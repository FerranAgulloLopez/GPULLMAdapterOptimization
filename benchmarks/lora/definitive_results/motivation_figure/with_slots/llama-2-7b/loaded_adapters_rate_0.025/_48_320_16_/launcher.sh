#!/bin/bash
#SBATCH --job-name=_48_320_16_
#SBATCH -D ./
#SBATCH --ntasks=1
#SBATCH --output=benchmarks/lora/definitive_results/model_with_offloading/example_maximum_variance/llama-2-7b/loaded_adapters/_48_320_16_/log_%j.out
#SBATCH --error=benchmarks/lora/definitive_results/model_with_offloading/example_maximum_variance/llama-2-7b/loaded_adapters/_48_320_16_/log_%j.err
#SBATCH --cpus-per-task=20
#SBATCH --gres gpu:1
#SBATCH --time=00:25:00
module load singularity
singularity exec --nv  --env PYTHONPATH=/usr/local/lib/python3.10/dist-packages --env TOKENIZERS_PARALLELISM=false --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/core:/usr/local/lib/python3.10/dist-packages/vllm/core --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/engine:/usr/local/lib/python3.10/dist-packages/vllm/engine --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/entrypoints:/usr/local/lib/python3.10/dist-packages/vllm/entrypoints --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/executor:/usr/local/lib/python3.10/dist-packages/vllm/executor --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/lora:/usr/local/lib/python3.10/dist-packages/vllm/lora --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/model_executor:/usr/local/lib/python3.10/dist-packages/vllm/model_executor --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/worker:/usr/local/lib/python3.10/dist-packages/vllm/worker --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/config.py:/usr/local/lib/python3.10/dist-packages/vllm/config.py --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/sequence.py:/usr/local/lib/python3.10/dist-packages/vllm/sequence.py --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/outputs.py:/usr/local/lib/python3.10/dist-packages/vllm/outputs.py /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/images/vllm-benchmark.sif python3 benchmarks/lora/benchmark_serving_by_time.py --port='10179' --result-dir='benchmarks/lora/definitive_results/model_with_offloading/example_maximum_variance/llama-2-7b/loaded_adapters/_48_320_16_' --backend='openai' --dataset-path='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/dummy_dataset_mean.json' --endpoint='/v1/completions' --model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b' --save-result --total-time='300' --adapter-rates='0.025' --launch-server --server-args='--port=10179 --disable-log-requests --model="/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b" --enable-lora --dummy-lora-modules="/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b/lora/yard1_sql-lora-test_dummy_rank_16/" --max-num-seqs="4096" --max-loras='48' --max-cpu-loras='320' --max-lora-rank='16''