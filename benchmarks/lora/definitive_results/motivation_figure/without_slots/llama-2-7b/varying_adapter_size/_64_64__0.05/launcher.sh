#!/bin/bash
#SBATCH --job-name=_64_64__0.05
#SBATCH -D ./
#SBATCH --ntasks=1
#SBATCH --output=benchmarks/lora/definitive_results/model_without_offloading/example_maximum_variance/llama-2-7b/varying_adapter_size/_64_64__0.05/log_%j.out
#SBATCH --error=benchmarks/lora/definitive_results/model_without_offloading/example_maximum_variance/llama-2-7b/varying_adapter_size/_64_64__0.05/log_%j.err
#SBATCH --cpus-per-task=20
#SBATCH --gres gpu:1
#SBATCH --time=02:15:00
module load singularity
singularity exec --nv --env PYTHONPATH=. --env TOKENIZERS_PARALLELISM=false --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/core:/usr/local/lib/python3.10/dist-packages/vllm/core --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/engine:/usr/local/lib/python3.10/dist-packages/vllm/engine --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/entrypoints:/usr/local/lib/python3.10/dist-packages/vllm/entrypoints --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/executor:/usr/local/lib/python3.10/dist-packages/vllm/executor --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/lora:/usr/local/lib/python3.10/dist-packages/vllm/lora --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/model_executor:/usr/local/lib/python3.10/dist-packages/vllm/model_executor --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/worker:/usr/local/lib/python3.10/dist-packages/vllm/worker --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/config.py:/usr/local/lib/python3.10/dist-packages/vllm/config.py --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/sequence.py:/usr/local/lib/python3.10/dist-packages/vllm/sequence.py --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/FairnessInLLMAdapterServing/vllm/outputs.py:/usr/local/lib/python3.10/dist-packages/vllm/outputs.py /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/images/vllm-benchmark.sif python3 benchmarks/lora/benchmark_serving.py --port='3864' --result-dir='benchmarks/lora/definitive_results/model_without_offloading/example_maximum_variance/llama-2-7b/varying_adapter_size/_64_64__0.05' --backend='openai' --disable-tqdm --dataset-path='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/dummy_dataset_mean.json' --endpoint='/v1/completions' --model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b' --num-prompts='4096' --save-result --infinite-behaviour --request-rate-by-lora='0.05' --launch-server --server-args='--port=3864 --disable-log-requests --model=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b --enable-lora --dummy-lora-modules=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-2-7b/lora/yard1_sql-lora-test_dummy_rank_64/ --max-num-seqs=4096 --max-loras='64' --max-lora-rank='64''