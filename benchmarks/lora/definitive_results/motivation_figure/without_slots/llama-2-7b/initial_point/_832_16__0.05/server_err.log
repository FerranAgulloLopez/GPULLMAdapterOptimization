/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py:34: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
[rank0]: Traceback (most recent call last):
[rank0]:   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank0]:     return _run_code(code, main_globals, None,
[rank0]:   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
[rank0]:     exec(code, run_globals)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py", line 196, in <module>
[rank0]:     engine = AsyncLLMEngine.from_engine_args(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 400, in from_engine_args
[rank0]:     engine = cls(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 351, in __init__
[rank0]:     self.engine = self._init_engine(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py", line 475, in _init_engine
[rank0]:     return engine_class(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py", line 229, in __init__
[rank0]:     self.model_executor = executor_class(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py", line 41, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py", line 24, in _init_executor
[rank0]:     self.driver_worker.load_model()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py", line 122, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py", line 183, in load_model
[rank0]:     self.model = self.lora_manager.create_lora_manager(self.model)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py", line 261, in create_lora_manager
[rank0]:     lora_manager = create_lora_manager(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/models.py", line 792, in create_lora_manager
[rank0]:     lora_manager = lora_manager_cls(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/models.py", line 736, in __init__
[rank0]:     super().__init__(model, max_num_seqs, max_num_batched_tokens,
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/models.py", line 430, in __init__
[rank0]:     self._create_lora_modules()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/models.py", line 578, in _create_lora_modules
[rank0]:     from_layer(module, self.lora_slots, self.lora_config,
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/utils.py", line 57, in from_layer
[rank0]:     ret.create_lora_weights(max_loras, lora_config, model_config)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/layers.py", line 531, in create_lora_weights
[rank0]:     self.lora_b_stacked = tuple(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/lora/layers.py", line 532, in <genexpr>
[rank0]:     torch.zeros(
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 280.00 MiB. GPU 
