INFO 06-01 15:57:38 [__init__.py:239] Automatically detected platform cuda.
INFO 06-01 15:57:40 [api_server.py:1046] vLLM API server version 0.1.dev6216+g1125ba0
INFO 06-01 15:57:40 [api_server.py:1047] args: Namespace(host=None, port=14222, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, dummy_lora_modules='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct/lora/lora-medical_dummy_rank_8/ /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct/lora/lora-medical_dummy_rank_8/ /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct/lora/lora-medical_dummy_rank_32/', prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', max_model_len=None, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=False, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=384, max_lora_rank=32, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=384, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=None, max_num_seqs=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)
INFO 06-01 15:57:51 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'score', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 06-01 15:57:51 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 06-01 15:57:58 [__init__.py:239] Automatically detected platform cuda.
INFO 06-01 15:58:01 [core.py:58] Initializing a V1 LLM engine (v0.1.dev6216+g1125ba0) with config: model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct', speculative_config=None, tokenizer='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-01 15:58:03 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fadfcca7f50>
INFO 06-01 15:58:03 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-01 15:58:03 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 06-01 15:58:03 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.
INFO 06-01 15:58:03 [gpu_model_runner.py:1329] Starting to load model /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct...
INFO 06-01 15:58:46 [loader.py:458] Loading weights took 43.07 seconds
INFO 06-01 15:58:46 [punica_selector.py:18] Using PunicaWrapperGPU.
ERROR 06-01 15:58:47 [core.py:396] EngineCore failed to start.
ERROR 06-01 15:58:47 [core.py:396] Traceback (most recent call last):
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 387, in run_engine_core
ERROR 06-01 15:58:47 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 06-01 15:58:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 329, in __init__
ERROR 06-01 15:58:47 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 64, in __init__
ERROR 06-01 15:58:47 [core.py:396]     self.model_executor = executor_class(vllm_config)
ERROR 06-01 15:58:47 [core.py:396]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 06-01 15:58:47 [core.py:396]     self._init_executor()
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 06-01 15:58:47 [core.py:396]     self.collective_rpc("load_model")
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 06-01 15:58:47 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 06-01 15:58:47 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/utils.py", line 2456, in run_method
ERROR 06-01 15:58:47 [core.py:396]     return func(*args, **kwargs)
ERROR 06-01 15:58:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 162, in load_model
ERROR 06-01 15:58:47 [core.py:396]     self.model_runner.load_model()
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 1334, in load_model
ERROR 06-01 15:58:47 [core.py:396]     self.model = self.load_lora_model(self.model,
ERROR 06-01 15:58:47 [core.py:396]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/lora_model_runner_mixin.py", line 53, in load_lora_model
ERROR 06-01 15:58:47 [core.py:396]     return self.lora_manager.create_lora_manager(model)
ERROR 06-01 15:58:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/worker_manager.py", line 218, in create_lora_manager
ERROR 06-01 15:58:47 [core.py:396]     lora_manager = create_lora_manager(
ERROR 06-01 15:58:47 [core.py:396]                    ^^^^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/models.py", line 794, in create_lora_manager
ERROR 06-01 15:58:47 [core.py:396]     lora_manager = lora_manager_cls(
ERROR 06-01 15:58:47 [core.py:396]                    ^^^^^^^^^^^^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/models.py", line 717, in __init__
ERROR 06-01 15:58:47 [core.py:396]     super().__init__(model, max_num_seqs, max_num_batched_tokens,
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/models.py", line 365, in __init__
ERROR 06-01 15:58:47 [core.py:396]     self._create_lora_modules()
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/models.py", line 490, in _create_lora_modules
ERROR 06-01 15:58:47 [core.py:396]     from_layer(module, self.lora_slots, self.lora_config,
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/utils.py", line 70, in from_layer
ERROR 06-01 15:58:47 [core.py:396]     instance_layer.create_lora_weights(max_loras, lora_config,
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/layers.py", line 647, in create_lora_weights
ERROR 06-01 15:58:47 [core.py:396]     self.lora_b_stacked = tuple(
ERROR 06-01 15:58:47 [core.py:396]                           ^^^^^^
ERROR 06-01 15:58:47 [core.py:396]   File "/usr/local/lib/python3.12/dist-packages/vllm/lora/layers.py", line 648, in <genexpr>
ERROR 06-01 15:58:47 [core.py:396]     torch.zeros(
ERROR 06-01 15:58:47 [core.py:396] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 444.00 MiB. GPU 0 has a total capacity of 63.43 GiB of which 389.50 MiB is free. Including non-PyTorch memory, this process has 63.04 GiB memory in use. Of the allocated memory 62.40 GiB is allocated by PyTorch, and 55.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
