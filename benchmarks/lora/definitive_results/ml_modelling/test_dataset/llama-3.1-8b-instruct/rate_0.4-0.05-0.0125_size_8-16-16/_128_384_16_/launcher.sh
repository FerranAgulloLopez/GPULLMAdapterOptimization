#!/bin/bash
#SBATCH --job-name=_128_384_16_
#SBATCH -D ./
#SBATCH --ntasks=1
#SBATCH --output=benchmarks/lora/definitive_results/finding_maximum/test_dataset/with_offloading/llama-3.1-8b-instruct/rate_0.4-0.05-0.0125_size_8-16-16/_128_384_16_/log_%j.out
#SBATCH --error=benchmarks/lora/definitive_results/finding_maximum/test_dataset/with_offloading/llama-3.1-8b-instruct/rate_0.4-0.05-0.0125_size_8-16-16/_128_384_16_/log_%j.err
#SBATCH --cpus-per-task=20
#SBATCH --gres gpu:1
#SBATCH --time=01:45:00
module load singularity
singularity exec --nv  --env PYTHONPATH=/usr/local/lib/python3.12/dist-packages --env TOKENIZERS_PARALLELISM=false --env VLLM_CACHE_ROOT=/scratch/tmp/_128_384_16__848004833 --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/core:/usr/local/lib/python3.12/dist-packages/vllm/core --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/engine:/usr/local/lib/python3.12/dist-packages/vllm/engine --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/entrypoints:/usr/local/lib/python3.12/dist-packages/vllm/entrypoints --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/executor:/usr/local/lib/python3.12/dist-packages/vllm/executor --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/lora:/usr/local/lib/python3.12/dist-packages/vllm/lora --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/model_executor:/usr/local/lib/python3.12/dist-packages/vllm/model_executor --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/worker:/usr/local/lib/python3.12/dist-packages/vllm/worker --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/config.py:/usr/local/lib/python3.12/dist-packages/vllm/config.py --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/sequence.py:/usr/local/lib/python3.12/dist-packages/vllm/sequence.py --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/outputs.py:/usr/local/lib/python3.12/dist-packages/vllm/outputs.py --bind /gpfs/home/bsc/bsc098069/llm_benchmarking/vLLMAdapterServingScaling/vllm/v1/core:/usr/local/lib/python3.12/dist-packages/vllm/v1/core /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/images/vllm_0.8.5.sif python3 benchmarks/lora/benchmark_serving_by_time.py --port='39067' --result-dir='benchmarks/lora/definitive_results/finding_maximum/test_dataset/with_offloading/llama-3.1-8b-instruct/rate_0.4-0.05-0.0125_size_8-16-16/_128_384_16_' --backend='openai' --dataset-path='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json' --endpoint='/v1/completions' --model='/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct' --save-result --total-time='3600' --adapter-rates='0.4 0.05 0.0125' --server-init-time='1800' --launch-server --server-args='--port=39067 --disable-log-requests --model="/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct" --enable-lora --dummy-lora-modules="/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct/lora/lora-finance_dummy_rank_8/ /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct/lora/lora-finance_dummy_rank_16/ /gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct/lora/lora-finance_dummy_rank_16/" --max-num-seqs="2048" --scheduler-cls="vllm.v1.core.sched.scheduler.Scheduler" --no-enable-prefix-caching --max-loras='128' --max-cpu-loras='384' --max-lora-rank='16''