INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 88.6050383108668,
    "estimated_duration": 3600.010245366038,
    "input_throughput": 7065.657113821269,
    "output_throughput": 6132.920601662098,
    "total_throughput": 13198.577715483369,
    "itl": 86.9465658114763,
    "ttft": 1958917.628553867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.745284499344421,
    "arrivals": 743719,
    "finished_requests": 102632,
    "scheduler_time": 309.60678826810096
}
#Debug simulation 
Total elapsed time: 88.60525792185217. Arrivals time: 0.5019321208819747 Scheduler time: 87.87223612749949 Scheduler overhead time: 0.08911681408062577 Adapter cache time: 0.019036265090107918 Engine time: 0.08823453262448311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 86.94139212416485,
    "estimated_duration": 3600.097522795796,
    "input_throughput": 7008.908464347522,
    "output_throughput": 6075.255145592508,
    "total_throughput": 13084.16360994003,
    "itl": 84.80767676082189,
    "ttft": 1966476.8980283665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.825818637898225,
    "arrivals": 743719,
    "finished_requests": 101735,
    "scheduler_time": 312.14652024208146
}
#Debug simulation 
Total elapsed time: 86.9416534230113. Arrivals time: 0.48728786827996373 Scheduler time: 86.22071195859462 Scheduler overhead time: 0.09072910668328404 Adapter cache time: 0.019323821179568768 Engine time: 0.0887163607403636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 89.6642177766189,
    "estimated_duration": 3600.0197618003635,
    "input_throughput": 6988.727469489709,
    "output_throughput": 6056.993973026195,
    "total_throughput": 13045.721442515904,
    "itl": 86.061185033552,
    "ttft": 1969137.2854820394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.364118826342742,
    "arrivals": 743719,
    "finished_requests": 101612,
    "scheduler_time": 313.0530873549976
}
#Debug simulation 
Total elapsed time: 89.66438338067383. Arrivals time: 0.5071258652023971 Scheduler time: 88.92315716668963 Scheduler overhead time: 0.09184511750936508 Adapter cache time: 0.01954976236447692 Engine time: 0.08816803619265556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 87.26657768711448,
    "estimated_duration": 3600.0213027705845,
    "input_throughput": 7033.497824169291,
    "output_throughput": 6103.466938678898,
    "total_throughput": 13136.964762848189,
    "itl": 85.31214946457813,
    "ttft": 1963826.4829397183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.861116613242809,
    "arrivals": 743719,
    "finished_requests": 102148,
    "scheduler_time": 310.62519686648926
}
#Debug simulation 
Total elapsed time: 87.2668084348552. Arrivals time: 0.5072495150379837 Scheduler time: 86.52431408176199 Scheduler overhead time: 0.0918108238838613 Adapter cache time: 0.01953155267983675 Engine time: 0.08922312082722783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 89.65691132796928,
    "estimated_duration": 3600.087036771875,
    "input_throughput": 6983.45699512406,
    "output_throughput": 6051.74396548362,
    "total_throughput": 13035.20096060768,
    "itl": 86.21071645394294,
    "ttft": 1972276.067577073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1600408655824,
    "arrivals": 743719,
    "finished_requests": 101487,
    "scheduler_time": 313.45547367902395
}
#Debug simulation 
Total elapsed time: 89.65708037279546. Arrivals time: 0.49853048427030444 Scheduler time: 88.92524907225743 Scheduler overhead time: 0.09082715958356857 Adapter cache time: 0.019170337356626987 Engine time: 0.08843769738450646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_384_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 135, 33, 17280, 17280, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 17280, 17280, 135, 135, 33, 17280, 33, 17280, 135, 135, 135, 33, 33, 135, 33, 17280, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 135, 33, 33, 33, 135, 33, 135, 33, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 17280, 17280, 135, 33, 33, 17280, 135, 135, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 17280, 17280, 33, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 17280, 33, 33, 17280, 33, 17280, 135, 33, 17280, 135, 135, 33, 17280, 17280, 17280, 33, 135, 17280, 33, 17280, 135, 135, 17280, 17280, 33, 135, 17280, 33, 135, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 17280, 33, 17280, 135, 135, 135, 135, 135, 33, 33, 17280, 33, 17280, 33, 135, 17280, 135, 17280, 135, 17280, 135, 17280, 33, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 33, 33, 33, 33, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33, 135, 17280, 17280, 135, 135, 33, 17280, 135, 17280, 33, 135, 135, 33, 33, 135, 33, 17280, 17280, 33, 17280, 17280, 33, 135, 17280, 33, 33, 135, 135, 33, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 135, 33, 17280, 17280, 17280, 135, 33, 17280, 17280, 135, 135, 17280, 17280, 33, 17280, 17280, 135, 33, 17280, 135, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 135, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 17280, 33, 33, 33, 135, 33, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 135, 135, 135, 135, 33, 135, 17280, 17280, 33, 17280, 17280, 33, 17280, 135, 17280, 135, 135, 135, 33, 135, 33, 33, 17280, 33, 135, 33, 33, 135, 17280, 135, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2233344 . Total input tokens: 498427686 . Total output tokens: 438646577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 86.7081353967078,
    "estimated_duration": 3600.043863272098,
    "input_throughput": 7009.0129338218185,
    "output_throughput": 6075.34569873848,
    "total_throughput": 13084.358632560297,
    "itl": 84.80709257690305,
    "ttft": 1966461.3249649585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7541560947895376,
    "arrivals": 743719,
    "finished_requests": 101735,
    "scheduler_time": 312.1458827714985
}
#Debug simulation 
Total elapsed time: 86.70837602997199. Arrivals time: 0.5074836728163064 Scheduler time: 85.96756892371923 Scheduler overhead time: 0.09066737443208694 Adapter cache time: 0.019078397657722235 Engine time: 0.08867547567933798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 88.24358014622703,
    "estimated_duration": 3600.0784825263904,
    "input_throughput": 7228.222141906942,
    "output_throughput": 6271.160228750514,
    "total_throughput": 13499.382370657455,
    "itl": 88.6374351996721,
    "ttft": 1955467.2425783363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4120030604862106,
    "arrivals": 740875,
    "finished_requests": 105180,
    "scheduler_time": 301.7655431638671
}
#Debug simulation 
Total elapsed time: 88.24375083530322. Arrivals time: 0.516473297495395 Scheduler time: 87.499162171036 Scheduler overhead time: 0.08930454170331359 Adapter cache time: 0.0189649211242795 Engine time: 0.08625802164897323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.90064105205238,
    "estimated_duration": 3600.06627949062,
    "input_throughput": 7173.119047034279,
    "output_throughput": 6229.4155881966535,
    "total_throughput": 13402.534635230933,
    "itl": 87.11620877979762,
    "ttft": 1955403.2913274465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7191916316561446,
    "arrivals": 740875,
    "finished_requests": 104426,
    "scheduler_time": 304.08785739042764
}
#Debug simulation 
Total elapsed time: 87.90089926216751. Arrivals time: 0.5121588376350701 Scheduler time: 87.1620580041781 Scheduler overhead time: 0.0882607689127326 Adapter cache time: 0.01883299509063363 Engine time: 0.0855045230127871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.25304519431666,
    "estimated_duration": 3600.0157769581765,
    "input_throughput": 7101.198045748731,
    "output_throughput": 6181.769852909882,
    "total_throughput": 13282.967898658611,
    "itl": 85.20396845051695,
    "ttft": 1956953.348901708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7595012445142806,
    "arrivals": 740875,
    "finished_requests": 103350,
    "scheduler_time": 307.7754760666466
}
#Debug simulation 
Total elapsed time: 87.25320551730692. Arrivals time: 0.5083107063546777 Scheduler time: 86.51380643807352 Scheduler overhead time: 0.09034759411588311 Adapter cache time: 0.018852942157536745 Engine time: 0.08751229615882039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 88.56816077698022,
    "estimated_duration": 3600.0893918135953,
    "input_throughput": 7179.2981193113255,
    "output_throughput": 6244.941320380438,
    "total_throughput": 13424.239439691764,
    "itl": 87.15472625220826,
    "ttft": 1955336.8172924283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 507,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.485139471092257,
    "arrivals": 740875,
    "finished_requests": 104518,
    "scheduler_time": 304.05035980902977
}
#Debug simulation 
Total elapsed time: 88.5684084626846. Arrivals time: 0.518267800565809 Scheduler time: 87.82191439671442 Scheduler overhead time: 0.0887202899903059 Adapter cache time: 0.018957521300762892 Engine time: 0.08592760702595115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 87.45671622687951,
    "estimated_duration": 3600.067068719154,
    "input_throughput": 7101.240758021654,
    "output_throughput": 6181.682334023233,
    "total_throughput": 13282.923092044886,
    "itl": 85.20258891482433,
    "ttft": 1957017.3083557854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7259482619026985,
    "arrivals": 740875,
    "finished_requests": 103351,
    "scheduler_time": 307.78275653107954
}
#Debug simulation 
Total elapsed time: 87.45688181277364. Arrivals time: 0.5148142403922975 Scheduler time: 86.71139874216169 Scheduler overhead time: 0.09021956659853458 Adapter cache time: 0.018973329570144415 Engine time: 0.0870179976336658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 88.00170967914164,
    "estimated_duration": 3600.0258632481628,
    "input_throughput": 7173.718462316433,
    "output_throughput": 6230.208574047096,
    "total_throughput": 13403.927036363528,
    "itl": 87.10592517076736,
    "ttft": 1955280.169049703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2430318378098164,
    "arrivals": 740875,
    "finished_requests": 104439,
    "scheduler_time": 304.1212592334339
}
#Debug simulation 
Total elapsed time: 88.00196476327255. Arrivals time: 0.5210503032431006 Scheduler time: 87.2505277171731 Scheduler overhead time: 0.0896002296358347 Adapter cache time: 0.01903581991791725 Engine time: 0.08721360564231873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_384_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 66, 33, 17280, 17280, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 17280, 17280, 66, 66, 33, 17280, 33, 17280, 66, 66, 66, 33, 33, 66, 33, 17280, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 66, 33, 33, 33, 66, 33, 66, 33, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 17280, 17280, 66, 33, 33, 17280, 66, 66, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 17280, 17280, 33, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 17280, 33, 33, 17280, 33, 17280, 66, 33, 17280, 66, 66, 33, 17280, 17280, 17280, 33, 66, 17280, 33, 17280, 66, 66, 17280, 17280, 33, 66, 17280, 33, 66, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 17280, 33, 17280, 66, 66, 66, 66, 66, 33, 33, 17280, 33, 17280, 33, 66, 17280, 66, 17280, 66, 17280, 66, 17280, 33, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 33, 33, 33, 33, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33, 66, 17280, 17280, 66, 66, 33, 17280, 66, 17280, 33, 66, 66, 33, 33, 66, 33, 17280, 17280, 33, 17280, 17280, 33, 66, 17280, 33, 33, 66, 66, 33, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 66, 33, 17280, 17280, 17280, 66, 33, 17280, 17280, 66, 66, 17280, 17280, 33, 17280, 17280, 66, 33, 17280, 66, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 66, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 17280, 33, 33, 33, 66, 33, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 66, 66, 66, 66, 33, 66, 17280, 17280, 33, 17280, 17280, 33, 17280, 66, 17280, 66, 66, 66, 33, 66, 33, 33, 17280, 33, 66, 33, 33, 66, 17280, 66, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 2224512 . Total input tokens: 496455911 . Total output tokens: 436938952
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.53704075794667,
    "estimated_duration": 3600.030183781484,
    "input_throughput": 7101.3135154179445,
    "output_throughput": 6181.745669872086,
    "total_throughput": 13283.05918529003,
    "itl": 85.20219832790934,
    "ttft": 1957002.4322624996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6897027559951256,
    "arrivals": 740875,
    "finished_requests": 103351,
    "scheduler_time": 307.78231936175973
}
#Debug simulation 
Total elapsed time: 87.53719681920484. Arrivals time: 0.5159335499629378 Scheduler time: 86.79096437711269 Scheduler overhead time: 0.09032209170982242 Adapter cache time: 0.0187447601929307 Engine time: 0.0866896016523242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.72305136313662,
    "estimated_duration": 3600.057671878783,
    "input_throughput": 6575.185221309706,
    "output_throughput": 5725.841883316937,
    "total_throughput": 12301.027104626644,
    "itl": 76.91163338673174,
    "ttft": 1998988.6325800351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.562562232045536,
    "arrivals": 599139,
    "finished_requests": 95558,
    "scheduler_time": 326.92972589733097
}
#Debug simulation 
Total elapsed time: 84.7232964891009. Arrivals time: 0.4866592166945338 Scheduler time: 84.00289574312046 Scheduler overhead time: 0.09149997774511576 Adapter cache time: 0.0203202897682786 Engine time: 0.08655057614669204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.59902102313936,
    "estimated_duration": 3600.01986950503,
    "input_throughput": 6273.4759303162355,
    "output_throughput": 5463.5120674220825,
    "total_throughput": 11736.987997738317,
    "itl": 71.66687120931489,
    "ttft": 2020914.8043771137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.684032641462989,
    "arrivals": 599139,
    "finished_requests": 91182,
    "scheduler_time": 340.87868137476505
}
#Debug simulation 
Total elapsed time: 81.59918987099081. Arrivals time: 0.48490473348647356 Scheduler time: 80.87745498120785 Scheduler overhead time: 0.09295732527971268 Adapter cache time: 0.019946342799812555 Engine time: 0.08715027337893844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.06337771890685,
    "estimated_duration": 3600.0018216579274,
    "input_throughput": 6273.189047887622,
    "output_throughput": 5466.9711225135325,
    "total_throughput": 11740.160170401155,
    "itl": 71.78578544801493,
    "ttft": 2020548.0991339732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.806887962091745,
    "arrivals": 599139,
    "finished_requests": 91154,
    "scheduler_time": 340.7744010753905
}
#Debug simulation 
Total elapsed time: 81.06354640284553. Arrivals time: 0.4728854550048709 Scheduler time: 80.3536028591916 Scheduler overhead time: 0.09363613231107593 Adapter cache time: 0.0200768462382257 Engine time: 0.08739982731640339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 84.63943199720234,
    "estimated_duration": 3600.006687477954,
    "input_throughput": 6559.73364775718,
    "output_throughput": 5710.788280341465,
    "total_throughput": 12270.521928098644,
    "itl": 75.5949799383055,
    "ttft": 1998276.2152148476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.669517426337109,
    "arrivals": 599139,
    "finished_requests": 95328,
    "scheduler_time": 327.7890502059394
}
#Debug simulation 
Total elapsed time: 84.6395990261808. Arrivals time: 0.501725759357214 Scheduler time: 83.90460534859449 Scheduler overhead time: 0.0917782480828464 Adapter cache time: 0.01999323582276702 Engine time: 0.0864140004850924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 80.97729575121775,
    "estimated_duration": 3600.0201162481144,
    "input_throughput": 6273.202724082647,
    "output_throughput": 5466.967506979221,
    "total_throughput": 11740.17023106187,
    "itl": 71.78499891916587,
    "ttft": 2020525.9419364054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7631862378260434,
    "arrivals": 599139,
    "finished_requests": 91155,
    "scheduler_time": 340.7807061975023
}
#Debug simulation 
Total elapsed time: 80.97745880391449. Arrivals time: 0.4704130506142974 Scheduler time: 80.27080635400489 Scheduler overhead time: 0.09284556098282337 Adapter cache time: 0.01982892584055662 Engine time: 0.08730672858655453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.79919822420925,
    "estimated_duration": 3600.093768650301,
    "input_throughput": 6558.63777927434,
    "output_throughput": 5706.2966467403385,
    "total_throughput": 12264.934426014677,
    "itl": 76.32858271369182,
    "ttft": 1997684.1340564596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.404905448993647,
    "arrivals": 599139,
    "finished_requests": 95319,
    "scheduler_time": 327.7531797628954
}
#Debug simulation 
Total elapsed time: 84.79936833819374. Arrivals time: 0.5035193562507629 Scheduler time: 84.06137625081465 Scheduler overhead time: 0.09204511623829603 Adapter cache time: 0.020327944308519363 Engine time: 0.08711370266973972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 8640, 4320, 1080, 8640, 8640, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 1080, 8640, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 8640, 1080, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 8640, 1080, 4320, 8640, 1080, 1080, 4320, 4320, 1080, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 8640, 4320, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1797120 . Total input tokens: 400824663 . Total output tokens: 352886804
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.91317126294598,
    "estimated_duration": 3600.0359625629617,
    "input_throughput": 6273.2839990636685,
    "output_throughput": 5466.99205359835,
    "total_throughput": 11740.276052662019,
    "itl": 71.7841653823066,
    "ttft": 2020497.7729997255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.717413341794194,
    "arrivals": 599139,
    "finished_requests": 91157,
    "scheduler_time": 340.7867353472633
}
#Debug simulation 
Total elapsed time: 80.91333569679409. Arrivals time: 0.46690018754452467 Scheduler time: 80.20936176087707 Scheduler overhead time: 0.09350222442299128 Adapter cache time: 0.020171698182821274 Engine time: 0.08734328113496304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.47662437986583,
    "estimated_duration": 3600.044536342647,
    "input_throughput": 6535.066375567966,
    "output_throughput": 5691.994583160806,
    "total_throughput": 12227.06095872877,
    "itl": 77.04672105304154,
    "ttft": 1984933.2823232564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.311290688831431,
    "arrivals": 576120,
    "finished_requests": 95175,
    "scheduler_time": 328.55610555561435
}
#Debug simulation 
Total elapsed time: 86.47679502796382. Arrivals time: 0.5012951851822436 Scheduler time: 85.74037795653567 Scheduler overhead time: 0.0917069623246789 Adapter cache time: 0.020404554437845945 Engine time: 0.08735978836193681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.2457043309696,
    "estimated_duration": 3600.0370194328743,
    "input_throughput": 6528.841751661396,
    "output_throughput": 5689.840657034925,
    "total_throughput": 12218.682408696322,
    "itl": 76.81622693650466,
    "ttft": 1982192.4110308387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 655,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.792285444331362,
    "arrivals": 576120,
    "finished_requests": 95126,
    "scheduler_time": 328.4796553432018
}
#Debug simulation 
Total elapsed time: 85.2458703010343. Arrivals time: 0.4824574254453182 Scheduler time: 84.52681605098769 Scheduler overhead time: 0.09315376775339246 Adapter cache time: 0.02043634233996272 Engine time: 0.08787775738164783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 84.12186724226922,
    "estimated_duration": 3600.044036812597,
    "input_throughput": 6418.909814353175,
    "output_throughput": 5597.9165237775305,
    "total_throughput": 12016.826338130704,
    "itl": 73.89613173945409,
    "ttft": 1991963.8097974262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 630,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.734203301728734,
    "arrivals": 576120,
    "finished_requests": 93481,
    "scheduler_time": 334.2068662185431
}
#Debug simulation 
Total elapsed time: 84.12203478394076. Arrivals time: 0.48857216257601976 Scheduler time: 83.39667072985321 Scheduler overhead time: 0.09307227376848459 Adapter cache time: 0.02022106619551778 Engine time: 0.08736953185871243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 84.15343048376963,
    "estimated_duration": 3600.0577785539444,
    "input_throughput": 6414.460106047427,
    "output_throughput": 5581.662083232284,
    "total_throughput": 11996.122189279711,
    "itl": 74.84265265214026,
    "ttft": 1996942.6173270545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.355033748471172,
    "arrivals": 576120,
    "finished_requests": 93430,
    "scheduler_time": 334.28072601549326
}
#Debug simulation 
Total elapsed time: 84.15359824383631. Arrivals time: 0.4843058669939637 Scheduler time: 83.43371935049072 Scheduler overhead time: 0.09255419531837106 Adapter cache time: 0.0199111788533628 Engine time: 0.0874667027965188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 84.1447045900859,
    "estimated_duration": 3600.0004996230955,
    "input_throughput": 6418.98744247934,
    "output_throughput": 5597.984223088276,
    "total_throughput": 12016.971665567617,
    "itl": 73.89541231220159,
    "ttft": 1991945.7968989897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 630,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.691122928992876,
    "arrivals": 576120,
    "finished_requests": 93481,
    "scheduler_time": 334.20640940177725
}
#Debug simulation 
Total elapsed time: 84.14487208519131. Arrivals time: 0.5011611506342888 Scheduler time: 83.40712789772078 Scheduler overhead time: 0.09328271774575114 Adapter cache time: 0.019938012585043907 Engine time: 0.08787048142403364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 85.79193200496957,
    "estimated_duration": 3600.067022636015,
    "input_throughput": 6530.082593514219,
    "output_throughput": 5691.212377762312,
    "total_throughput": 12221.294971276531,
    "itl": 76.80526526118092,
    "ttft": 1982221.897353909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 656,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.18785213701425,
    "arrivals": 576120,
    "finished_requests": 95147,
    "scheduler_time": 328.5356265484429
}
#Debug simulation 
Total elapsed time: 85.79210375901312. Arrivals time: 0.4955666484311223 Scheduler time: 85.06121358415112 Scheduler overhead time: 0.09267411381006241 Adapter cache time: 0.020320373121649027 Engine time: 0.08685115212574601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 4320, 540, 8640, 8640, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 4320, 4320, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 4320, 540, 8640, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 8640, 540, 8640, 4320, 4320, 8640, 8640, 540, 4320, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 8640, 540, 8640, 4320, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 8640, 540, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 8640, 540, 4320, 4320, 540, 540, 4320, 540, 8640, 8640, 540, 8640, 8640, 540, 4320, 8640, 540, 540, 4320, 4320, 540, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 4320, 540, 8640, 8640, 8640, 4320, 540, 8640, 8640, 4320, 4320, 8640, 8640, 540, 8640, 8640, 4320, 540, 8640, 4320, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 4320, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 8640, 540, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 8640, 8640, 540, 8640, 8640, 540, 8640, 4320, 8640, 4320, 4320, 4320, 540, 4320, 540, 540, 8640, 540, 4320, 540, 540, 4320, 8640, 4320, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1728000 . Total input tokens: 385422565 . Total output tokens: 339338536
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.05048866383731,
    "estimated_duration": 3600.0419791020704,
    "input_throughput": 6510.27630679067,
    "output_throughput": 5682.273462017319,
    "total_throughput": 12192.549768807989,
    "itl": 76.48728291911969,
    "ttft": 1982965.6718002423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.815565214809061,
    "arrivals": 576120,
    "finished_requests": 94882,
    "scheduler_time": 329.44395871108395
}
#Debug simulation 
Total elapsed time: 85.05066093057394. Arrivals time: 0.4976794016547501 Scheduler time: 84.31305050570518 Scheduler overhead time: 0.09353444259613752 Adapter cache time: 0.020646988414227962 Engine time: 0.09029062744230032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.90864313393831,
    "estimated_duration": 3600.005737670807,
    "input_throughput": 6617.341953297293,
    "output_throughput": 5789.349939615517,
    "total_throughput": 12406.69189291281,
    "itl": 80.28009693383001,
    "ttft": 1969657.984985809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 655,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.331127915927281,
    "arrivals": 564654,
    "finished_requests": 96264,
    "scheduler_time": 323.52993408285266
}
#Debug simulation 
Total elapsed time: 86.90880900295451. Arrivals time: 0.5385453901253641 Scheduler time: 86.1341234096326 Scheduler overhead time: 0.09269547509029508 Adapter cache time: 0.020071551203727722 Engine time: 0.0882073468528688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 86.74651950178668,
    "estimated_duration": 3600.0566006362596,
    "input_throughput": 6611.8646567371015,
    "output_throughput": 5787.188178185267,
    "total_throughput": 12399.052834922368,
    "itl": 80.02894360834362,
    "ttft": 1973528.7929583304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 663,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.855850800615742,
    "arrivals": 564654,
    "finished_requests": 96156,
    "scheduler_time": 323.92286706496566
}
#Debug simulation 
Total elapsed time: 86.7466928716749. Arrivals time: 0.4901864556595683 Scheduler time: 86.0195068498142 Scheduler overhead time: 0.09270391333848238 Adapter cache time: 0.020481090992689133 Engine time: 0.08798936055973172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 84.88902766490355,
    "estimated_duration": 3600.0012183738504,
    "input_throughput": 6574.9247192454595,
    "output_throughput": 5735.176947891772,
    "total_throughput": 12310.10166713723,
    "itl": 78.19045554929878,
    "ttft": 1981188.5224755914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.938586831660972,
    "arrivals": 564654,
    "finished_requests": 95627,
    "scheduler_time": 325.588248893253
}
#Debug simulation 
Total elapsed time: 84.8891889359802. Arrivals time: 0.502578763756901 Scheduler time: 84.15006827097386 Scheduler overhead time: 0.0925223776139319 Adapter cache time: 0.020446044858545065 Engine time: 0.08772824052721262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 86.07752399984747,
    "estimated_duration": 3600.0665333001944,
    "input_throughput": 6612.093909880828,
    "output_throughput": 5787.315264115615,
    "total_throughput": 12399.409173996442,
    "itl": 80.02171042639723,
    "ttft": 1973467.2425749,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 663,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.543501081620336,
    "arrivals": 564654,
    "finished_requests": 96160,
    "scheduler_time": 323.9502196019066
}
#Debug simulation 
Total elapsed time: 86.07769367471337. Arrivals time: 0.5075989929027855 Scheduler time: 85.33422260405496 Scheduler overhead time: 0.09207340562716126 Adapter cache time: 0.020586999598890543 Engine time: 0.08781349333003163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 84.99088378902525,
    "estimated_duration": 3600.051344588883,
    "input_throughput": 6575.099834500593,
    "output_throughput": 5735.356533465748,
    "total_throughput": 12310.45636796634,
    "itl": 78.1900443487131,
    "ttft": 1981203.6509590694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.892813935629123,
    "arrivals": 564654,
    "finished_requests": 95631,
    "scheduler_time": 325.5952857712388
}
#Debug simulation 
Total elapsed time: 84.99104385497048. Arrivals time: 0.5022890823893249 Scheduler time: 84.25069995597005 Scheduler overhead time: 0.09314614441245794 Adapter cache time: 0.02027271967381239 Engine time: 0.08855569548904896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.37683685915545,
    "estimated_duration": 3600.0056822017277,
    "input_throughput": 6612.249007740907,
    "output_throughput": 5787.562809416835,
    "total_throughput": 12399.81181715774,
    "itl": 80.01493214868461,
    "ttft": 1973348.5547677225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 663,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.232539583598244,
    "arrivals": 564654,
    "finished_requests": 96162,
    "scheduler_time": 323.9741355846606
}
#Debug simulation 
Total elapsed time: 86.37700885208324. Arrivals time: 0.4956962442956865 Scheduler time: 85.6454474828206 Scheduler overhead time: 0.09233212703838944 Adapter cache time: 0.020252814050763845 Engine time: 0.08783589815720916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 4320, 270, 8640, 8640, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 4320, 4320, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 4320, 270, 8640, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 8640, 270, 8640, 4320, 4320, 8640, 8640, 270, 4320, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 8640, 270, 8640, 4320, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 8640, 270, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 8640, 270, 4320, 4320, 270, 270, 4320, 270, 8640, 8640, 270, 8640, 8640, 270, 4320, 8640, 270, 270, 4320, 4320, 270, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 4320, 270, 8640, 8640, 8640, 4320, 270, 8640, 8640, 4320, 4320, 8640, 8640, 270, 8640, 8640, 4320, 270, 8640, 4320, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 4320, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 8640, 270, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 8640, 8640, 270, 8640, 8640, 270, 8640, 4320, 8640, 4320, 4320, 4320, 270, 4320, 270, 270, 8640, 270, 4320, 270, 270, 4320, 8640, 4320, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1693440 . Total input tokens: 377656713 . Total output tokens: 332588680
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 84.98641731310636,
    "estimated_duration": 3600.00536850105,
    "input_throughput": 6575.183805866343,
    "output_throughput": 5735.429780372001,
    "total_throughput": 12310.613586238343,
    "itl": 78.18935099097715,
    "ttft": 1981184.234759947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.847041039597275,
    "arrivals": 564654,
    "finished_requests": 95631,
    "scheduler_time": 325.59488031699533
}
#Debug simulation 
Total elapsed time: 84.98657958116382. Arrivals time: 0.508970276452601 Scheduler time: 84.24052632972598 Scheduler overhead time: 0.09250008035451174 Adapter cache time: 0.020500268321484327 Engine time: 0.08807733422145247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.32492202799767,
    "estimated_duration": 3600.0121554476314,
    "input_throughput": 6627.070956940565,
    "output_throughput": 5826.3667160845835,
    "total_throughput": 12453.437673025148,
    "itl": 82.42853157598624,
    "ttft": 1966798.2461671662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.198879735288278,
    "arrivals": 558926,
    "finished_requests": 96828,
    "scheduler_time": 320.8043309413759
}
#Debug simulation 
Total elapsed time: 86.32508964184672. Arrivals time: 0.49677709909155965 Scheduler time: 85.59224646771327 Scheduler overhead time: 0.09244027873501182 Adapter cache time: 0.019770681858062744 Engine time: 0.08852890506386757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.21173883089796,
    "estimated_duration": 3600.0191082935107,
    "input_throughput": 6519.2520633939175,
    "output_throughput": 5735.958443228473,
    "total_throughput": 12255.21050662239,
    "itl": 79.62884326730654,
    "ttft": 1972969.900780644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.54469127093908,
    "arrivals": 558926,
    "finished_requests": 95213,
    "scheduler_time": 326.19648258150767
}
#Debug simulation 
Total elapsed time: 85.2119061499834. Arrivals time: 0.5075258947908878 Scheduler time: 84.46748426789418 Scheduler overhead time: 0.09280254412442446 Adapter cache time: 0.020326522644609213 Engine time: 0.08805124368518591 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 84.6496263248846,
    "estimated_duration": 3600.0539091585483,
    "input_throughput": 6509.6572416265735,
    "output_throughput": 5719.272966335244,
    "total_throughput": 12228.930207961817,
    "itl": 79.19255324675575,
    "ttft": 1978928.664097124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.66322744776032,
    "arrivals": 558926,
    "finished_requests": 95077,
    "scheduler_time": 326.5037311842595
}
#Debug simulation 
Total elapsed time: 84.64979592710733. Arrivals time: 0.48648460395634174 Scheduler time: 83.92471327912062 Scheduler overhead time: 0.0933995614759624 Adapter cache time: 0.02004443109035492 Engine time: 0.08898729551583529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 85.07155470084399,
    "estimated_duration": 3600.0325643532587,
    "input_throughput": 6520.375185610855,
    "output_throughput": 5719.307987342862,
    "total_throughput": 12239.683172953717,
    "itl": 79.39639350988504,
    "ttft": 1985035.1911383756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.274262034264387,
    "arrivals": 558926,
    "finished_requests": 95240,
    "scheduler_time": 326.31695462252105
}
#Debug simulation 
Total elapsed time: 85.0717207849957. Arrivals time: 0.5102946688421071 Scheduler time: 84.32430901378393 Scheduler overhead time: 0.0933761429041624 Adapter cache time: 0.020309183280915022 Engine time: 0.08814303344115615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 84.61534397490323,
    "estimated_duration": 3600.011082986565,
    "input_throughput": 6509.734681304996,
    "output_throughput": 5719.341003505694,
    "total_throughput": 12229.07568481069,
    "itl": 79.1917903777594,
    "ttft": 1978910.5540763843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.620768426554307,
    "arrivals": 558926,
    "finished_requests": 95077,
    "scheduler_time": 326.503364033482
}
#Debug simulation 
Total elapsed time: 84.61551354592666. Arrivals time: 0.5046085077337921 Scheduler time: 83.87340861419216 Scheduler overhead time: 0.09267577063292265 Adapter cache time: 0.020381729118525982 Engine time: 0.08875393075868487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 85.34898347826675,
    "estimated_duration": 3600.021371118967,
    "input_throughput": 6591.284204689198,
    "output_throughput": 5789.693129938704,
    "total_throughput": 12380.977334627903,
    "itl": 80.40051068270391,
    "ttft": 1970612.232460684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.028254113499988,
    "arrivals": 558926,
    "finished_requests": 96285,
    "scheduler_time": 322.8155608408447
}
#Debug simulation 
Total elapsed time: 85.34914718940854. Arrivals time: 0.5080612772144377 Scheduler time: 84.6045607374981 Scheduler overhead time: 0.09287408599629998 Adapter cache time: 0.02014659997075796 Engine time: 0.08797178417444229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 4320, 135, 8640, 8640, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 4320, 4320, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 4320, 135, 8640, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 8640, 135, 8640, 4320, 4320, 8640, 8640, 135, 4320, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 8640, 135, 8640, 4320, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 8640, 135, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 8640, 135, 4320, 4320, 135, 135, 4320, 135, 8640, 8640, 135, 8640, 8640, 135, 4320, 8640, 135, 135, 4320, 4320, 135, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 4320, 135, 8640, 8640, 8640, 4320, 135, 8640, 8640, 4320, 4320, 8640, 8640, 135, 8640, 8640, 4320, 135, 8640, 4320, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 4320, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 8640, 135, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 8640, 8640, 135, 8640, 8640, 135, 8640, 4320, 8640, 4320, 4320, 4320, 135, 4320, 135, 135, 8640, 135, 4320, 135, 135, 4320, 8640, 4320, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1676160 . Total input tokens: 373811137 . Total output tokens: 329234407
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 84.814412987791,
    "estimated_duration": 3600.0740241132135,
    "input_throughput": 6509.650869129746,
    "output_throughput": 5719.374341218404,
    "total_throughput": 12229.025210348149,
    "itl": 79.1903620509221,
    "ttft": 1978897.5586834264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.577273819465221,
    "arrivals": 558926,
    "finished_requests": 95080,
    "scheduler_time": 326.51530629657265
}
#Debug simulation 
Total elapsed time: 84.81458910368383. Arrivals time: 0.512887142598629 Scheduler time: 84.06303098239005 Scheduler overhead time: 0.09340024599805474 Adapter cache time: 0.020331164821982384 Engine time: 0.08927395427599549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.56958580436185,
    "estimated_duration": 3600.027851789267,
    "input_throughput": 6719.466347455362,
    "output_throughput": 5833.044038690736,
    "total_throughput": 12552.510386146098,
    "itl": 81.18526490550379,
    "ttft": 1960452.2128501644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 637,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.212104553352178,
    "arrivals": 556060,
    "finished_requests": 97647,
    "scheduler_time": 320.92040528263385
}
#Debug simulation 
Total elapsed time: 86.56974788196385. Arrivals time: 0.5141400210559368 Scheduler time: 85.82038184069097 Scheduler overhead time: 0.09186829207465053 Adapter cache time: 0.020171663258224726 Engine time: 0.08771973103284836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 86.10543655510992,
    "estimated_duration": 3600.0264954282547,
    "input_throughput": 6698.639587965386,
    "output_throughput": 5811.580283247655,
    "total_throughput": 12510.219871213041,
    "itl": 79.35884843446085,
    "ttft": 1962891.2742930432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 630,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.610475885244093,
    "arrivals": 556060,
    "finished_requests": 97325,
    "scheduler_time": 321.5544165807216
}
#Debug simulation 
Total elapsed time: 86.10561127122492. Arrivals time: 0.4888327191583812 Scheduler time: 85.38182685058564 Scheduler overhead time: 0.09185091499239206 Adapter cache time: 0.020111487712711096 Engine time: 0.08748238068073988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.47290335595608,
    "estimated_duration": 3600.006196386439,
    "input_throughput": 6728.412863376049,
    "output_throughput": 5830.070798507879,
    "total_throughput": 12558.483661883929,
    "itl": 79.84377841773926,
    "ttft": 1967190.405673242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.819622496049888,
    "arrivals": 556060,
    "finished_requests": 97760,
    "scheduler_time": 320.84501377304974
}
#Debug simulation 
Total elapsed time: 85.47307398030534. Arrivals time: 0.5105423396453261 Scheduler time: 84.7242311812006 Scheduler overhead time: 0.093381283339113 Adapter cache time: 0.02052636258304119 Engine time: 0.0886988933198154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 86.88629211112857,
    "estimated_duration": 3600.00884544862,
    "input_throughput": 6699.122984236624,
    "output_throughput": 5811.839608797598,
    "total_throughput": 12510.96259303422,
    "itl": 79.35322180424372,
    "ttft": 1962770.9402880103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 630,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3203377018216935,
    "arrivals": 556060,
    "finished_requests": 97330,
    "scheduler_time": 321.5802730467973
}
#Debug simulation 
Total elapsed time: 86.88646112941206. Arrivals time: 0.5193895138800144 Scheduler time: 86.1320221987553 Scheduler overhead time: 0.09280453389510512 Adapter cache time: 0.020154912024736404 Engine time: 0.0872040749527514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 85.42267126077786,
    "estimated_duration": 3600.0627171810447,
    "input_throughput": 6728.509168575642,
    "output_throughput": 5829.9848221628945,
    "total_throughput": 12558.493990738538,
    "itl": 79.84272733557793,
    "ttft": 1967253.9345565063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.775506537430957,
    "arrivals": 556060,
    "finished_requests": 97762,
    "scheduler_time": 320.8528441755483
}
#Debug simulation 
Total elapsed time: 85.42284857761115. Arrivals time: 0.507209250703454 Scheduler time: 84.67731824098155 Scheduler overhead time: 0.09337486699223518 Adapter cache time: 0.020238651428371668 Engine time: 0.0889597050845623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 85.62282150192186,
    "estimated_duration": 3600.066646338817,
    "input_throughput": 6717.755079505245,
    "output_throughput": 5819.508653070711,
    "total_throughput": 12537.263732575955,
    "itl": 80.34760229262058,
    "ttft": 1964189.7921574777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.130396848549116,
    "arrivals": 556060,
    "finished_requests": 97601,
    "scheduler_time": 320.8633338179427
}
#Debug simulation 
Total elapsed time: 85.62299305573106. Arrivals time: 0.5138418017886579 Scheduler time: 84.87266304995865 Scheduler overhead time: 0.09331774432212114 Adapter cache time: 0.02039307216182351 Engine time: 0.08774751238524914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 4320, 66, 8640, 8640, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 4320, 4320, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 4320, 66, 8640, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 8640, 66, 8640, 4320, 4320, 8640, 8640, 66, 4320, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 8640, 66, 8640, 4320, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 8640, 66, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 8640, 66, 4320, 4320, 66, 66, 4320, 66, 8640, 8640, 66, 8640, 8640, 66, 4320, 8640, 66, 66, 4320, 4320, 66, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 4320, 66, 8640, 8640, 8640, 4320, 66, 8640, 8640, 4320, 4320, 8640, 8640, 66, 8640, 8640, 4320, 66, 8640, 4320, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 4320, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 8640, 66, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 8640, 8640, 66, 8640, 8640, 66, 8640, 4320, 8640, 4320, 4320, 4320, 66, 4320, 66, 66, 8640, 66, 4320, 66, 66, 4320, 8640, 4320, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1667328 . Total input tokens: 371849222 . Total output tokens: 327495170
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.55590711673722,
    "estimated_duration": 3600.016998559037,
    "input_throughput": 6728.594617663099,
    "output_throughput": 5830.058860388965,
    "total_throughput": 12558.653478052063,
    "itl": 79.84180477587584,
    "ttft": 1967235.1597795605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.730147875752336,
    "arrivals": 556060,
    "finished_requests": 97762,
    "scheduler_time": 320.85248421521914
}
#Debug simulation 
Total elapsed time: 85.55607539694756. Arrivals time: 0.5122950240038335 Scheduler time: 84.80637763813138 Scheduler overhead time: 0.09276764746755362 Adapter cache time: 0.02023813920095563 Engine time: 0.08868690114468336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.9947412898764,
    "estimated_duration": 3600.049975360369,
    "input_throughput": 6660.109488507837,
    "output_throughput": 5827.235772720084,
    "total_throughput": 12487.345261227922,
    "itl": 80.06591846400153,
    "ttft": 1967338.32842326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 596,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9409957830422226,
    "arrivals": 554588,
    "finished_requests": 96848,
    "scheduler_time": 320.849533213652
}
#Debug simulation 
Total elapsed time: 86.99491015681997. Arrivals time: 0.5009933030232787 Scheduler time: 86.26305778604001 Scheduler overhead time: 0.09089354332536459 Adapter cache time: 0.01954579073935747 Engine time: 0.08554802555590868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 86.64730779826641,
    "estimated_duration": 3600.057458710992,
    "input_throughput": 6656.191539948333,
    "output_throughput": 5826.829499413277,
    "total_throughput": 12483.021039361609,
    "itl": 79.83893601612222,
    "ttft": 1970620.6623923874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.390362800513401,
    "arrivals": 554588,
    "finished_requests": 96783,
    "scheduler_time": 321.2797757924075
}
#Debug simulation 
Total elapsed time: 86.64747395319864. Arrivals time: 0.5124566019512713 Scheduler time: 85.90055976249278 Scheduler overhead time: 0.0917630372568965 Adapter cache time: 0.020299613941460848 Engine time: 0.08707212004810572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.05198646895587,
    "estimated_duration": 3600.037650134112,
    "input_throughput": 6595.322690337833,
    "output_throughput": 5764.044717484267,
    "total_throughput": 12359.3674078221,
    "itl": 78.60644103684761,
    "ttft": 1977089.0030092678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.541781416558696,
    "arrivals": 554588,
    "finished_requests": 95939,
    "scheduler_time": 323.8124449258256
}
#Debug simulation 
Total elapsed time: 85.05215528002009. Arrivals time: 0.5133181703276932 Scheduler time: 84.30364573234692 Scheduler overhead time: 0.09164624661207199 Adapter cache time: 0.020248721353709698 Engine time: 0.08758412254974246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 86.342507887166,
    "estimated_duration": 3600.0491819822932,
    "input_throughput": 6656.338230025287,
    "output_throughput": 5826.887339480568,
    "total_throughput": 12483.225569505856,
    "itl": 79.83328894014592,
    "ttft": 1970502.15683902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1141068268241305,
    "arrivals": 554588,
    "finished_requests": 96785,
    "scheduler_time": 321.30139458250386
}
#Debug simulation 
Total elapsed time: 86.34267674991861. Arrivals time: 0.5090682241134346 Scheduler time: 85.5987369576469 Scheduler overhead time: 0.0917395781725645 Adapter cache time: 0.020081446040421724 Engine time: 0.08718364918604493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 84.76823568111286,
    "estimated_duration": 3600.0939722102057,
    "input_throughput": 6595.265063434749,
    "output_throughput": 5764.007039866339,
    "total_throughput": 12359.27210330109,
    "itl": 78.60582643484223,
    "ttft": 1977067.4024709088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.500150864059142,
    "arrivals": 554588,
    "finished_requests": 95940,
    "scheduler_time": 323.82022061545797
}
#Debug simulation 
Total elapsed time: 84.76840443909168. Arrivals time: 0.5074471151456237 Scheduler time: 84.02665536943823 Scheduler overhead time: 0.09204266965389252 Adapter cache time: 0.019828548189252615 Engine time: 0.0872757425531745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.6006487570703,
    "estimated_duration": 3600.053805440419,
    "input_throughput": 6656.660787620723,
    "output_throughput": 5826.97291032118,
    "total_throughput": 12483.633697941903,
    "itl": 79.8277498622228,
    "ttft": 1970381.427492217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8239686434017317,
    "arrivals": 554588,
    "finished_requests": 96789,
    "scheduler_time": 321.3266688302249
}
#Debug simulation 
Total elapsed time: 86.60081171104684. Arrivals time: 0.5203063804656267 Scheduler time: 85.84574500471354 Scheduler overhead time: 0.09184104483574629 Adapter cache time: 0.01988517213612795 Engine time: 0.08763308776542544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 4320, 33, 8640, 8640, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 4320, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 4320, 4320, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 4320, 33, 8640, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 8640, 33, 8640, 4320, 4320, 8640, 8640, 33, 4320, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 8640, 33, 8640, 4320, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 8640, 33, 4320, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 8640, 33, 4320, 4320, 33, 33, 4320, 33, 8640, 8640, 33, 8640, 8640, 33, 4320, 8640, 33, 33, 4320, 4320, 33, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 4320, 33, 8640, 8640, 8640, 4320, 33, 8640, 8640, 4320, 4320, 8640, 8640, 33, 8640, 8640, 4320, 33, 8640, 4320, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 4320, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 8640, 33, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 8640, 8640, 33, 8640, 8640, 33, 8640, 4320, 8640, 4320, 4320, 4320, 33, 4320, 33, 33, 8640, 33, 4320, 33, 33, 4320, 8640, 4320, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1663104 . Total input tokens: 370908458 . Total output tokens: 326676223
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 84.53155880980194,
    "estimated_duration": 3600.0500368353664,
    "input_throughput": 6595.345552716776,
    "output_throughput": 5764.077384391355,
    "total_throughput": 12359.422937108131,
    "itl": 78.60506845772274,
    "ttft": 1977048.9561689487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.456656256970055,
    "arrivals": 554588,
    "finished_requests": 95940,
    "scheduler_time": 323.8197798477079
}
#Debug simulation 
Total elapsed time: 84.53172301594168. Arrivals time: 0.5171335535123944 Scheduler time: 83.77854688512161 Scheduler overhead time: 0.09252925543114543 Adapter cache time: 0.019988706335425377 Engine time: 0.08786953566595912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 89.18315448006615,
    "estimated_duration": 3600.0310531094738,
    "input_throughput": 6618.866795445781,
    "output_throughput": 5777.437386834541,
    "total_throughput": 12396.30418228032,
    "itl": 81.06896132356684,
    "ttft": 1901636.5917479056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.119530826904876,
    "arrivals": 438250,
    "finished_requests": 96277,
    "scheduler_time": 320.3775729358941
}
#Debug simulation 
Total elapsed time: 89.18332095723599. Arrivals time: 0.5016779392026365 Scheduler time: 88.44183550216258 Scheduler overhead time: 0.09433113597333431 Adapter cache time: 0.02014523232355714 Engine time: 0.08982941694557667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 89.14005302125588,
    "estimated_duration": 3600.0178720803838,
    "input_throughput": 6620.447410786583,
    "output_throughput": 5784.4462833086245,
    "total_throughput": 12404.893694095208,
    "itl": 80.15007926792137,
    "ttft": 1903269.2589581022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.580775538561872,
    "arrivals": 438250,
    "finished_requests": 96357,
    "scheduler_time": 320.6162959318429
}
#Debug simulation 
Total elapsed time: 89.14021928887814. Arrivals time: 0.5004666200838983 Scheduler time: 88.39999904530123 Scheduler overhead time: 0.09410042315721512 Adapter cache time: 0.020342159550637007 Engine time: 0.08925010217353702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 88.15204223338515,
    "estimated_duration": 3600.042924905277,
    "input_throughput": 6542.516711969408,
    "output_throughput": 5718.926809891223,
    "total_throughput": 12261.44352186063,
    "itl": 78.62350981375194,
    "ttft": 1904201.3196924573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.663695406359652,
    "arrivals": 438250,
    "finished_requests": 95184,
    "scheduler_time": 323.8451748794265
}
#Debug simulation 
Total elapsed time: 88.15219917194918. Arrivals time: 0.5002348083071411 Scheduler time: 87.40914139384404 Scheduler overhead time: 0.09521878231316805 Adapter cache time: 0.020349517930299044 Engine time: 0.09023589314892888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 89.19724511681125,
    "estimated_duration": 3600.039890600061,
    "input_throughput": 6614.4519848725695,
    "output_throughput": 5773.761022557834,
    "total_throughput": 12388.213007430402,
    "itl": 79.94390615943858,
    "ttft": 1905644.8520250125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.224021703787142,
    "arrivals": 438250,
    "finished_requests": 96239,
    "scheduler_time": 321.0378884737986
}
#Debug simulation 
Total elapsed time: 89.19741286477074. Arrivals time: 0.4881510017439723 Scheduler time: 88.47046470828354 Scheduler overhead time: 0.09373827278614044 Adapter cache time: 0.020117128267884254 Engine time: 0.08932163892313838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 86.34674494108185,
    "estimated_duration": 3600.0040972202855,
    "input_throughput": 6556.948092983129,
    "output_throughput": 5739.848189604878,
    "total_throughput": 12296.796282588006,
    "itl": 78.54299247177578,
    "ttft": 1904162.0631878034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.698784768013314,
    "arrivals": 438250,
    "finished_requests": 95420,
    "scheduler_time": 323.5537800780922
}
#Debug simulation 
Total elapsed time: 86.34690089290962. Arrivals time: 0.4905017544515431 Scheduler time: 85.61572481691837 Scheduler overhead time: 0.0943371937610209 Adapter cache time: 0.020152879878878593 Engine time: 0.0902843875810504 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 87.90520129026845,
    "estimated_duration": 3600.0349833657565,
    "input_throughput": 6595.32007597375,
    "output_throughput": 5774.41249767099,
    "total_throughput": 12369.73257364474,
    "itl": 80.44700573076203,
    "ttft": 1897337.8455163199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 634,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.047405876321699,
    "arrivals": 438250,
    "finished_requests": 96033,
    "scheduler_time": 321.6505359387871
}
#Debug simulation 
Total elapsed time: 87.9053694549948. Arrivals time: 0.4783031395636499 Scheduler time: 87.18772364314646 Scheduler overhead time: 0.09413337660953403 Adapter cache time: 0.020174602512270212 Engine time: 0.08927125250920653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 8640, 1080, 540, 8640, 8640, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 1080, 1080, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 1080, 540, 8640, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 8640, 540, 8640, 1080, 1080, 8640, 8640, 540, 1080, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 8640, 540, 8640, 1080, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 8640, 540, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 8640, 540, 1080, 1080, 540, 540, 1080, 540, 8640, 8640, 540, 8640, 8640, 540, 1080, 8640, 540, 540, 1080, 1080, 540, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 540, 540, 1080, 540, 8640, 8640, 8640, 1080, 540, 8640, 8640, 1080, 1080, 8640, 8640, 540, 8640, 8640, 1080, 540, 8640, 1080, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 8640, 540, 1080, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 8640, 540, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 8640, 8640, 540, 8640, 8640, 540, 8640, 1080, 8640, 1080, 1080, 1080, 540, 1080, 540, 540, 8640, 540, 1080, 540, 540, 1080, 8640, 1080, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1313280 . Total input tokens: 292791595 . Total output tokens: 257996321
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.3038685307838,
    "estimated_duration": 3600.0082118122828,
    "input_throughput": 6542.579798211903,
    "output_throughput": 5718.981954664928,
    "total_throughput": 12261.56175287683,
    "itl": 78.62199756566231,
    "ttft": 1904203.7453343947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.576913309358093,
    "arrivals": 438250,
    "finished_requests": 95184,
    "scheduler_time": 323.8507555840539
}
#Debug simulation 
Total elapsed time: 87.30403102003038. Arrivals time: 0.4783539609052241 Scheduler time: 86.58369111502543 Scheduler overhead time: 0.09521047445014119 Adapter cache time: 0.02045690966770053 Engine time: 0.09015905670821667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 88.33237825613469,
    "estimated_duration": 3600.033677605302,
    "input_throughput": 6599.3032642415,
    "output_throughput": 5759.44417658674,
    "total_throughput": 12358.74744082824,
    "itl": 81.20094626785281,
    "ttft": 1904641.779536547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.093081190777076,
    "arrivals": 426621,
    "finished_requests": 96155,
    "scheduler_time": 321.41697096307263
}
#Debug simulation 
Total elapsed time: 88.3325425372459. Arrivals time: 0.48102455073967576 Scheduler time: 87.61025106254965 Scheduler overhead time: 0.09456211281940341 Adapter cache time: 0.02010863972827792 Engine time: 0.09034985536709428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.16256505204365,
    "estimated_duration": 3600.0156127421164,
    "input_throughput": 6564.949028651058,
    "output_throughput": 5718.731865253932,
    "total_throughput": 12283.68089390499,
    "itl": 80.15314697114943,
    "ttft": 1899843.2689893136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.441991351963958,
    "arrivals": 426621,
    "finished_requests": 95574,
    "scheduler_time": 322.71263807075565
}
#Debug simulation 
Total elapsed time: 87.16272790916264. Arrivals time: 0.4791627498343587 Scheduler time: 86.44387790700421 Scheduler overhead time: 0.09385409951210022 Adapter cache time: 0.019934108946472406 Engine time: 0.08996384544298053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.98708678409457,
    "estimated_duration": 3600.027365969953,
    "input_throughput": 6537.559748148991,
    "output_throughput": 5704.467469920728,
    "total_throughput": 12242.02721806972,
    "itl": 78.70880475884141,
    "ttft": 1906862.2588541918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.564300830285098,
    "arrivals": 426621,
    "finished_requests": 95202,
    "scheduler_time": 324.73726413808714
}
#Debug simulation 
Total elapsed time: 85.98725646035746. Arrivals time: 0.48055998235940933 Scheduler time: 85.26370068266988 Scheduler overhead time: 0.0956606580875814 Adapter cache time: 0.020345374010503292 Engine time: 0.09087656391784549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 87.12942462181672,
    "estimated_duration": 3600.07197098623,
    "input_throughput": 6565.649851029161,
    "output_throughput": 5719.302882258616,
    "total_throughput": 12284.952733287777,
    "itl": 80.15020535575462,
    "ttft": 1899891.0524219975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.166566415322007,
    "arrivals": 426621,
    "finished_requests": 95585,
    "scheduler_time": 322.7403469798427
}
#Debug simulation 
Total elapsed time: 87.12959434697405. Arrivals time: 0.4771487913094461 Scheduler time: 86.41006784280762 Scheduler overhead time: 0.0951039376668632 Adapter cache time: 0.020404386799782515 Engine time: 0.09061115048825741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 86.06422762619331,
    "estimated_duration": 3600.0362694348955,
    "input_throughput": 6537.543579719099,
    "output_throughput": 5704.45336186116,
    "total_throughput": 12241.99694158026,
    "itl": 78.70807763227393,
    "ttft": 1906844.8773414532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.522877394962159,
    "arrivals": 426621,
    "finished_requests": 95202,
    "scheduler_time": 324.7431254116275
}
#Debug simulation 
Total elapsed time: 86.06439656810835. Arrivals time: 0.4780162493698299 Scheduler time: 85.34424227150157 Scheduler overhead time: 0.09441946865990758 Adapter cache time: 0.020204150583595037 Engine time: 0.09086978435516357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 87.02378054196015,
    "estimated_duration": 3600.0576918451234,
    "input_throughput": 6566.4461582236745,
    "output_throughput": 5720.4269383375095,
    "total_throughput": 12286.873096561185,
    "itl": 80.1462927099089,
    "ttft": 1899903.9940058722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8750400109262957,
    "arrivals": 426621,
    "finished_requests": 95601,
    "scheduler_time": 322.7647255750492
}
#Debug simulation 
Total elapsed time: 87.02395052276552. Arrivals time: 0.4655931065790355 Scheduler time: 86.31771486625075 Scheduler overhead time: 0.09477465273812413 Adapter cache time: 0.02024116413667798 Engine time: 0.08919326914474368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 1080, 270, 8640, 8640, 270, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 270, 270, 1080, 270, 8640, 270, 8640, 1080, 8640, 1080, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 8640, 8640, 270, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 8640, 1080, 1080, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 270, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 1080, 270, 8640, 1080, 1080, 270, 8640, 8640, 8640, 270, 1080, 8640, 270, 8640, 1080, 1080, 8640, 8640, 270, 1080, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 8640, 8640, 1080, 270, 8640, 270, 8640, 1080, 1080, 1080, 1080, 1080, 270, 270, 8640, 270, 8640, 270, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 8640, 270, 1080, 1080, 270, 270, 1080, 270, 8640, 8640, 270, 8640, 8640, 270, 1080, 8640, 270, 270, 1080, 1080, 270, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 1080, 270, 8640, 8640, 8640, 1080, 270, 8640, 8640, 1080, 1080, 8640, 8640, 270, 8640, 8640, 1080, 270, 8640, 1080, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 1080, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 8640, 270, 270, 270, 1080, 270, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 270, 1080, 1080, 1080, 1080, 270, 1080, 8640, 8640, 270, 8640, 8640, 270, 8640, 1080, 8640, 1080, 1080, 1080, 270, 1080, 270, 270, 8640, 270, 1080, 270, 270, 1080, 8640, 1080, 8640, 270, 8640, 8640, 1080, 8640, 1080, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1278720 . Total input tokens: 285043161 . Total output tokens: 251156809
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.94456224981695,
    "estimated_duration": 3600.042180095965,
    "input_throughput": 6537.711455195397,
    "output_throughput": 5704.546494911502,
    "total_throughput": 12242.2579501069,
    "itl": 78.7067100954107,
    "ttft": 1906812.6911645506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.478761436343228,
    "arrivals": 426621,
    "finished_requests": 95204,
    "scheduler_time": 324.7487875358117
}
#Debug simulation 
Total elapsed time: 85.94473422178999. Arrivals time: 0.47667955281212926 Scheduler time: 85.22481823759153 Scheduler overhead time: 0.09599984623491764 Adapter cache time: 0.020348782185465097 Engine time: 0.09034942090511322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 88.4597828509286,
    "estimated_duration": 3600.0261485796955,
    "input_throughput": 6578.381384630582,
    "output_throughput": 5774.8802764119055,
    "total_throughput": 12353.261661042488,
    "itl": 81.83544760084241,
    "ttft": 1892837.4867401994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.914546146914422,
    "arrivals": 420933,
    "finished_requests": 96144,
    "scheduler_time": 319.3312076328352
}
#Debug simulation 
Total elapsed time: 88.45994789898396. Arrivals time: 0.4837311669252813 Scheduler time: 87.73578259907663 Scheduler overhead time: 0.09414041973650455 Adapter cache time: 0.020078844390809536 Engine time: 0.08993272576481104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 88.51779400464147,
    "estimated_duration": 3600.017963242912,
    "input_throughput": 6541.394859815319,
    "output_throughput": 5740.978020393685,
    "total_throughput": 12282.372880209005,
    "itl": 80.742791331623,
    "ttft": 1899335.1309036482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1719117898773455,
    "arrivals": 420933,
    "finished_requests": 95524,
    "scheduler_time": 321.7659000522498
}
#Debug simulation 
Total elapsed time: 88.51796164503321. Arrivals time: 0.487732095643878 Scheduler time: 87.78792135277763 Scheduler overhead time: 0.09541378170251846 Adapter cache time: 0.020248448010534048 Engine time: 0.09021897381171584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.04945381032303,
    "estimated_duration": 3600.0307127593496,
    "input_throughput": 6516.469683676332,
    "output_throughput": 5724.119221251075,
    "total_throughput": 12240.588904927406,
    "itl": 79.83620928754529,
    "ttft": 1901139.7828467323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 581,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.369234221521798,
    "arrivals": 420933,
    "finished_requests": 95187,
    "scheduler_time": 322.5305514836198
}
#Debug simulation 
Total elapsed time: 87.04961756523699. Arrivals time: 0.478234083391726 Scheduler time: 86.32588578294963 Scheduler overhead time: 0.09628253849223256 Adapter cache time: 0.02055728854611516 Engine time: 0.09196690283715725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 88.31672037299722,
    "estimated_duration": 3600.0335302731287,
    "input_throughput": 6541.725737263693,
    "output_throughput": 5741.545134562071,
    "total_throughput": 12283.270871825765,
    "itl": 80.73733470358272,
    "ttft": 1899185.1642065197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.909538025921205,
    "arrivals": 420933,
    "finished_requests": 95530,
    "scheduler_time": 321.79582538245563
}
#Debug simulation 
Total elapsed time: 88.31689587188885. Arrivals time: 0.49331744853407145 Scheduler time: 87.5817385292612 Scheduler overhead time: 0.09466772293671966 Adapter cache time: 0.02025563968345523 Engine time: 0.0902074258774519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 86.95966343441978,
    "estimated_duration": 3600.0588596453385,
    "input_throughput": 6516.8390058798295,
    "output_throughput": 5724.166688216354,
    "total_throughput": 12241.005694096182,
    "itl": 79.83433380240345,
    "ttft": 1901181.1616927267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 581,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.329053489258546,
    "arrivals": 420933,
    "finished_requests": 95191,
    "scheduler_time": 322.53712003622576
}
#Debug simulation 
Total elapsed time: 86.9598278501071. Arrivals time: 0.4920155219733715 Scheduler time: 86.22404233599082 Scheduler overhead time: 0.0958101162686944 Adapter cache time: 0.02045785589143634 Engine time: 0.09128315141424537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 88.52236223267391,
    "estimated_duration": 3600.022113298571,
    "input_throughput": 6604.623597218457,
    "output_throughput": 5807.735158838447,
    "total_throughput": 12412.358756056905,
    "itl": 81.80620858559253,
    "ttft": 1889666.2466357895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7026741455308922,
    "arrivals": 420933,
    "finished_requests": 96539,
    "scheduler_time": 317.9541786953158
}
#Debug simulation 
Total elapsed time: 88.5225263047032. Arrivals time: 0.4902964374050498 Scheduler time: 87.79025697242469 Scheduler overhead time: 0.09561859956011176 Adapter cache time: 0.020123380236327648 Engine time: 0.09031533682718873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 1080, 135, 8640, 8640, 135, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 135, 135, 1080, 135, 8640, 135, 8640, 1080, 8640, 1080, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 8640, 8640, 135, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 8640, 1080, 1080, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 135, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 1080, 135, 8640, 1080, 1080, 135, 8640, 8640, 8640, 135, 1080, 8640, 135, 8640, 1080, 1080, 8640, 8640, 135, 1080, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 8640, 8640, 1080, 135, 8640, 135, 8640, 1080, 1080, 1080, 1080, 1080, 135, 135, 8640, 135, 8640, 135, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 8640, 135, 1080, 1080, 135, 135, 1080, 135, 8640, 8640, 135, 8640, 8640, 135, 1080, 8640, 135, 135, 1080, 1080, 135, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 1080, 135, 8640, 8640, 8640, 1080, 135, 8640, 8640, 1080, 1080, 8640, 8640, 135, 8640, 8640, 1080, 135, 8640, 1080, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 1080, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 8640, 135, 135, 135, 1080, 135, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 135, 1080, 1080, 1080, 1080, 135, 1080, 8640, 8640, 135, 8640, 8640, 135, 8640, 1080, 8640, 1080, 1080, 1080, 135, 1080, 135, 135, 8640, 135, 1080, 135, 135, 1080, 8640, 1080, 8640, 135, 8640, 8640, 1080, 8640, 1080, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1261440 . Total input tokens: 281189536 . Total output tokens: 247710118
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.45320415077731,
    "estimated_duration": 3600.0170088448313,
    "input_throughput": 6516.914765224439,
    "output_throughput": 5724.233232612547,
    "total_throughput": 12241.147997836986,
    "itl": 79.8335546880324,
    "ttft": 1901163.5107070485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 581,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.288044288288836,
    "arrivals": 420933,
    "finished_requests": 95191,
    "scheduler_time": 322.5366819914435
}
#Debug simulation 
Total elapsed time: 87.45337129384279. Arrivals time: 0.4949432145804167 Scheduler time: 86.71385256107897 Scheduler overhead time: 0.09637210052460432 Adapter cache time: 0.02035184670239687 Engine time: 0.09106023190543056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 90.98670242307708,
    "estimated_duration": 3600.0592063859604,
    "input_throughput": 6619.522522776289,
    "output_throughput": 5800.800154329773,
    "total_throughput": 12420.322677106064,
    "itl": 82.57620247571931,
    "ttft": 1889165.2318491077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 587,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8814841017546713,
    "arrivals": 418062,
    "finished_requests": 96791,
    "scheduler_time": 318.3304404854536
}
#Debug simulation 
Total elapsed time: 90.98687852034345. Arrivals time: 0.4982779175043106 Scheduler time: 90.24594391044229 Scheduler overhead time: 0.09531287755817175 Adapter cache time: 0.020323473494499922 Engine time: 0.09083875687792897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 89.17256020521745,
    "estimated_duration": 3600.0482748997097,
    "input_throughput": 6542.35115795944,
    "output_throughput": 5739.664977291349,
    "total_throughput": 12282.016135250788,
    "itl": 80.981357899872,
    "ttft": 1893337.288815752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.123059680373414,
    "arrivals": 418062,
    "finished_requests": 95627,
    "scheduler_time": 321.85489617835617
}
#Debug simulation 
Total elapsed time: 89.1727290940471. Arrivals time: 0.5088014486245811 Scheduler time: 88.41972179338336 Scheduler overhead time: 0.09637553244829178 Adapter cache time: 0.02010331442579627 Engine time: 0.09088939521461725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 89.23918719124049,
    "estimated_duration": 3600.0122457282564,
    "input_throughput": 6578.917898988663,
    "output_throughput": 5755.251811875073,
    "total_throughput": 12334.169710863736,
    "itl": 80.24621363158782,
    "ttft": 1897547.8669109023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2923590591736485,
    "arrivals": 418062,
    "finished_requests": 96153,
    "scheduler_time": 320.07423967694416
}
#Debug simulation 
Total elapsed time: 89.23935946496204. Arrivals time: 0.4999979701824486 Scheduler time: 88.49721195502207 Scheduler overhead time: 0.09507490275427699 Adapter cache time: 0.02016700804233551 Engine time: 0.09060225635766983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 89.4289290108718,
    "estimated_duration": 3600.0674071892086,
    "input_throughput": 6569.948649507859,
    "output_throughput": 5758.5980080818035,
    "total_throughput": 12328.546657589663,
    "itl": 81.40065207612491,
    "ttft": 1887902.8384800346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9570019144518245,
    "arrivals": 418062,
    "finished_requests": 96003,
    "scheduler_time": 320.68683072017103
}
#Debug simulation 
Total elapsed time: 89.42909321282059. Arrivals time: 0.5027580666355789 Scheduler time: 88.68252910440788 Scheduler overhead time: 0.09561322163790464 Adapter cache time: 0.020082058385014534 Engine time: 0.09143348690122366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 88.93702720012516,
    "estimated_duration": 3600.0570499990595,
    "input_throughput": 6578.914631368463,
    "output_throughput": 5755.488235944876,
    "total_throughput": 12334.40286731334,
    "itl": 80.24601305806108,
    "ttft": 1897509.546857872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.25321391279347,
    "arrivals": 418062,
    "finished_requests": 96156,
    "scheduler_time": 320.08126034521774
}
#Debug simulation 
Total elapsed time: 88.93719412200153. Arrivals time: 0.49092033272609115 Scheduler time: 88.20278645586222 Scheduler overhead time: 0.09535969467833638 Adapter cache time: 0.01993373455479741 Engine time: 0.09134980849921703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 89.37119719572365,
    "estimated_duration": 3600.032868166553,
    "input_throughput": 6543.325814688143,
    "output_throughput": 5740.190369574136,
    "total_throughput": 12283.516184262278,
    "itl": 80.96641829584397,
    "ttft": 1893554.6450708557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6005314104817643,
    "arrivals": 418062,
    "finished_requests": 95645,
    "scheduler_time": 321.89916262657295
}
#Debug simulation 
Total elapsed time: 89.37135572172701. Arrivals time: 0.4810917880386114 Scheduler time: 88.64768825378269 Scheduler overhead time: 0.09518011752516031 Adapter cache time: 0.020291228778660297 Engine time: 0.09060212504118681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 1080, 66, 8640, 8640, 66, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 66, 66, 1080, 66, 8640, 66, 8640, 1080, 8640, 1080, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 8640, 8640, 66, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 8640, 1080, 1080, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 66, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 1080, 66, 8640, 1080, 1080, 66, 8640, 8640, 8640, 66, 1080, 8640, 66, 8640, 1080, 1080, 8640, 8640, 66, 1080, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 8640, 8640, 1080, 66, 8640, 66, 8640, 1080, 1080, 1080, 1080, 1080, 66, 66, 8640, 66, 8640, 66, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 8640, 66, 1080, 1080, 66, 66, 1080, 66, 8640, 8640, 66, 8640, 8640, 66, 1080, 8640, 66, 66, 1080, 1080, 66, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 1080, 66, 8640, 8640, 8640, 1080, 66, 8640, 8640, 1080, 1080, 8640, 8640, 66, 8640, 8640, 1080, 66, 8640, 1080, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 1080, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 8640, 66, 66, 66, 1080, 66, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 66, 1080, 1080, 1080, 1080, 66, 1080, 8640, 8640, 66, 8640, 8640, 66, 8640, 1080, 8640, 1080, 1080, 1080, 66, 1080, 66, 66, 8640, 66, 1080, 66, 66, 1080, 8640, 1080, 8640, 66, 8640, 8640, 1080, 8640, 1080, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1252608 . Total input tokens: 279188928 . Total output tokens: 245992403
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 89.11198014393449,
    "estimated_duration": 3600.0155413481853,
    "input_throughput": 6578.990487116148,
    "output_throughput": 5755.554597478334,
    "total_throughput": 12334.545084594482,
    "itl": 80.24525459202346,
    "ttft": 1897492.6479099654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.211997594647144,
    "arrivals": 418062,
    "finished_requests": 96156,
    "scheduler_time": 320.08096801248945
}
#Debug simulation 
Total elapsed time: 89.11214909190312. Arrivals time: 0.4892020276747644 Scheduler time: 88.37892703479156 Scheduler overhead time: 0.09636291628703475 Adapter cache time: 0.020245952997356653 Engine time: 0.09108552662655711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 89.70172516489401,
    "estimated_duration": 3600.0472241335215,
    "input_throughput": 6779.823563529662,
    "output_throughput": 5926.296148833161,
    "total_throughput": 12706.119712362823,
    "itl": 83.05916429660468,
    "ttft": 1861284.073597467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.033569509489524,
    "arrivals": 416618,
    "finished_requests": 98919,
    "scheduler_time": 312.359406736938
}
#Debug simulation 
Total elapsed time: 89.70188330905512. Arrivals time: 0.4976972700096667 Scheduler time: 88.96379974996671 Scheduler overhead time: 0.09373555332422256 Adapter cache time: 0.020307086873799562 Engine time: 0.09044542536139488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 89.1993072219193,
    "estimated_duration": 3600.014086439668,
    "input_throughput": 6575.2187162717355,
    "output_throughput": 5737.257550685459,
    "total_throughput": 12312.476266957196,
    "itl": 79.91542044391271,
    "ttft": 1886481.12411016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.129443601313984,
    "arrivals": 416618,
    "finished_requests": 95827,
    "scheduler_time": 322.5983074077926
}
#Debug simulation 
Total elapsed time: 89.1994681940414. Arrivals time: 0.4793737083673477 Scheduler time: 88.48136797640473 Scheduler overhead time: 0.09389024274423718 Adapter cache time: 0.01965387910604477 Engine time: 0.08947202004492283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 88.63768481463194,
    "estimated_duration": 3600.0357367990605,
    "input_throughput": 6649.367048029424,
    "output_throughput": 5801.870738808676,
    "total_throughput": 12451.237786838099,
    "itl": 80.22295222637263,
    "ttft": 1874104.6008735413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.427475541532075,
    "arrivals": 416618,
    "finished_requests": 96953,
    "scheduler_time": 318.63493356915154
}
#Debug simulation 
Total elapsed time: 88.63784568058327. Arrivals time: 0.4904621997848153 Scheduler time: 87.90580241195858 Scheduler overhead time: 0.095700996927917 Adapter cache time: 0.02020384231582284 Engine time: 0.08936346136033535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 90.0252636182122,
    "estimated_duration": 3600.019923714243,
    "input_throughput": 6733.976620603915,
    "output_throughput": 5876.3455337141095,
    "total_throughput": 12610.322154318024,
    "itl": 81.02201134456605,
    "ttft": 1865985.9963389286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.084963664067904,
    "arrivals": 416618,
    "finished_requests": 98211,
    "scheduler_time": 314.81276077410763
}
#Debug simulation 
Total elapsed time: 90.02542510908097. Arrivals time: 0.4877718584612012 Scheduler time: 89.29702170845121 Scheduler overhead time: 0.09373244503512979 Adapter cache time: 0.02008922165259719 Engine time: 0.0908448500558734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 88.06296892091632,
    "estimated_duration": 3600.051785059479,
    "input_throughput": 6649.46184922851,
    "output_throughput": 5801.988206582118,
    "total_throughput": 12451.450055810628,
    "itl": 80.22207621460058,
    "ttft": 1874151.012635524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.387709043622053,
    "arrivals": 416618,
    "finished_requests": 96954,
    "scheduler_time": 318.64082161478
}
#Debug simulation 
Total elapsed time: 88.06314272806048. Arrivals time: 0.48755302652716637 Scheduler time: 87.33605068316683 Scheduler overhead time: 0.09417218901216984 Adapter cache time: 0.02016711700707674 Engine time: 0.08947021374478936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 89.05082251923159,
    "estimated_duration": 3600.050465196186,
    "input_throughput": 6717.873605886257,
    "output_throughput": 5871.486303969935,
    "total_throughput": 12589.359909856192,
    "itl": 81.76144849382825,
    "ttft": 1853987.9683067405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.996334508797135,
    "arrivals": 416618,
    "finished_requests": 98014,
    "scheduler_time": 315.4446121627069
}
#Debug simulation 
Total elapsed time: 89.05099683115259. Arrivals time: 0.4830459919758141 Scheduler time: 88.32747100060806 Scheduler overhead time: 0.095301846973598 Adapter cache time: 0.020361330825835466 Engine time: 0.08927818946540356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 8640, 1080, 33, 8640, 8640, 33, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 33, 33, 1080, 33, 8640, 33, 8640, 1080, 8640, 1080, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 8640, 8640, 33, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 8640, 1080, 1080, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 8640, 8640, 33, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 1080, 33, 8640, 1080, 1080, 33, 8640, 8640, 8640, 33, 1080, 8640, 33, 8640, 1080, 1080, 8640, 8640, 33, 1080, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 8640, 8640, 1080, 33, 8640, 33, 8640, 1080, 1080, 1080, 1080, 1080, 33, 33, 8640, 33, 8640, 33, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 8640, 33, 1080, 1080, 33, 33, 1080, 33, 8640, 8640, 33, 8640, 8640, 33, 1080, 8640, 33, 33, 1080, 1080, 33, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 33, 33, 1080, 33, 8640, 8640, 8640, 1080, 33, 8640, 8640, 1080, 1080, 8640, 8640, 33, 8640, 8640, 1080, 33, 8640, 1080, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 33, 8640, 33, 1080, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 8640, 33, 33, 33, 1080, 33, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 33, 1080, 1080, 1080, 1080, 33, 1080, 8640, 8640, 33, 8640, 8640, 33, 8640, 1080, 8640, 1080, 1080, 1080, 33, 1080, 33, 33, 8640, 33, 1080, 33, 33, 1080, 8640, 1080, 8640, 33, 8640, 8640, 1080, 8640, 1080, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1248384 . Total input tokens: 278260102 . Total output tokens: 245186076
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 89.36542702419683,
    "estimated_duration": 3600.029811575058,
    "input_throughput": 6527.5137234818585,
    "output_throughput": 5686.633464583234,
    "total_throughput": 12214.147188065093,
    "itl": 78.08208143562716,
    "ttft": 1897652.924864959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.161780479028858,
    "arrivals": 416618,
    "finished_requests": 95172,
    "scheduler_time": 324.29025873545316
}
#Debug simulation 
Total elapsed time: 89.36558964988217. Arrivals time: 0.4909274368546903 Scheduler time: 88.63348215213045 Scheduler overhead time: 0.0946275107562542 Adapter cache time: 0.020019537769258022 Engine time: 0.0903590377420187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 87.17997784679756,
    "estimated_duration": 3600.0295254478456,
    "input_throughput": 6618.819326776435,
    "output_throughput": 5727.682468780554,
    "total_throughput": 12346.50179555699,
    "itl": 83.26924496184945,
    "ttft": 1873153.366100064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.198879735288278,
    "arrivals": 403575,
    "finished_requests": 95801,
    "scheduler_time": 322.34964047460915
}
#Debug simulation 
Total elapsed time: 87.18014554493129. Arrivals time: 0.494224741589278 Scheduler time: 86.44224801007658 Scheduler overhead time: 0.09547579474747181 Adapter cache time: 0.020437491592019796 Engine time: 0.09122947603464127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 89.42597297113389,
    "estimated_duration": 3600.0310728592876,
    "input_throughput": 6585.313715409321,
    "output_throughput": 5682.898448915597,
    "total_throughput": 12268.21216432492,
    "itl": 81.73881605779069,
    "ttft": 1901938.0417188243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.136658559301873,
    "arrivals": 403575,
    "finished_requests": 95288,
    "scheduler_time": 323.25369429290134
}
#Debug simulation 
Total elapsed time: 89.42613584408537. Arrivals time: 0.5040071690455079 Scheduler time: 88.67792274570093 Scheduler overhead time: 0.09617204312235117 Adapter cache time: 0.020454090554267168 Engine time: 0.0908923507668078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.86909628007561,
    "estimated_duration": 3600.094434917929,
    "input_throughput": 6549.967626206883,
    "output_throughput": 5648.10739484492,
    "total_throughput": 12198.075021051804,
    "itl": 79.86638978522859,
    "ttft": 1908462.9085119597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.217497633816708,
    "arrivals": 403575,
    "finished_requests": 94810,
    "scheduler_time": 325.20827942319613
}
#Debug simulation 
Total elapsed time: 87.86925729224458. Arrivals time: 0.4948324942961335 Scheduler time: 87.1279717781581 Scheduler overhead time: 0.09716920368373394 Adapter cache time: 0.020048370584845543 Engine time: 0.09176908526569605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 86.73648617276922,
    "estimated_duration": 3600.001150486109,
    "input_throughput": 6572.822621683022,
    "output_throughput": 5695.702624214791,
    "total_throughput": 12268.525245897812,
    "itl": 82.06218666128339,
    "ttft": 1874373.2680185053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.297021276080043,
    "arrivals": 403575,
    "finished_requests": 95119,
    "scheduler_time": 323.9600531110619
}
#Debug simulation 
Total elapsed time: 86.73664485383779. Arrivals time: 0.48463081289082766 Scheduler time: 86.00779258040711 Scheduler overhead time: 0.09641326870769262 Adapter cache time: 0.020489064510911703 Engine time: 0.09080499690026045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_384_slots_16_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 87.1617256430909,
    "estimated_duration": 3600.003086278463,
    "input_throughput": 6524.402462188111,
    "output_throughput": 5652.912931537935,
    "total_throughput": 12177.315393726045,
    "itl": 79.86873001575641,
    "ttft": 1900445.037362289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.285876861130853,
    "arrivals": 403575,
    "finished_requests": 94377,
    "scheduler_time": 326.3088819431587
}
#Debug simulation 
Total elapsed time: 87.16188946599141. Arrivals time: 0.4694428681395948 Scheduler time: 86.44626670284197 Scheduler overhead time: 0.09701935620978475 Adapter cache time: 0.020172812044620514 Engine time: 0.09209370240569115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_384_slots_16_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_384_slots_16_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.5480050551705,
    "estimated_duration": 3600.0039972188315,
    "input_throughput": 6573.020757277125,
    "output_throughput": 5696.076175426957,
    "total_throughput": 12269.096932704082,
    "itl": 82.05487329741162,
    "ttft": 1874247.8783179997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.002718429737706,
    "arrivals": 403575,
    "finished_requests": 95123,
    "scheduler_time": 324.0018296721607
}
#Debug simulation 
Total elapsed time: 86.54816933395341. Arrivals time: 0.4738584985025227 Scheduler time: 85.83026497485116 Scheduler overhead time: 0.09632720379158854 Adapter cache time: 0.020395401399582624 Engine time: 0.09036502474918962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_384_slots_16_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_384_slots_16_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 8640, 540, 270, 8640, 8640, 270, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 270, 8640, 270, 8640, 540, 540, 540, 270, 270, 540, 270, 8640, 270, 8640, 540, 8640, 540, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 8640, 8640, 270, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 540, 270, 270, 270, 540, 270, 540, 270, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 8640, 8640, 540, 270, 270, 8640, 540, 540, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 270, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 8640, 270, 270, 8640, 270, 8640, 540, 270, 8640, 540, 540, 270, 8640, 8640, 8640, 270, 540, 8640, 270, 8640, 540, 540, 8640, 8640, 270, 540, 8640, 270, 540, 540, 270, 540, 270, 540, 8640, 8640, 540, 270, 8640, 270, 8640, 540, 540, 540, 540, 540, 270, 270, 8640, 270, 8640, 270, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 270, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 270, 270, 270, 270, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270, 540, 8640, 8640, 540, 540, 270, 8640, 540, 8640, 270, 540, 540, 270, 270, 540, 270, 8640, 8640, 270, 8640, 8640, 270, 540, 8640, 270, 270, 540, 540, 270, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 270, 270, 540, 270, 8640, 8640, 8640, 540, 270, 8640, 8640, 540, 540, 8640, 8640, 270, 8640, 8640, 540, 270, 8640, 540, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 8640, 270, 540, 8640, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 8640, 270, 270, 270, 540, 270, 540, 540, 270, 8640, 540, 8640, 270, 8640, 270, 540, 540, 540, 540, 270, 540, 8640, 8640, 270, 8640, 8640, 270, 8640, 540, 8640, 540, 540, 540, 270, 540, 270, 270, 8640, 270, 540, 270, 270, 540, 8640, 540, 8640, 270, 8640, 8640, 540, 8640, 540, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1209600 . Total input tokens: 269647645 . Total output tokens: 237532542
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.35658020526171,
    "estimated_duration": 3600.001484675841,
    "input_throughput": 6524.405364825828,
    "output_throughput": 5652.915446459168,
    "total_throughput": 12177.320811284995,
    "itl": 79.86805005267325,
    "ttft": 1900427.8961377288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.245696128867601,
    "arrivals": 403575,
    "finished_requests": 94377,
    "scheduler_time": 326.3141198804702
}
#Debug simulation 
Total elapsed time: 87.35675201099366. Arrivals time: 0.46502726851031184 Scheduler time: 86.64626744482666 Scheduler overhead time: 0.09631186863407493 Adapter cache time: 0.020413983147591352 Engine time: 0.09151816973462701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 89.7695149439387,
    "estimated_duration": 3600.0028490541026,
    "input_throughput": 6656.584731952767,
    "output_throughput": 5786.6065315680335,
    "total_throughput": 12443.191263520801,
    "itl": 82.8385527224584,
    "ttft": 1884349.9869222497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7558483301476184,
    "arrivals": 397780,
    "finished_requests": 96745,
    "scheduler_time": 318.0486827998647
}
#Debug simulation 
Total elapsed time: 89.76967926230282. Arrivals time: 0.48058288684114814 Scheduler time: 89.04664823878556 Scheduler overhead time: 0.09504590928554535 Adapter cache time: 0.02023855270817876 Engine time: 0.09127385215833783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 88.72215066291392,
    "estimated_duration": 3600.0851386131817,
    "input_throughput": 6661.494958210428,
    "output_throughput": 5778.028629626541,
    "total_throughput": 12439.52358783697,
    "itl": 82.71829740077847,
    "ttft": 1882261.0285087093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.188287110752433,
    "arrivals": 397780,
    "finished_requests": 96772,
    "scheduler_time": 317.80094855199684
}
#Debug simulation 
Total elapsed time: 88.72232455713674. Arrivals time: 0.49265370052307844 Scheduler time: 87.98471289779991 Scheduler overhead time: 0.09653537953272462 Adapter cache time: 0.020224427338689566 Engine time: 0.09163264883682132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 87.30254596518353,
    "estimated_duration": 3600.029706563926,
    "input_throughput": 6571.974658115194,
    "output_throughput": 5716.689771330517,
    "total_throughput": 12288.664429445711,
    "itl": 80.58437138718638,
    "ttft": 1887987.1410544591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3127017079061245,
    "arrivals": 397780,
    "finished_requests": 95549,
    "scheduler_time": 321.703566197844
}
#Debug simulation 
Total elapsed time: 87.30270931124687. Arrivals time: 0.49653839971870184 Scheduler time: 86.56140670645982 Scheduler overhead time: 0.0956692467443645 Adapter cache time: 0.01964946649968624 Engine time: 0.09284239681437612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 88.75517782988027,
    "estimated_duration": 3600.0395409839375,
    "input_throughput": 6662.040715654187,
    "output_throughput": 5778.289311320877,
    "total_throughput": 12440.330026975063,
    "itl": 82.71388910638065,
    "ttft": 1882241.8678371354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.938407335556107,
    "arrivals": 397780,
    "finished_requests": 96777,
    "scheduler_time": 317.8153704851682
}
#Debug simulation 
Total elapsed time: 88.75534517597407. Arrivals time: 0.4936533351428807 Scheduler time: 88.01990767940879 Scheduler overhead time: 0.09558330848813057 Adapter cache time: 0.019603327848017216 Engine time: 0.0907375612296164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 86.92182826716453,
    "estimated_duration": 3600.071778628206,
    "input_throughput": 6571.897855052015,
    "output_throughput": 5716.6229635126965,
    "total_throughput": 12288.520818564712,
    "itl": 80.58377974060791,
    "ttft": 1888013.6598885672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.27500638176225,
    "arrivals": 397780,
    "finished_requests": 95549,
    "scheduler_time": 321.7105512194501
}
#Debug simulation 
Total elapsed time: 86.92198194283992. Arrivals time: 0.442656961735338 Scheduler time: 86.23684476921335 Scheduler overhead time: 0.09552778908982873 Adapter cache time: 0.019345459062606096 Engine time: 0.09113860921934247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.964993362315,
    "estimated_duration": 3600.0920770267003,
    "input_throughput": 6662.331542311304,
    "output_throughput": 5778.836361647232,
    "total_throughput": 12441.167903958534,
    "itl": 82.7093179658132,
    "ttft": 1882192.921391392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6579866989468988,
    "arrivals": 397780,
    "finished_requests": 96785,
    "scheduler_time": 317.84094164457866
}
#Debug simulation 
Total elapsed time: 86.9651703001. Arrivals time: 0.38590124528855085 Scheduler time: 86.3507613837719 Scheduler overhead time: 0.08985199825838208 Adapter cache time: 0.018485291860997677 Engine time: 0.08552830433472991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_384_slots_16_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 8640, 540, 135, 8640, 8640, 135, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 135, 8640, 135, 8640, 540, 540, 540, 135, 135, 540, 135, 8640, 135, 8640, 540, 8640, 540, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 8640, 8640, 135, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 540, 135, 135, 135, 540, 135, 540, 135, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 8640, 8640, 540, 135, 135, 8640, 540, 540, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 135, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 8640, 135, 135, 8640, 135, 8640, 540, 135, 8640, 540, 540, 135, 8640, 8640, 8640, 135, 540, 8640, 135, 8640, 540, 540, 8640, 8640, 135, 540, 8640, 135, 540, 540, 135, 540, 135, 540, 8640, 8640, 540, 135, 8640, 135, 8640, 540, 540, 540, 540, 540, 135, 135, 8640, 135, 8640, 135, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 135, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 135, 135, 135, 135, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135, 540, 8640, 8640, 540, 540, 135, 8640, 540, 8640, 135, 540, 540, 135, 135, 540, 135, 8640, 8640, 135, 8640, 8640, 135, 540, 8640, 135, 135, 540, 540, 135, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 135, 135, 540, 135, 8640, 8640, 8640, 540, 135, 8640, 8640, 540, 540, 8640, 8640, 135, 8640, 8640, 540, 135, 8640, 540, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 8640, 135, 540, 8640, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 8640, 135, 135, 135, 540, 135, 540, 540, 135, 8640, 540, 8640, 135, 8640, 135, 540, 540, 540, 540, 135, 540, 8640, 8640, 135, 8640, 8640, 135, 8640, 540, 8640, 540, 540, 540, 135, 540, 135, 135, 8640, 135, 540, 135, 135, 540, 8640, 540, 8640, 135, 8640, 8640, 540, 8640, 540, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1192320 . Total input tokens: 265793457 . Total output tokens: 234171413
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 85.77342645591125,
    "estimated_duration": 3600.0290390049613,
    "input_throughput": 6571.975876766642,
    "output_throughput": 5716.690831385163,
    "total_throughput": 12288.666708151804,
    "itl": 80.58304542624502,
    "ttft": 1887995.7543804212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.232961594909465,
    "arrivals": 397780,
    "finished_requests": 95549,
    "scheduler_time": 321.7099575142806
}
#Debug simulation 
Total elapsed time: 85.77359174191952. Arrivals time: 0.4373452067375183 Scheduler time: 85.10256004240364 Scheduler overhead time: 0.09179387055337429 Adapter cache time: 0.01904453570023179 Engine time: 0.08761731209233403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_384_slots_16_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_384_slots_16_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 8640, 540, 66, 8640, 8640, 66, 540, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 8640, 8640, 540, 540, 66, 8640, 66, 8640, 540, 540, 540, 66, 66, 540, 66, 8640, 66, 8640, 540, 8640, 540, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 8640, 8640, 66, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 540, 66, 66, 66, 540, 66, 540, 66, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 8640, 8640, 540, 66, 66, 8640, 540, 540, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 8640, 8640, 66, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 8640, 66, 66, 8640, 66, 8640, 540, 66, 8640, 540, 540, 66, 8640, 8640, 8640, 66, 540, 8640, 66, 8640, 540, 540, 8640, 8640, 66, 540, 8640, 66, 540, 540, 66, 540, 66, 540, 8640, 8640, 540, 66, 8640, 66, 8640, 540, 540, 540, 540, 540, 66, 66, 8640, 66, 8640, 66, 540, 8640, 540, 8640, 540, 8640, 540, 8640, 66, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 66, 66, 66, 66, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66, 540, 8640, 8640, 540, 540, 66, 8640, 540, 8640, 66, 540, 540, 66, 66, 540, 66, 8640, 8640, 66, 8640, 8640, 66, 540, 8640, 66, 66, 540, 540, 66, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 66, 66, 540, 66, 8640, 8640, 8640, 540, 66, 8640, 8640, 540, 540, 8640, 8640, 66, 8640, 8640, 540, 66, 8640, 540, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 8640, 66, 540, 8640, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 8640, 66, 66, 66, 540, 66, 540, 540, 66, 8640, 540, 8640, 66, 8640, 66, 540, 540, 540, 540, 66, 540, 8640, 8640, 66, 8640, 8640, 66, 8640, 540, 8640, 540, 540, 540, 66, 540, 66, 66, 8640, 66, 540, 66, 66, 540, 8640, 540, 8640, 66, 8640, 8640, 540, 8640, 540, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1183488 . Total input tokens: 263821695 . Total output tokens: 232444508
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 87.80951388087124,
    "estimated_duration": 3600.056086997076,
    "input_throughput": 6639.0310101908735,
    "output_throughput": 5832.903847205272,
    "total_throughput": 12471.934857396145,
    "itl": 84.07915860503662,
    "ttft": 1868857.3738218925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.643437376604466,
    "arrivals": 394936,
    "finished_requests": 96825,
    "scheduler_time": 315.8631816233852
}
#Debug simulation 
Total elapsed time: 87.80968337878585. Arrivals time: 0.4486889452673495 Scheduler time: 87.12977626919746 Scheduler overhead time: 0.09151208447292447 Adapter cache time: 0.018798534758388996 Engine time: 0.08623112738132477 
