INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.150080513674766,
    "estimated_duration": 3600.057092371748,
    "input_throughput": 5703.778154938114,
    "output_throughput": 4970.477006577494,
    "total_throughput": 10674.255161515608,
    "itl": 88.21309029808451,
    "ttft": 1983426.558287303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 507129,
    "finished_requests": 83514,
    "scheduler_time": 28.21543915902307
}
#Debug simulation 
Total elapsed time: 6.150190813001245. Arrivals time: 0.30851510539650917 Scheduler time: 5.660977026447654 Scheduler overhead time: 0.05858884984627366 Adapter cache time: 0.03329500602558255 Engine time: 0.06134373275563121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.169245983939618,
    "estimated_duration": 3600.0636458701333,
    "input_throughput": 5703.836937315285,
    "output_throughput": 4970.674621413492,
    "total_throughput": 10674.511558728776,
    "itl": 88.21219712035078,
    "ttft": 1983356.5841112928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454804,
    "arrivals": 507129,
    "finished_requests": 83515,
    "scheduler_time": 28.215610782231384
}
#Debug simulation 
Total elapsed time: 6.169354988727719. Arrivals time: 0.3073023450560868 Scheduler time: 5.683163166977465 Scheduler overhead time: 0.058344872668385506 Adapter cache time: 0.03224644297733903 Engine time: 0.06088556069880724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.178077456075698,
    "estimated_duration": 3600.037991662343,
    "input_throughput": 5703.768140101872,
    "output_throughput": 4970.576155430293,
    "total_throughput": 10674.344295532164,
    "itl": 88.21080861141417,
    "ttft": 1983357.5451069353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 507129,
    "finished_requests": 83514,
    "scheduler_time": 28.215704228498105
}
#Debug simulation 
Total elapsed time: 6.178222785703838. Arrivals time: 0.3087418442592025 Scheduler time: 5.690076988656074 Scheduler overhead time: 0.05834784545004368 Adapter cache time: 0.032576202880591154 Engine time: 0.06100029544904828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.432838985230774,
    "estimated_duration": 3600.1125664454894,
    "input_throughput": 5971.994931599464,
    "output_throughput": 5229.433428129742,
    "total_throughput": 11201.428359729205,
    "itl": 99.63618815761227,
    "ttft": 1964050.2353689221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 505238,
    "finished_requests": 87235,
    "scheduler_time": 37.249203576388496
}
#Debug simulation 
Total elapsed time: 6.4329314939677715. Arrivals time: 0.30908977426588535 Scheduler time: 5.963172625284642 Scheduler overhead time: 0.05305400816723704 Adapter cache time: 0.02751190960407257 Engine time: 0.05529451556503773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2247460298240185,
    "estimated_duration": 3600.0030879873934,
    "input_throughput": 5711.252878811975,
    "output_throughput": 5010.989035035842,
    "total_throughput": 10722.241913847816,
    "itl": 87.0140370477559,
    "ttft": 1989591.3346883042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 505238,
    "finished_requests": 83374,
    "scheduler_time": 28.21638185958099
}
#Debug simulation 
Total elapsed time: 6.224847032222897. Arrivals time: 0.30460481252521276 Scheduler time: 5.743412118870765 Scheduler overhead time: 0.059091296046972275 Adapter cache time: 0.028186774346977472 Engine time: 0.06167884869500995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.189502164721489,
    "estimated_duration": 3600.0048872367765,
    "input_throughput": 5711.342524254659,
    "output_throughput": 5011.119585964463,
    "total_throughput": 10722.462110219121,
    "itl": 87.01239884069363,
    "ttft": 1989454.5289074597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454801,
    "arrivals": 505238,
    "finished_requests": 83377,
    "scheduler_time": 28.216369581496952
}
#Debug simulation 
Total elapsed time: 6.189596786629409. Arrivals time: 0.30198515858501196 Scheduler time: 5.710335368756205 Scheduler overhead time: 0.05914980359375477 Adapter cache time: 0.0285484716296196 Engine time: 0.06173771247267723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.18481419282034,
    "estimated_duration": 3600.032172722844,
    "input_throughput": 5711.206737480147,
    "output_throughput": 5010.9485511503,
    "total_throughput": 10722.155288630447,
    "itl": 87.01071125005984,
    "ttft": 1989479.7077815738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 505238,
    "finished_requests": 83374,
    "scheduler_time": 28.217539323876036
}
#Debug simulation 
Total elapsed time: 6.184913158882409. Arrivals time: 0.2999116647988558 Scheduler time: 5.708008463028818 Scheduler overhead time: 0.05912378570064902 Adapter cache time: 0.028691024519503117 Engine time: 0.06141982600092888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.45219903299585,
    "estimated_duration": 3600.0513659292433,
    "input_throughput": 6104.903448878169,
    "output_throughput": 5274.0503037514645,
    "total_throughput": 11378.953752629634,
    "itl": 98.8899825428237,
    "ttft": 1947927.8976612617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 504263,
    "finished_requests": 88735,
    "scheduler_time": 37.6081709980166
}
#Debug simulation 
Total elapsed time: 6.452298288233578. Arrivals time: 0.2711477540433407 Scheduler time: 6.021745131816715 Scheduler overhead time: 0.053468658588826656 Adapter cache time: 0.025606172624975443 Engine time: 0.0554281803779304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.243807573337108,
    "estimated_duration": 3600.052218312369,
    "input_throughput": 5846.43686359267,
    "output_throughput": 5050.478409039118,
    "total_throughput": 10896.915272631788,
    "itl": 86.48559766379563,
    "ttft": 1976414.1965676383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979342,
    "arrivals": 504263,
    "finished_requests": 84905,
    "scheduler_time": 28.52591227614273
}
#Debug simulation 
Total elapsed time: 6.243901581969112. Arrivals time: 0.26190323242917657 Scheduler time: 5.805327735375613 Scheduler overhead time: 0.059740726836025715 Adapter cache time: 0.02657031686976552 Engine time: 0.06227346742525697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.248828728683293,
    "estimated_duration": 3600.0372249670363,
    "input_throughput": 5846.461212687244,
    "output_throughput": 5050.4994431457535,
    "total_throughput": 10896.960655832998,
    "itl": 86.48417649324851,
    "ttft": 1976466.6212718366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454801,
    "arrivals": 504263,
    "finished_requests": 84905,
    "scheduler_time": 28.525668325884574
}
#Debug simulation 
Total elapsed time: 6.248924965038896. Arrivals time: 0.26716645155102015 Scheduler time: 5.805374436546117 Scheduler overhead time: 0.05970765044912696 Adapter cache time: 0.026515203062444925 Engine time: 0.06211969116702676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.238257083110511,
    "estimated_duration": 3600.0566785057395,
    "input_throughput": 5846.694616134901,
    "output_throughput": 5050.599649877432,
    "total_throughput": 10897.294266012334,
    "itl": 86.48254895048997,
    "ttft": 1976572.3290188813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 504263,
    "finished_requests": 84910,
    "scheduler_time": 28.526277072515253
}
#Debug simulation 
Total elapsed time: 6.238383484072983. Arrivals time: 0.31123305251821876 Scheduler time: 5.751238587778062 Scheduler overhead time: 0.05939276982098818 Adapter cache time: 0.026282834820449352 Engine time: 0.06228930130600929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.421923346817493,
    "estimated_duration": 3600.0240539516844,
    "input_throughput": 6022.5044819356035,
    "output_throughput": 5283.015811831365,
    "total_throughput": 11305.52029376697,
    "itl": 98.47657122866684,
    "ttft": 1951310.3936023402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8067139018978915,
    "arrivals": 503772,
    "finished_requests": 88097,
    "scheduler_time": 37.55694331506439
}
#Debug simulation 
Total elapsed time: 6.422050565015525. Arrivals time: 0.2678335518576205 Scheduler time: 5.996184474788606 Scheduler overhead time: 0.0536886896006763 Adapter cache time: 0.023502216208726168 Engine time: 0.05575825413689017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.227010924834758,
    "estimated_duration": 3600.091098070414,
    "input_throughput": 5773.266685151389,
    "output_throughput": 5056.420102745955,
    "total_throughput": 10829.686787897343,
    "itl": 86.10786709000833,
    "ttft": 1980750.8783577161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8771281907334927,
    "arrivals": 503772,
    "finished_requests": 84344,
    "scheduler_time": 28.41510032758292
}
#Debug simulation 
Total elapsed time: 6.227134445682168. Arrivals time: 0.2662753430195153 Scheduler time: 5.785729991272092 Scheduler overhead time: 0.0596617478877306 Adapter cache time: 0.025005312636494637 Engine time: 0.06241007288917899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.195876910816878,
    "estimated_duration": 3600.034391180766,
    "input_throughput": 5773.565677849752,
    "output_throughput": 5056.561694131312,
    "total_throughput": 10830.127371981065,
    "itl": 86.10545608286428,
    "ttft": 1980701.7323453354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8215993518009775,
    "arrivals": 503772,
    "finished_requests": 84346,
    "scheduler_time": 28.414293154068275
}
#Debug simulation 
Total elapsed time: 6.195969101041555. Arrivals time: 0.26311762165278196 Scheduler time: 5.7575750863179564 Scheduler overhead time: 0.05979426112025976 Adapter cache time: 0.02513602189719677 Engine time: 0.06226109014824033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.198299578856677,
    "estimated_duration": 3600.038935585781,
    "input_throughput": 5773.014228974798,
    "output_throughput": 5056.432534677027,
    "total_throughput": 10829.446763651826,
    "itl": 86.10474326096829,
    "ttft": 1980631.5425115705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7660705128684632,
    "arrivals": 503772,
    "finished_requests": 84339,
    "scheduler_time": 28.414396946206253
}
#Debug simulation 
Total elapsed time: 6.198420676868409. Arrivals time: 0.2635224717669189 Scheduler time: 5.759438570123166 Scheduler overhead time: 0.060049745719879866 Adapter cache time: 0.024910839274525642 Engine time: 0.062419950030744076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.563058423809707,
    "estimated_duration": 3600.0799843186883,
    "input_throughput": 6146.046225744446,
    "output_throughput": 5368.843771302447,
    "total_throughput": 11514.889997046894,
    "itl": 97.38552198608289,
    "ttft": 1942563.4482202607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 501405,
    "finished_requests": 89905,
    "scheduler_time": 38.3369895799059
}
#Debug simulation 
Total elapsed time: 6.56315331812948. Arrivals time: 0.27433528611436486 Scheduler time: 6.130505511071533 Scheduler overhead time: 0.054145831149071455 Adapter cache time: 0.022306465078145266 Engine time: 0.05646859481930733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.309424831066281,
    "estimated_duration": 3600.0866332612036,
    "input_throughput": 5860.323972506265,
    "output_throughput": 5122.609225460257,
    "total_throughput": 10982.933197966522,
    "itl": 85.13551036194376,
    "ttft": 1973177.1303152319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979344,
    "arrivals": 501405,
    "finished_requests": 85767,
    "scheduler_time": 28.849368697205456
}
#Debug simulation 
Total elapsed time: 6.309520564042032. Arrivals time: 0.31528577394783497 Scheduler time: 5.819899907335639 Scheduler overhead time: 0.06031786696985364 Adapter cache time: 0.022737011313438416 Engine time: 0.06301953829824924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.321200985927135,
    "estimated_duration": 3600.050532192171,
    "input_throughput": 5857.520557401541,
    "output_throughput": 5120.768676759212,
    "total_throughput": 10978.289234160753,
    "itl": 85.0468492070468,
    "ttft": 1973250.847778136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454804,
    "arrivals": 501405,
    "finished_requests": 85724,
    "scheduler_time": 28.76801774692319
}
#Debug simulation 
Total elapsed time: 6.321324985008687. Arrivals time: 0.31220981711521745 Scheduler time: 5.834695323370397 Scheduler overhead time: 0.06042517954483628 Adapter cache time: 0.0229011089541018 Engine time: 0.06277513178065419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.345623228698969,
    "estimated_duration": 3600.058172092908,
    "input_throughput": 5866.74881081753,
    "output_throughput": 5128.4382411150455,
    "total_throughput": 10995.187051932577,
    "itl": 85.38752502779838,
    "ttft": 1972507.3736971172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 501405,
    "finished_requests": 85865,
    "scheduler_time": 29.079392212104644
}
#Debug simulation 
Total elapsed time: 6.345725351013243. Arrivals time: 0.3148619616404176 Scheduler time: 5.854786086361855 Scheduler overhead time: 0.060532300267368555 Adapter cache time: 0.023520113434642553 Engine time: 0.06369974464178085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.607487205881625,
    "estimated_duration": 3600.022295751545,
    "input_throughput": 6154.61132730972,
    "output_throughput": 5405.21652406537,
    "total_throughput": 11559.82785137509,
    "itl": 96.69755422935627,
    "ttft": 1936907.8948133758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 500396,
    "finished_requests": 90157,
    "scheduler_time": 38.56699837028686
}
#Debug simulation 
Total elapsed time: 6.607585948891938. Arrivals time: 0.3218379239551723 Scheduler time: 6.129013560246676 Scheduler overhead time: 0.0544951930642128 Adapter cache time: 0.019954339135438204 Engine time: 0.056822077836841345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.383410859853029,
    "estimated_duration": 3600.051392276142,
    "input_throughput": 5872.150893555481,
    "output_throughput": 5156.398611371852,
    "total_throughput": 11028.549504927332,
    "itl": 84.89068763187544,
    "ttft": 1966738.978521645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979347,
    "arrivals": 500396,
    "finished_requests": 86013,
    "scheduler_time": 29.20263122241195
}
#Debug simulation 
Total elapsed time: 6.383505149744451. Arrivals time: 0.3160560820251703 Scheduler time: 5.8939303695224226 Scheduler overhead time: 0.060572159476578236 Adapter cache time: 0.020956146996468306 Engine time: 0.06340041942894459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.407337591983378,
    "estimated_duration": 3600.004679245047,
    "input_throughput": 5872.038200915093,
    "output_throughput": 5156.239686858607,
    "total_throughput": 11028.277887773698,
    "itl": 84.88910950082392,
    "ttft": 1966792.7709467197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454806,
    "arrivals": 500396,
    "finished_requests": 86010,
    "scheduler_time": 29.203617512044605
}
#Debug simulation 
Total elapsed time: 6.4074478931725025. Arrivals time: 0.32377982372418046 Scheduler time: 5.909834285732359 Scheduler overhead time: 0.06068558758124709 Adapter cache time: 0.021119221579283476 Engine time: 0.06349633261561394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.33762500109151,
    "estimated_duration": 3600.0802470668655,
    "input_throughput": 5872.814367742429,
    "output_throughput": 5156.826994377602,
    "total_throughput": 11029.641362120032,
    "itl": 84.88692509058905,
    "ttft": 1966654.2310142233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 500396,
    "finished_requests": 86019,
    "scheduler_time": 29.20296518154226
}
#Debug simulation 
Total elapsed time: 6.3377217031084. Arrivals time: 0.3141330215148628 Scheduler time: 5.850214794743806 Scheduler overhead time: 0.060731046833097935 Adapter cache time: 0.020720850210636854 Engine time: 0.06342784641310573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.90303443884477,
    "estimated_duration": 3600.0684061756524,
    "input_throughput": 6213.875814588762,
    "output_throughput": 5435.320330700759,
    "total_throughput": 11649.196145289521,
    "itl": 96.12365364828797,
    "ttft": 1935762.8616857617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8001014928659416,
    "arrivals": 499947,
    "finished_requests": 90770,
    "scheduler_time": 38.7444416195664
}
#Debug simulation 
Total elapsed time: 6.903100069146603. Arrivals time: 0.5700611183419824 Scheduler time: 6.177664214745164 Scheduler overhead time: 0.05463803606107831 Adapter cache time: 0.018206057138741016 Engine time: 0.056857440154999495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.344945766963065,
    "estimated_duration": 3600.0560039099614,
    "input_throughput": 5916.794898986449,
    "output_throughput": 5181.1574541456785,
    "total_throughput": 11097.952353132128,
    "itl": 84.49492189143656,
    "ttft": 1966606.186675861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8835121116740636,
    "arrivals": 499947,
    "finished_requests": 86427,
    "scheduler_time": 29.324901383548255
}
#Debug simulation 
Total elapsed time: 6.345039261970669. Arrivals time: 0.27327633230015635 Scheduler time: 5.899210379458964 Scheduler overhead time: 0.060905168298631907 Adapter cache time: 0.019324200693517923 Engine time: 0.06368834432214499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.3337165070697665,
    "estimated_duration": 3600.024384646276,
    "input_throughput": 5916.70992308928,
    "output_throughput": 5181.135183292014,
    "total_throughput": 11097.845106381294,
    "itl": 84.49441421963205,
    "ttft": 1966631.687486388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8293714937148616,
    "arrivals": 499947,
    "finished_requests": 86424,
    "scheduler_time": 29.325899795775978
}
#Debug simulation 
Total elapsed time: 6.3338127001188695. Arrivals time: 0.2694691554643214 Scheduler time: 5.891411507036537 Scheduler overhead time: 0.061049255076795816 Adapter cache time: 0.01960100932046771 Engine time: 0.06362460181117058 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.611881936900318,
    "estimated_duration": 3600.051902608184,
    "input_throughput": 5916.767751200471,
    "output_throughput": 5181.151412424528,
    "total_throughput": 11097.919163624998,
    "itl": 84.494523575465,
    "ttft": 1966599.8659843984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7724544338090337,
    "arrivals": 499947,
    "finished_requests": 86426,
    "scheduler_time": 29.32691567163373
}
#Debug simulation 
Total elapsed time: 6.6120028141886. Arrivals time: 0.5521010812371969 Scheduler time: 5.887869225349277 Scheduler overhead time: 0.06072760373353958 Adapter cache time: 0.019441937562078238 Engine time: 0.06325758434832096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.66231890488416,
    "estimated_duration": 3600.0221478352805,
    "input_throughput": 6310.544231973839,
    "output_throughput": 5478.079908996811,
    "total_throughput": 11788.62414097065,
    "itl": 94.6597850915481,
    "ttft": 1923330.2443925207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8397759470576411,
    "arrivals": 498492,
    "finished_requests": 91469,
    "scheduler_time": 38.81335619443902
}
#Debug simulation 
Total elapsed time: 6.662449819967151. Arrivals time: 0.28280597599223256 Scheduler time: 6.22403912525624 Scheduler overhead time: 0.05580695532262325 Adapter cache time: 0.015985659789294004 Engine time: 0.05774628929793835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.388813802972436,
    "estimated_duration": 3600.0287974703115,
    "input_throughput": 5998.653681652415,
    "output_throughput": 5208.394725390979,
    "total_throughput": 11207.048407043394,
    "itl": 82.99150814380856,
    "ttft": 1956533.9366764936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9287567421840508,
    "arrivals": 498492,
    "finished_requests": 86886,
    "scheduler_time": 28.93628616886142
}
#Debug simulation 
Total elapsed time: 6.388907855376601. Arrivals time: 0.2697230027988553 Scheduler time: 5.945849768817425 Scheduler overhead time: 0.06266086548566818 Adapter cache time: 0.016953987535089254 Engine time: 0.06452632742002606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.63909225165844,
    "estimated_duration": 3600.0615438395007,
    "input_throughput": 6007.587019452413,
    "output_throughput": 5214.189194104753,
    "total_throughput": 11221.776213557167,
    "itl": 83.20924900028226,
    "ttft": 1955895.2170415292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8690632403315968,
    "arrivals": 498492,
    "finished_requests": 87000,
    "scheduler_time": 29.14982657690246
}
#Debug simulation 
Total elapsed time: 6.639197373762727. Arrivals time: 0.27210100181400776 Scheduler time: 6.195920162834227 Scheduler overhead time: 0.061470836866647005 Adapter cache time: 0.01667553326115012 Engine time: 0.06408084137365222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.361106788739562,
    "estimated_duration": 3600.042344620402,
    "input_throughput": 6007.448226912568,
    "output_throughput": 5214.198390763456,
    "total_throughput": 11221.646617676024,
    "itl": 83.20713057303398,
    "ttft": 1955948.459433403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8107579594524567,
    "arrivals": 498492,
    "finished_requests": 86998,
    "scheduler_time": 29.149351458118677
}
#Debug simulation 
Total elapsed time: 6.361207190901041. Arrivals time: 0.27087527234107256 Scheduler time: 5.918389735277742 Scheduler overhead time: 0.06175387650728226 Adapter cache time: 0.016807018779218197 Engine time: 0.06440588505938649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.755196330603212,
    "estimated_duration": 3600.027707684044,
    "input_throughput": 6276.590025062976,
    "output_throughput": 5490.678573892469,
    "total_throughput": 11767.268598955445,
    "itl": 95.00689173226309,
    "ttft": 1921616.7345435354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7934890838339916,
    "arrivals": 497997,
    "finished_requests": 91537,
    "scheduler_time": 39.100713546269844
}
#Debug simulation 
Total elapsed time: 6.755304483696818. Arrivals time: 0.32923605805262923 Scheduler time: 6.270730073098093 Scheduler overhead time: 0.05575226014479995 Adapter cache time: 0.015451207756996155 Engine time: 0.058029184117913246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.424091960769147,
    "estimated_duration": 3600.0294884421605,
    "input_throughput": 5966.626959296116,
    "output_throughput": 5224.168596501807,
    "total_throughput": 11190.795555797922,
    "itl": 83.6341648832799,
    "ttft": 1952810.8708438165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8693560488196096,
    "arrivals": 497997,
    "finished_requests": 87009,
    "scheduler_time": 29.495274943820856
}
#Debug simulation 
Total elapsed time: 6.42418599082157. Arrivals time: 0.31464138021692634 Scheduler time: 5.940165430307388 Scheduler overhead time: 0.06105264648795128 Adapter cache time: 0.01576721156015992 Engine time: 0.06373683549463749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.472826198209077,
    "estimated_duration": 3600.0740401907287,
    "input_throughput": 5966.553398680102,
    "output_throughput": 5224.229499180964,
    "total_throughput": 11190.782897861067,
    "itl": 83.63398219386706,
    "ttft": 1952796.4346094036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8152154308604074,
    "arrivals": 497997,
    "finished_requests": 87010,
    "scheduler_time": 29.496567511398315
}
#Debug simulation 
Total elapsed time: 6.472922849003226. Arrivals time: 0.29767798399552703 Scheduler time: 6.00490420171991 Scheduler overhead time: 0.06143273925408721 Adapter cache time: 0.016002747230231762 Engine time: 0.0640587704256177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.458228474017233,
    "estimated_duration": 3600.0527788428544,
    "input_throughput": 5966.633635550272,
    "output_throughput": 5224.436738965098,
    "total_throughput": 11191.07037451537,
    "itl": 83.63123881212972,
    "ttft": 1952680.7134465352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7596865919278927,
    "arrivals": 497997,
    "finished_requests": 87014,
    "scheduler_time": 29.4952052388296
}
#Debug simulation 
Total elapsed time: 6.458326466847211. Arrivals time: 0.2711514378897846 Scheduler time: 6.0166480811312795 Scheduler overhead time: 0.06170409778133035 Adapter cache time: 0.0158996619284153 Engine time: 0.06412866152822971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.9994327439926565,
    "estimated_duration": 3600.017177915081,
    "input_throughput": 6315.480142560671,
    "output_throughput": 5532.321101738309,
    "total_throughput": 11847.80124429898,
    "itl": 94.39113895212506,
    "ttft": 1921998.0469538365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8133263109298414,
    "arrivals": 497033,
    "finished_requests": 92312,
    "scheduler_time": 39.40090985770768
}
#Debug simulation 
Total elapsed time: 6.999497157987207. Arrivals time: 0.5656887320801616 Scheduler time: 6.280105398967862 Scheduler overhead time: 0.05583647033199668 Adapter cache time: 0.013700854033231735 Engine time: 0.05805795732885599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.7064908389002085,
    "estimated_duration": 3600.044609568935,
    "input_throughput": 6000.939805739462,
    "output_throughput": 5258.231231269837,
    "total_throughput": 11259.171037009299,
    "itl": 83.0913685482621,
    "ttft": 1954098.741304648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8990563955018303,
    "arrivals": 497033,
    "finished_requests": 87649,
    "scheduler_time": 29.640856263784205
}
#Debug simulation 
Total elapsed time: 6.706558160949498. Arrivals time: 0.5519374711439013 Scheduler time: 5.982880151830614 Scheduler overhead time: 0.06226193392649293 Adapter cache time: 0.014613451436161995 Engine time: 0.06576007464900613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.724613258615136,
    "estimated_duration": 3600.0041116021657,
    "input_throughput": 6001.021479496413,
    "output_throughput": 5258.513994189576,
    "total_throughput": 11259.53547368599,
    "itl": 83.09001691335133,
    "ttft": 1954023.1205579222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8421393355960016,
    "arrivals": 497033,
    "finished_requests": 87648,
    "scheduler_time": 29.639631567625095
}
#Debug simulation 
Total elapsed time: 6.7247078088112175. Arrivals time: 0.27746058302000165 Scheduler time: 6.27714067324996 Scheduler overhead time: 0.06195496395230293 Adapter cache time: 0.014526024460792542 Engine time: 0.06462569953873754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.763793309684843,
    "estimated_duration": 3600.054939190318,
    "input_throughput": 6001.40619100086,
    "output_throughput": 5258.599193560583,
    "total_throughput": 11260.005384561444,
    "itl": 83.08779765069048,
    "ttft": 1954006.0516449271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7852222756901747,
    "arrivals": 497033,
    "finished_requests": 87653,
    "scheduler_time": 29.640102874922245
}
#Debug simulation 
Total elapsed time: 6.7638695780187845. Arrivals time: 0.27293334528803825 Scheduler time: 6.320585044566542 Scheduler overhead time: 0.062104553915560246 Adapter cache time: 0.014509616419672966 Engine time: 0.06466384371742606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.644108393229544,
    "estimated_duration": 3600.068636488237,
    "input_throughput": 4170.6954828079315,
    "output_throughput": 3656.088905249128,
    "total_throughput": 7826.78438805706,
    "itl": 142.5438281736243,
    "ttft": 2141176.8596695703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 432423,
    "finished_requests": 60772,
    "scheduler_time": 26.173657894444617
}
#Debug simulation 
Total elapsed time: 4.644202182069421. Arrivals time: 0.2103918343782425 Scheduler time: 4.302075586747378 Scheduler overhead time: 0.03787267114967108 Adapter cache time: 0.03701180126518011 Engine time: 0.039233030285686255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.523065991699696,
    "estimated_duration": 3600.1110498655953,
    "input_throughput": 4011.6865285417207,
    "output_throughput": 3513.3544006850043,
    "total_throughput": 7525.040929226725,
    "itl": 124.16030847887818,
    "ttft": 2166983.105451931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.936528884097934,
    "arrivals": 432423,
    "finished_requests": 58390,
    "scheduler_time": 19.950219722614694
}
#Debug simulation 
Total elapsed time: 4.523160655982792. Arrivals time: 0.2466307100839913 Scheduler time: 4.11729311523959 Scheduler overhead time: 0.04249966936185956 Adapter cache time: 0.0526434276252985 Engine time: 0.04420744674280286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 4.800256218761206,
    "estimated_duration": 3600.071100222407,
    "input_throughput": 4011.85604337307,
    "output_throughput": 3513.7467143964236,
    "total_throughput": 7525.602757769493,
    "itl": 124.17444344768592,
    "ttft": 2167033.2300588405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454802,
    "arrivals": 432423,
    "finished_requests": 58394,
    "scheduler_time": 19.956092516270875
}
#Debug simulation 
Total elapsed time: 4.800369217991829. Arrivals time: 0.5250047775916755 Scheduler time: 4.116869776044041 Scheduler overhead time: 0.04252599971368909 Adapter cache time: 0.05181344132870436 Engine time: 0.044317763298749924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.538118012715131,
    "estimated_duration": 3600.0937667786898,
    "input_throughput": 4012.0655004283703,
    "output_throughput": 3513.813478063679,
    "total_throughput": 7525.878978492049,
    "itl": 124.17325123200688,
    "ttft": 2167078.904003207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 432423,
    "finished_requests": 58396,
    "scheduler_time": 19.956717044093065
}
#Debug simulation 
Total elapsed time: 4.538239966612309. Arrivals time: 0.24908586917445064 Scheduler time: 4.126672693528235 Scheduler overhead time: 0.044447436928749084 Adapter cache time: 0.052824192214757204 Engine time: 0.044685741886496544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.987616040743887,
    "estimated_duration": 3600.0162997974035,
    "input_throughput": 4551.4213368761975,
    "output_throughput": 3973.1014553475597,
    "total_throughput": 8524.522792223757,
    "itl": 130.27553344952375,
    "ttft": 2067818.976307931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 386952,
    "finished_requests": 66455,
    "scheduler_time": 28.43013324150951
}
#Debug simulation 
Total elapsed time: 4.987717546988279. Arrivals time: 0.22263490967452526 Scheduler time: 4.622191859874874 Scheduler overhead time: 0.04131141630932689 Adapter cache time: 0.039636232890188694 Engine time: 0.04267020430415869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.86308402614668,
    "estimated_duration": 3600.0754117995207,
    "input_throughput": 4367.930446251352,
    "output_throughput": 3820.8227402448633,
    "total_throughput": 8188.753186496216,
    "itl": 113.55764437588341,
    "ttft": 2094040.0261785015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 386952,
    "finished_requests": 63853,
    "scheduler_time": 21.692746964795354
}
#Debug simulation 
Total elapsed time: 4.8631882960908115. Arrivals time: 0.2254168102517724 Scheduler time: 4.474007989279926 Scheduler overhead time: 0.04643507627770305 Adapter cache time: 0.0475211339071393 Engine time: 0.048110098112374544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 4.845656430814415,
    "estimated_duration": 3600.120093619154,
    "input_throughput": 4367.875401676389,
    "output_throughput": 3821.0266997429844,
    "total_throughput": 8188.902101419373,
    "itl": 113.55218712770055,
    "ttft": 2093895.8704926781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454802,
    "arrivals": 386952,
    "finished_requests": 63854,
    "scheduler_time": 21.69178710920661
}
#Debug simulation 
Total elapsed time: 4.8457507961429656. Arrivals time: 0.21715692663565278 Scheduler time: 4.464800353627652 Scheduler overhead time: 0.0462881438434124 Adapter cache time: 0.04740814492106438 Engine time: 0.04848931822925806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.117821882944554,
    "estimated_duration": 3600.038482130389,
    "input_throughput": 4368.085529656416,
    "output_throughput": 3821.249986155495,
    "total_throughput": 8189.335515811911,
    "itl": 113.55189510927792,
    "ttft": 2093913.0117004747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 386952,
    "finished_requests": 63856,
    "scheduler_time": 21.69277780089241
}
#Debug simulation 
Total elapsed time: 5.1178922560065985. Arrivals time: 0.48392203357070684 Scheduler time: 4.470840012654662 Scheduler overhead time: 0.04630920384079218 Adapter cache time: 0.04656273080036044 Engine time: 0.048679746221750975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.15398408100009,
    "estimated_duration": 3600.099990003459,
    "input_throughput": 4707.9464590048,
    "output_throughput": 4117.109258397503,
    "total_throughput": 8825.055717402303,
    "itl": 125.96100825335729,
    "ttft": 2045508.2394382202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 379429,
    "finished_requests": 68473,
    "scheduler_time": 29.47429943857467
}
#Debug simulation 
Total elapsed time: 5.154078253079206. Arrivals time: 0.2320510004647076 Scheduler time: 4.785260729957372 Scheduler overhead time: 0.04263788880780339 Adapter cache time: 0.03020810056477785 Engine time: 0.04405036801472306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.975620879326016,
    "estimated_duration": 3600.0679285936653,
    "input_throughput": 4496.858759641263,
    "output_throughput": 3936.3262808032046,
    "total_throughput": 8433.185040444467,
    "itl": 109.5789094538423,
    "ttft": 2076100.6534946724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 379429,
    "finished_requests": 65449,
    "scheduler_time": 22.134708266593258
}
#Debug simulation 
Total elapsed time: 4.975718056317419. Arrivals time: 0.22052076132968068 Scheduler time: 4.5999780730344355 Scheduler overhead time: 0.04747694777324796 Adapter cache time: 0.03620569175109267 Engine time: 0.04922748310491443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 4.981249140575528,
    "estimated_duration": 3600.112906682676,
    "input_throughput": 4491.8613441212765,
    "output_throughput": 3931.833352705364,
    "total_throughput": 8423.69469682664,
    "itl": 109.40472089665403,
    "ttft": 2076352.997678959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454803,
    "arrivals": 379429,
    "finished_requests": 65377,
    "scheduler_time": 22.027915384286395
}
#Debug simulation 
Total elapsed time: 4.981345192994922. Arrivals time: 0.22046661004424095 Scheduler time: 4.604004147928208 Scheduler overhead time: 0.047858186066150665 Adapter cache time: 0.036847399082034826 Engine time: 0.049779105465859175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.995807784143835,
    "estimated_duration": 3600.0800435646197,
    "input_throughput": 4503.772639439917,
    "output_throughput": 3941.5829171258524,
    "total_throughput": 8445.35555656577,
    "itl": 109.96482357196105,
    "ttft": 2075852.1595150551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 379429,
    "finished_requests": 65534,
    "scheduler_time": 22.338488918484217
}
#Debug simulation 
Total elapsed time: 4.9959014020860195. Arrivals time: 0.22563225170597434 Scheduler time: 4.614595849998295 Scheduler overhead time: 0.047530659940093756 Adapter cache time: 0.036465875804424286 Engine time: 0.0493775405921042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.2754341210238636,
    "estimated_duration": 3600.0404699079813,
    "input_throughput": 4818.792217756213,
    "output_throughput": 4226.898316059473,
    "total_throughput": 9045.690533815687,
    "itl": 122.98051546912349,
    "ttft": 2019277.7574894633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 375670,
    "finished_requests": 70639,
    "scheduler_time": 30.424157860375633
}
#Debug simulation 
Total elapsed time: 5.275545570999384. Arrivals time: 0.2411089395172894 Scheduler time: 4.9020864348858595 Scheduler overhead time: 0.0437005371786654 Adapter cache time: 0.023007880430668592 Engine time: 0.04524431098252535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.072430951055139,
    "estimated_duration": 3600.0102908530957,
    "input_throughput": 4594.797421004093,
    "output_throughput": 4034.058468360256,
    "total_throughput": 8628.85588936435,
    "itl": 107.67264151941058,
    "ttft": 2051203.3328795764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979342,
    "arrivals": 375670,
    "finished_requests": 67384,
    "scheduler_time": 23.00108874740264
}
#Debug simulation 
Total elapsed time: 5.07254145713523. Arrivals time: 0.23219300946220756 Scheduler time: 4.68908845866099 Scheduler overhead time: 0.048649626318365335 Adapter cache time: 0.02928601484745741 Engine time: 0.050480055157095194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.090509257744998,
    "estimated_duration": 3600.022971800059,
    "input_throughput": 4595.061234214463,
    "output_throughput": 4034.160646685616,
    "total_throughput": 8629.221880900079,
    "itl": 107.71013257952575,
    "ttft": 2051437.1433379948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454804,
    "arrivals": 375670,
    "finished_requests": 67388,
    "scheduler_time": 23.01710152824555
}
#Debug simulation 
Total elapsed time: 5.090604284778237. Arrivals time: 0.23373271664604545 Scheduler time: 4.70535871386528 Scheduler overhead time: 0.04884622525423765 Adapter cache time: 0.02942059375345707 Engine time: 0.050465946551412344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.340807575266808,
    "estimated_duration": 3600.0930859229375,
    "input_throughput": 4595.173403900733,
    "output_throughput": 4034.327350254214,
    "total_throughput": 8629.500754154948,
    "itl": 107.70717671507231,
    "ttft": 2051406.1999543575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 375670,
    "finished_requests": 67390,
    "scheduler_time": 23.017278539192038
}
#Debug simulation 
Total elapsed time: 5.340873318258673. Arrivals time: 0.23518186947330832 Scheduler time: 4.954289246816188 Scheduler overhead time: 0.04857523087412119 Adapter cache time: 0.029799566604197025 Engine time: 0.050267570186406374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.340586852747947,
    "estimated_duration": 3600.090148957747,
    "input_throughput": 4913.592512432281,
    "output_throughput": 4279.458114253133,
    "total_throughput": 9193.050626685414,
    "itl": 121.67935278544324,
    "ttft": 2011310.9092543935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 373765,
    "finished_requests": 71690,
    "scheduler_time": 30.87972274545899
}
#Debug simulation 
Total elapsed time: 5.340683072805405. Arrivals time: 0.2365140300244093 Scheduler time: 4.975085745565593 Scheduler overhead time: 0.04375910805538297 Adapter cache time: 0.019382426515221596 Engine time: 0.04544657934457064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.13555002771318,
    "estimated_duration": 3600.093923493872,
    "input_throughput": 4673.458625677058,
    "output_throughput": 4073.9181564924984,
    "total_throughput": 8747.376782169556,
    "itl": 106.67688865006863,
    "ttft": 2044875.3219373045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 373765,
    "finished_requests": 68210,
    "scheduler_time": 23.289927072716072
}
#Debug simulation 
Total elapsed time: 5.135642597917467. Arrivals time: 0.23139546252787113 Scheduler time: 4.756318905390799 Scheduler overhead time: 0.048950822558254004 Adapter cache time: 0.02515401178970933 Engine time: 0.05092219216749072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.160438785795122,
    "estimated_duration": 3600.0395885119374,
    "input_throughput": 4678.119666723258,
    "output_throughput": 4078.105153857065,
    "total_throughput": 8756.224820580323,
    "itl": 106.9372423782687,
    "ttft": 2044500.0181041746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454803,
    "arrivals": 373765,
    "finished_requests": 68281,
    "scheduler_time": 23.438409505157992
}
#Debug simulation 
Total elapsed time: 5.160567155107856. Arrivals time: 0.2618105844594538 Scheduler time: 4.750712724868208 Scheduler overhead time: 0.04943947959691286 Adapter cache time: 0.024493652395904064 Engine time: 0.05103092873468995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.13462457805872,
    "estimated_duration": 3600.0859533218354,
    "input_throughput": 4678.148582663689,
    "output_throughput": 4078.188462820708,
    "total_throughput": 8756.337045484397,
    "itl": 106.9349630808869,
    "ttft": 2044427.8472197237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 373765,
    "finished_requests": 68283,
    "scheduler_time": 23.43880837487714
}
#Debug simulation 
Total elapsed time: 5.13474785676226. Arrivals time: 0.23345935018733144 Scheduler time: 4.755051632877439 Scheduler overhead time: 0.04852149821817875 Adapter cache time: 0.024477548897266388 Engine time: 0.05043057072907686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.319155537057668,
    "estimated_duration": 3600.0595293632737,
    "input_throughput": 4925.449108652979,
    "output_throughput": 4302.4677435652,
    "total_throughput": 9227.916852218179,
    "itl": 120.81659892089974,
    "ttft": 2008909.5109143981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 372804,
    "finished_requests": 71563,
    "scheduler_time": 30.923935576462977
}
#Debug simulation 
Total elapsed time: 5.31924984883517. Arrivals time: 0.23524606367573142 Scheduler time: 4.9569771704263985 Scheduler overhead time: 0.04420045204460621 Adapter cache time: 0.016029775142669678 Engine time: 0.04590232577174902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.135511955711991,
    "estimated_duration": 3600.0809199423043,
    "input_throughput": 4692.688963299955,
    "output_throughput": 4097.32317912354,
    "total_throughput": 8790.012142423495,
    "itl": 106.21768742067631,
    "ttft": 2044134.4971775687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979342,
    "arrivals": 372804,
    "finished_requests": 68165,
    "scheduler_time": 23.406163424126156
}
#Debug simulation 
Total elapsed time: 5.135606181807816. Arrivals time: 0.22932225465774536 Scheduler time: 4.76173276361078 Scheduler overhead time: 0.04906965512782335 Adapter cache time: 0.02149791643023491 Engine time: 0.05091411853209138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.192251020111144,
    "estimated_duration": 3600.0982885152252,
    "input_throughput": 4692.789375749998,
    "output_throughput": 4097.3045227838975,
    "total_throughput": 8790.093898533894,
    "itl": 106.21288517364897,
    "ttft": 2044104.8493541055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454802,
    "arrivals": 372804,
    "finished_requests": 68167,
    "scheduler_time": 23.406250357703446
}
#Debug simulation 
Total elapsed time: 5.192359020933509. Arrivals time: 0.23668736685067415 Scheduler time: 4.810816141311079 Scheduler overhead time: 0.04940915061160922 Adapter cache time: 0.021205757278949022 Engine time: 0.050969659350812435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.10919273365289,
    "estimated_duration": 3600.0477455277787,
    "input_throughput": 4692.760261579047,
    "output_throughput": 4097.343991707951,
    "total_throughput": 8790.104253287,
    "itl": 106.21487898736311,
    "ttft": 2044179.783457567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 372804,
    "finished_requests": 68167,
    "scheduler_time": 23.40765503357143
}
#Debug simulation 
Total elapsed time: 5.109321681782603. Arrivals time: 0.2289169980213046 Scheduler time: 4.736033989582211 Scheduler overhead time: 0.04924384457990527 Adapter cache time: 0.02123395213857293 Engine time: 0.050938323605805635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.391692659817636,
    "estimated_duration": 3600.043814835008,
    "input_throughput": 4944.947593871403,
    "output_throughput": 4321.459904429804,
    "total_throughput": 9266.407498301207,
    "itl": 120.05674848413621,
    "ttft": 2004894.1321042245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8067139018978915,
    "arrivals": 372307,
    "finished_requests": 71920,
    "scheduler_time": 30.976405790131896
}
#Debug simulation 
Total elapsed time: 5.391785807907581. Arrivals time: 0.26581332879140973 Scheduler time: 4.997859302442521 Scheduler overhead time: 0.04619763046503067 Adapter cache time: 0.013978892005980015 Engine time: 0.046456755604594946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.211752987001091,
    "estimated_duration": 3600.038844750732,
    "input_throughput": 4704.468126696747,
    "output_throughput": 4110.275927043382,
    "total_throughput": 8814.744053740129,
    "itl": 105.61520398535544,
    "ttft": 2039756.135682492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8940606955345727,
    "arrivals": 372307,
    "finished_requests": 68348,
    "scheduler_time": 23.372064894465822
}
#Debug simulation 
Total elapsed time: 5.211847153026611. Arrivals time: 0.2656183927319944 Scheduler time: 4.802977747283876 Scheduler overhead time: 0.0494408025406301 Adapter cache time: 0.01902940170839429 Engine time: 0.051453308667987585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.1724089863710105,
    "estimated_duration": 3600.073980296418,
    "input_throughput": 4704.556932078792,
    "output_throughput": 4110.178868819154,
    "total_throughput": 8814.735800897945,
    "itl": 105.61381404418778,
    "ttft": 2039908.6267712833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8357554146554314,
    "arrivals": 372307,
    "finished_requests": 68350,
    "scheduler_time": 23.37348369729429
}
#Debug simulation 
Total elapsed time: 5.172500296030194. Arrivals time: 0.2631035726517439 Scheduler time: 4.765893334522843 Scheduler overhead time: 0.04952939134091139 Adapter cache time: 0.019464661367237568 Engine time: 0.05127210123464465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.18646843591705,
    "estimated_duration": 3600.0433360492484,
    "input_throughput": 4704.797809069567,
    "output_throughput": 4110.428297299132,
    "total_throughput": 8815.2261063687,
    "itl": 105.60958077998677,
    "ttft": 2039850.3861394606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7788383547496042,
    "arrivals": 372307,
    "finished_requests": 68353,
    "scheduler_time": 23.372555925438594
}
#Debug simulation 
Total elapsed time: 5.186561779584736. Arrivals time: 0.2631397838704288 Scheduler time: 4.7805096488446 Scheduler overhead time: 0.04946079757064581 Adapter cache time: 0.0190645232796669 Engine time: 0.05119703011587262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.018008894752711,
    "estimated_duration": 3600.036596196054,
    "input_throughput": 4536.215831043362,
    "output_throughput": 3976.0551365296396,
    "total_throughput": 8512.270967573002,
    "itl": 130.36216060578312,
    "ttft": 2023036.5918529092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 324936,
    "finished_requests": 66373,
    "scheduler_time": 28.779052697262188
}
#Debug simulation 
Total elapsed time: 5.0181027189828455. Arrivals time: 0.22339479578658938 Scheduler time: 4.642268491908908 Scheduler overhead time: 0.04136522160843015 Adapter cache time: 0.04924459056928754 Engine time: 0.04266184428706765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.9250244200229645,
    "estimated_duration": 3600.0381194152847,
    "input_throughput": 4386.976047510611,
    "output_throughput": 3848.1014757282746,
    "total_throughput": 8235.077523238886,
    "itl": 112.72769345590608,
    "ttft": 2047406.209990803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979342,
    "arrivals": 324936,
    "finished_requests": 64173,
    "scheduler_time": 22.16965543890403
}
#Debug simulation 
Total elapsed time: 4.925115745980293. Arrivals time: 0.24757660320028663 Scheduler time: 4.500216040760279 Scheduler overhead time: 0.04656661581248045 Adapter cache time: 0.060601712204515934 Engine time: 0.0483094141818583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 4.896396363154054,
    "estimated_duration": 3600.118628819724,
    "input_throughput": 4387.284317122129,
    "output_throughput": 3848.397908082871,
    "total_throughput": 8235.682225205,
    "itl": 112.75291665397383,
    "ttft": 2047372.0241234452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.87683538224548,
    "arrivals": 324936,
    "finished_requests": 64179,
    "scheduler_time": 22.184450314582662
}
#Debug simulation 
Total elapsed time: 4.896494341082871. Arrivals time: 0.24249657476320863 Scheduler time: 4.472731550689787 Scheduler overhead time: 0.048424400854855776 Adapter cache time: 0.061406638007611036 Engine time: 0.04890388064086437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.913297419901937,
    "estimated_duration": 3600.1249236586996,
    "input_throughput": 4387.450806553578,
    "output_throughput": 3848.398123340639,
    "total_throughput": 8235.848929894217,
    "itl": 112.74929270117754,
    "ttft": 2047323.055147976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 324936,
    "finished_requests": 64180,
    "scheduler_time": 22.185094618392075
}
#Debug simulation 
Total elapsed time: 4.91339709982276. Arrivals time: 0.24391911923885345 Scheduler time: 4.49227446038276 Scheduler overhead time: 0.046594247221946716 Adapter cache time: 0.060277043376117945 Engine time: 0.048461838625371456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.185522550251335,
    "estimated_duration": 3600.00088240976,
    "input_throughput": 4750.042446810051,
    "output_throughput": 4144.540650782466,
    "total_throughput": 8894.583097592518,
    "itl": 124.99767978074091,
    "ttft": 1991403.2630369777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 317347,
    "finished_requests": 69003,
    "scheduler_time": 29.921956930202857
}
#Debug simulation 
Total elapsed time: 5.1856496669352055. Arrivals time: 0.22488197591155767 Scheduler time: 4.811519698239863 Scheduler overhead time: 0.04268731828778982 Adapter cache time: 0.04195987340062857 Engine time: 0.04448866983875632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.292227501049638,
    "estimated_duration": 3600.1095833572845,
    "input_throughput": 4574.061877484178,
    "output_throughput": 3992.8467917898424,
    "total_throughput": 8566.90866927402,
    "itl": 108.659080798993,
    "ttft": 2019487.3144876114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 317347,
    "finished_requests": 66430,
    "scheduler_time": 22.96517728824682
}
#Debug simulation 
Total elapsed time: 5.292294653132558. Arrivals time: 0.4702308871783316 Scheduler time: 4.6493666977621615 Scheduler overhead time: 0.04812057642266154 Adapter cache time: 0.05183501774445176 Engine time: 0.05014826264232397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.043743286747485,
    "estimated_duration": 3600.069018882363,
    "input_throughput": 4572.7387207455995,
    "output_throughput": 3991.5023641577436,
    "total_throughput": 8564.241084903342,
    "itl": 108.51210654053494,
    "ttft": 2019557.406855893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454803,
    "arrivals": 317347,
    "finished_requests": 66409,
    "scheduler_time": 22.893037463557892
}
#Debug simulation 
Total elapsed time: 5.043871846981347. Arrivals time: 0.22190035553649068 Scheduler time: 4.648003246169537 Scheduler overhead time: 0.048307287972420454 Adapter cache time: 0.052512756548821926 Engine time: 0.050378506537526846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.041765959002078,
    "estimated_duration": 3600.0184012501,
    "input_throughput": 4573.9605092802,
    "output_throughput": 3992.8457018465638,
    "total_throughput": 8566.806211126765,
    "itl": 108.65668294555304,
    "ttft": 2019430.9301727458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 317347,
    "finished_requests": 66429,
    "scheduler_time": 22.965713395950036
}
#Debug simulation 
Total elapsed time: 5.041859847959131. Arrivals time: 0.22358188079670072 Scheduler time: 4.644340765662491 Scheduler overhead time: 0.04831360373646021 Adapter cache time: 0.05293697863817215 Engine time: 0.05011910852044821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209178264 . Total output tokens: 184716690
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.31059126323089,
    "estimated_duration": 3600.081857819603,
    "input_throughput": 4875.818021157779,
    "output_throughput": 4261.529211252942,
    "total_throughput": 9137.347232410722,
    "itl": 122.47031415982107,
    "ttft": 1958707.206529683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 313505,
    "finished_requests": 71283,
    "scheduler_time": 31.094253698899024
}
#Debug simulation 
Total elapsed time: 5.310705114156008. Arrivals time: 0.23733312031254172 Scheduler time: 4.929407458752394 Scheduler overhead time: 0.04386972589418292 Adapter cache time: 0.03439738741144538 Engine time: 0.04529187548905611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209178264 . Total output tokens: 184716690
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.1596204889938235,
    "estimated_duration": 3600.0792542922527,
    "input_throughput": 4688.404006627406,
    "output_throughput": 4097.222577090125,
    "total_throughput": 8785.62658371753,
    "itl": 106.58637185318011,
    "ttft": 1989568.3039379471,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 313505,
    "finished_requests": 68468,
    "scheduler_time": 23.920696895086387
}
#Debug simulation 
Total elapsed time: 5.159717170055956. Arrivals time: 0.23204834992066026 Scheduler time: 4.7609547772444785 Scheduler overhead time: 0.04897284181788564 Adapter cache time: 0.04368001315742731 Engine time: 0.05098101496696472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209178264 . Total output tokens: 184716690
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.164325973019004,
    "estimated_duration": 3600.109604963337,
    "input_throughput": 4688.03837992368,
    "output_throughput": 4096.82749093696,
    "total_throughput": 8784.86587086064,
    "itl": 106.58572676464887,
    "ttft": 1989577.230794657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454802,
    "arrivals": 313505,
    "finished_requests": 68465,
    "scheduler_time": 23.92177480119337
}
#Debug simulation 
Total elapsed time: 5.164417963009328. Arrivals time: 0.2399840261787176 Scheduler time: 4.757933136075735 Scheduler overhead time: 0.04891120782122016 Adapter cache time: 0.043590088840574026 Engine time: 0.051018612924963236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209178264 . Total output tokens: 184716690
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.135380386840552,
    "estimated_duration": 3600.01949286392,
    "input_throughput": 4688.24628129256,
    "output_throughput": 4097.001704917579,
    "total_throughput": 8785.247986210139,
    "itl": 106.58295285218111,
    "ttft": 1989531.9660519194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 313505,
    "finished_requests": 68464,
    "scheduler_time": 23.920891593286054
}
#Debug simulation 
Total elapsed time: 5.135476545896381. Arrivals time: 0.2274175682105124 Scheduler time: 4.73807877721265 Scheduler overhead time: 0.050408108625561 Adapter cache time: 0.044662711676210165 Engine time: 0.05143067101016641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207911889 . Total output tokens: 183599983
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.386106594000012,
    "estimated_duration": 3600.090611110692,
    "input_throughput": 4977.653602577342,
    "output_throughput": 4321.814276557832,
    "total_throughput": 9299.467879135174,
    "itl": 120.26298518482537,
    "ttft": 1948358.7274488555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 311621,
    "finished_requests": 72493,
    "scheduler_time": 31.449409458721032
}
#Debug simulation 
Total elapsed time: 5.386205167043954. Arrivals time: 0.2362000192515552 Scheduler time: 5.010174061637372 Scheduler overhead time: 0.044226194731891155 Adapter cache time: 0.028790309093892574 Engine time: 0.04612860642373562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207911889 . Total output tokens: 183599983
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.227903491817415,
    "estimated_duration": 3600.113892320742,
    "input_throughput": 4769.614104883487,
    "output_throughput": 4146.2904914870705,
    "total_throughput": 8915.904596370558,
    "itl": 104.99402392705774,
    "ttft": 1980762.7141751822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979343,
    "arrivals": 311621,
    "finished_requests": 69475,
    "scheduler_time": 24.113978809986172
}
#Debug simulation 
Total elapsed time: 5.227997764013708. Arrivals time: 0.26209495309740305 Scheduler time: 4.80377106834203 Scheduler overhead time: 0.049670648761093616 Adapter cache time: 0.037632559426128864 Engine time: 0.051633195951581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207911889 . Total output tokens: 183599983
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.244265255983919,
    "estimated_duration": 3600.0112585704405,
    "input_throughput": 4770.001471278454,
    "output_throughput": 4146.66619846292,
    "total_throughput": 8916.667669741373,
    "itl": 104.98845137336274,
    "ttft": 1980635.084067192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454801,
    "arrivals": 311621,
    "finished_requests": 69479,
    "scheduler_time": 24.11333412852076
}
#Debug simulation 
Total elapsed time: 5.2443606979213655. Arrivals time: 0.2579619213938713 Scheduler time: 4.823313674889505 Scheduler overhead time: 0.05007148301228881 Adapter cache time: 0.037615200504660606 Engine time: 0.052031188271939754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207911889 . Total output tokens: 183599983
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.239465308841318,
    "estimated_duration": 3600.0007187171504,
    "input_throughput": 4769.962936596064,
    "output_throughput": 4146.677783253211,
    "total_throughput": 8916.640719849274,
    "itl": 104.98625893607374,
    "ttft": 1980661.2099467753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 311621,
    "finished_requests": 69478,
    "scheduler_time": 24.113047830462
}
#Debug simulation 
Total elapsed time: 5.239566328004003. Arrivals time: 0.2311113770119846 Scheduler time: 4.84596933145076 Scheduler overhead time: 0.049657996743917465 Adapter cache time: 0.03779200557619333 Engine time: 0.05174913117662072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207282110 . Total output tokens: 183031835
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.403556091710925,
    "estimated_duration": 3600.038175879,
    "input_throughput": 4960.948780949892,
    "output_throughput": 4353.163837262245,
    "total_throughput": 9314.112618212137,
    "itl": 119.579587869059,
    "ttft": 1947863.5314706713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 310685,
    "finished_requests": 72540,
    "scheduler_time": 31.676693916267745
}
#Debug simulation 
Total elapsed time: 5.403649717569351. Arrivals time: 0.23386945482343435 Scheduler time: 5.030595364049077 Scheduler overhead time: 0.04484595498070121 Adapter cache time: 0.026889140252023935 Engine time: 0.0464957351796329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207282110 . Total output tokens: 183031835
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.198222300037742,
    "estimated_duration": 3600.0398770885654,
    "input_throughput": 4743.732731597148,
    "output_throughput": 4170.374638222555,
    "total_throughput": 8914.107369819703,
    "itl": 104.57160108760587,
    "ttft": 1980668.123821054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979342,
    "arrivals": 310685,
    "finished_requests": 69393,
    "scheduler_time": 24.28091891423213
}
#Debug simulation 
Total elapsed time: 5.19834541529417. Arrivals time: 0.23061508405953646 Scheduler time: 4.807012937031686 Scheduler overhead time: 0.04988614935427904 Adapter cache time: 0.03587260050699115 Engine time: 0.051633286755532026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207282110 . Total output tokens: 183031835
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.219256810378283,
    "estimated_duration": 3600.049490371595,
    "input_throughput": 4743.810063077,
    "output_throughput": 4170.364057536974,
    "total_throughput": 8914.174120613974,
    "itl": 104.57007861589689,
    "ttft": 1980709.6381335314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454801,
    "arrivals": 310685,
    "finished_requests": 69394,
    "scheduler_time": 24.280880084569585
}
#Debug simulation 
Total elapsed time: 5.21937806205824. Arrivals time: 0.23361207032576203 Scheduler time: 4.823833521921188 Scheduler overhead time: 0.04986092308536172 Adapter cache time: 0.03671707399189472 Engine time: 0.051972873508930206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207282110 . Total output tokens: 183031835
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.478086524177343,
    "estimated_duration": 3600.0235939501067,
    "input_throughput": 4743.950575407449,
    "output_throughput": 4170.577111003257,
    "total_throughput": 8914.527686410705,
    "itl": 104.57033245821926,
    "ttft": 1980612.8832630548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 310685,
    "finished_requests": 69397,
    "scheduler_time": 24.282102799178375
}
#Debug simulation 
Total elapsed time: 5.478152373805642. Arrivals time: 0.23338679037988186 Scheduler time: 5.083184437826276 Scheduler overhead time: 0.04987861914560199 Adapter cache time: 0.03643558965995908 Engine time: 0.05185507284477353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206969333 . Total output tokens: 182762594
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.4373958427459,
    "estimated_duration": 3600.088905374983,
    "input_throughput": 4956.7139781903015,
    "output_throughput": 4378.441314731409,
    "total_throughput": 9335.15529292171,
    "itl": 119.11693089537182,
    "ttft": 1947739.9690708835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8133263109298414,
    "arrivals": 310219,
    "finished_requests": 72792,
    "scheduler_time": 31.89869534152229
}
#Debug simulation 
Total elapsed time: 5.437520574778318. Arrivals time: 0.26176285138353705 Scheduler time: 5.037206292618066 Scheduler overhead time: 0.04492114204913378 Adapter cache time: 0.02588303480297327 Engine time: 0.046692294999957085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206969333 . Total output tokens: 182762594
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.226906148251146,
    "estimated_duration": 3600.080896741013,
    "input_throughput": 4725.274372417466,
    "output_throughput": 4183.404326728785,
    "total_throughput": 8908.678699146252,
    "itl": 103.76603514027015,
    "ttft": 1982570.9438587832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8990563955018299,
    "arrivals": 310219,
    "finished_requests": 69470,
    "scheduler_time": 24.18939039373307
}
#Debug simulation 
Total elapsed time: 5.227001389954239. Arrivals time: 0.25732785603031516 Scheduler time: 4.810263536404818 Scheduler overhead time: 0.05028380546718836 Adapter cache time: 0.03340394888073206 Engine time: 0.05222009588032961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206969333 . Total output tokens: 182762594
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.2982723959721625,
    "estimated_duration": 3600.087122768764,
    "input_throughput": 4734.849857436317,
    "output_throughput": 4191.136349054723,
    "total_throughput": 8925.98620649104,
    "itl": 104.3036976307244,
    "ttft": 1981264.5272896155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8435275565693148,
    "arrivals": 310219,
    "finished_requests": 69612,
    "scheduler_time": 24.502832728406037
}
#Debug simulation 
Total elapsed time: 5.2983804498799145. Arrivals time: 0.26004326716065407 Scheduler time: 4.879361604806036 Scheduler overhead time: 0.050115336664021015 Adapter cache time: 0.03323026979342103 Engine time: 0.05217452347278595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206969333 . Total output tokens: 182762594
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.261308850720525,
    "estimated_duration": 3600.02017781723,
    "input_throughput": 4736.249842448979,
    "output_throughput": 4192.274002517055,
    "total_throughput": 8928.523844966034,
    "itl": 104.38796617615891,
    "ttft": 1980861.2690059082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7852222756901747,
    "arrivals": 310219,
    "finished_requests": 69628,
    "scheduler_time": 24.55043211719584
}
#Debug simulation 
Total elapsed time: 5.261403470765799. Arrivals time: 0.25137880723923445 Scheduler time: 4.85127083119005 Scheduler overhead time: 0.04995376896113157 Adapter cache time: 0.03352770954370499 Engine time: 0.05181094631552696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180767437 . Total output tokens: 159578454
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.625827314797789,
    "estimated_duration": 3600.12176511389,
    "input_throughput": 5116.375834420505,
    "output_throughput": 4486.835183334138,
    "total_throughput": 9603.211017754642,
    "itl": 116.13614496524124,
    "ttft": 1880217.7771906324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 270597,
    "finished_requests": 74771,
    "scheduler_time": 33.00254767107441
}
#Debug simulation 
Total elapsed time: 5.625923722051084. Arrivals time: 0.24729094561189413 Scheduler time: 5.205910189542919 Scheduler overhead time: 0.046148499473929405 Adapter cache time: 0.056972764898091555 Engine time: 0.048067765310406685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180767437 . Total output tokens: 159578454
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.501766071654856,
    "estimated_duration": 3600.0626636685815,
    "input_throughput": 4969.692383567645,
    "output_throughput": 4362.785725511446,
    "total_throughput": 9332.478109079091,
    "itl": 100.07897618043921,
    "ttft": 1904322.2921706738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979344,
    "arrivals": 270597,
    "finished_requests": 72639,
    "scheduler_time": 25.794740317122518
}
#Debug simulation 
Total elapsed time: 5.501859987620264. Arrivals time: 0.26193995447829366 Scheduler time: 5.045887705404311 Scheduler overhead time: 0.05237531941384077 Adapter cache time: 0.06257185013964772 Engine time: 0.05463205557316542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_128_slots_128_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_128_slots_128_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180767437 . Total output tokens: 159578454
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.489769503939897,
    "estimated_duration": 3600.0726004445314,
    "input_throughput": 4969.516725243511,
    "output_throughput": 4362.812016085618,
    "total_throughput": 9332.32874132913,
    "itl": 100.07799202162677,
    "ttft": 1904293.4800780835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454805,
    "arrivals": 270597,
    "finished_requests": 72639,
    "scheduler_time": 25.7939720936448
}
#Debug simulation 
Total elapsed time: 5.489866215735674. Arrivals time: 0.24917871644720435 Scheduler time: 5.04678995674476 Scheduler overhead time: 0.05192934861406684 Adapter cache time: 0.06217977870255709 Engine time: 0.05542588559910655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_128_slots_128_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_128_slots_128_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180767437 . Total output tokens: 159578454
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.488247627392411,
    "estimated_duration": 3600.067681372294,
    "input_throughput": 4969.9379521608835,
    "output_throughput": 4362.839088073173,
    "total_throughput": 9332.777040234057,
    "itl": 100.07150821903772,
    "ttft": 1904211.2359717616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 270597,
    "finished_requests": 72643,
    "scheduler_time": 25.793258781475842
}
#Debug simulation 
Total elapsed time: 5.488350181374699. Arrivals time: 0.2346480875276029 Scheduler time: 5.060379386879504 Scheduler overhead time: 0.052091157995164394 Adapter cache time: 0.06234088400378823 Engine time: 0.05437876842916012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178243290 . Total output tokens: 157358313
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.736869913991541,
    "estimated_duration": 3600.0323569045304,
    "input_throughput": 5282.971960939062,
    "output_throughput": 4622.621785074506,
    "total_throughput": 9905.593746013568,
    "itl": 112.86161376441495,
    "ttft": 1848192.7458801025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 266811,
    "finished_requests": 77153,
    "scheduler_time": 34.08320673221746
}
#Debug simulation 
Total elapsed time: 5.7369623822160065. Arrivals time: 0.26532619865611196 Scheduler time: 5.302095426246524 Scheduler overhead time: 0.04747320944443345 Adapter cache time: 0.050581712275743484 Engine time: 0.0493360823020339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178243290 . Total output tokens: 157358313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.60060105426237,
    "estimated_duration": 3600.0381508082132,
    "input_throughput": 5120.154906097821,
    "output_throughput": 4482.897770507478,
    "total_throughput": 9603.052676605299,
    "itl": 97.44693122250769,
    "ttft": 1875060.9664406327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979342,
    "arrivals": 266811,
    "finished_requests": 74810,
    "scheduler_time": 26.5816347489815
}
#Debug simulation 
Total elapsed time: 5.600695022381842. Arrivals time: 0.2565512298606336 Scheduler time: 5.154517962131649 Scheduler overhead time: 0.0535394586622715 Adapter cache time: 0.05518106557428837 Engine time: 0.05586843239143491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_128_slots_128_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_128_slots_128_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178243290 . Total output tokens: 157358313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.608561038970947,
    "estimated_duration": 3600.092382598631,
    "input_throughput": 5119.891114209489,
    "output_throughput": 4482.746631171447,
    "total_throughput": 9602.637745380936,
    "itl": 97.44658612904456,
    "ttft": 1875127.4138324542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454804,
    "arrivals": 266811,
    "finished_requests": 74808,
    "scheduler_time": 26.58299893824905
}
#Debug simulation 
Total elapsed time: 5.608656499069184. Arrivals time: 0.2412938168272376 Scheduler time: 5.1775695430114865 Scheduler overhead time: 0.05364205362275243 Adapter cache time: 0.05514371441677213 Engine time: 0.05590334115549922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_128_slots_128_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_128_slots_128_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178243290 . Total output tokens: 157358313
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.606579654850066,
    "estimated_duration": 3600.033253138285,
    "input_throughput": 5119.763819940475,
    "output_throughput": 4482.3097080931775,
    "total_throughput": 9602.073528033652,
    "itl": 97.44862990545954,
    "ttft": 1875149.7140720452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 266811,
    "finished_requests": 74803,
    "scheduler_time": 26.584552528737884
}
#Debug simulation 
Total elapsed time: 5.606672195252031. Arrivals time: 0.23744995007291436 Scheduler time: 5.179661247879267 Scheduler overhead time: 0.053442811127752066 Adapter cache time: 0.05541970394551754 Engine time: 0.05571382399648428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176975216 . Total output tokens: 156255932
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.855077389162034,
    "estimated_duration": 3600.016306065064,
    "input_throughput": 5378.610637784719,
    "output_throughput": 4707.190067847912,
    "total_throughput": 10085.800705632631,
    "itl": 110.32653062035166,
    "ttft": 1832393.6536793455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 264887,
    "finished_requests": 78452,
    "scheduler_time": 34.61165748648924
}
#Debug simulation 
Total elapsed time: 5.855173544026911. Arrivals time: 0.268408068921417 Scheduler time: 5.419647759757936 Scheduler overhead time: 0.049377334769815207 Adapter cache time: 0.0447932118549943 Engine time: 0.05032197665423155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176975216 . Total output tokens: 156255932
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.933621650096029,
    "estimated_duration": 3600.0500990529035,
    "input_throughput": 5189.673056193057,
    "output_throughput": 4549.159469838626,
    "total_throughput": 9738.832526031683,
    "itl": 95.57765019790575,
    "ttft": 1860962.037834621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979342,
    "arrivals": 264887,
    "finished_requests": 75759,
    "scheduler_time": 26.881786609137524
}
#Debug simulation 
Total elapsed time: 5.9336882792413235. Arrivals time: 0.5191582920961082 Scheduler time: 5.228762079495937 Scheduler overhead time: 0.05449146777391434 Adapter cache time: 0.04900183528661728 Engine time: 0.056807586923241615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176975216 . Total output tokens: 156255932
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.698499062098563,
    "estimated_duration": 3600.084882147228,
    "input_throughput": 5189.892630769338,
    "output_throughput": 4549.402176937394,
    "total_throughput": 9739.294807706732,
    "itl": 95.57524968336297,
    "ttft": 1860812.6246136867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8768353822454801,
    "arrivals": 264887,
    "finished_requests": 75764,
    "scheduler_time": 26.88235197999601
}
#Debug simulation 
Total elapsed time: 5.6985896662808955. Arrivals time: 0.24211613228544593 Scheduler time: 5.270255092531443 Scheduler overhead time: 0.05455422028899193 Adapter cache time: 0.049356346018612385 Engine time: 0.05683596804738045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176975216 . Total output tokens: 156255932
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.688131120055914,
    "estimated_duration": 3600.058038066228,
    "input_throughput": 5190.086327062783,
    "output_throughput": 4549.527209510697,
    "total_throughput": 9739.613536573479,
    "itl": 95.57278418754076,
    "ttft": 1860825.6969620057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171418803930272,
    "arrivals": 264887,
    "finished_requests": 75767,
    "scheduler_time": 26.88413293106229
}
#Debug simulation 
Total elapsed time: 5.688223619945347. Arrivals time: 0.2428904282860458 Scheduler time: 5.259053628426045 Scheduler overhead time: 0.054709503427147865 Adapter cache time: 0.04941823659464717 Engine time: 0.056574694346636534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 253568,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176338065 . Total output tokens: 155698882
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.883527038153261,
    "estimated_duration": 3600.0166745952993,
    "input_throughput": 5431.096788516395,
    "output_throughput": 4751.763268408484,
    "total_throughput": 10182.860056924877,
    "itl": 109.28228884309716,
    "ttft": 1817525.881942287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 263940,
    "finished_requests": 79409,
    "scheduler_time": 34.98522434683342
}
#Debug simulation 
Total elapsed time: 5.883624630048871. Arrivals time: 0.2684245016425848 Scheduler time: 5.450857021380216 Scheduler overhead time: 0.04891861695796251 Adapter cache time: 0.041853317990899086 Engine time: 0.05073090409860015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 212032,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176338065 . Total output tokens: 155698882
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.737682464066893,
    "estimated_duration": 3600.0121578950675,
    "input_throughput": 5243.9550679290405,
    "output_throughput": 4587.096175157233,
    "total_throughput": 9831.051243086273,
    "itl": 94.70414303167806,
    "ttft": 1849009.243804142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365288840979341,
    "arrivals": 263940,
    "finished_requests": 76661,
    "scheduler_time": 27.172681677768413
}
#Debug simulation 
Total elapsed time: 5.737777294125408. Arrivals time: 0.2469168151728809 Scheduler time: 5.30598024232313 Scheduler overhead time: 0.05497461464256048 Adapter cache time: 0.047039732336997986 Engine time: 0.05717636924237013 
