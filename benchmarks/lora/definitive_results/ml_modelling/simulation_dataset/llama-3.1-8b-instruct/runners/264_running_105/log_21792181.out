INFO 05-31 19:31:04 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:05 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.44684679200873,
    "estimated_duration": 3600.01612097928,
    "input_throughput": 6819.354740367647,
    "output_throughput": 5958.008597518569,
    "total_throughput": 12777.363337886216,
    "itl": 83.1967175552033,
    "ttft": 2065341.0067572617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.544691270939081,
    "arrivals": 1669342,
    "finished_requests": 99340,
    "scheduler_time": 323.6654687193062
}
#Debug simulation 
Total elapsed time: 82.44707248685881. Arrivals time: 0.5266189561225474 Scheduler time: 81.69443321879953 Scheduler overhead time: 0.08853459125384688 Adapter cache time: 0.020270274952054024 Engine time: 0.08340751333162189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.08523082360625,
    "estimated_duration": 3600.0108278945913,
    "input_throughput": 6836.1172164564905,
    "output_throughput": 5973.031201291045,
    "total_throughput": 12809.148417747536,
    "itl": 81.9914878025084,
    "ttft": 2074640.1732988202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.428248430728016,
    "arrivals": 1669342,
    "finished_requests": 99593,
    "scheduler_time": 322.0891992555458
}
#Debug simulation 
Total elapsed time: 82.08544322196394. Arrivals time: 0.5263933632522821 Scheduler time: 81.33021233230829 Scheduler overhead time: 0.09050257783383131 Adapter cache time: 0.01976480893790722 Engine time: 0.08481526933610439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 82.70785548398271,
    "estimated_duration": 3600.0189154198915,
    "input_throughput": 6810.845325000239,
    "output_throughput": 5951.034287135464,
    "total_throughput": 12761.879612135703,
    "itl": 82.8091371013152,
    "ttft": 2062859.5593918804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.208477419959375,
    "arrivals": 1669342,
    "finished_requests": 99224,
    "scheduler_time": 324.0861637058524
}
#Debug simulation 
Total elapsed time: 82.70804175687954. Arrivals time: 0.6185058588162065 Scheduler time: 81.86232133116573 Scheduler overhead time: 0.08938770927488804 Adapter cache time: 0.019847177900373936 Engine time: 0.08421655604615808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 83.12466425821185,
    "estimated_duration": 3600.0522948039984,
    "input_throughput": 6836.336804196321,
    "output_throughput": 5973.154343073438,
    "total_throughput": 12809.49114726976,
    "itl": 81.99054013261724,
    "ttft": 2074681.2034668855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.388067698464764,
    "arrivals": 1669342,
    "finished_requests": 99596,
    "scheduler_time": 322.0963452976285
}
#Debug simulation 
Total elapsed time: 83.12484529707581. Arrivals time: 0.5398230068385601 Scheduler time: 82.35792138194665 Scheduler overhead time: 0.08867786452174187 Adapter cache time: 0.019757714588195086 Engine time: 0.08495493000373244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.37558346893638,
    "estimated_duration": 3600.015058989129,
    "input_throughput": 6850.875786871111,
    "output_throughput": 5987.9246744187285,
    "total_throughput": 12838.80046128984,
    "itl": 82.98858871761647,
    "ttft": 2056705.6901176225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.117629006667975,
    "arrivals": 1669342,
    "finished_requests": 99904,
    "scheduler_time": 321.79310832282323
}
#Debug simulation 
Total elapsed time: 82.37578553287312. Arrivals time: 0.5172840096056461 Scheduler time: 81.63163271546364 Scheduler overhead time: 0.08944634441286325 Adapter cache time: 0.020215395372360945 Engine time: 0.08359490009024739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.58167567802593,
    "estimated_duration": 3600.0491832267976,
    "input_throughput": 6748.215583606242,
    "output_throughput": 5881.384648479153,
    "total_throughput": 12629.600232085395,
    "itl": 80.23117597709171,
    "ttft": 2081462.3915085127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.594513421077315,
    "arrivals": 1669342,
    "finished_requests": 98243,
    "scheduler_time": 326.50914221014654
}
#Debug simulation 
Total elapsed time: 80.58185898000374. Arrivals time: 0.5197718795388937 Scheduler time: 79.83468918921426 Scheduler overhead time: 0.08915055450052023 Adapter cache time: 0.020051841624081135 Engine time: 0.0845547616481781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 4320, 135, 34560, 34560, 135, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 135, 135, 4320, 135, 34560, 135, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 34560, 34560, 135, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 34560, 4320, 4320, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 4320, 135, 34560, 4320, 4320, 135, 34560, 34560, 34560, 135, 4320, 34560, 135, 34560, 4320, 4320, 34560, 34560, 135, 4320, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 34560, 135, 4320, 4320, 135, 135, 4320, 135, 34560, 34560, 135, 34560, 34560, 135, 4320, 34560, 135, 135, 4320, 4320, 135, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 4320, 135, 34560, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 135, 34560, 34560, 4320, 135, 34560, 4320, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 4320, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 34560, 135, 135, 135, 4320, 135, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 4320, 4320, 4320, 135, 4320, 135, 135, 34560, 135, 4320, 135, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4993920 . Total input tokens: 1114032545 . Total output tokens: 980536160
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.01335068279877,
    "estimated_duration": 3600.0528076392684,
    "input_throughput": 6908.710602028533,
    "output_throughput": 6032.577342731063,
    "total_throughput": 12941.287944759595,
    "itl": 83.16965598167666,
    "ttft": 2066742.912857059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8880965107866214,
    "arrivals": 1663594,
    "finished_requests": 100858,
    "scheduler_time": 319.87245421393317
}
#Debug simulation 
Total elapsed time: 83.01355339772999. Arrivals time: 0.5411589280702174 Scheduler time: 82.2493880125694 Scheduler overhead time: 0.08780704252421856 Adapter cache time: 0.019477407913655043 Engine time: 0.08267142390832305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 4320, 135, 34560, 34560, 135, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 135, 135, 4320, 135, 34560, 135, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 34560, 34560, 135, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 34560, 4320, 4320, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 4320, 135, 34560, 4320, 4320, 135, 34560, 34560, 34560, 135, 4320, 34560, 135, 34560, 4320, 4320, 34560, 34560, 135, 4320, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 34560, 135, 4320, 4320, 135, 135, 4320, 135, 34560, 34560, 135, 34560, 34560, 135, 4320, 34560, 135, 135, 4320, 4320, 135, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 4320, 135, 34560, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 135, 34560, 34560, 4320, 135, 34560, 4320, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 4320, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 34560, 135, 135, 135, 4320, 135, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 4320, 4320, 4320, 135, 4320, 135, 135, 34560, 135, 4320, 135, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4993920 . Total input tokens: 1114032545 . Total output tokens: 980536160
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.9837427008897,
    "estimated_duration": 3600.012661839184,
    "input_throughput": 6784.2047498584625,
    "output_throughput": 5937.601338624087,
    "total_throughput": 12721.806088482548,
    "itl": 81.49847628204292,
    "ttft": 2067947.5161540413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.199666731660259,
    "arrivals": 1663594,
    "finished_requests": 99145,
    "scheduler_time": 324.8149137910372
}
#Debug simulation 
Total elapsed time: 82.98392542824149. Arrivals time: 0.5408079833723605 Scheduler time: 82.21706370543689 Scheduler overhead time: 0.08864489290863276 Adapter cache time: 0.019525683019310236 Engine time: 0.08390729175880551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 4320, 135, 34560, 34560, 135, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 135, 135, 4320, 135, 34560, 135, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 34560, 34560, 135, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 34560, 4320, 4320, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 4320, 135, 34560, 4320, 4320, 135, 34560, 34560, 34560, 135, 4320, 34560, 135, 34560, 4320, 4320, 34560, 34560, 135, 4320, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 34560, 135, 4320, 4320, 135, 135, 4320, 135, 34560, 34560, 135, 34560, 34560, 135, 4320, 34560, 135, 135, 4320, 4320, 135, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 4320, 135, 34560, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 135, 34560, 34560, 4320, 135, 34560, 4320, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 4320, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 34560, 135, 135, 135, 4320, 135, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 4320, 4320, 4320, 135, 4320, 135, 135, 34560, 135, 4320, 135, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4993920 . Total input tokens: 1114032545 . Total output tokens: 980536160
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.412665292155,
    "estimated_duration": 3600.0148882904027,
    "input_throughput": 6797.9179973952005,
    "output_throughput": 5943.45125338104,
    "total_throughput": 12741.36925077624,
    "itl": 81.40674807233906,
    "ttft": 2059617.2973774641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.477945718765286,
    "arrivals": 1663594,
    "finished_requests": 99263,
    "scheduler_time": 324.3350901585413
}
#Debug simulation 
Total elapsed time: 81.41285738814622. Arrivals time: 0.5336403949186206 Scheduler time: 80.65231902105734 Scheduler overhead time: 0.08887188462540507 Adapter cache time: 0.02005327632650733 Engine time: 0.08419353468343616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 4320, 135, 34560, 34560, 135, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 135, 135, 4320, 135, 34560, 135, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 34560, 34560, 135, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 34560, 4320, 4320, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 4320, 135, 34560, 4320, 4320, 135, 34560, 34560, 34560, 135, 4320, 34560, 135, 34560, 4320, 4320, 34560, 34560, 135, 4320, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 34560, 135, 4320, 4320, 135, 135, 4320, 135, 34560, 34560, 135, 34560, 34560, 135, 4320, 34560, 135, 135, 4320, 4320, 135, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 4320, 135, 34560, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 135, 34560, 34560, 4320, 135, 34560, 4320, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 4320, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 34560, 135, 135, 135, 4320, 135, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 4320, 4320, 4320, 135, 4320, 135, 135, 34560, 135, 4320, 135, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4993920 . Total input tokens: 1114032545 . Total output tokens: 980536160
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.5829441039823,
    "estimated_duration": 3600.039065308164,
    "input_throughput": 6881.293383375948,
    "output_throughput": 6013.546410821195,
    "total_throughput": 12894.839794197143,
    "itl": 82.51734552192111,
    "ttft": 2063333.1640831812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.166009231396012,
    "arrivals": 1663594,
    "finished_requests": 100523,
    "scheduler_time": 320.69887735263984
}
#Debug simulation 
Total elapsed time: 81.58312542969361. Arrivals time: 0.5369673534296453 Scheduler time: 80.81950184563175 Scheduler overhead time: 0.0891862497664988 Adapter cache time: 0.019985560793429613 Engine time: 0.08401822112500668 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 4320, 135, 34560, 34560, 135, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 135, 135, 4320, 135, 34560, 135, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 34560, 34560, 135, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 34560, 4320, 4320, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 4320, 135, 34560, 4320, 4320, 135, 34560, 34560, 34560, 135, 4320, 34560, 135, 34560, 4320, 4320, 34560, 34560, 135, 4320, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 34560, 135, 4320, 4320, 135, 135, 4320, 135, 34560, 34560, 135, 34560, 34560, 135, 4320, 34560, 135, 135, 4320, 4320, 135, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 4320, 135, 34560, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 135, 34560, 34560, 4320, 135, 34560, 4320, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 4320, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 34560, 135, 135, 135, 4320, 135, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 4320, 4320, 4320, 135, 4320, 135, 135, 34560, 135, 4320, 135, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4993920 . Total input tokens: 1114032545 . Total output tokens: 980536160
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 81.4906159597449,
    "estimated_duration": 3600.0110471153826,
    "input_throughput": 6779.341974396387,
    "output_throughput": 5924.369042447672,
    "total_throughput": 12703.711016844058,
    "itl": 80.78596130506111,
    "ttft": 2074232.8729855868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.284996523442707,
    "arrivals": 1663594,
    "finished_requests": 99040,
    "scheduler_time": 325.446821692066
}
#Debug simulation 
Total elapsed time: 81.4908183440566. Arrivals time: 0.5492523517459631 Scheduler time: 80.71362432651222 Scheduler overhead time: 0.08916031150147319 Adapter cache time: 0.01969219883903861 Engine time: 0.0848171510733664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 4320, 135, 34560, 34560, 135, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 135, 135, 4320, 135, 34560, 135, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 34560, 34560, 135, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 34560, 4320, 4320, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 4320, 135, 34560, 4320, 4320, 135, 34560, 34560, 34560, 135, 4320, 34560, 135, 34560, 4320, 4320, 34560, 34560, 135, 4320, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 34560, 135, 4320, 4320, 135, 135, 4320, 135, 34560, 34560, 135, 34560, 34560, 135, 4320, 34560, 135, 135, 4320, 4320, 135, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 4320, 135, 34560, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 135, 34560, 34560, 4320, 135, 34560, 4320, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 4320, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 34560, 135, 135, 135, 4320, 135, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 4320, 4320, 4320, 135, 4320, 135, 135, 34560, 135, 4320, 135, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4993920 . Total input tokens: 1114032545 . Total output tokens: 980536160
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.27961053000763,
    "estimated_duration": 3600.0554216767514,
    "input_throughput": 6932.279111518869,
    "output_throughput": 6040.3225653320305,
    "total_throughput": 12972.6016768509,
    "itl": 83.31596399857267,
    "ttft": 2055072.9364837862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.05378979726227,
    "arrivals": 1663594,
    "finished_requests": 101122,
    "scheduler_time": 318.99540161692425
}
#Debug simulation 
Total elapsed time: 81.27980311308056. Arrivals time: 0.5385132529772818 Scheduler time: 80.51462831441313 Scheduler overhead time: 0.08829326927661896 Adapter cache time: 0.020271089393645525 Engine time: 0.08459981624037027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 4320, 135, 34560, 34560, 135, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 135, 135, 4320, 135, 34560, 135, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 34560, 34560, 135, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 34560, 4320, 4320, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 135, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 4320, 135, 34560, 4320, 4320, 135, 34560, 34560, 34560, 135, 4320, 34560, 135, 34560, 4320, 4320, 34560, 34560, 135, 4320, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 4320, 4320, 4320, 135, 135, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 34560, 135, 4320, 4320, 135, 135, 4320, 135, 34560, 34560, 135, 34560, 34560, 135, 4320, 34560, 135, 135, 4320, 4320, 135, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 4320, 135, 34560, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 135, 34560, 34560, 4320, 135, 34560, 4320, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 4320, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 34560, 135, 135, 135, 4320, 135, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 135, 4320, 4320, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 4320, 4320, 4320, 135, 4320, 135, 135, 34560, 135, 4320, 135, 135, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4993920 . Total input tokens: 1114032545 . Total output tokens: 980536160
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.50748604303226,
    "estimated_duration": 3600.049764040666,
    "input_throughput": 6778.032416033115,
    "output_throughput": 5902.904235453771,
    "total_throughput": 12680.936651486887,
    "itl": 80.44589437313309,
    "ttft": 2055392.0218819932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.774718744605807,
    "arrivals": 1663594,
    "finished_requests": 98985,
    "scheduler_time": 325.4477555053622
}
#Debug simulation 
Total elapsed time: 79.50768019910902. Arrivals time: 0.5512535744346678 Scheduler time: 78.727427163627 Scheduler overhead time: 0.0898294672369957 Adapter cache time: 0.02066982164978981 Engine time: 0.08440806018188596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 4320, 66, 34560, 34560, 66, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 66, 66, 4320, 66, 34560, 66, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 34560, 34560, 66, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 34560, 4320, 4320, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 4320, 66, 34560, 4320, 4320, 66, 34560, 34560, 34560, 66, 4320, 34560, 66, 34560, 4320, 4320, 34560, 34560, 66, 4320, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 34560, 66, 4320, 4320, 66, 66, 4320, 66, 34560, 34560, 66, 34560, 34560, 66, 4320, 34560, 66, 66, 4320, 4320, 66, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 4320, 66, 34560, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 66, 34560, 34560, 4320, 66, 34560, 4320, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 4320, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 34560, 66, 66, 66, 4320, 66, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 4320, 4320, 4320, 66, 4320, 66, 66, 34560, 66, 4320, 66, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4985088 . Total input tokens: 1112070492 . Total output tokens: 978798904
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.16816220106557,
    "estimated_duration": 3600.063689866677,
    "input_throughput": 6976.926566799207,
    "output_throughput": 6101.418722626393,
    "total_throughput": 13078.3452894256,
    "itl": 84.55846836731874,
    "ttft": 2050823.2104194672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1724300991604775,
    "arrivals": 1660646,
    "finished_requests": 101953,
    "scheduler_time": 314.77028126493246
}
#Debug simulation 
Total elapsed time: 82.16835516225547. Arrivals time: 0.5595998540520668 Scheduler time: 81.38706152979285 Scheduler overhead time: 0.08702325774356723 Adapter cache time: 0.019669895991683006 Engine time: 0.08197630196809769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 4320, 66, 34560, 34560, 66, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 66, 66, 4320, 66, 34560, 66, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 34560, 34560, 66, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 34560, 4320, 4320, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 4320, 66, 34560, 4320, 4320, 66, 34560, 34560, 34560, 66, 4320, 34560, 66, 34560, 4320, 4320, 34560, 34560, 66, 4320, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 34560, 66, 4320, 4320, 66, 66, 4320, 66, 34560, 34560, 66, 34560, 34560, 66, 4320, 34560, 66, 66, 4320, 4320, 66, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 4320, 66, 34560, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 66, 34560, 34560, 4320, 66, 34560, 4320, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 4320, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 34560, 66, 66, 66, 4320, 66, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 4320, 4320, 4320, 66, 4320, 66, 66, 34560, 66, 4320, 66, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4985088 . Total input tokens: 1112070492 . Total output tokens: 978798904
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.976914615836,
    "estimated_duration": 3600.0688401781185,
    "input_throughput": 6589.9012083408425,
    "output_throughput": 5769.208012970218,
    "total_throughput": 12359.10922131106,
    "itl": 80.69949712436608,
    "ttft": 2089401.3187385495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 506,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.706423789775004,
    "arrivals": 1660646,
    "finished_requests": 96417,
    "scheduler_time": 332.4733916235045
}
#Debug simulation 
Total elapsed time: 83.97711438499391. Arrivals time: 0.5276815802790225 Scheduler time: 83.22439563693479 Scheduler overhead time: 0.08851546421647072 Adapter cache time: 0.01903540873900056 Engine time: 0.08363440167158842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 4320, 66, 34560, 34560, 66, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 66, 66, 4320, 66, 34560, 66, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 34560, 34560, 66, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 34560, 4320, 4320, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 4320, 66, 34560, 4320, 4320, 66, 34560, 34560, 34560, 66, 4320, 34560, 66, 34560, 4320, 4320, 34560, 34560, 66, 4320, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 34560, 66, 4320, 4320, 66, 66, 4320, 66, 34560, 34560, 66, 34560, 34560, 66, 4320, 34560, 66, 66, 4320, 4320, 66, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 4320, 66, 34560, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 66, 34560, 34560, 4320, 66, 34560, 4320, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 4320, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 34560, 66, 66, 66, 4320, 66, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 4320, 4320, 4320, 66, 4320, 66, 66, 34560, 66, 4320, 66, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4985088 . Total input tokens: 1112070492 . Total output tokens: 978798904
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.25919007882476,
    "estimated_duration": 3600.0386817151257,
    "input_throughput": 6574.441858142482,
    "output_throughput": 5763.892511875511,
    "total_throughput": 12338.334370017992,
    "itl": 79.50602621785657,
    "ttft": 2088360.6369974082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8772247323301023,
    "arrivals": 1660646,
    "finished_requests": 96275,
    "scheduler_time": 332.893355811769
}
#Debug simulation 
Total elapsed time: 80.25940142385662. Arrivals time: 0.548379756975919 Scheduler time: 79.4825215372257 Scheduler overhead time: 0.09000370232388377 Adapter cache time: 0.01919768750667572 Engine time: 0.08498699450865388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 4320, 66, 34560, 34560, 66, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 66, 66, 4320, 66, 34560, 66, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 34560, 34560, 66, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 34560, 4320, 4320, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 4320, 66, 34560, 4320, 4320, 66, 34560, 34560, 34560, 66, 4320, 34560, 66, 34560, 4320, 4320, 34560, 34560, 66, 4320, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 34560, 66, 4320, 4320, 66, 66, 4320, 66, 34560, 34560, 66, 34560, 34560, 66, 4320, 34560, 66, 66, 4320, 4320, 66, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 4320, 66, 34560, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 66, 34560, 34560, 4320, 66, 34560, 4320, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 4320, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 34560, 66, 66, 66, 4320, 66, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 4320, 4320, 4320, 66, 4320, 66, 66, 34560, 66, 4320, 66, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4985088 . Total input tokens: 1112070492 . Total output tokens: 978798904
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 84.0281826639548,
    "estimated_duration": 3600.0267323752864,
    "input_throughput": 6938.932362737775,
    "output_throughput": 6082.066503309921,
    "total_throughput": 13020.998866047697,
    "itl": 83.56325108243412,
    "ttft": 2052436.2515261918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.133258589645842,
    "arrivals": 1660646,
    "finished_requests": 101463,
    "scheduler_time": 316.89995626998643
}
#Debug simulation 
Total elapsed time: 84.02838968485594. Arrivals time: 0.9970874674618244 Scheduler time: 82.80675693135709 Scheduler overhead time: 0.08759837271645665 Adapter cache time: 0.01981254480779171 Engine time: 0.08334459224715829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 4320, 66, 34560, 34560, 66, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 66, 66, 4320, 66, 34560, 66, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 34560, 34560, 66, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 34560, 4320, 4320, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 4320, 66, 34560, 4320, 4320, 66, 34560, 34560, 34560, 66, 4320, 34560, 66, 34560, 4320, 4320, 34560, 34560, 66, 4320, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 34560, 66, 4320, 4320, 66, 66, 4320, 66, 34560, 34560, 66, 34560, 34560, 66, 4320, 34560, 66, 66, 4320, 4320, 66, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 4320, 66, 34560, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 66, 34560, 34560, 4320, 66, 34560, 4320, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 4320, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 34560, 66, 66, 66, 4320, 66, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 4320, 4320, 4320, 66, 4320, 66, 66, 34560, 66, 4320, 66, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4985088 . Total input tokens: 1112070492 . Total output tokens: 978798904
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 80.80216627987102,
    "estimated_duration": 3600.0746736503474,
    "input_throughput": 6558.748398420938,
    "output_throughput": 5745.678874773527,
    "total_throughput": 12304.427273194466,
    "itl": 79.05596075838358,
    "ttft": 2090692.4799582697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 507,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7833611132065155,
    "arrivals": 1660646,
    "finished_requests": 95954,
    "scheduler_time": 333.9932046178646
}
#Debug simulation 
Total elapsed time: 80.80235587619245. Arrivals time: 0.539765530731529 Scheduler time: 80.03257227595896 Scheduler overhead time: 0.09017884312197566 Adapter cache time: 0.019397947005927563 Engine time: 0.08536163670942187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 4320, 66, 34560, 34560, 66, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 66, 66, 4320, 66, 34560, 66, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 34560, 34560, 66, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 34560, 4320, 4320, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 4320, 66, 34560, 4320, 4320, 66, 34560, 34560, 34560, 66, 4320, 34560, 66, 34560, 4320, 4320, 34560, 34560, 66, 4320, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 34560, 66, 4320, 4320, 66, 66, 4320, 66, 34560, 34560, 66, 34560, 34560, 66, 4320, 34560, 66, 66, 4320, 4320, 66, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 4320, 66, 34560, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 66, 34560, 34560, 4320, 66, 34560, 4320, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 4320, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 34560, 66, 66, 66, 4320, 66, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 4320, 4320, 4320, 66, 4320, 66, 66, 34560, 66, 4320, 66, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4985088 . Total input tokens: 1112070492 . Total output tokens: 978798904
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.1135871950537,
    "estimated_duration": 3600.003118554905,
    "input_throughput": 6924.779001304578,
    "output_throughput": 6064.903912851154,
    "total_throughput": 12989.682914155732,
    "itl": 83.07426447820112,
    "ttft": 2056458.8782097076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.970798825034853,
    "arrivals": 1660646,
    "finished_requests": 101199,
    "scheduler_time": 317.36435988195643
}
#Debug simulation 
Total elapsed time: 82.11378745129332. Arrivals time: 0.557339676655829 Scheduler time: 81.33252882445231 Scheduler overhead time: 0.0874967323616147 Adapter cache time: 0.020246337167918682 Engine time: 0.08319073496386409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 4320, 66, 34560, 34560, 66, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 66, 66, 4320, 66, 34560, 66, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 34560, 34560, 66, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 34560, 4320, 4320, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 66, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 4320, 66, 34560, 4320, 4320, 66, 34560, 34560, 34560, 66, 4320, 34560, 66, 34560, 4320, 4320, 34560, 34560, 66, 4320, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 4320, 4320, 4320, 66, 66, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 34560, 66, 4320, 4320, 66, 66, 4320, 66, 34560, 34560, 66, 34560, 34560, 66, 4320, 34560, 66, 66, 4320, 4320, 66, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 4320, 66, 34560, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 66, 34560, 34560, 4320, 66, 34560, 4320, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 4320, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 34560, 66, 66, 66, 4320, 66, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 66, 4320, 4320, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 4320, 4320, 4320, 66, 4320, 66, 66, 34560, 66, 4320, 66, 66, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4985088 . Total input tokens: 1112070492 . Total output tokens: 978798904
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.17074212990701,
    "estimated_duration": 3600.0001252002617,
    "input_throughput": 6527.921717417359,
    "output_throughput": 5729.051189644803,
    "total_throughput": 12256.972907062163,
    "itl": 79.13479098717981,
    "ttft": 2094403.3095493573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 507,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7462871385924834,
    "arrivals": 1660646,
    "finished_requests": 95529,
    "scheduler_time": 335.4855688152151
}
#Debug simulation 
Total elapsed time: 80.1709362459369. Arrivals time: 0.521902572363615 Scheduler time: 79.41902097919956 Scheduler overhead time: 0.09035611525177956 Adapter cache time: 0.01926869759336114 Engine time: 0.08527467073872685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 4320, 33, 34560, 34560, 33, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 33, 33, 4320, 33, 34560, 33, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 34560, 34560, 33, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 34560, 4320, 4320, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 4320, 33, 34560, 4320, 4320, 33, 34560, 34560, 34560, 33, 4320, 34560, 33, 34560, 4320, 4320, 34560, 34560, 33, 4320, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 34560, 33, 4320, 4320, 33, 33, 4320, 33, 34560, 34560, 33, 34560, 34560, 33, 4320, 34560, 33, 33, 4320, 4320, 33, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 4320, 33, 34560, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 33, 34560, 34560, 4320, 33, 34560, 4320, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 4320, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 34560, 33, 33, 33, 4320, 33, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 4320, 4320, 4320, 33, 4320, 33, 33, 34560, 33, 4320, 33, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4980864 . Total input tokens: 1111142889 . Total output tokens: 977965913
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.4846317670308,
    "estimated_duration": 3600.0348198358593,
    "input_throughput": 6984.947162579531,
    "output_throughput": 6075.7326233298145,
    "total_throughput": 13060.679785909346,
    "itl": 83.45804731628472,
    "ttft": 2045369.8200404162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.736011103051768,
    "arrivals": 1659330,
    "finished_requests": 101560,
    "scheduler_time": 317.6089172701619
}
#Debug simulation 
Total elapsed time: 84.48483232501894. Arrivals time: 0.575527532491833 Scheduler time: 83.68255531368777 Scheduler overhead time: 0.08966593258082867 Adapter cache time: 0.019495943561196327 Engine time: 0.0839749095030129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 4320, 33, 34560, 34560, 33, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 33, 33, 4320, 33, 34560, 33, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 34560, 34560, 33, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 34560, 4320, 4320, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 4320, 33, 34560, 4320, 4320, 33, 34560, 34560, 34560, 33, 4320, 34560, 33, 34560, 4320, 4320, 34560, 34560, 33, 4320, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 34560, 33, 4320, 4320, 33, 33, 4320, 33, 34560, 34560, 33, 34560, 34560, 33, 4320, 34560, 33, 33, 4320, 4320, 33, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 4320, 33, 34560, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 33, 34560, 34560, 4320, 33, 34560, 4320, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 4320, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 34560, 33, 33, 33, 4320, 33, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 4320, 4320, 4320, 33, 4320, 33, 33, 34560, 33, 4320, 33, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4980864 . Total input tokens: 1111142889 . Total output tokens: 977965913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.78605504799634,
    "estimated_duration": 3600.028255399856,
    "input_throughput": 7008.204161219209,
    "output_throughput": 6092.8477344863895,
    "total_throughput": 13101.0518957056,
    "itl": 83.09387287211152,
    "ttft": 2049271.2763821236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.271004229858524,
    "arrivals": 1659330,
    "finished_requests": 101867,
    "scheduler_time": 316.30970730089064
}
#Debug simulation 
Total elapsed time: 81.7862536474131. Arrivals time: 0.5472351959906518 Scheduler time: 81.01440045237541 Scheduler overhead time: 0.088015615940094 Adapter cache time: 0.019580146297812462 Engine time: 0.0838177464902401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 4320, 33, 34560, 34560, 33, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 33, 33, 4320, 33, 34560, 33, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 34560, 34560, 33, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 34560, 4320, 4320, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 4320, 33, 34560, 4320, 4320, 33, 34560, 34560, 34560, 33, 4320, 34560, 33, 34560, 4320, 4320, 34560, 34560, 33, 4320, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 34560, 33, 4320, 4320, 33, 33, 4320, 33, 34560, 34560, 33, 34560, 34560, 33, 4320, 34560, 33, 33, 4320, 4320, 33, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 4320, 33, 34560, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 33, 34560, 34560, 4320, 33, 34560, 4320, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 4320, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 34560, 33, 33, 33, 4320, 33, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 4320, 4320, 4320, 33, 4320, 33, 33, 34560, 33, 4320, 33, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4980864 . Total input tokens: 1111142889 . Total output tokens: 977965913
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.45610284619033,
    "estimated_duration": 3600.0079497515885,
    "input_throughput": 6918.413055648562,
    "output_throughput": 6011.913668549934,
    "total_throughput": 12930.326724198496,
    "itl": 80.99091878016462,
    "ttft": 2053726.2475512708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.267662880453314,
    "arrivals": 1659330,
    "finished_requests": 100594,
    "scheduler_time": 320.8789935940936
}
#Debug simulation 
Total elapsed time: 82.45629619108513. Arrivals time: 0.572185053024441 Scheduler time: 81.65839829435572 Scheduler overhead time: 0.0883842958137393 Adapter cache time: 0.01989763742312789 Engine time: 0.08378079952672124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 4320, 33, 34560, 34560, 33, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 33, 33, 4320, 33, 34560, 33, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 34560, 34560, 33, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 34560, 4320, 4320, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 4320, 33, 34560, 4320, 4320, 33, 34560, 34560, 34560, 33, 4320, 34560, 33, 34560, 4320, 4320, 34560, 34560, 33, 4320, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 34560, 33, 4320, 4320, 33, 33, 4320, 33, 34560, 34560, 33, 34560, 34560, 33, 4320, 34560, 33, 33, 4320, 4320, 33, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 4320, 33, 34560, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 33, 34560, 34560, 4320, 33, 34560, 4320, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 4320, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 34560, 33, 33, 33, 4320, 33, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 4320, 4320, 4320, 33, 4320, 33, 33, 34560, 33, 4320, 33, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4980864 . Total input tokens: 1111142889 . Total output tokens: 977965913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.86093793576583,
    "estimated_duration": 3600.0008593661246,
    "input_throughput": 7008.750549143668,
    "output_throughput": 6093.536323172585,
    "total_throughput": 13102.286872316252,
    "itl": 83.08713084054139,
    "ttft": 2049207.4836440852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.008630465902383,
    "arrivals": 1659330,
    "finished_requests": 101874,
    "scheduler_time": 316.33823449929616
}
#Debug simulation 
Total elapsed time: 81.86113462178037. Arrivals time: 0.5486327665857971 Scheduler time: 81.08818218484521 Scheduler overhead time: 0.0882188118994236 Adapter cache time: 0.019537162967026234 Engine time: 0.08364068111404777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 4320, 33, 34560, 34560, 33, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 33, 33, 4320, 33, 34560, 33, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 34560, 34560, 33, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 34560, 4320, 4320, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 4320, 33, 34560, 4320, 4320, 33, 34560, 34560, 34560, 33, 4320, 34560, 33, 34560, 4320, 4320, 34560, 34560, 33, 4320, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 34560, 33, 4320, 4320, 33, 33, 4320, 33, 34560, 34560, 33, 34560, 34560, 33, 4320, 34560, 33, 33, 4320, 4320, 33, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 4320, 33, 34560, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 33, 34560, 34560, 4320, 33, 34560, 4320, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 4320, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 34560, 33, 33, 33, 4320, 33, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 4320, 4320, 4320, 33, 4320, 33, 33, 34560, 33, 4320, 33, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4980864 . Total input tokens: 1111142889 . Total output tokens: 977965913
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 82.73007489088923,
    "estimated_duration": 3600.0220898483594,
    "input_throughput": 6918.416992560486,
    "output_throughput": 6012.118109228513,
    "total_throughput": 12930.535101788999,
    "itl": 80.98985971798699,
    "ttft": 2053713.0112426612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.22913908560298,
    "arrivals": 1659330,
    "finished_requests": 100596,
    "scheduler_time": 320.88496697211144
}
#Debug simulation 
Total elapsed time: 82.7302743960172. Arrivals time: 0.5624805027619004 Scheduler time: 81.9406069945544 Scheduler overhead time: 0.0887857866473496 Adapter cache time: 0.019600391387939453 Engine time: 0.08540498651564121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 4320, 33, 34560, 34560, 33, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 33, 33, 4320, 33, 34560, 33, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 34560, 34560, 33, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 34560, 4320, 4320, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 4320, 33, 34560, 4320, 4320, 33, 34560, 34560, 34560, 33, 4320, 34560, 33, 34560, 4320, 4320, 34560, 34560, 33, 4320, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 34560, 33, 4320, 4320, 33, 33, 4320, 33, 34560, 34560, 33, 34560, 34560, 33, 4320, 34560, 33, 33, 4320, 4320, 33, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 4320, 33, 34560, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 33, 34560, 34560, 4320, 33, 34560, 4320, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 4320, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 34560, 33, 33, 33, 4320, 33, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 4320, 4320, 4320, 33, 4320, 33, 33, 34560, 33, 4320, 33, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4980864 . Total input tokens: 1111142889 . Total output tokens: 977965913
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.24083989392966,
    "estimated_duration": 3600.00346729797,
    "input_throughput": 7008.8610272751075,
    "output_throughput": 6093.6780198340475,
    "total_throughput": 13102.539047109156,
    "itl": 83.0810350698479,
    "ttft": 2049122.1107055931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7282098292931742,
    "arrivals": 1659330,
    "finished_requests": 101876,
    "scheduler_time": 316.3724706648641
}
#Debug simulation 
Total elapsed time: 82.24102960014716. Arrivals time: 0.560313614550978 Scheduler time: 81.45707944687456 Scheduler overhead time: 0.08756917109712958 Adapter cache time: 0.019375843927264214 Engine time: 0.08365595806390047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 4320, 33, 34560, 34560, 33, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 33, 33, 4320, 33, 34560, 33, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 34560, 34560, 33, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 4320, 33, 33, 33, 4320, 33, 4320, 33, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 34560, 4320, 4320, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 33, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 4320, 33, 34560, 4320, 4320, 33, 34560, 34560, 34560, 33, 4320, 34560, 33, 34560, 4320, 4320, 34560, 34560, 33, 4320, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 4320, 4320, 4320, 33, 33, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 33, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 34560, 33, 4320, 4320, 33, 33, 4320, 33, 34560, 34560, 33, 34560, 34560, 33, 4320, 34560, 33, 33, 4320, 4320, 33, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 33, 33, 33, 33, 33, 33, 33, 33, 4320, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 4320, 33, 34560, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 33, 34560, 34560, 4320, 33, 34560, 4320, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 4320, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 33, 34560, 33, 33, 33, 4320, 33, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 33, 4320, 4320, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 4320, 4320, 4320, 33, 4320, 33, 33, 34560, 33, 4320, 33, 33, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4980864 . Total input tokens: 1111142889 . Total output tokens: 977965913
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.89940438885242,
    "estimated_duration": 3600.014429414057,
    "input_throughput": 6947.8404852035665,
    "output_throughput": 6035.33386490797,
    "total_throughput": 12983.174350111536,
    "itl": 81.2752297167465,
    "ttft": 2058139.4174365087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.173013323769008,
    "arrivals": 1659330,
    "finished_requests": 100973,
    "scheduler_time": 319.40864793413346
}
#Debug simulation 
Total elapsed time: 80.89959160098806. Arrivals time: 0.538207605946809 Scheduler time: 80.13382087554783 Scheduler overhead time: 0.08914926135912538 Adapter cache time: 0.019553713034838438 Engine time: 0.0848927772603929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 1080, 540, 34560, 34560, 540, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 540, 540, 1080, 540, 34560, 540, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 34560, 34560, 540, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 34560, 1080, 1080, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 1080, 540, 34560, 1080, 1080, 540, 34560, 34560, 34560, 540, 1080, 34560, 540, 34560, 1080, 1080, 34560, 34560, 540, 1080, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 34560, 540, 1080, 1080, 540, 540, 1080, 540, 34560, 34560, 540, 34560, 34560, 540, 1080, 34560, 540, 540, 1080, 1080, 540, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 1080, 540, 34560, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 540, 34560, 34560, 1080, 540, 34560, 1080, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 1080, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 34560, 540, 540, 540, 1080, 540, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 1080, 1080, 1080, 540, 1080, 540, 540, 34560, 540, 1080, 540, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 4631040 . Total input tokens: 1033253728 . Total output tokens: 909312487
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.1964087090455,
    "estimated_duration": 3600.044489550755,
    "input_throughput": 6838.452433423161,
    "output_throughput": 5958.567196117306,
    "total_throughput": 12797.019629540468,
    "itl": 85.96163728693786,
    "ttft": 2073284.2129518276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.696336648860067,
    "arrivals": 1542191,
    "finished_requests": 99651,
    "scheduler_time": 323.52885171462395
}
#Debug simulation 
Total elapsed time: 82.19659959804267. Arrivals time: 0.6585715566761792 Scheduler time: 81.31013744929805 Scheduler overhead time: 0.089121307246387 Adapter cache time: 0.019641077145934105 Engine time: 0.08502014959231019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 1080, 540, 34560, 34560, 540, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 540, 540, 1080, 540, 34560, 540, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 34560, 34560, 540, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 34560, 1080, 1080, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 1080, 540, 34560, 1080, 1080, 540, 34560, 34560, 34560, 540, 1080, 34560, 540, 34560, 1080, 1080, 34560, 34560, 540, 1080, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 34560, 540, 1080, 1080, 540, 540, 1080, 540, 34560, 34560, 540, 34560, 34560, 540, 1080, 34560, 540, 540, 1080, 1080, 540, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 1080, 540, 34560, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 540, 34560, 34560, 1080, 540, 34560, 1080, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 1080, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 34560, 540, 540, 540, 1080, 540, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 1080, 1080, 1080, 540, 1080, 540, 540, 34560, 540, 1080, 540, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 4631040 . Total input tokens: 1033253728 . Total output tokens: 909312487
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.4774213777855,
    "estimated_duration": 3600.0600921700247,
    "input_throughput": 6653.520326534798,
    "output_throughput": 5786.205637318623,
    "total_throughput": 12439.725963853421,
    "itl": 84.3262939772722,
    "ttft": 2079730.4354979624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7871955039817884,
    "arrivals": 1542191,
    "finished_requests": 96872,
    "scheduler_time": 331.85572286932603
}
#Debug simulation 
Total elapsed time: 81.47761027188972. Arrivals time: 0.5134796504862607 Scheduler time: 80.73356729187071 Scheduler overhead time: 0.0901028267107904 Adapter cache time: 0.019594816025346518 Engine time: 0.0866178385913372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 1080, 540, 34560, 34560, 540, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 540, 540, 1080, 540, 34560, 540, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 34560, 34560, 540, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 34560, 1080, 1080, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 1080, 540, 34560, 1080, 1080, 540, 34560, 34560, 34560, 540, 1080, 34560, 540, 34560, 1080, 1080, 34560, 34560, 540, 1080, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 34560, 540, 1080, 1080, 540, 540, 1080, 540, 34560, 34560, 540, 34560, 34560, 540, 1080, 34560, 540, 540, 1080, 1080, 540, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 1080, 540, 34560, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 540, 34560, 34560, 1080, 540, 34560, 1080, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 1080, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 34560, 540, 540, 540, 1080, 540, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 1080, 1080, 1080, 540, 1080, 540, 540, 34560, 540, 1080, 540, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 4631040 . Total input tokens: 1033253728 . Total output tokens: 909312487
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.5244916412048,
    "estimated_duration": 3600.0605964362317,
    "input_throughput": 6702.246629927634,
    "output_throughput": 5829.784926613736,
    "total_throughput": 12532.03155654137,
    "itl": 83.20011085714675,
    "ttft": 2081135.7102242114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.023366942252069,
    "arrivals": 1542191,
    "finished_requests": 97608,
    "scheduler_time": 329.3142068353308
}
#Debug simulation 
Total elapsed time: 80.52468880778179. Arrivals time: 0.5366443679668009 Scheduler time: 79.75471121678129 Scheduler overhead time: 0.09138662507757545 Adapter cache time: 0.0197456912137568 Engine time: 0.08731247251853347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 1080, 540, 34560, 34560, 540, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 540, 540, 1080, 540, 34560, 540, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 34560, 34560, 540, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 34560, 1080, 1080, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 1080, 540, 34560, 1080, 1080, 540, 34560, 34560, 34560, 540, 1080, 34560, 540, 34560, 1080, 1080, 34560, 34560, 540, 1080, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 34560, 540, 1080, 1080, 540, 540, 1080, 540, 34560, 34560, 540, 34560, 34560, 540, 1080, 34560, 540, 540, 1080, 1080, 540, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 1080, 540, 34560, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 540, 34560, 34560, 1080, 540, 34560, 1080, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 1080, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 34560, 540, 540, 540, 1080, 540, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 1080, 1080, 1080, 540, 1080, 540, 540, 34560, 540, 1080, 540, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 4631040 . Total input tokens: 1033253728 . Total output tokens: 909312487
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.55801997799426,
    "estimated_duration": 3600.1054326534004,
    "input_throughput": 6798.335898168763,
    "output_throughput": 5912.009078109991,
    "total_throughput": 12710.344976278755,
    "itl": 84.90240610874518,
    "ttft": 2082020.7814633427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.710522108911531,
    "arrivals": 1542191,
    "finished_requests": 98977,
    "scheduler_time": 325.59896913226737
}
#Debug simulation 
Total elapsed time: 81.55821935320273. Arrivals time: 0.5221848981454968 Scheduler time: 80.80799918947741 Scheduler overhead time: 0.0892882808111608 Adapter cache time: 0.019536703824996948 Engine time: 0.08497713645920157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 1080, 540, 34560, 34560, 540, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 540, 540, 1080, 540, 34560, 540, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 34560, 34560, 540, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 34560, 1080, 1080, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 1080, 540, 34560, 1080, 1080, 540, 34560, 34560, 34560, 540, 1080, 34560, 540, 34560, 1080, 1080, 34560, 34560, 540, 1080, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 34560, 540, 1080, 1080, 540, 540, 1080, 540, 34560, 34560, 540, 34560, 34560, 540, 1080, 34560, 540, 540, 1080, 1080, 540, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 1080, 540, 34560, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 540, 34560, 34560, 1080, 540, 34560, 1080, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 1080, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 34560, 540, 540, 540, 1080, 540, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 1080, 1080, 1080, 540, 1080, 540, 540, 34560, 540, 1080, 540, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 4631040 . Total input tokens: 1033253728 . Total output tokens: 909312487
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 79.553878209088,
    "estimated_duration": 3600.0333844633074,
    "input_throughput": 6803.709683834363,
    "output_throughput": 5915.24820072597,
    "total_throughput": 12718.957884560332,
    "itl": 83.67377841762847,
    "ttft": 2078859.9569271295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.200979349655135,
    "arrivals": 1542191,
    "finished_requests": 99106,
    "scheduler_time": 324.71805701243767
}
#Debug simulation 
Total elapsed time: 79.55407964996994. Arrivals time: 0.6276080599054694 Scheduler time: 78.69651066930965 Scheduler overhead time: 0.09032432967796922 Adapter cache time: 0.020020716823637486 Engine time: 0.08557846676558256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 1080, 540, 34560, 34560, 540, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 540, 540, 1080, 540, 34560, 540, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 34560, 34560, 540, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 34560, 1080, 1080, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 1080, 540, 34560, 1080, 1080, 540, 34560, 34560, 34560, 540, 1080, 34560, 540, 34560, 1080, 1080, 34560, 34560, 540, 1080, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 34560, 540, 1080, 1080, 540, 540, 1080, 540, 34560, 34560, 540, 34560, 34560, 540, 1080, 34560, 540, 540, 1080, 1080, 540, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 1080, 540, 34560, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 540, 34560, 34560, 1080, 540, 34560, 1080, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 1080, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 34560, 540, 540, 540, 1080, 540, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 1080, 1080, 1080, 540, 1080, 540, 540, 34560, 540, 1080, 540, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 4631040 . Total input tokens: 1033253728 . Total output tokens: 909312487
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.92951284302399,
    "estimated_duration": 3600.0389856076827,
    "input_throughput": 6865.873702705952,
    "output_throughput": 5981.783276817758,
    "total_throughput": 12847.65697952371,
    "itl": 85.73265175958261,
    "ttft": 2067735.1465226817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4345494660269313,
    "arrivals": 1542191,
    "finished_requests": 100057,
    "scheduler_time": 321.68424942033
}
#Debug simulation 
Total elapsed time: 81.92969676479697. Arrivals time: 0.5417201858945191 Scheduler time: 81.15939631685615 Scheduler overhead time: 0.08948021801188588 Adapter cache time: 0.019605213310569525 Engine time: 0.08546946849673986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 1080, 540, 34560, 34560, 540, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 540, 540, 1080, 540, 34560, 540, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 34560, 34560, 540, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 1080, 540, 540, 540, 1080, 540, 1080, 540, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 34560, 1080, 1080, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 540, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 1080, 540, 34560, 1080, 1080, 540, 34560, 34560, 34560, 540, 1080, 34560, 540, 34560, 1080, 1080, 34560, 34560, 540, 1080, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 1080, 1080, 1080, 540, 540, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 540, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 540, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 34560, 540, 1080, 1080, 540, 540, 1080, 540, 34560, 34560, 540, 34560, 34560, 540, 1080, 34560, 540, 540, 1080, 1080, 540, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 540, 540, 540, 540, 540, 540, 540, 540, 1080, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 1080, 540, 34560, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 540, 34560, 34560, 1080, 540, 34560, 1080, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 1080, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 34560, 540, 540, 540, 1080, 540, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 540, 1080, 1080, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 1080, 1080, 1080, 540, 1080, 540, 540, 34560, 540, 1080, 540, 540, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 4631040 . Total input tokens: 1033253728 . Total output tokens: 909312487
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.80432563088834,
    "estimated_duration": 3600.0440025225816,
    "input_throughput": 6821.531343170286,
    "output_throughput": 5922.889827196286,
    "total_throughput": 12744.421170366573,
    "itl": 84.06838759412513,
    "ttft": 2077912.5816682894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.025785514246708,
    "arrivals": 1542191,
    "finished_requests": 99316,
    "scheduler_time": 324.33734289873667
}
#Debug simulation 
Total elapsed time: 81.80451446399093. Arrivals time: 0.5174411050975323 Scheduler time: 81.05576635710895 Scheduler overhead time: 0.09057638607919216 Adapter cache time: 0.01938203303143382 Engine time: 0.08666723733767867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 1080, 270, 34560, 34560, 270, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 270, 270, 1080, 270, 34560, 270, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 34560, 34560, 270, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 34560, 1080, 1080, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 1080, 270, 34560, 1080, 1080, 270, 34560, 34560, 34560, 270, 1080, 34560, 270, 34560, 1080, 1080, 34560, 34560, 270, 1080, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 34560, 270, 1080, 1080, 270, 270, 1080, 270, 34560, 34560, 270, 34560, 34560, 270, 1080, 34560, 270, 270, 1080, 1080, 270, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 1080, 270, 34560, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 270, 34560, 34560, 1080, 270, 34560, 1080, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 1080, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 34560, 270, 270, 270, 1080, 270, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 1080, 1080, 1080, 270, 1080, 270, 270, 34560, 270, 1080, 270, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4596480 . Total input tokens: 1025578370 . Total output tokens: 902576643
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.20442855684087,
    "estimated_duration": 3600.0058943772615,
    "input_throughput": 6926.056437558451,
    "output_throughput": 6018.899589537534,
    "total_throughput": 12944.956027095985,
    "itl": 86.8188462438572,
    "ttft": 2044209.774951433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.099693599809026,
    "arrivals": 1530695,
    "finished_requests": 100565,
    "scheduler_time": 320.7908938931996
}
#Debug simulation 
Total elapsed time: 83.20462716091424. Arrivals time: 0.5181386643089354 Scheduler time: 82.45942657859996 Scheduler overhead time: 0.08848705235868692 Adapter cache time: 0.019834520760923624 Engine time: 0.08525751903653145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 1080, 270, 34560, 34560, 270, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 270, 270, 1080, 270, 34560, 270, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 34560, 34560, 270, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 34560, 1080, 1080, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 1080, 270, 34560, 1080, 1080, 270, 34560, 34560, 34560, 270, 1080, 34560, 270, 34560, 1080, 1080, 34560, 34560, 270, 1080, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 34560, 270, 1080, 1080, 270, 270, 1080, 270, 34560, 34560, 270, 34560, 34560, 270, 1080, 34560, 270, 270, 1080, 1080, 270, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 1080, 270, 34560, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 270, 34560, 34560, 1080, 270, 34560, 1080, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 1080, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 34560, 270, 270, 270, 1080, 270, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 1080, 1080, 1080, 270, 1080, 270, 270, 34560, 270, 1080, 270, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4596480 . Total input tokens: 1025578370 . Total output tokens: 902576643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.32751813484356,
    "estimated_duration": 3600.0233488855793,
    "input_throughput": 7003.862629892456,
    "output_throughput": 6084.389704522491,
    "total_throughput": 13088.252334414947,
    "itl": 86.82324939288725,
    "ttft": 2051299.4382730527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.432830989076763,
    "arrivals": 1530695,
    "finished_requests": 101691,
    "scheduler_time": 317.1846381471632
}
#Debug simulation 
Total elapsed time: 82.32770112901926. Arrivals time: 0.5526944193989038 Scheduler time: 81.54837176389992 Scheduler overhead time: 0.08845562441274524 Adapter cache time: 0.02002008305862546 Engine time: 0.08471328765153885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 1080, 270, 34560, 34560, 270, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 270, 270, 1080, 270, 34560, 270, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 34560, 34560, 270, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 34560, 1080, 1080, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 1080, 270, 34560, 1080, 1080, 270, 34560, 34560, 34560, 270, 1080, 34560, 270, 34560, 1080, 1080, 34560, 34560, 270, 1080, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 34560, 270, 1080, 1080, 270, 270, 1080, 270, 34560, 34560, 270, 34560, 34560, 270, 1080, 34560, 270, 270, 1080, 1080, 270, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 1080, 270, 34560, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 270, 34560, 34560, 1080, 270, 34560, 1080, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 1080, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 34560, 270, 270, 270, 1080, 270, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 1080, 1080, 1080, 270, 1080, 270, 270, 34560, 270, 1080, 270, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4596480 . Total input tokens: 1025578370 . Total output tokens: 902576643
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.1222998611629,
    "estimated_duration": 3600.0386406556295,
    "input_throughput": 6924.802339193752,
    "output_throughput": 5995.819532667284,
    "total_throughput": 12920.621871861036,
    "itl": 84.92129329135489,
    "ttft": 2055362.954755277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.708734233812459,
    "arrivals": 1530695,
    "finished_requests": 100583,
    "scheduler_time": 320.6794797045037
}
#Debug simulation 
Total elapsed time: 83.12250246899202. Arrivals time: 0.5256399037316442 Scheduler time: 82.36510195722803 Scheduler overhead time: 0.09005912905558944 Adapter cache time: 0.02023816527798772 Engine time: 0.08687538048252463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 1080, 270, 34560, 34560, 270, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 270, 270, 1080, 270, 34560, 270, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 34560, 34560, 270, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 34560, 1080, 1080, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 1080, 270, 34560, 1080, 1080, 270, 34560, 34560, 34560, 270, 1080, 34560, 270, 34560, 1080, 1080, 34560, 34560, 270, 1080, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 34560, 270, 1080, 1080, 270, 270, 1080, 270, 34560, 34560, 270, 34560, 34560, 270, 1080, 34560, 270, 270, 1080, 1080, 270, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 1080, 270, 34560, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 270, 34560, 34560, 1080, 270, 34560, 1080, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 1080, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 34560, 270, 270, 270, 1080, 270, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 1080, 1080, 1080, 270, 1080, 270, 270, 34560, 270, 1080, 270, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4596480 . Total input tokens: 1025578370 . Total output tokens: 902576643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 82.5188762480393,
    "estimated_duration": 3600.037792631725,
    "input_throughput": 7004.083138129275,
    "output_throughput": 6084.64640144427,
    "total_throughput": 13088.729539573545,
    "itl": 86.81723455981391,
    "ttft": 2051191.5713491393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.153798573440867,
    "arrivals": 1530695,
    "finished_requests": 101694,
    "scheduler_time": 317.2235895372634
}
#Debug simulation 
Total elapsed time: 82.51906496612355. Arrivals time: 0.5633087940514088 Scheduler time: 81.72906951466575 Scheduler overhead time: 0.08806078601628542 Adapter cache time: 0.019659350160509348 Engine time: 0.0855477056466043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 1080, 270, 34560, 34560, 270, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 270, 270, 1080, 270, 34560, 270, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 34560, 34560, 270, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 34560, 1080, 1080, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 1080, 270, 34560, 1080, 1080, 270, 34560, 34560, 34560, 270, 1080, 34560, 270, 34560, 1080, 1080, 34560, 34560, 270, 1080, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 34560, 270, 1080, 1080, 270, 270, 1080, 270, 34560, 34560, 270, 34560, 34560, 270, 1080, 34560, 270, 270, 1080, 1080, 270, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 1080, 270, 34560, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 270, 34560, 34560, 1080, 270, 34560, 1080, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 1080, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 34560, 270, 270, 270, 1080, 270, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 1080, 1080, 1080, 270, 1080, 270, 270, 34560, 270, 1080, 270, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4596480 . Total input tokens: 1025578370 . Total output tokens: 902576643
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 82.7823994839564,
    "estimated_duration": 3600.0554804900858,
    "input_throughput": 6960.734115294425,
    "output_throughput": 6032.003428192668,
    "total_throughput": 12992.737543487092,
    "itl": 84.5829428054521,
    "ttft": 2063987.5719212424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.270814024512689,
    "arrivals": 1530695,
    "finished_requests": 101132,
    "scheduler_time": 318.6268067182748
}
#Debug simulation 
Total elapsed time: 82.7826002817601. Arrivals time: 0.5410259729251266 Scheduler time: 82.01014105789363 Scheduler overhead time: 0.08895541494712234 Adapter cache time: 0.022250340320169926 Engine time: 0.08632528828456998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 1080, 270, 34560, 34560, 270, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 270, 270, 1080, 270, 34560, 270, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 34560, 34560, 270, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 34560, 1080, 1080, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 1080, 270, 34560, 1080, 1080, 270, 34560, 34560, 34560, 270, 1080, 34560, 270, 34560, 1080, 1080, 34560, 34560, 270, 1080, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 34560, 270, 1080, 1080, 270, 270, 1080, 270, 34560, 34560, 270, 34560, 34560, 270, 1080, 34560, 270, 270, 1080, 1080, 270, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 1080, 270, 34560, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 270, 34560, 34560, 1080, 270, 34560, 1080, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 1080, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 34560, 270, 270, 270, 1080, 270, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 1080, 1080, 1080, 270, 1080, 270, 270, 34560, 270, 1080, 270, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4596480 . Total input tokens: 1025578370 . Total output tokens: 902576643
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.59478196501732,
    "estimated_duration": 3600.0329683935865,
    "input_throughput": 7004.584741692591,
    "output_throughput": 6085.053162658983,
    "total_throughput": 13089.637904351574,
    "itl": 86.80665758364121,
    "ttft": 2051065.1288155296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8622721690451547,
    "arrivals": 1530695,
    "finished_requests": 101702,
    "scheduler_time": 317.26603527596484
}
#Debug simulation 
Total elapsed time: 82.59497503424063. Arrivals time: 0.5660178326070309 Scheduler time: 81.79992911359295 Scheduler overhead time: 0.09039551205933094 Adapter cache time: 0.01976517215371132 Engine time: 0.08521731430664659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 1080, 270, 34560, 34560, 270, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 270, 270, 1080, 270, 34560, 270, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 34560, 34560, 270, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 1080, 270, 270, 270, 1080, 270, 1080, 270, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 34560, 1080, 1080, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 270, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 1080, 270, 34560, 1080, 1080, 270, 34560, 34560, 34560, 270, 1080, 34560, 270, 34560, 1080, 1080, 34560, 34560, 270, 1080, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 1080, 1080, 1080, 270, 270, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 270, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 270, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 34560, 270, 1080, 1080, 270, 270, 1080, 270, 34560, 34560, 270, 34560, 34560, 270, 1080, 34560, 270, 270, 1080, 1080, 270, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 270, 270, 270, 270, 270, 270, 270, 270, 1080, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 1080, 270, 34560, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 270, 34560, 34560, 1080, 270, 34560, 1080, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 1080, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 34560, 270, 270, 270, 1080, 270, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 270, 1080, 1080, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 1080, 1080, 1080, 270, 1080, 270, 270, 34560, 270, 1080, 270, 270, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4596480 . Total input tokens: 1025578370 . Total output tokens: 902576643
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.69130983715877,
    "estimated_duration": 3600.0315022488494,
    "input_throughput": 7009.431718649363,
    "output_throughput": 6079.190414397434,
    "total_throughput": 13088.622133046796,
    "itl": 85.36276751389838,
    "ttft": 2058497.462813804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.344628670886193,
    "arrivals": 1530695,
    "finished_requests": 101879,
    "scheduler_time": 316.7250395397441
}
#Debug simulation 
Total elapsed time: 81.69150365702808. Arrivals time: 0.6677020890638232 Scheduler time: 80.79569776169956 Scheduler overhead time: 0.08866287348791957 Adapter cache time: 0.02015226474031806 Engine time: 0.08575134444981813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 1080, 135, 34560, 34560, 135, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 135, 135, 1080, 135, 34560, 135, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 34560, 34560, 135, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 34560, 1080, 1080, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 1080, 135, 34560, 1080, 1080, 135, 34560, 34560, 34560, 135, 1080, 34560, 135, 34560, 1080, 1080, 34560, 34560, 135, 1080, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 34560, 135, 1080, 1080, 135, 135, 1080, 135, 34560, 34560, 135, 34560, 34560, 135, 1080, 34560, 135, 135, 1080, 1080, 135, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 1080, 135, 34560, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 135, 34560, 34560, 1080, 135, 34560, 1080, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 1080, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 34560, 135, 135, 135, 1080, 135, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 1080, 1080, 1080, 135, 1080, 135, 135, 34560, 135, 1080, 135, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4579200 . Total input tokens: 1021748362 . Total output tokens: 899193034
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.69930795719847,
    "estimated_duration": 3600.0292023207876,
    "input_throughput": 6910.2272792572985,
    "output_throughput": 6011.156238968668,
    "total_throughput": 12921.383518225966,
    "itl": 86.98953109983343,
    "ttft": 2073942.0426200645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.438452696614011,
    "arrivals": 1525074,
    "finished_requests": 100219,
    "scheduler_time": 320.4190735950478
}
#Debug simulation 
Total elapsed time: 86.699494688306. Arrivals time: 0.5294677424244583 Scheduler time: 85.94043619930744 Scheduler overhead time: 0.09030188806355 Adapter cache time: 0.01935915369540453 Engine time: 0.08597332332283258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 1080, 135, 34560, 34560, 135, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 135, 135, 1080, 135, 34560, 135, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 34560, 34560, 135, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 34560, 1080, 1080, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 1080, 135, 34560, 1080, 1080, 135, 34560, 34560, 34560, 135, 1080, 34560, 135, 34560, 1080, 1080, 34560, 34560, 135, 1080, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 34560, 135, 1080, 1080, 135, 135, 1080, 135, 34560, 34560, 135, 34560, 34560, 135, 1080, 34560, 135, 135, 1080, 1080, 135, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 1080, 135, 34560, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 135, 34560, 34560, 1080, 135, 34560, 1080, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 1080, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 34560, 135, 135, 135, 1080, 135, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 1080, 1080, 1080, 135, 1080, 135, 135, 34560, 135, 1080, 135, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4579200 . Total input tokens: 1021748362 . Total output tokens: 899193034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.46944056358188,
    "estimated_duration": 3600.0667937010803,
    "input_throughput": 6973.825331220955,
    "output_throughput": 6037.757976610698,
    "total_throughput": 13011.583307831654,
    "itl": 85.75545897743112,
    "ttft": 2074633.8049766254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9440265632327702,
    "arrivals": 1525074,
    "finished_requests": 100957,
    "scheduler_time": 318.0754273088743
}
#Debug simulation 
Total elapsed time: 81.46963839558885. Arrivals time: 0.5570932067930698 Scheduler time: 80.68707000184804 Scheduler overhead time: 0.08745910180732608 Adapter cache time: 0.01975495321676135 Engine time: 0.08460007980465889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 1080, 135, 34560, 34560, 135, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 135, 135, 1080, 135, 34560, 135, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 34560, 34560, 135, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 34560, 1080, 1080, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 1080, 135, 34560, 1080, 1080, 135, 34560, 34560, 34560, 135, 1080, 34560, 135, 34560, 1080, 1080, 34560, 34560, 135, 1080, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 34560, 135, 1080, 1080, 135, 135, 1080, 135, 34560, 34560, 135, 34560, 34560, 135, 1080, 34560, 135, 135, 1080, 1080, 135, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 1080, 135, 34560, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 135, 34560, 34560, 1080, 135, 34560, 1080, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 1080, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 34560, 135, 135, 135, 1080, 135, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 1080, 1080, 1080, 135, 1080, 135, 135, 34560, 135, 1080, 135, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4579200 . Total input tokens: 1021748362 . Total output tokens: 899193034
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.70988502912223,
    "estimated_duration": 3600.0087215944327,
    "input_throughput": 6908.709651398991,
    "output_throughput": 5992.509926599663,
    "total_throughput": 12901.219577998654,
    "itl": 84.11609938208848,
    "ttft": 2074202.343554075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.051480733761597,
    "arrivals": 1525074,
    "finished_requests": 100114,
    "scheduler_time": 320.57667937348907
}
#Debug simulation 
Total elapsed time: 79.71010257815942. Arrivals time: 0.5275366781279445 Scheduler time: 78.9549088026397 Scheduler overhead time: 0.08890919294208288 Adapter cache time: 0.019451530184596777 Engine time: 0.08532478753477335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 1080, 135, 34560, 34560, 135, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 135, 135, 1080, 135, 34560, 135, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 34560, 34560, 135, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 34560, 1080, 1080, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 1080, 135, 34560, 1080, 1080, 135, 34560, 34560, 34560, 135, 1080, 34560, 135, 34560, 1080, 1080, 34560, 34560, 135, 1080, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 34560, 135, 1080, 1080, 135, 135, 1080, 135, 34560, 34560, 135, 34560, 34560, 135, 1080, 34560, 135, 135, 1080, 1080, 135, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 1080, 135, 34560, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 135, 34560, 34560, 1080, 135, 34560, 1080, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 1080, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 34560, 135, 135, 135, 1080, 135, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 1080, 1080, 1080, 135, 1080, 135, 135, 34560, 135, 1080, 135, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4579200 . Total input tokens: 1021748362 . Total output tokens: 899193034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 82.80308281909674,
    "estimated_duration": 3600.020256854708,
    "input_throughput": 6842.2226105807495,
    "output_throughput": 5933.118003802953,
    "total_throughput": 12775.340614383704,
    "itl": 84.81207746190718,
    "ttft": 2080447.945988235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5453712015040177,
    "arrivals": 1525074,
    "finished_requests": 99145,
    "scheduler_time": 323.73647054875687
}
#Debug simulation 
Total elapsed time: 82.80328608723357. Arrivals time: 0.551683543715626 Scheduler time: 82.02205732138827 Scheduler overhead time: 0.09031672729179263 Adapter cache time: 0.019509383011609316 Engine time: 0.08586899610236287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 1080, 135, 34560, 34560, 135, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 135, 135, 1080, 135, 34560, 135, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 34560, 34560, 135, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 34560, 1080, 1080, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 1080, 135, 34560, 1080, 1080, 135, 34560, 34560, 34560, 135, 1080, 34560, 135, 34560, 1080, 1080, 34560, 34560, 135, 1080, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 34560, 135, 1080, 1080, 135, 135, 1080, 135, 34560, 34560, 135, 34560, 34560, 135, 1080, 34560, 135, 135, 1080, 1080, 135, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 1080, 135, 34560, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 135, 34560, 34560, 1080, 135, 34560, 1080, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 1080, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 34560, 135, 135, 135, 1080, 135, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 1080, 1080, 1080, 135, 1080, 135, 135, 34560, 135, 1080, 135, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4579200 . Total input tokens: 1021748362 . Total output tokens: 899193034
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 80.61050437903032,
    "estimated_duration": 3600.0014027083485,
    "input_throughput": 6938.384518741697,
    "output_throughput": 6044.569589231049,
    "total_throughput": 12982.954107972746,
    "itl": 84.47394804177043,
    "ttft": 2062026.6194012584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.14801969702359,
    "arrivals": 1525074,
    "finished_requests": 100485,
    "scheduler_time": 319.25715335812237
}
#Debug simulation 
Total elapsed time: 80.61069417884573. Arrivals time: 0.5360550014302135 Scheduler time: 79.84590174816549 Scheduler overhead time: 0.0888208388350904 Adapter cache time: 0.019809842109680176 Engine time: 0.08596600638702512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 1080, 135, 34560, 34560, 135, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 135, 135, 1080, 135, 34560, 135, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 34560, 34560, 135, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 34560, 1080, 1080, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 1080, 135, 34560, 1080, 1080, 135, 34560, 34560, 34560, 135, 1080, 34560, 135, 34560, 1080, 1080, 34560, 34560, 135, 1080, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 34560, 135, 1080, 1080, 135, 135, 1080, 135, 34560, 34560, 135, 34560, 34560, 135, 1080, 34560, 135, 135, 1080, 1080, 135, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 1080, 135, 34560, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 135, 34560, 34560, 1080, 135, 34560, 1080, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 1080, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 34560, 135, 135, 135, 1080, 135, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 1080, 1080, 1080, 135, 1080, 135, 135, 34560, 135, 1080, 135, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4579200 . Total input tokens: 1021748362 . Total output tokens: 899193034
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.77235684311017,
    "estimated_duration": 3600.0348392183573,
    "input_throughput": 6890.020265855409,
    "output_throughput": 6010.489055349824,
    "total_throughput": 12900.509321205232,
    "itl": 86.0796758843678,
    "ttft": 2056965.3395819063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.568611805778912,
    "arrivals": 1525074,
    "finished_requests": 99857,
    "scheduler_time": 321.24094113546806
}
#Debug simulation 
Total elapsed time: 82.77254529902712. Arrivals time: 0.5451706987805665 Scheduler time: 82.00055571645498 Scheduler overhead time: 0.088252326939255 Adapter cache time: 0.0197554980404675 Engine time: 0.08501419005915523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 1080, 135, 34560, 34560, 135, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 135, 135, 1080, 135, 34560, 135, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 34560, 34560, 135, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 1080, 135, 135, 135, 1080, 135, 1080, 135, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 34560, 1080, 1080, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 135, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 1080, 135, 34560, 1080, 1080, 135, 34560, 34560, 34560, 135, 1080, 34560, 135, 34560, 1080, 1080, 34560, 34560, 135, 1080, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 1080, 1080, 1080, 135, 135, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 135, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 135, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 34560, 135, 1080, 1080, 135, 135, 1080, 135, 34560, 34560, 135, 34560, 34560, 135, 1080, 34560, 135, 135, 1080, 1080, 135, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 135, 135, 135, 135, 135, 135, 135, 135, 1080, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 1080, 135, 34560, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 135, 34560, 34560, 1080, 135, 34560, 1080, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 1080, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 135, 1080, 1080, 135, 34560, 135, 135, 135, 1080, 135, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 135, 1080, 1080, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 1080, 1080, 1080, 135, 1080, 135, 135, 34560, 135, 1080, 135, 135, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4579200 . Total input tokens: 1021748362 . Total output tokens: 899193034
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.54281128011644,
    "estimated_duration": 3600.0356576883873,
    "input_throughput": 6768.837677470931,
    "output_throughput": 5888.817782880702,
    "total_throughput": 12657.655460351634,
    "itl": 83.77947656876428,
    "ttft": 2066142.7963724127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 571,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.216863172408226,
    "arrivals": 1525074,
    "finished_requests": 98067,
    "scheduler_time": 327.0607781747269
}
#Debug simulation 
Total elapsed time: 80.54301109630615. Arrivals time: 0.9715240527875721 Scheduler time: 79.3396376655437 Scheduler overhead time: 0.09025319712236524 Adapter cache time: 0.02026661392301321 Engine time: 0.08663360308855772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 1080, 66, 34560, 34560, 66, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 66, 66, 1080, 66, 34560, 66, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 34560, 34560, 66, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 34560, 1080, 1080, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 1080, 66, 34560, 1080, 1080, 66, 34560, 34560, 34560, 66, 1080, 34560, 66, 34560, 1080, 1080, 34560, 34560, 66, 1080, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 34560, 66, 1080, 1080, 66, 66, 1080, 66, 34560, 34560, 66, 34560, 34560, 66, 1080, 34560, 66, 66, 1080, 1080, 66, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 1080, 66, 34560, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 66, 34560, 34560, 1080, 66, 34560, 1080, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 1080, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 34560, 66, 66, 66, 1080, 66, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 1080, 1080, 1080, 66, 1080, 66, 66, 34560, 66, 1080, 66, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4570368 . Total input tokens: 1019791627 . Total output tokens: 897472288
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.81082340935245,
    "estimated_duration": 3600.0518987710616,
    "input_throughput": 7042.221254825369,
    "output_throughput": 6132.863253315018,
    "total_throughput": 13175.084508140388,
    "itl": 87.60220357576378,
    "ttft": 2065080.4128053337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3591037882306094,
    "arrivals": 1522091,
    "finished_requests": 102368,
    "scheduler_time": 313.39730123536157
}
#Debug simulation 
Total elapsed time: 83.81102796830237. Arrivals time: 0.5407532141543925 Scheduler time: 83.04587912000716 Scheduler overhead time: 0.0879051280207932 Adapter cache time: 0.018714148551225662 Engine time: 0.0842208000831306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 1080, 66, 34560, 34560, 66, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 66, 66, 1080, 66, 34560, 66, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 34560, 34560, 66, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 34560, 1080, 1080, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 1080, 66, 34560, 1080, 1080, 66, 34560, 34560, 34560, 66, 1080, 34560, 66, 34560, 1080, 1080, 34560, 34560, 66, 1080, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 34560, 66, 1080, 1080, 66, 66, 1080, 66, 34560, 34560, 66, 34560, 34560, 66, 1080, 34560, 66, 66, 1080, 1080, 66, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 1080, 66, 34560, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 66, 34560, 34560, 1080, 66, 34560, 1080, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 1080, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 34560, 66, 66, 66, 1080, 66, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 1080, 1080, 1080, 66, 1080, 66, 66, 34560, 66, 1080, 66, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4570368 . Total input tokens: 1019791627 . Total output tokens: 897472288
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 86.63309238199145,
    "estimated_duration": 3600.0994515848815,
    "input_throughput": 6875.057295737719,
    "output_throughput": 5983.183878577955,
    "total_throughput": 12858.241174315674,
    "itl": 85.10439552287238,
    "ttft": 2070273.8106912938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4893610001122624,
    "arrivals": 1522091,
    "finished_requests": 100017,
    "scheduler_time": 321.21661278018223
}
#Debug simulation 
Total elapsed time: 86.6333068520762. Arrivals time: 0.9869128623977304 Scheduler time: 85.4178914828226 Scheduler overhead time: 0.08965943707153201 Adapter cache time: 0.019292532466351986 Engine time: 0.08537443540990353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 1080, 66, 34560, 34560, 66, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 66, 66, 1080, 66, 34560, 66, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 34560, 34560, 66, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 34560, 1080, 1080, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 1080, 66, 34560, 1080, 1080, 66, 34560, 34560, 34560, 66, 1080, 34560, 66, 34560, 1080, 1080, 34560, 34560, 66, 1080, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 34560, 66, 1080, 1080, 66, 66, 1080, 66, 34560, 34560, 66, 34560, 34560, 66, 1080, 34560, 66, 66, 1080, 1080, 66, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 1080, 66, 34560, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 66, 34560, 34560, 1080, 66, 34560, 1080, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 1080, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 34560, 66, 66, 66, 1080, 66, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 1080, 1080, 1080, 66, 1080, 66, 66, 34560, 66, 1080, 66, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4570368 . Total input tokens: 1019791627 . Total output tokens: 897472288
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.80645008804277,
    "estimated_duration": 3600.0956210999884,
    "input_throughput": 6867.737027619719,
    "output_throughput": 5975.200179107284,
    "total_throughput": 12842.937206727003,
    "itl": 83.96953018758558,
    "ttft": 2072778.8513363285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6481450236775275,
    "arrivals": 1522091,
    "finished_requests": 99881,
    "scheduler_time": 321.6295157853891
}
#Debug simulation 
Total elapsed time: 83.80666283797473. Arrivals time: 0.5356007488444448 Scheduler time: 83.03985733073205 Scheduler overhead time: 0.0914100706577301 Adapter cache time: 0.019732782617211342 Engine time: 0.0861199707724154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 1080, 66, 34560, 34560, 66, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 66, 66, 1080, 66, 34560, 66, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 34560, 34560, 66, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 34560, 1080, 1080, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 1080, 66, 34560, 1080, 1080, 66, 34560, 34560, 34560, 66, 1080, 34560, 66, 34560, 1080, 1080, 34560, 34560, 66, 1080, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 34560, 66, 1080, 1080, 66, 66, 1080, 66, 34560, 34560, 66, 34560, 34560, 66, 1080, 34560, 66, 66, 1080, 1080, 66, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 1080, 66, 34560, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 66, 34560, 34560, 1080, 66, 34560, 1080, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 1080, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 34560, 66, 66, 66, 1080, 66, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 1080, 1080, 1080, 66, 1080, 66, 66, 34560, 66, 1080, 66, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4570368 . Total input tokens: 1019791627 . Total output tokens: 897472288
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 86.41653582826257,
    "estimated_duration": 3600.013998856126,
    "input_throughput": 6875.243820680812,
    "output_throughput": 5983.357566621743,
    "total_throughput": 12858.601387302555,
    "itl": 85.10064615848793,
    "ttft": 2070279.8257548262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.270022086328822,
    "arrivals": 1522091,
    "finished_requests": 100019,
    "scheduler_time": 321.22799214612536
}
#Debug simulation 
Total elapsed time: 86.41674193320796. Arrivals time: 0.5492849526926875 Scheduler time: 85.63848227355629 Scheduler overhead time: 0.08914043940603733 Adapter cache time: 0.019536414183676243 Engine time: 0.08627670165151358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 1080, 66, 34560, 34560, 66, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 66, 66, 1080, 66, 34560, 66, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 34560, 34560, 66, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 34560, 1080, 1080, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 1080, 66, 34560, 1080, 1080, 66, 34560, 34560, 34560, 66, 1080, 34560, 66, 34560, 1080, 1080, 34560, 34560, 66, 1080, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 34560, 66, 1080, 1080, 66, 66, 1080, 66, 34560, 34560, 66, 34560, 34560, 66, 1080, 34560, 66, 66, 1080, 1080, 66, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 1080, 66, 34560, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 66, 34560, 34560, 1080, 66, 34560, 1080, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 1080, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 34560, 66, 66, 66, 1080, 66, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 1080, 1080, 1080, 66, 1080, 66, 66, 34560, 66, 1080, 66, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4570368 . Total input tokens: 1019791627 . Total output tokens: 897472288
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 83.91477619297802,
    "estimated_duration": 3600.086182108534,
    "input_throughput": 6867.755033997299,
    "output_throughput": 5975.215845360973,
    "total_throughput": 12842.97087935827,
    "itl": 83.96817803785432,
    "ttft": 2072783.582872551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6150062754191747,
    "arrivals": 1522091,
    "finished_requests": 99881,
    "scheduler_time": 321.6342349833517
}
#Debug simulation 
Total elapsed time: 83.91498189419508. Arrivals time: 0.5411919825710356 Scheduler time: 83.14337721513584 Scheduler overhead time: 0.08994270209223032 Adapter cache time: 0.01962444232776761 Engine time: 0.08669026289135218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 1080, 66, 34560, 34560, 66, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 66, 66, 1080, 66, 34560, 66, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 34560, 34560, 66, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 34560, 1080, 1080, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 1080, 66, 34560, 1080, 1080, 66, 34560, 34560, 34560, 66, 1080, 34560, 66, 34560, 1080, 1080, 34560, 34560, 66, 1080, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 34560, 66, 1080, 1080, 66, 66, 1080, 66, 34560, 34560, 66, 34560, 34560, 66, 1080, 34560, 66, 66, 1080, 1080, 66, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 1080, 66, 34560, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 66, 34560, 34560, 1080, 66, 34560, 1080, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 1080, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 34560, 66, 66, 66, 1080, 66, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 1080, 1080, 1080, 66, 1080, 66, 66, 34560, 66, 1080, 66, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4570368 . Total input tokens: 1019791627 . Total output tokens: 897472288
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 86.4579185009934,
    "estimated_duration": 3600.0872234978933,
    "input_throughput": 6875.729798554556,
    "output_throughput": 5983.913350596242,
    "total_throughput": 12859.643149150797,
    "itl": 85.09755238019837,
    "ttft": 2070210.0155158201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.045130288652131,
    "arrivals": 1522091,
    "finished_requests": 100030,
    "scheduler_time": 321.25082460125583
}
#Debug simulation 
Total elapsed time: 86.45813579764217. Arrivals time: 0.9542855634354055 Scheduler time: 85.27658559754491 Scheduler overhead time: 0.08928206469863653 Adapter cache time: 0.018635811749845743 Engine time: 0.08511934289708734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 1080, 66, 34560, 34560, 66, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 66, 66, 1080, 66, 34560, 66, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 34560, 34560, 66, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 1080, 66, 66, 66, 1080, 66, 1080, 66, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 34560, 1080, 1080, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 66, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 1080, 66, 34560, 1080, 1080, 66, 34560, 34560, 34560, 66, 1080, 34560, 66, 34560, 1080, 1080, 34560, 34560, 66, 1080, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 1080, 1080, 1080, 66, 66, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 66, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 66, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 34560, 66, 1080, 1080, 66, 66, 1080, 66, 34560, 34560, 66, 34560, 34560, 66, 1080, 34560, 66, 66, 1080, 1080, 66, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 66, 66, 66, 66, 66, 66, 66, 66, 1080, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 1080, 66, 34560, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 66, 34560, 34560, 1080, 66, 34560, 1080, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 1080, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 66, 1080, 1080, 66, 34560, 66, 66, 66, 1080, 66, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 66, 1080, 1080, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 1080, 1080, 1080, 66, 1080, 66, 66, 34560, 66, 1080, 66, 66, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4570368 . Total input tokens: 1019791627 . Total output tokens: 897472288
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.93808306287974,
    "estimated_duration": 3600.0298404687273,
    "input_throughput": 6879.343254770206,
    "output_throughput": 5992.790047871107,
    "total_throughput": 12872.133302641314,
    "itl": 84.53111926116448,
    "ttft": 2071423.086380963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.543195966687082,
    "arrivals": 1522091,
    "finished_requests": 100088,
    "scheduler_time": 320.67863028166005
}
#Debug simulation 
Total elapsed time: 82.93829226819798. Arrivals time: 0.5467402571812272 Scheduler time: 82.15882984222844 Scheduler overhead time: 0.08942250348627567 Adapter cache time: 0.019469675607979298 Engine time: 0.08909739693626761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 1080, 33, 34560, 34560, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 1080, 1080, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 1080, 33, 34560, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 34560, 33, 34560, 1080, 1080, 34560, 34560, 33, 1080, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 34560, 33, 1080, 1080, 33, 33, 1080, 33, 34560, 34560, 33, 34560, 34560, 33, 1080, 34560, 33, 33, 1080, 1080, 33, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 1080, 33, 34560, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 33, 34560, 34560, 1080, 33, 34560, 1080, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 1080, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 34560, 33, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 1080, 1080, 1080, 33, 1080, 33, 33, 34560, 33, 1080, 33, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4566144 . Total input tokens: 1018851721 . Total output tokens: 896629367
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 85.27490269998088,
    "estimated_duration": 3600.064018089024,
    "input_throughput": 7035.453778803795,
    "output_throughput": 6149.448701123779,
    "total_throughput": 13184.902479927574,
    "itl": 87.67914844229266,
    "ttft": 2056299.4584024819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0946074269526034,
    "arrivals": 1520700,
    "finished_requests": 102907,
    "scheduler_time": 312.977359802313
}
#Debug simulation 
Total elapsed time: 85.27510577021167. Arrivals time: 0.5409386162646115 Scheduler time: 84.51042662700638 Scheduler overhead time: 0.08739948412403464 Adapter cache time: 0.01882041059434414 Engine time: 0.08423956390470266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 1080, 33, 34560, 34560, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 1080, 1080, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 1080, 33, 34560, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 34560, 33, 34560, 1080, 1080, 34560, 34560, 33, 1080, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 34560, 33, 1080, 1080, 33, 33, 1080, 33, 34560, 34560, 33, 34560, 34560, 33, 1080, 34560, 33, 33, 1080, 1080, 33, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 1080, 33, 34560, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 33, 34560, 34560, 1080, 33, 34560, 1080, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 1080, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 34560, 33, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 1080, 1080, 1080, 33, 1080, 33, 33, 34560, 33, 1080, 33, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4566144 . Total input tokens: 1018851721 . Total output tokens: 896629367
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.04663013434038,
    "estimated_duration": 3600.007154453069,
    "input_throughput": 6967.082542870811,
    "output_throughput": 6099.147878870206,
    "total_throughput": 13066.230421741016,
    "itl": 86.62873392767989,
    "ttft": 2046179.7822448653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5962255820073237,
    "arrivals": 1520700,
    "finished_requests": 101788,
    "scheduler_time": 316.58057580524553
}
#Debug simulation 
Total elapsed time: 83.04682184336707. Arrivals time: 0.5144387292675674 Scheduler time: 82.30445707263425 Scheduler overhead time: 0.0894886776804924 Adapter cache time: 0.018916997127234936 Engine time: 0.08560296753421426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 1080, 33, 34560, 34560, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 1080, 1080, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 1080, 33, 34560, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 34560, 33, 34560, 1080, 1080, 34560, 34560, 33, 1080, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 34560, 33, 1080, 1080, 33, 33, 1080, 33, 34560, 34560, 33, 34560, 34560, 33, 1080, 34560, 33, 33, 1080, 1080, 33, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 1080, 33, 34560, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 33, 34560, 34560, 1080, 33, 34560, 1080, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 1080, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 34560, 33, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 1080, 1080, 1080, 33, 1080, 33, 33, 34560, 33, 1080, 33, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4566144 . Total input tokens: 1018851721 . Total output tokens: 896629367
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.90278141293675,
    "estimated_duration": 3600.029273072039,
    "input_throughput": 6907.241612171861,
    "output_throughput": 6018.458283676972,
    "total_throughput": 12925.699895848833,
    "itl": 84.13743540572925,
    "ttft": 2071565.9626813203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.555585673181369,
    "arrivals": 1520700,
    "finished_requests": 100908,
    "scheduler_time": 319.2794870601936
}
#Debug simulation 
Total elapsed time: 81.90297125279903. Arrivals time: 0.530618742108345 Scheduler time: 81.14400505833328 Scheduler overhead time: 0.08955294732004404 Adapter cache time: 0.019105796236544847 Engine time: 0.08561790501698852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 1080, 33, 34560, 34560, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 1080, 1080, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 1080, 33, 34560, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 34560, 33, 34560, 1080, 1080, 34560, 34560, 33, 1080, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 34560, 33, 1080, 1080, 33, 33, 1080, 33, 34560, 34560, 33, 34560, 34560, 33, 1080, 34560, 33, 33, 1080, 1080, 33, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 1080, 33, 34560, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 33, 34560, 34560, 1080, 33, 34560, 1080, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 1080, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 34560, 33, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 1080, 1080, 1080, 33, 1080, 33, 33, 34560, 33, 1080, 33, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4566144 . Total input tokens: 1018851721 . Total output tokens: 896629367
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 82.94737814832479,
    "estimated_duration": 3600.0127661853035,
    "input_throughput": 6996.199079229482,
    "output_throughput": 6119.2244113464585,
    "total_throughput": 13115.42349057594,
    "itl": 87.01421328989684,
    "ttft": 2048223.0129493587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2622499444149398,
    "arrivals": 1520700,
    "finished_requests": 102165,
    "scheduler_time": 315.4956429704499
}
#Debug simulation 
Total elapsed time: 82.94756818003953. Arrivals time: 0.507837335113436 Scheduler time: 82.21312677953392 Scheduler overhead time: 0.0887396726757288 Adapter cache time: 0.01868123561143875 Engine time: 0.08562322845682502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 1080, 33, 34560, 34560, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 1080, 1080, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 1080, 33, 34560, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 34560, 33, 34560, 1080, 1080, 34560, 34560, 33, 1080, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 34560, 33, 1080, 1080, 33, 33, 1080, 33, 34560, 34560, 33, 34560, 34560, 33, 1080, 34560, 33, 33, 1080, 1080, 33, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 1080, 33, 34560, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 33, 34560, 34560, 1080, 33, 34560, 1080, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 1080, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 34560, 33, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 1080, 1080, 1080, 33, 1080, 33, 33, 34560, 33, 1080, 33, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4566144 . Total input tokens: 1018851721 . Total output tokens: 896629367
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 81.38947916589677,
    "estimated_duration": 3600.0552345076876,
    "input_throughput": 6907.19180129482,
    "output_throughput": 6018.414882171368,
    "total_throughput": 12925.606683466187,
    "itl": 84.13678762493247,
    "ttft": 2071597.9659847636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.522654042099631,
    "arrivals": 1520700,
    "finished_requests": 100908,
    "scheduler_time": 319.2899013621778
}
#Debug simulation 
Total elapsed time: 81.38967494899407. Arrivals time: 0.5244592316448689 Scheduler time: 80.63635511416942 Scheduler overhead time: 0.08949916576966643 Adapter cache time: 0.018843429163098335 Engine time: 0.08609419455751777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 1080, 33, 34560, 34560, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 1080, 1080, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 1080, 33, 34560, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 34560, 33, 34560, 1080, 1080, 34560, 34560, 33, 1080, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 34560, 33, 1080, 1080, 33, 33, 1080, 33, 34560, 34560, 33, 34560, 34560, 33, 1080, 34560, 33, 33, 1080, 1080, 33, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 1080, 33, 34560, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 33, 34560, 34560, 1080, 33, 34560, 1080, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 1080, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 34560, 33, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 1080, 1080, 1080, 33, 1080, 33, 33, 34560, 33, 1080, 33, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4566144 . Total input tokens: 1018851721 . Total output tokens: 896629367
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.28673666808754,
    "estimated_duration": 3600.0114842618395,
    "input_throughput": 6977.53829670089,
    "output_throughput": 6093.053062717563,
    "total_throughput": 13070.591359418453,
    "itl": 86.6397909851324,
    "ttft": 2052186.8638657234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2047283121663934,
    "arrivals": 1520700,
    "finished_requests": 101876,
    "scheduler_time": 316.01163984606796
}
#Debug simulation 
Total elapsed time: 83.28692082921043. Arrivals time: 0.5214931210502982 Scheduler time: 82.53925778158009 Scheduler overhead time: 0.08873708173632622 Adapter cache time: 0.01911634625867009 Engine time: 0.08455000119283795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 1080, 33, 34560, 34560, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 1080, 33, 33, 33, 1080, 33, 1080, 33, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 1080, 1080, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 1080, 33, 34560, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 34560, 33, 34560, 1080, 1080, 34560, 34560, 33, 1080, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 33, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 33, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 34560, 33, 1080, 1080, 33, 33, 1080, 33, 34560, 34560, 33, 34560, 34560, 33, 1080, 34560, 33, 33, 1080, 1080, 33, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 33, 33, 33, 33, 33, 33, 33, 33, 1080, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 1080, 33, 34560, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 33, 34560, 34560, 1080, 33, 34560, 1080, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 1080, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 33, 34560, 33, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 1080, 1080, 1080, 33, 1080, 33, 33, 34560, 33, 1080, 33, 33, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4566144 . Total input tokens: 1018851721 . Total output tokens: 896629367
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.6730216210708,
    "estimated_duration": 3600.093513150971,
    "input_throughput": 6907.46473922417,
    "output_throughput": 6018.690603688036,
    "total_throughput": 12926.155342912207,
    "itl": 84.13680165232772,
    "ttft": 2071617.501377336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4888939423114347,
    "arrivals": 1520700,
    "finished_requests": 100911,
    "scheduler_time": 319.2923618246773
}
#Debug simulation 
Total elapsed time: 81.67321610683575. Arrivals time: 0.5305850612930954 Scheduler time: 80.91026113042608 Scheduler overhead time: 0.08954275492578745 Adapter cache time: 0.018772925715893507 Engine time: 0.09002942545339465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 540, 270, 34560, 34560, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 270, 34560, 270, 34560, 540, 540, 540, 270, 270, 540, 270, 34560, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 540, 270, 270, 270, 540, 270, 540, 270, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 34560, 34560, 540, 270, 270, 34560, 540, 540, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 34560, 270, 270, 34560, 270, 34560, 540, 270, 34560, 540, 540, 270, 34560, 34560, 34560, 270, 540, 34560, 270, 34560, 540, 540, 34560, 34560, 270, 540, 34560, 270, 540, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 34560, 270, 34560, 540, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 270, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 270, 270, 270, 270, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270, 540, 34560, 34560, 540, 540, 270, 34560, 540, 34560, 270, 540, 540, 270, 270, 540, 270, 34560, 34560, 270, 34560, 34560, 270, 540, 34560, 270, 270, 540, 540, 270, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 540, 270, 34560, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 270, 34560, 34560, 540, 270, 34560, 540, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 540, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 34560, 270, 270, 270, 540, 270, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 540, 540, 540, 540, 270, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 540, 540, 540, 270, 540, 270, 270, 34560, 270, 540, 270, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4527360 . Total input tokens: 1010229528 . Total output tokens: 889011302
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.03674309002236,
    "estimated_duration": 3600.0346240748754,
    "input_throughput": 6929.232244928868,
    "output_throughput": 6102.976858353415,
    "total_throughput": 13032.209103282283,
    "itl": 88.40795923479453,
    "ttft": 2054278.1129352797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7029490578920172,
    "arrivals": 1507762,
    "finished_requests": 101195,
    "scheduler_time": 316.4528644842855
}
#Debug simulation 
Total elapsed time: 82.03695367323235. Arrivals time: 0.5197528935968876 Scheduler time: 81.2918807300739 Scheduler overhead time: 0.08824842143803835 Adapter cache time: 0.01919467654079199 Engine time: 0.08437623688951135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 540, 270, 34560, 34560, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 270, 34560, 270, 34560, 540, 540, 540, 270, 270, 540, 270, 34560, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 540, 270, 270, 270, 540, 270, 540, 270, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 34560, 34560, 540, 270, 270, 34560, 540, 540, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 34560, 270, 270, 34560, 270, 34560, 540, 270, 34560, 540, 540, 270, 34560, 34560, 34560, 270, 540, 34560, 270, 34560, 540, 540, 34560, 34560, 270, 540, 34560, 270, 540, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 34560, 270, 34560, 540, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 270, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 270, 270, 270, 270, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270, 540, 34560, 34560, 540, 540, 270, 34560, 540, 34560, 270, 540, 540, 270, 270, 540, 270, 34560, 34560, 270, 34560, 34560, 270, 540, 34560, 270, 270, 540, 540, 270, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 540, 270, 34560, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 270, 34560, 34560, 540, 270, 34560, 540, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 540, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 34560, 270, 270, 270, 540, 270, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 540, 540, 540, 540, 270, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 540, 540, 540, 270, 540, 270, 270, 34560, 270, 540, 270, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4527360 . Total input tokens: 1010229528 . Total output tokens: 889011302
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.22569279093295,
    "estimated_duration": 3600.0658170648408,
    "input_throughput": 6657.220233695616,
    "output_throughput": 5843.208171441362,
    "total_throughput": 12500.428405136978,
    "itl": 86.12059088811493,
    "ttft": 2078217.4913339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7927483878750405,
    "arrivals": 1507762,
    "finished_requests": 97293,
    "scheduler_time": 328.68974660793685
}
#Debug simulation 
Total elapsed time: 82.22588943922892. Arrivals time: 0.6219161651097238 Scheduler time: 81.37352483998984 Scheduler overhead time: 0.08988538244739175 Adapter cache time: 0.019597124308347702 Engine time: 0.08642920292913914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 540, 270, 34560, 34560, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 270, 34560, 270, 34560, 540, 540, 540, 270, 270, 540, 270, 34560, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 540, 270, 270, 270, 540, 270, 540, 270, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 34560, 34560, 540, 270, 270, 34560, 540, 540, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 34560, 270, 270, 34560, 270, 34560, 540, 270, 34560, 540, 540, 270, 34560, 34560, 34560, 270, 540, 34560, 270, 34560, 540, 540, 34560, 34560, 270, 540, 34560, 270, 540, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 34560, 270, 34560, 540, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 270, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 270, 270, 270, 270, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270, 540, 34560, 34560, 540, 540, 270, 34560, 540, 34560, 270, 540, 540, 270, 270, 540, 270, 34560, 34560, 270, 34560, 34560, 270, 540, 34560, 270, 270, 540, 540, 270, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 540, 270, 34560, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 270, 34560, 34560, 540, 270, 34560, 540, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 540, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 34560, 270, 270, 270, 540, 270, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 540, 540, 540, 540, 270, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 540, 540, 540, 270, 540, 270, 270, 34560, 270, 540, 270, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4527360 . Total input tokens: 1010229528 . Total output tokens: 889011302
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.81115824123845,
    "estimated_duration": 3600.0018289668596,
    "input_throughput": 6744.237962503409,
    "output_throughput": 5914.367550782709,
    "total_throughput": 12658.605513286118,
    "itl": 84.70010274148382,
    "ttft": 2072955.655735318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9927714551519946,
    "arrivals": 1507762,
    "finished_requests": 98476,
    "scheduler_time": 324.76591484891054
}
#Debug simulation 
Total elapsed time: 82.81135003780946. Arrivals time: 0.6252184337936342 Scheduler time: 81.9550283481367 Scheduler overhead time: 0.08990605873987079 Adapter cache time: 0.019753344357013702 Engine time: 0.08702882844954729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 540, 270, 34560, 34560, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 270, 34560, 270, 34560, 540, 540, 540, 270, 270, 540, 270, 34560, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 540, 270, 270, 270, 540, 270, 540, 270, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 34560, 34560, 540, 270, 270, 34560, 540, 540, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 34560, 270, 270, 34560, 270, 34560, 540, 270, 34560, 540, 540, 270, 34560, 34560, 34560, 270, 540, 34560, 270, 34560, 540, 540, 34560, 34560, 270, 540, 34560, 270, 540, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 34560, 270, 34560, 540, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 270, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 270, 270, 270, 270, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270, 540, 34560, 34560, 540, 540, 270, 34560, 540, 34560, 270, 540, 540, 270, 270, 540, 270, 34560, 34560, 270, 34560, 34560, 270, 540, 34560, 270, 270, 540, 540, 270, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 540, 270, 34560, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 270, 34560, 34560, 540, 270, 34560, 540, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 540, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 34560, 270, 270, 270, 540, 270, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 540, 540, 540, 540, 270, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 540, 540, 540, 270, 540, 270, 270, 34560, 270, 540, 270, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4527360 . Total input tokens: 1010229528 . Total output tokens: 889011302
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 83.9953296892345,
    "estimated_duration": 3600.0498857005887,
    "input_throughput": 6938.633017064116,
    "output_throughput": 6086.6920447508555,
    "total_throughput": 13025.32506181497,
    "itl": 87.6660001112466,
    "ttft": 2061800.809133056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 532,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.65445504141971,
    "arrivals": 1507762,
    "finished_requests": 101212,
    "scheduler_time": 315.8598645951119
}
#Debug simulation 
Total elapsed time: 83.99551901128143. Arrivals time: 0.5209728647023439 Scheduler time: 83.24781792424619 Scheduler overhead time: 0.0885241492651403 Adapter cache time: 0.019352753181010485 Engine time: 0.08508811891078949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 540, 270, 34560, 34560, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 270, 34560, 270, 34560, 540, 540, 540, 270, 270, 540, 270, 34560, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 540, 270, 270, 270, 540, 270, 540, 270, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 34560, 34560, 540, 270, 270, 34560, 540, 540, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 34560, 270, 270, 34560, 270, 34560, 540, 270, 34560, 540, 540, 270, 34560, 34560, 34560, 270, 540, 34560, 270, 34560, 540, 540, 34560, 34560, 270, 540, 34560, 270, 540, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 34560, 270, 34560, 540, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 270, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 270, 270, 270, 270, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270, 540, 34560, 34560, 540, 540, 270, 34560, 540, 34560, 270, 540, 540, 270, 270, 540, 270, 34560, 34560, 270, 34560, 34560, 270, 540, 34560, 270, 270, 540, 540, 270, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 540, 270, 34560, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 270, 34560, 34560, 540, 270, 34560, 540, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 540, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 34560, 270, 270, 270, 540, 270, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 540, 540, 540, 540, 270, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 540, 540, 540, 270, 540, 270, 270, 34560, 270, 540, 270, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4527360 . Total input tokens: 1010229528 . Total output tokens: 889011302
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 79.95564816705883,
    "estimated_duration": 3600.0983907154573,
    "input_throughput": 6838.4914322042605,
    "output_throughput": 5998.450780037811,
    "total_throughput": 12836.942212242071,
    "itl": 85.2030563136141,
    "ttft": 2072221.2271660268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.049399865409393,
    "arrivals": 1507762,
    "finished_requests": 99878,
    "scheduler_time": 320.43295490145533
}
#Debug simulation 
Total elapsed time: 79.9558428581804. Arrivals time: 0.5299288830719888 Scheduler time: 79.19565736688673 Scheduler overhead time: 0.08955598529428244 Adapter cache time: 0.019556558690965176 Engine time: 0.0869247829541564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 540, 270, 34560, 34560, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 270, 34560, 270, 34560, 540, 540, 540, 270, 270, 540, 270, 34560, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 540, 270, 270, 270, 540, 270, 540, 270, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 34560, 34560, 540, 270, 270, 34560, 540, 540, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 34560, 270, 270, 34560, 270, 34560, 540, 270, 34560, 540, 540, 270, 34560, 34560, 34560, 270, 540, 34560, 270, 34560, 540, 540, 34560, 34560, 270, 540, 34560, 270, 540, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 34560, 270, 34560, 540, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 270, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 270, 270, 270, 270, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270, 540, 34560, 34560, 540, 540, 270, 34560, 540, 34560, 270, 540, 540, 270, 270, 540, 270, 34560, 34560, 270, 34560, 34560, 270, 540, 34560, 270, 270, 540, 540, 270, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 540, 270, 34560, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 270, 34560, 34560, 540, 270, 34560, 540, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 540, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 34560, 270, 270, 270, 540, 270, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 540, 540, 540, 540, 270, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 540, 540, 540, 270, 540, 270, 270, 34560, 270, 540, 270, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4527360 . Total input tokens: 1010229528 . Total output tokens: 889011302
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 81.5521984747611,
    "estimated_duration": 3600.0693160596484,
    "input_throughput": 6974.505154106867,
    "output_throughput": 6124.019029758791,
    "total_throughput": 13098.524183865657,
    "itl": 88.1578421872946,
    "ttft": 2063461.8602993165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6132992523629053,
    "arrivals": 1507762,
    "finished_requests": 101881,
    "scheduler_time": 313.76470082423623
}
#Debug simulation 
Total elapsed time: 81.55238544102758. Arrivals time: 0.5200256095267832 Scheduler time: 80.80820336006582 Scheduler overhead time: 0.08671526843681931 Adapter cache time: 0.01968472544103861 Engine time: 0.0842926581390202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 540, 270, 34560, 34560, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 270, 34560, 270, 34560, 540, 540, 540, 270, 270, 540, 270, 34560, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 540, 270, 270, 270, 540, 270, 540, 270, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 34560, 34560, 540, 270, 270, 34560, 540, 540, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 270, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 34560, 270, 270, 34560, 270, 34560, 540, 270, 34560, 540, 540, 270, 34560, 34560, 34560, 270, 540, 34560, 270, 34560, 540, 540, 34560, 34560, 270, 540, 34560, 270, 540, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 34560, 270, 34560, 540, 540, 540, 540, 540, 270, 270, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 270, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 270, 270, 270, 270, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270, 540, 34560, 34560, 540, 540, 270, 34560, 540, 34560, 270, 540, 540, 270, 270, 540, 270, 34560, 34560, 270, 34560, 34560, 270, 540, 34560, 270, 270, 540, 540, 270, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 270, 270, 270, 270, 270, 270, 270, 270, 540, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 540, 270, 34560, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 270, 34560, 34560, 540, 270, 34560, 540, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 540, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 540, 540, 270, 34560, 270, 270, 270, 540, 270, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 540, 540, 540, 540, 270, 540, 34560, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 540, 540, 540, 270, 540, 270, 270, 34560, 270, 540, 270, 270, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 4527360 . Total input tokens: 1010229528 . Total output tokens: 889011302
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.61344084097072,
    "estimated_duration": 3600.0343823349363,
    "input_throughput": 6696.184102654274,
    "output_throughput": 5873.546126047175,
    "total_throughput": 12569.73022870145,
    "itl": 84.15642197417264,
    "ttft": 2077934.858232802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.828340589106115,
    "arrivals": 1507762,
    "finished_requests": 97772,
    "scheduler_time": 326.91052292050495
}
#Debug simulation 
Total elapsed time: 81.61363469436765. Arrivals time: 0.514286978635937 Scheduler time: 80.86866456642747 Scheduler overhead time: 0.09063931694254279 Adapter cache time: 0.019362224265933037 Engine time: 0.0862641641870141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 540, 135, 34560, 34560, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 135, 34560, 135, 34560, 540, 540, 540, 135, 135, 540, 135, 34560, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 540, 135, 135, 135, 540, 135, 540, 135, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 34560, 34560, 540, 135, 135, 34560, 540, 540, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 34560, 135, 135, 34560, 135, 34560, 540, 135, 34560, 540, 540, 135, 34560, 34560, 34560, 135, 540, 34560, 135, 34560, 540, 540, 34560, 34560, 135, 540, 34560, 135, 540, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 34560, 135, 34560, 540, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 135, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 135, 135, 135, 135, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135, 540, 34560, 34560, 540, 540, 135, 34560, 540, 34560, 135, 540, 540, 135, 135, 540, 135, 34560, 34560, 135, 34560, 34560, 135, 540, 34560, 135, 135, 540, 540, 135, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 540, 135, 34560, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 135, 34560, 34560, 540, 135, 34560, 540, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 540, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 34560, 135, 135, 135, 540, 135, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 540, 540, 540, 540, 135, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 540, 540, 540, 135, 540, 135, 135, 34560, 135, 540, 135, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4510080 . Total input tokens: 1006359752 . Total output tokens: 885614170
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.27738806931302,
    "estimated_duration": 3600.084309343184,
    "input_throughput": 6949.992236310951,
    "output_throughput": 6089.724327594936,
    "total_throughput": 13039.716563905886,
    "itl": 88.50619336490604,
    "ttft": 2064575.3428260256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.511189195965463,
    "arrivals": 1502157,
    "finished_requests": 101486,
    "scheduler_time": 315.35927836969876
}
#Debug simulation 
Total elapsed time: 82.27757811220363. Arrivals time: 0.5080594252794981 Scheduler time: 81.54325906932354 Scheduler overhead time: 0.08819822175428271 Adapter cache time: 0.01956574758514762 Engine time: 0.08473041793331504 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 540, 135, 34560, 34560, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 135, 34560, 135, 34560, 540, 540, 540, 135, 135, 540, 135, 34560, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 540, 135, 135, 135, 540, 135, 540, 135, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 34560, 34560, 540, 135, 135, 34560, 540, 540, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 34560, 135, 135, 34560, 135, 34560, 540, 135, 34560, 540, 540, 135, 34560, 34560, 34560, 135, 540, 34560, 135, 34560, 540, 540, 34560, 34560, 135, 540, 34560, 135, 540, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 34560, 135, 34560, 540, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 135, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 135, 135, 135, 135, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135, 540, 34560, 34560, 540, 540, 135, 34560, 540, 34560, 135, 540, 540, 135, 135, 540, 135, 34560, 34560, 135, 34560, 34560, 135, 540, 34560, 135, 135, 540, 540, 135, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 540, 135, 34560, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 135, 34560, 34560, 540, 135, 34560, 540, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 540, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 34560, 135, 135, 135, 540, 135, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 540, 540, 540, 540, 135, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 540, 540, 540, 135, 540, 135, 135, 34560, 135, 540, 135, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4510080 . Total input tokens: 1006359752 . Total output tokens: 885614170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.44447003910318,
    "estimated_duration": 3600.0171110386364,
    "input_throughput": 6953.168340018858,
    "output_throughput": 6098.204070387372,
    "total_throughput": 13051.37241040623,
    "itl": 87.68893797284683,
    "ttft": 2050033.2902671644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.259067425024701,
    "arrivals": 1502157,
    "finished_requests": 101365,
    "scheduler_time": 316.2096238677025
}
#Debug simulation 
Total elapsed time: 82.44467213517055. Arrivals time: 0.5070508345961571 Scheduler time: 81.70712667284533 Scheduler overhead time: 0.09036422753706574 Adapter cache time: 0.019845489412546158 Engine time: 0.08683405071496964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 540, 135, 34560, 34560, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 135, 34560, 135, 34560, 540, 540, 540, 135, 135, 540, 135, 34560, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 540, 135, 135, 135, 540, 135, 540, 135, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 34560, 34560, 540, 135, 135, 34560, 540, 540, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 34560, 135, 135, 34560, 135, 34560, 540, 135, 34560, 540, 540, 135, 34560, 34560, 34560, 135, 540, 34560, 135, 34560, 540, 540, 34560, 34560, 135, 540, 34560, 135, 540, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 34560, 135, 34560, 540, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 135, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 135, 135, 135, 135, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135, 540, 34560, 34560, 540, 540, 135, 34560, 540, 34560, 135, 540, 540, 135, 135, 540, 135, 34560, 34560, 135, 34560, 34560, 135, 540, 34560, 135, 135, 540, 540, 135, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 540, 135, 34560, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 135, 34560, 34560, 540, 135, 34560, 540, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 540, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 34560, 135, 135, 135, 540, 135, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 540, 540, 540, 540, 135, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 540, 540, 540, 135, 540, 135, 135, 34560, 135, 540, 135, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4510080 . Total input tokens: 1006359752 . Total output tokens: 885614170
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 79.76642791787162,
    "estimated_duration": 3600.0139912014943,
    "input_throughput": 6918.9386654819455,
    "output_throughput": 6074.346392387688,
    "total_throughput": 12993.285057869634,
    "itl": 85.4521487138926,
    "ttft": 2053236.703545639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3964120958326705,
    "arrivals": 1502157,
    "finished_requests": 100896,
    "scheduler_time": 317.73852592740906
}
#Debug simulation 
Total elapsed time: 79.76662232587114. Arrivals time: 0.5150871090590954 Scheduler time: 79.02250420255587 Scheduler overhead time: 0.08922099648043513 Adapter cache time: 0.01981278695166111 Engine time: 0.08602622337639332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 540, 135, 34560, 34560, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 135, 34560, 135, 34560, 540, 540, 540, 135, 135, 540, 135, 34560, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 540, 135, 135, 135, 540, 135, 540, 135, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 34560, 34560, 540, 135, 135, 34560, 540, 540, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 34560, 135, 135, 34560, 135, 34560, 540, 135, 34560, 540, 540, 135, 34560, 34560, 34560, 135, 540, 34560, 135, 34560, 540, 540, 34560, 34560, 135, 540, 34560, 135, 540, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 34560, 135, 34560, 540, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 135, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 135, 135, 135, 135, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135, 540, 34560, 34560, 540, 540, 135, 34560, 540, 34560, 135, 540, 540, 135, 135, 540, 135, 34560, 34560, 135, 34560, 34560, 135, 540, 34560, 135, 135, 540, 540, 135, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 540, 135, 34560, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 135, 34560, 34560, 540, 135, 34560, 540, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 540, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 34560, 135, 135, 135, 540, 135, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 540, 540, 540, 540, 135, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 540, 540, 540, 135, 540, 135, 135, 34560, 135, 540, 135, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4510080 . Total input tokens: 1006359752 . Total output tokens: 885614170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 81.9509301413782,
    "estimated_duration": 3600.0004825253523,
    "input_throughput": 6881.66241095095,
    "output_throughput": 6059.670576682033,
    "total_throughput": 12941.332987632984,
    "itl": 87.54544407498878,
    "ttft": 2051428.6480529308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8506945164827577,
    "arrivals": 1502157,
    "finished_requests": 100523,
    "scheduler_time": 318.66959242873173
}
#Debug simulation 
Total elapsed time: 81.95113272499293. Arrivals time: 0.4937502988614142 Scheduler time: 81.22789994487539 Scheduler overhead time: 0.08957404503598809 Adapter cache time: 0.01990380883216858 Engine time: 0.08576115313917398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 540, 135, 34560, 34560, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 135, 34560, 135, 34560, 540, 540, 540, 135, 135, 540, 135, 34560, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 540, 135, 135, 135, 540, 135, 540, 135, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 34560, 34560, 540, 135, 135, 34560, 540, 540, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 34560, 135, 135, 34560, 135, 34560, 540, 135, 34560, 540, 540, 135, 34560, 34560, 34560, 135, 540, 34560, 135, 34560, 540, 540, 34560, 34560, 135, 540, 34560, 135, 540, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 34560, 135, 34560, 540, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 135, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 135, 135, 135, 135, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135, 540, 34560, 34560, 540, 540, 135, 34560, 540, 34560, 135, 540, 540, 135, 135, 540, 135, 34560, 34560, 135, 34560, 34560, 135, 540, 34560, 135, 135, 540, 540, 135, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 540, 135, 34560, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 135, 34560, 34560, 540, 135, 34560, 540, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 540, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 34560, 135, 135, 135, 540, 135, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 540, 540, 540, 540, 135, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 540, 540, 540, 135, 540, 135, 135, 34560, 135, 540, 135, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4510080 . Total input tokens: 1006359752 . Total output tokens: 885614170
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 82.23544096481055,
    "estimated_duration": 3600.090001298618,
    "input_throughput": 6936.542417270702,
    "output_throughput": 6078.1599882521805,
    "total_throughput": 13014.702405522881,
    "itl": 85.49911353533035,
    "ttft": 2065133.5422218395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.238302613841402,
    "arrivals": 1502157,
    "finished_requests": 101320,
    "scheduler_time": 316.18466741910714
}
#Debug simulation 
Total elapsed time: 82.2356353639625. Arrivals time: 0.5022395765408874 Scheduler time: 81.50571829872206 Scheduler overhead time: 0.0889559406787157 Adapter cache time: 0.019729656632989645 Engine time: 0.0851713758893311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 540, 135, 34560, 34560, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 135, 34560, 135, 34560, 540, 540, 540, 135, 135, 540, 135, 34560, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 540, 135, 135, 135, 540, 135, 540, 135, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 34560, 34560, 540, 135, 135, 34560, 540, 540, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 34560, 135, 135, 34560, 135, 34560, 540, 135, 34560, 540, 540, 135, 34560, 34560, 34560, 135, 540, 34560, 135, 34560, 540, 540, 34560, 34560, 135, 540, 34560, 135, 540, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 34560, 135, 34560, 540, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 135, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 135, 135, 135, 135, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135, 540, 34560, 34560, 540, 540, 135, 34560, 540, 34560, 135, 540, 540, 135, 135, 540, 135, 34560, 34560, 135, 34560, 34560, 135, 540, 34560, 135, 135, 540, 540, 135, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 540, 135, 34560, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 135, 34560, 34560, 540, 135, 34560, 540, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 540, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 34560, 135, 135, 135, 540, 135, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 540, 540, 540, 540, 135, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 540, 540, 540, 135, 540, 135, 135, 34560, 135, 540, 135, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4510080 . Total input tokens: 1006359752 . Total output tokens: 885614170
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 83.76211808202788,
    "estimated_duration": 3600.0045585635075,
    "input_throughput": 6901.883204813081,
    "output_throughput": 6053.916778565526,
    "total_throughput": 12955.799983378607,
    "itl": 87.36592011179211,
    "ttft": 2067307.9747996512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2685675215720984,
    "arrivals": 1502157,
    "finished_requests": 100738,
    "scheduler_time": 317.59308672799995
}
#Debug simulation 
Total elapsed time: 83.76230780407786. Arrivals time: 0.5124479806981981 Scheduler time: 83.02276151720434 Scheduler overhead time: 0.08880220307037234 Adapter cache time: 0.019194625318050385 Engine time: 0.08523664390668273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 540, 135, 34560, 34560, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 135, 34560, 135, 34560, 540, 540, 540, 135, 135, 540, 135, 34560, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 540, 135, 135, 135, 540, 135, 540, 135, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 34560, 34560, 540, 135, 135, 34560, 540, 540, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 135, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 34560, 135, 135, 34560, 135, 34560, 540, 135, 34560, 540, 540, 135, 34560, 34560, 34560, 135, 540, 34560, 135, 34560, 540, 540, 34560, 34560, 135, 540, 34560, 135, 540, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 34560, 135, 34560, 540, 540, 540, 540, 540, 135, 135, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 135, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 135, 135, 135, 135, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135, 540, 34560, 34560, 540, 540, 135, 34560, 540, 34560, 135, 540, 540, 135, 135, 540, 135, 34560, 34560, 135, 34560, 34560, 135, 540, 34560, 135, 135, 540, 540, 135, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 135, 135, 135, 135, 135, 135, 135, 135, 540, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 540, 135, 34560, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 135, 34560, 34560, 540, 135, 34560, 540, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 540, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 540, 540, 135, 34560, 135, 135, 135, 540, 135, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 540, 540, 540, 540, 135, 540, 34560, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 540, 540, 540, 135, 540, 135, 135, 34560, 135, 540, 135, 135, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4510080 . Total input tokens: 1006359752 . Total output tokens: 885614170
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.54095061821863,
    "estimated_duration": 3600.0078722773956,
    "input_throughput": 6960.808667384243,
    "output_throughput": 6105.551370946098,
    "total_throughput": 13066.360038330342,
    "itl": 85.15535817519279,
    "ttft": 2058232.5234676155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.132947522569482,
    "arrivals": 1502157,
    "finished_requests": 101542,
    "scheduler_time": 315.2646986424588
}
#Debug simulation 
Total elapsed time: 80.54113418003544. Arrivals time: 0.49857292231172323 Scheduler time: 79.8152618422173 Scheduler overhead time: 0.08837757958099246 Adapter cache time: 0.01971516665071249 Engine time: 0.0854129302315414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 540, 66, 34560, 34560, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 66, 34560, 66, 34560, 540, 540, 540, 66, 66, 540, 66, 34560, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 540, 66, 66, 66, 540, 66, 540, 66, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 34560, 34560, 540, 66, 66, 34560, 540, 540, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 34560, 66, 66, 34560, 66, 34560, 540, 66, 34560, 540, 540, 66, 34560, 34560, 34560, 66, 540, 34560, 66, 34560, 540, 540, 34560, 34560, 66, 540, 34560, 66, 540, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 34560, 66, 34560, 540, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 66, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 66, 66, 66, 66, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66, 540, 34560, 34560, 540, 540, 66, 34560, 540, 34560, 66, 540, 540, 66, 66, 540, 66, 34560, 34560, 66, 34560, 34560, 66, 540, 34560, 66, 66, 540, 540, 66, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 540, 66, 34560, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 66, 34560, 34560, 540, 66, 34560, 540, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 540, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 34560, 66, 66, 66, 540, 66, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 540, 540, 540, 540, 66, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 540, 540, 540, 66, 540, 66, 66, 34560, 66, 540, 66, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4501248 . Total input tokens: 1004448505 . Total output tokens: 883874642
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.03635263210163,
    "estimated_duration": 3600.0638383311075,
    "input_throughput": 6922.963069330926,
    "output_throughput": 6084.434327740773,
    "total_throughput": 13007.3973970717,
    "itl": 86.5270714809376,
    "ttft": 2068794.874310403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2731424708152574,
    "arrivals": 1499241,
    "finished_requests": 101003,
    "scheduler_time": 317.1635047248018
}
#Debug simulation 
Total elapsed time: 84.0365509330295. Arrivals time: 0.5158837079070508 Scheduler time: 83.29583805287257 Scheduler overhead time: 0.08734658034518361 Adapter cache time: 0.01886781258508563 Engine time: 0.08529938291758299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 540, 66, 34560, 34560, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 66, 34560, 66, 34560, 540, 540, 540, 66, 66, 540, 66, 34560, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 540, 66, 66, 66, 540, 66, 540, 66, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 34560, 34560, 540, 66, 66, 34560, 540, 540, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 34560, 66, 66, 34560, 66, 34560, 540, 66, 34560, 540, 540, 66, 34560, 34560, 34560, 66, 540, 34560, 66, 34560, 540, 540, 34560, 34560, 66, 540, 34560, 66, 540, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 34560, 66, 34560, 540, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 66, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 66, 66, 66, 66, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66, 540, 34560, 34560, 540, 540, 66, 34560, 540, 34560, 66, 540, 540, 66, 66, 540, 66, 34560, 34560, 66, 34560, 34560, 66, 540, 34560, 66, 66, 540, 540, 66, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 540, 66, 34560, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 66, 34560, 34560, 540, 66, 34560, 540, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 540, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 34560, 66, 66, 66, 540, 66, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 540, 540, 540, 540, 66, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 540, 540, 540, 66, 540, 66, 66, 34560, 66, 540, 66, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4501248 . Total input tokens: 1004448505 . Total output tokens: 883874642
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.7433633711189,
    "estimated_duration": 3600.040173573857,
    "input_throughput": 7012.735909260003,
    "output_throughput": 6140.4811985902925,
    "total_throughput": 13153.217107850294,
    "itl": 86.33007909924378,
    "ttft": 2064856.4683410937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7772041040472737,
    "arrivals": 1499241,
    "finished_requests": 102281,
    "scheduler_time": 312.89577456761623
}
#Debug simulation 
Total elapsed time: 81.743547514081. Arrivals time: 0.5189593089744449 Scheduler time: 81.0001874519512 Scheduler overhead time: 0.08666985761374235 Adapter cache time: 0.018916886299848557 Engine time: 0.0854463642463088 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 540, 66, 34560, 34560, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 66, 34560, 66, 34560, 540, 540, 540, 66, 66, 540, 66, 34560, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 540, 66, 66, 66, 540, 66, 540, 66, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 34560, 34560, 540, 66, 66, 34560, 540, 540, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 34560, 66, 66, 34560, 66, 34560, 540, 66, 34560, 540, 540, 66, 34560, 34560, 34560, 66, 540, 34560, 66, 34560, 540, 540, 34560, 34560, 66, 540, 34560, 66, 540, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 34560, 66, 34560, 540, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 66, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 66, 66, 66, 66, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66, 540, 34560, 34560, 540, 540, 66, 34560, 540, 34560, 66, 540, 540, 66, 66, 540, 66, 34560, 34560, 66, 34560, 34560, 66, 540, 34560, 66, 66, 540, 540, 66, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 540, 66, 34560, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 66, 34560, 34560, 540, 66, 34560, 540, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 540, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 34560, 66, 66, 66, 540, 66, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 540, 540, 540, 540, 66, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 540, 540, 540, 66, 540, 66, 66, 34560, 66, 540, 66, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4501248 . Total input tokens: 1004448505 . Total output tokens: 883874642
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.90079489536583,
    "estimated_duration": 3600.028605111332,
    "input_throughput": 6877.685628621357,
    "output_throughput": 6031.899571344783,
    "total_throughput": 12909.585199966139,
    "itl": 85.24469049469157,
    "ttft": 2067217.7348610412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.817274605925227,
    "arrivals": 1499241,
    "finished_requests": 100388,
    "scheduler_time": 318.5850288750787
}
#Debug simulation 
Total elapsed time: 80.90100133325905. Arrivals time: 0.49869857588782907 Scheduler time: 80.17409473331645 Scheduler overhead time: 0.08982825791463256 Adapter cache time: 0.01923100044950843 Engine time: 0.08574918564409018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 540, 66, 34560, 34560, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 66, 34560, 66, 34560, 540, 540, 540, 66, 66, 540, 66, 34560, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 540, 66, 66, 66, 540, 66, 540, 66, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 34560, 34560, 540, 66, 66, 34560, 540, 540, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 34560, 66, 66, 34560, 66, 34560, 540, 66, 34560, 540, 540, 66, 34560, 34560, 34560, 66, 540, 34560, 66, 34560, 540, 540, 34560, 34560, 66, 540, 34560, 66, 540, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 34560, 66, 34560, 540, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 66, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 66, 66, 66, 66, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66, 540, 34560, 34560, 540, 540, 66, 34560, 540, 34560, 66, 540, 540, 66, 66, 540, 66, 34560, 34560, 66, 34560, 34560, 66, 540, 34560, 66, 66, 540, 540, 66, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 540, 66, 34560, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 66, 34560, 34560, 540, 66, 34560, 540, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 540, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 34560, 66, 66, 66, 540, 66, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 540, 540, 540, 540, 66, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 540, 540, 540, 66, 540, 66, 66, 34560, 66, 540, 66, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4501248 . Total input tokens: 1004448505 . Total output tokens: 883874642
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 80.61489707278088,
    "estimated_duration": 3600.028035544333,
    "input_throughput": 6936.804867472114,
    "output_throughput": 6091.420339920836,
    "total_throughput": 13028.22520739295,
    "itl": 86.61619139658427,
    "ttft": 2071550.931709268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5176162597211036,
    "arrivals": 1499241,
    "finished_requests": 101386,
    "scheduler_time": 315.429551996093
}
#Debug simulation 
Total elapsed time: 80.61509718187153. Arrivals time: 0.627341159619391 Scheduler time: 79.7611549589783 Scheduler overhead time: 0.08830137923359871 Adapter cache time: 0.019305019173771143 Engine time: 0.08521129889413714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 540, 66, 34560, 34560, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 66, 34560, 66, 34560, 540, 540, 540, 66, 66, 540, 66, 34560, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 540, 66, 66, 66, 540, 66, 540, 66, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 34560, 34560, 540, 66, 66, 34560, 540, 540, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 34560, 66, 66, 34560, 66, 34560, 540, 66, 34560, 540, 540, 66, 34560, 34560, 34560, 66, 540, 34560, 66, 34560, 540, 540, 34560, 34560, 66, 540, 34560, 66, 540, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 34560, 66, 34560, 540, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 66, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 66, 66, 66, 66, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66, 540, 34560, 34560, 540, 540, 66, 34560, 540, 34560, 66, 540, 540, 66, 66, 540, 66, 34560, 34560, 66, 34560, 34560, 66, 540, 34560, 66, 66, 540, 540, 66, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 540, 66, 34560, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 66, 34560, 34560, 540, 66, 34560, 540, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 540, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 34560, 66, 66, 66, 540, 66, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 540, 540, 540, 540, 66, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 540, 540, 540, 66, 540, 66, 66, 34560, 66, 540, 66, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4501248 . Total input tokens: 1004448505 . Total output tokens: 883874642
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 80.84076145896688,
    "estimated_duration": 3600.032627991104,
    "input_throughput": 6946.705928594655,
    "output_throughput": 6095.7619743145915,
    "total_throughput": 13042.467902909246,
    "itl": 84.97285123176535,
    "ttft": 2065482.0472154205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8379239790048776,
    "arrivals": 1499241,
    "finished_requests": 101340,
    "scheduler_time": 316.0401768957512
}
#Debug simulation 
Total elapsed time: 80.8409638130106. Arrivals time: 0.6060505788773298 Scheduler time: 80.0093635651283 Scheduler overhead time: 0.08800033433362842 Adapter cache time: 0.018568984232842922 Engine time: 0.08504232158884406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 540, 66, 34560, 34560, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 66, 34560, 66, 34560, 540, 540, 540, 66, 66, 540, 66, 34560, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 540, 66, 66, 66, 540, 66, 540, 66, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 34560, 34560, 540, 66, 66, 34560, 540, 540, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 34560, 66, 66, 34560, 66, 34560, 540, 66, 34560, 540, 540, 66, 34560, 34560, 34560, 66, 540, 34560, 66, 34560, 540, 540, 34560, 34560, 66, 540, 34560, 66, 540, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 34560, 66, 34560, 540, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 66, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 66, 66, 66, 66, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66, 540, 34560, 34560, 540, 540, 66, 34560, 540, 34560, 66, 540, 540, 66, 66, 540, 66, 34560, 34560, 66, 34560, 34560, 66, 540, 34560, 66, 66, 540, 540, 66, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 540, 66, 34560, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 66, 34560, 34560, 540, 66, 34560, 540, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 540, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 34560, 66, 66, 66, 540, 66, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 540, 540, 540, 540, 66, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 540, 540, 540, 66, 540, 66, 66, 34560, 66, 540, 66, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4501248 . Total input tokens: 1004448505 . Total output tokens: 883874642
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 80.51244453759864,
    "estimated_duration": 3600.079126342123,
    "input_throughput": 6937.444184838326,
    "output_throughput": 6091.9572126970525,
    "total_throughput": 13029.40139753538,
    "itl": 86.61239249751694,
    "ttft": 2071581.5742178843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.262183600631528,
    "arrivals": 1499241,
    "finished_requests": 101395,
    "scheduler_time": 315.45148739625375
}
#Debug simulation 
Total elapsed time: 80.51264345273376. Arrivals time: 0.5064414558000863 Scheduler time: 79.78150945948437 Scheduler overhead time: 0.08770368294790387 Adapter cache time: 0.018810844980180264 Engine time: 0.08402880327776074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 540, 66, 34560, 34560, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 66, 34560, 66, 34560, 540, 540, 540, 66, 66, 540, 66, 34560, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 540, 66, 66, 66, 540, 66, 540, 66, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 34560, 34560, 540, 66, 66, 34560, 540, 540, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 66, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 34560, 66, 66, 34560, 66, 34560, 540, 66, 34560, 540, 540, 66, 34560, 34560, 34560, 66, 540, 34560, 66, 34560, 540, 540, 34560, 34560, 66, 540, 34560, 66, 540, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 34560, 66, 34560, 540, 540, 540, 540, 540, 66, 66, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 66, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 66, 66, 66, 66, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66, 540, 34560, 34560, 540, 540, 66, 34560, 540, 34560, 66, 540, 540, 66, 66, 540, 66, 34560, 34560, 66, 34560, 34560, 66, 540, 34560, 66, 66, 540, 540, 66, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 66, 66, 66, 66, 66, 66, 66, 66, 540, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 540, 66, 34560, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 66, 34560, 34560, 540, 66, 34560, 540, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 540, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 540, 540, 66, 34560, 66, 66, 66, 540, 66, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 540, 540, 540, 540, 66, 540, 34560, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 540, 540, 540, 66, 540, 66, 66, 34560, 66, 540, 66, 66, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4501248 . Total input tokens: 1004448505 . Total output tokens: 883874642
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.50828633178025,
    "estimated_duration": 3600.057759307713,
    "input_throughput": 6941.8597341631985,
    "output_throughput": 6083.862666751153,
    "total_throughput": 13025.722400914352,
    "itl": 85.50804874987776,
    "ttft": 2057295.7701386323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 546,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.029149402789807,
    "arrivals": 1499241,
    "finished_requests": 101234,
    "scheduler_time": 315.69964845202463
}
#Debug simulation 
Total elapsed time: 80.50848460011184. Arrivals time: 0.48913477594032884 Scheduler time: 79.79063241602853 Scheduler overhead time: 0.09007562696933746 Adapter cache time: 0.019178050104528666 Engine time: 0.08552005840465426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 540, 33, 34560, 34560, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 33, 34560, 33, 34560, 540, 540, 540, 33, 33, 540, 33, 34560, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 540, 33, 33, 33, 540, 33, 540, 33, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 34560, 34560, 540, 33, 33, 34560, 540, 540, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 34560, 33, 33, 34560, 33, 34560, 540, 33, 34560, 540, 540, 33, 34560, 34560, 34560, 33, 540, 34560, 33, 34560, 540, 540, 34560, 34560, 33, 540, 34560, 33, 540, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 34560, 33, 34560, 540, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 33, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 33, 33, 33, 33, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33, 540, 34560, 34560, 540, 540, 33, 34560, 540, 34560, 33, 540, 540, 33, 33, 540, 33, 34560, 34560, 33, 34560, 34560, 33, 540, 34560, 33, 33, 540, 540, 33, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 540, 33, 34560, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 33, 34560, 34560, 540, 33, 34560, 540, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 540, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 34560, 33, 33, 33, 540, 33, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 540, 540, 540, 540, 33, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 540, 540, 540, 33, 540, 33, 33, 34560, 33, 540, 33, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4497024 . Total input tokens: 1003489601 . Total output tokens: 883067534
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.96172101097181,
    "estimated_duration": 3600.098147294448,
    "input_throughput": 6963.839032788877,
    "output_throughput": 6101.820867441848,
    "total_throughput": 13065.659900230727,
    "itl": 87.649393865069,
    "ttft": 2062247.3795462733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.982196473409451,
    "arrivals": 1497804,
    "finished_requests": 101585,
    "scheduler_time": 314.87301449758337
}
#Debug simulation 
Total elapsed time: 84.96191362803802. Arrivals time: 0.5205100369639695 Scheduler time: 84.21573842130601 Scheduler overhead time: 0.08790895249694586 Adapter cache time: 0.018565291538834572 Engine time: 0.08531752740964293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 540, 33, 34560, 34560, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 33, 34560, 33, 34560, 540, 540, 540, 33, 33, 540, 33, 34560, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 540, 33, 33, 33, 540, 33, 540, 33, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 34560, 34560, 540, 33, 33, 34560, 540, 540, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 34560, 33, 33, 34560, 33, 34560, 540, 33, 34560, 540, 540, 33, 34560, 34560, 34560, 33, 540, 34560, 33, 34560, 540, 540, 34560, 34560, 33, 540, 34560, 33, 540, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 34560, 33, 34560, 540, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 33, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 33, 33, 33, 33, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33, 540, 34560, 34560, 540, 540, 33, 34560, 540, 34560, 33, 540, 540, 33, 33, 540, 33, 34560, 34560, 33, 34560, 34560, 33, 540, 34560, 33, 33, 540, 540, 33, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 540, 33, 34560, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 33, 34560, 34560, 540, 33, 34560, 540, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 540, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 34560, 33, 33, 33, 540, 33, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 540, 540, 540, 540, 33, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 540, 540, 540, 33, 540, 33, 33, 34560, 33, 540, 33, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4497024 . Total input tokens: 1003489601 . Total output tokens: 883067534
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 84.94679480791092,
    "estimated_duration": 3600.082538750208,
    "input_throughput": 6789.308505266999,
    "output_throughput": 5936.034735308821,
    "total_throughput": 12725.34324057582,
    "itl": 86.37870646368619,
    "ttft": 2066234.8448716374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.22899898483884,
    "arrivals": 1497804,
    "finished_requests": 98966,
    "scheduler_time": 323.7573129458819
}
#Debug simulation 
Total elapsed time: 84.9470052071847. Arrivals time: 0.49413250386714935 Scheduler time: 84.22455908264965 Scheduler overhead time: 0.08952956646680832 Adapter cache time: 0.018552185501903296 Engine time: 0.08577910484746099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 540, 33, 34560, 34560, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 33, 34560, 33, 34560, 540, 540, 540, 33, 33, 540, 33, 34560, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 540, 33, 33, 33, 540, 33, 540, 33, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 34560, 34560, 540, 33, 33, 34560, 540, 540, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 34560, 33, 33, 34560, 33, 34560, 540, 33, 34560, 540, 540, 33, 34560, 34560, 34560, 33, 540, 34560, 33, 34560, 540, 540, 34560, 34560, 33, 540, 34560, 33, 540, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 34560, 33, 34560, 540, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 33, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 33, 33, 33, 33, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33, 540, 34560, 34560, 540, 540, 33, 34560, 540, 34560, 33, 540, 540, 33, 33, 540, 33, 34560, 34560, 33, 34560, 34560, 33, 540, 34560, 33, 33, 540, 540, 33, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 540, 33, 34560, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 33, 34560, 34560, 540, 33, 34560, 540, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 540, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 34560, 33, 33, 33, 540, 33, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 540, 540, 540, 540, 33, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 540, 540, 540, 33, 540, 33, 33, 34560, 33, 540, 33, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4497024 . Total input tokens: 1003489601 . Total output tokens: 883067534
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 82.8243978372775,
    "estimated_duration": 3600.0130194645412,
    "input_throughput": 6867.402663915146,
    "output_throughput": 6015.5329669394005,
    "total_throughput": 12882.935630854547,
    "itl": 85.0218611709282,
    "ttft": 2066735.7927952204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4574319449020816,
    "arrivals": 1497804,
    "finished_requests": 100114,
    "scheduler_time": 319.3878074381556
}
#Debug simulation 
Total elapsed time: 82.82457658322528. Arrivals time: 0.4900753702968359 Scheduler time: 82.10868971142918 Scheduler overhead time: 0.08790352661162615 Adapter cache time: 0.018750326707959175 Engine time: 0.08536681300029159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 540, 33, 34560, 34560, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 33, 34560, 33, 34560, 540, 540, 540, 33, 33, 540, 33, 34560, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 540, 33, 33, 33, 540, 33, 540, 33, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 34560, 34560, 540, 33, 33, 34560, 540, 540, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 34560, 33, 33, 34560, 33, 34560, 540, 33, 34560, 540, 540, 33, 34560, 34560, 34560, 33, 540, 34560, 33, 34560, 540, 540, 34560, 34560, 33, 540, 34560, 33, 540, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 34560, 33, 34560, 540, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 33, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 33, 33, 33, 33, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33, 540, 34560, 34560, 540, 540, 33, 34560, 540, 34560, 33, 540, 540, 33, 33, 540, 33, 34560, 34560, 33, 34560, 34560, 33, 540, 34560, 33, 33, 540, 540, 33, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 540, 33, 34560, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 33, 34560, 34560, 540, 33, 34560, 540, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 540, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 34560, 33, 33, 33, 540, 33, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 540, 540, 540, 540, 33, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 540, 540, 540, 33, 540, 33, 33, 34560, 33, 540, 33, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4497024 . Total input tokens: 1003489601 . Total output tokens: 883067534
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 82.77100400207564,
    "estimated_duration": 3600.0303524141427,
    "input_throughput": 6995.741017325391,
    "output_throughput": 6132.084687896109,
    "total_throughput": 13127.825705221501,
    "itl": 88.08329504724216,
    "ttft": 2059209.2260594887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2167314607836266,
    "arrivals": 1497804,
    "finished_requests": 102104,
    "scheduler_time": 313.2450545044461
}
#Debug simulation 
Total elapsed time: 82.77119195414707. Arrivals time: 0.5143814915791154 Scheduler time: 82.03312398865819 Scheduler overhead time: 0.08728076471015811 Adapter cache time: 0.018442934844642878 Engine time: 0.08474928792566061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 540, 33, 34560, 34560, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 33, 34560, 33, 34560, 540, 540, 540, 33, 33, 540, 33, 34560, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 540, 33, 33, 33, 540, 33, 540, 33, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 34560, 34560, 540, 33, 33, 34560, 540, 540, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 34560, 33, 33, 34560, 33, 34560, 540, 33, 34560, 540, 540, 33, 34560, 34560, 34560, 33, 540, 34560, 33, 34560, 540, 540, 34560, 34560, 33, 540, 34560, 33, 540, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 34560, 33, 34560, 540, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 33, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 33, 33, 33, 33, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33, 540, 34560, 34560, 540, 540, 33, 34560, 540, 34560, 33, 540, 540, 33, 33, 540, 33, 34560, 34560, 33, 34560, 34560, 33, 540, 34560, 33, 33, 540, 540, 33, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 540, 33, 34560, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 33, 34560, 34560, 540, 33, 34560, 540, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 540, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 34560, 33, 33, 33, 540, 33, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 540, 540, 540, 540, 33, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 540, 540, 540, 33, 540, 33, 33, 34560, 33, 540, 33, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4497024 . Total input tokens: 1003489601 . Total output tokens: 883067534
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 82.35480050975457,
    "estimated_duration": 3600.023633370823,
    "input_throughput": 6821.072443073762,
    "output_throughput": 5974.918553481719,
    "total_throughput": 12795.99099655548,
    "itl": 84.79003079099219,
    "ttft": 2065178.7587867416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3734565847600138,
    "arrivals": 1497804,
    "finished_requests": 99535,
    "scheduler_time": 321.6263598483739
}
#Debug simulation 
Total elapsed time: 82.35498580010608. Arrivals time: 0.4883360876701772 Scheduler time: 81.63883181242272 Scheduler overhead time: 0.08929773047566414 Adapter cache time: 0.018813644535839558 Engine time: 0.08572624064981937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 540, 33, 34560, 34560, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 33, 34560, 33, 34560, 540, 540, 540, 33, 33, 540, 33, 34560, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 540, 33, 33, 33, 540, 33, 540, 33, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 34560, 34560, 540, 33, 33, 34560, 540, 540, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 34560, 33, 33, 34560, 33, 34560, 540, 33, 34560, 540, 540, 33, 34560, 34560, 34560, 33, 540, 34560, 33, 34560, 540, 540, 34560, 34560, 33, 540, 34560, 33, 540, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 34560, 33, 34560, 540, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 33, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 33, 33, 33, 33, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33, 540, 34560, 34560, 540, 540, 33, 34560, 540, 34560, 33, 540, 540, 33, 33, 540, 33, 34560, 34560, 33, 34560, 34560, 33, 540, 34560, 33, 33, 540, 540, 33, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 540, 33, 34560, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 33, 34560, 34560, 540, 33, 34560, 540, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 540, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 34560, 33, 33, 33, 540, 33, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 540, 540, 540, 540, 33, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 540, 540, 540, 33, 540, 33, 33, 34560, 33, 540, 33, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4497024 . Total input tokens: 1003489601 . Total output tokens: 883067534
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.77647462813184,
    "estimated_duration": 3600.032195620402,
    "input_throughput": 7017.099188927275,
    "output_throughput": 6152.651642100907,
    "total_throughput": 13169.750831028183,
    "itl": 87.6125247176027,
    "ttft": 2057484.8893473423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9366036326624325,
    "arrivals": 1497804,
    "finished_requests": 102398,
    "scheduler_time": 312.15031490954897
}
#Debug simulation 
Total elapsed time: 84.77665860485286. Arrivals time: 0.5198990083299577 Scheduler time: 84.03334521641955 Scheduler overhead time: 0.08740456262603402 Adapter cache time: 0.018651513382792473 Engine time: 0.0840602396056056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 540, 33, 34560, 34560, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 33, 34560, 33, 34560, 540, 540, 540, 33, 33, 540, 33, 34560, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 540, 33, 33, 33, 540, 33, 540, 33, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 34560, 34560, 540, 33, 33, 34560, 540, 540, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 34560, 34560, 33, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 34560, 33, 33, 34560, 33, 34560, 540, 33, 34560, 540, 540, 33, 34560, 34560, 34560, 33, 540, 34560, 33, 34560, 540, 540, 34560, 34560, 33, 540, 34560, 33, 540, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 34560, 33, 34560, 540, 540, 540, 540, 540, 33, 33, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 33, 540, 540, 540, 540, 540, 540, 540, 540, 34560, 33, 33, 33, 33, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33, 540, 34560, 34560, 540, 540, 33, 34560, 540, 34560, 33, 540, 540, 33, 33, 540, 33, 34560, 34560, 33, 34560, 34560, 33, 540, 34560, 33, 33, 540, 540, 33, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 33, 33, 33, 33, 33, 33, 33, 33, 540, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 540, 33, 34560, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 33, 34560, 34560, 540, 33, 34560, 540, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 540, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 540, 540, 33, 34560, 33, 33, 33, 540, 33, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 540, 540, 540, 540, 33, 540, 34560, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 540, 540, 540, 33, 540, 33, 33, 34560, 33, 540, 33, 33, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4497024 . Total input tokens: 1003489601 . Total output tokens: 883067534
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.79765638383105,
    "estimated_duration": 3600.014781110681,
    "input_throughput": 6926.499060736321,
    "output_throughput": 6095.77969377935,
    "total_throughput": 13022.27875451567,
    "itl": 85.53788723399063,
    "ttft": 2050035.2445645058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 489,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.609150994699478,
    "arrivals": 1497804,
    "finished_requests": 101097,
    "scheduler_time": 316.80938523545314
}
#Debug simulation 
Total elapsed time: 80.79782920982689. Arrivals time: 0.4927082466892898 Scheduler time: 80.08033147873357 Scheduler overhead time: 0.08768771076574922 Adapter cache time: 0.018819639924913645 Engine time: 0.08473247662186623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 85.80890095792711,
    "estimated_duration": 3600.0275878055404,
    "input_throughput": 7067.867225848163,
    "output_throughput": 6150.942308055476,
    "total_throughput": 13218.809533903639,
    "itl": 88.38513675862498,
    "ttft": 2046054.9276805734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.511189195965463,
    "arrivals": 1490652,
    "finished_requests": 102698,
    "scheduler_time": 313.26412464180925
}
#Debug simulation 
Total elapsed time: 85.8090882319957. Arrivals time: 0.5251305331476033 Scheduler time: 85.05938621470705 Scheduler overhead time: 0.08727554325014353 Adapter cache time: 0.019363341853022575 Engine time: 0.08430602541193366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.49646345013753,
    "estimated_duration": 3600.060153841137,
    "input_throughput": 7095.822266398514,
    "output_throughput": 6162.031758366813,
    "total_throughput": 13257.854024765327,
    "itl": 87.6237160689121,
    "ttft": 2054240.4860921046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.866305144093936,
    "arrivals": 1490652,
    "finished_requests": 103152,
    "scheduler_time": 311.44158429447685
}
#Debug simulation 
Total elapsed time: 83.49664705712348. Arrivals time: 0.5201621875166893 Scheduler time: 82.75337553815916 Scheduler overhead time: 0.0868609338067472 Adapter cache time: 0.01908332947641611 Engine time: 0.08407430583611131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.85639280779287,
    "estimated_duration": 3600.054451705148,
    "input_throughput": 7005.841533328476,
    "output_throughput": 6081.688289361796,
    "total_throughput": 13087.529822690272,
    "itl": 85.12367375691092,
    "ttft": 2061539.9992975735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.09900125680495,
    "arrivals": 1490652,
    "finished_requests": 101850,
    "scheduler_time": 315.83590616224575
}
#Debug simulation 
Total elapsed time: 80.85656038392335. Arrivals time: 0.50157081335783 Scheduler time: 80.12989505240694 Scheduler overhead time: 0.0872625675983727 Adapter cache time: 0.019347427412867546 Engine time: 0.08468872494995594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 84.0989622110501,
    "estimated_duration": 3600.0595624772004,
    "input_throughput": 7062.607870438817,
    "output_throughput": 6141.326724268612,
    "total_throughput": 13203.934594707429,
    "itl": 87.21529568460645,
    "ttft": 2046309.4087338552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9328544516628554,
    "arrivals": 1490652,
    "finished_requests": 102786,
    "scheduler_time": 312.9388550317565
}
#Debug simulation 
Total elapsed time: 84.09914719685912. Arrivals time: 0.5274750790558755 Scheduler time: 83.34662290941924 Scheduler overhead time: 0.0873151384294033 Adapter cache time: 0.019579756073653698 Engine time: 0.08462400548160076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 82.57215132704005,
    "estimated_duration": 3600.0169099973136,
    "input_throughput": 7052.625761143701,
    "output_throughput": 6115.118775932758,
    "total_throughput": 13167.744537076458,
    "itl": 85.32906686092625,
    "ttft": 2054028.9860927528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.152938999030774,
    "arrivals": 1490652,
    "finished_requests": 102471,
    "scheduler_time": 313.894637110594
}
#Debug simulation 
Total elapsed time: 82.5723437750712. Arrivals time: 0.502726127859205 Scheduler time: 81.84267141064629 Scheduler overhead time: 0.0874476064927876 Adapter cache time: 0.019563698675483465 Engine time: 0.08628668263554573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 87.89858391880989,
    "estimated_duration": 3600.0201013285778,
    "input_throughput": 6934.901833127666,
    "output_throughput": 6025.006913710095,
    "total_throughput": 12959.908746837762,
    "itl": 86.79755266943523,
    "ttft": 2059381.1899853004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 493,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.147273023701259,
    "arrivals": 1490652,
    "finished_requests": 100820,
    "scheduler_time": 318.9960503535461
}
#Debug simulation 
Total elapsed time: 87.8987616142258. Arrivals time: 0.521387224085629 Scheduler time: 87.15117146074772 Scheduler overhead time: 0.08840662660077214 Adapter cache time: 0.018804005347192287 Engine time: 0.08532266644760966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_384_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 270, 135, 34560, 34560, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 135, 34560, 135, 34560, 270, 270, 270, 135, 135, 270, 135, 34560, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 270, 135, 135, 135, 270, 135, 270, 135, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 34560, 34560, 270, 135, 135, 34560, 270, 270, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 135, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 34560, 135, 135, 34560, 135, 34560, 270, 135, 34560, 270, 270, 135, 34560, 34560, 34560, 135, 270, 34560, 135, 34560, 270, 270, 34560, 34560, 135, 270, 34560, 135, 270, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 34560, 135, 34560, 270, 270, 270, 270, 270, 135, 135, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 135, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 135, 135, 135, 135, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135, 270, 34560, 34560, 270, 270, 135, 34560, 270, 34560, 135, 270, 270, 135, 135, 270, 135, 34560, 34560, 135, 34560, 34560, 135, 270, 34560, 135, 135, 270, 270, 135, 135, 34560, 34560, 34560, 34560, 270, 135, 270, 270, 135, 135, 135, 135, 135, 135, 135, 135, 270, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 270, 135, 34560, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 135, 34560, 34560, 270, 135, 34560, 270, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 270, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 270, 270, 135, 34560, 135, 135, 135, 270, 135, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 270, 270, 270, 270, 135, 270, 34560, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 270, 270, 270, 135, 270, 135, 135, 34560, 135, 270, 135, 135, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 4475520 . Total input tokens: 998749284 . Total output tokens: 878814277
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 81.57245728792623,
    "estimated_duration": 3600.023864500033,
    "input_throughput": 7021.9062293662655,
    "output_throughput": 6103.922314707144,
    "total_throughput": 13125.828544073409,
    "itl": 85.47942509146822,
    "ttft": 2045780.6180164875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.140816478766537,
    "arrivals": 1490652,
    "finished_requests": 102053,
    "scheduler_time": 315.13680209298764
}
#Debug simulation 
Total elapsed time: 81.57264489494264. Arrivals time: 0.49967668764293194 Scheduler time: 80.84733710484579 Scheduler overhead time: 0.08745901705697179 Adapter cache time: 0.019652739632874727 Engine time: 0.08455923246219754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 84.28100503003225,
    "estimated_duration": 3600.015976911253,
    "input_throughput": 7105.478743443529,
    "output_throughput": 6192.797516173244,
    "total_throughput": 13298.276259616774,
    "itl": 88.51432056590542,
    "ttft": 2054771.9112558947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.306204515975008,
    "arrivals": 1487737,
    "finished_requests": 103440,
    "scheduler_time": 310.05074898742924
}
#Debug simulation 
Total elapsed time: 84.28118797298521. Arrivals time: 0.9471979946829379 Scheduler time: 83.1122115557082 Scheduler overhead time: 0.08623690344393253 Adapter cache time: 0.019086407963186502 Engine time: 0.08325484488159418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 83.11735593900084,
    "estimated_duration": 3600.0166243528215,
    "input_throughput": 7039.205827155207,
    "output_throughput": 6151.477704351186,
    "total_throughput": 13190.683531506393,
    "itl": 87.39328027152824,
    "ttft": 2047914.0859608692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 480,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5196185307204773,
    "arrivals": 1487737,
    "finished_requests": 102412,
    "scheduler_time": 313.76672861221584
}
#Debug simulation 
Total elapsed time: 83.1175302322954. Arrivals time: 0.5182977207005024 Scheduler time: 82.3750011138618 Scheduler overhead time: 0.08730889251455665 Adapter cache time: 0.0186360664665699 Engine time: 0.0849044225178659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.37462766608223,
    "estimated_duration": 3600.0191797963334,
    "input_throughput": 6988.5766557008565,
    "output_throughput": 6084.38980628964,
    "total_throughput": 13072.966461990496,
    "itl": 85.9696230576345,
    "ttft": 2059413.6739391924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.534775065849564,
    "arrivals": 1487737,
    "finished_requests": 101694,
    "scheduler_time": 315.7367285167155
}
#Debug simulation 
Total elapsed time: 80.37482014112175. Arrivals time: 0.497771926689893 Scheduler time: 79.65141392732039 Scheduler overhead time: 0.08793905656784773 Adapter cache time: 0.018683344591408968 Engine time: 0.08523902762681246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 85.34010135009885,
    "estimated_duration": 3600.0810781206455,
    "input_throughput": 7037.278452969048,
    "output_throughput": 6139.127291971323,
    "total_throughput": 13176.405744940372,
    "itl": 87.5798444774359,
    "ttft": 2059475.9356723207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 479,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2925074750231533,
    "arrivals": 1487737,
    "finished_requests": 102471,
    "scheduler_time": 313.09475434899105
}
#Debug simulation 
Total elapsed time: 85.34029655111954. Arrivals time: 0.9727029842324555 Scheduler time: 84.14283760031685 Scheduler overhead time: 0.08747040713205934 Adapter cache time: 0.019004316069185734 Engine time: 0.08477558428421617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 82.23996887402609,
    "estimated_duration": 3600.0479944507465,
    "input_throughput": 6856.382758798718,
    "output_throughput": 5993.990089371633,
    "total_throughput": 12850.372848170351,
    "itl": 84.98971376546837,
    "ttft": 2056932.9780166226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4371829787968005,
    "arrivals": 1487737,
    "finished_requests": 99788,
    "scheduler_time": 321.655985465868
}
#Debug simulation 
Total elapsed time: 82.24016177421436. Arrivals time: 0.5199261009693146 Scheduler time: 81.49089347291738 Scheduler overhead time: 0.08964873244985938 Adapter cache time: 0.018896508496254683 Engine time: 0.08697833586484194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 82.6618400667794,
    "estimated_duration": 3600.0014302449545,
    "input_throughput": 7056.219974410274,
    "output_throughput": 6169.710882167261,
    "total_throughput": 13225.930856577534,
    "itl": 87.74843772992233,
    "ttft": 2046573.56825013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0132106839492785,
    "arrivals": 1487737,
    "finished_requests": 102681,
    "scheduler_time": 312.5283719404612
}
#Debug simulation 
Total elapsed time: 82.6620272458531. Arrivals time: 0.5164798158220947 Scheduler time: 81.92218732694164 Scheduler overhead time: 0.08692898228764534 Adapter cache time: 0.018864423967897892 Engine time: 0.08402767451480031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_384_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 270, 66, 34560, 34560, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 66, 34560, 66, 34560, 270, 270, 270, 66, 66, 270, 66, 34560, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 270, 66, 66, 66, 270, 66, 270, 66, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 34560, 34560, 270, 66, 66, 34560, 270, 270, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 66, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 34560, 66, 66, 34560, 66, 34560, 270, 66, 34560, 270, 270, 66, 34560, 34560, 34560, 66, 270, 34560, 66, 34560, 270, 270, 34560, 34560, 66, 270, 34560, 66, 270, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 34560, 66, 34560, 270, 270, 270, 270, 270, 66, 66, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 66, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 66, 66, 66, 66, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66, 270, 34560, 34560, 270, 270, 66, 34560, 270, 34560, 66, 270, 270, 66, 66, 270, 66, 34560, 34560, 66, 34560, 34560, 66, 270, 34560, 66, 66, 270, 270, 66, 66, 34560, 34560, 34560, 34560, 270, 66, 270, 270, 66, 66, 66, 66, 66, 66, 66, 66, 270, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 270, 66, 34560, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 66, 34560, 34560, 270, 66, 34560, 270, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 270, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 270, 270, 66, 34560, 66, 66, 66, 270, 66, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 270, 270, 270, 270, 66, 270, 34560, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 270, 270, 270, 66, 270, 66, 66, 34560, 66, 270, 66, 66, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4466688 . Total input tokens: 996746799 . Total output tokens: 877136313
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 80.7485747220926,
    "estimated_duration": 3600.0233159534882,
    "input_throughput": 6963.656009922323,
    "output_throughput": 6058.059930710118,
    "total_throughput": 13021.715940632443,
    "itl": 84.95584200395251,
    "ttft": 2064847.9961429124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5188680778816632,
    "arrivals": 1487737,
    "finished_requests": 101257,
    "scheduler_time": 317.1812789797174
}
#Debug simulation 
Total elapsed time: 80.7487800871022. Arrivals time: 0.5278535597026348 Scheduler time: 79.99690095055848 Scheduler overhead time: 0.08691750932484865 Adapter cache time: 0.018936617765575647 Engine time: 0.08442104235291481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_384_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 87.21491386368871,
    "estimated_duration": 3600.0025153853744,
    "input_throughput": 7055.091459368369,
    "output_throughput": 6138.323211042103,
    "total_throughput": 13193.414670410471,
    "itl": 87.50840345478896,
    "ttft": 2058593.924350038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9160723830899493,
    "arrivals": 1486263,
    "finished_requests": 102727,
    "scheduler_time": 312.9055485751896
}
#Debug simulation 
Total elapsed time: 87.21510818460956. Arrivals time: 0.5475352597422898 Scheduler time: 86.44366044504568 Scheduler overhead time: 0.08726334758102894 Adapter cache time: 0.018772162031382322 Engine time: 0.08467873511835933 
