INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 47.45594529109076,
    "estimated_duration": 3600.0249775220595,
    "input_throughput": 7915.670357269177,
    "output_throughput": 6962.583358866823,
    "total_throughput": 14878.253716136,
    "itl": 88.12892359176075,
    "ttft": 948634.7774607439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 58,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3835197238530961,
    "arrivals": 145872,
    "finished_requests": 115552,
    "scheduler_time": 143.61076397891756
}
#Debug simulation 
Total elapsed time: 47.45616465713829. Arrivals time: 0.40279851481318474 Scheduler time: 46.84671928733587 Scheduler overhead time: 0.08046148112043738 Adapter cache time: 0.013914016541093588 Engine time: 0.07972833048552275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 43.25397558417171,
    "estimated_duration": 3600.0391610849033,
    "input_throughput": 7871.92770188081,
    "output_throughput": 6924.139956435357,
    "total_throughput": 14796.067658316166,
    "itl": 87.00283981306958,
    "ttft": 957441.6395346725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 58,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42579625348560496,
    "arrivals": 145872,
    "finished_requests": 114889,
    "scheduler_time": 144.30125732617807
}
#Debug simulation 
Total elapsed time: 43.25413471227512. Arrivals time: 0.4000390004366636 Scheduler time: 42.649960548616946 Scheduler overhead time: 0.07962346635758877 Adapter cache time: 0.013067566324025393 Engine time: 0.0788725414313376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 30.988091343082488,
    "estimated_duration": 3600.0702404593294,
    "input_throughput": 7794.666805284243,
    "output_throughput": 6852.64296311407,
    "total_throughput": 14647.309768398312,
    "itl": 84.58851456631253,
    "ttft": 974655.2005860461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 71,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.534098662454635,
    "arrivals": 145872,
    "finished_requests": 113738,
    "scheduler_time": 146.02911125731003
}
#Debug simulation 
Total elapsed time: 30.98825410194695. Arrivals time: 0.37223888374865055 Scheduler time: 30.418636003509164 Scheduler overhead time: 0.07568637980148196 Adapter cache time: 0.01289967866614461 Engine time: 0.07600674545392394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 43.43645434966311,
    "estimated_duration": 3600.056077609774,
    "input_throughput": 7872.357926940048,
    "output_throughput": 6921.899954554293,
    "total_throughput": 14794.25788149434,
    "itl": 86.95070414561451,
    "ttft": 957244.4005456015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 56,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3838757711648942,
    "arrivals": 145872,
    "finished_requests": 114908,
    "scheduler_time": 144.28936893427164
}
#Debug simulation 
Total elapsed time: 43.436587324831635. Arrivals time: 0.396804160438478 Scheduler time: 42.835602779872715 Scheduler overhead time: 0.08025359455496073 Adapter cache time: 0.013317283242940903 Engine time: 0.07854835269972682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 29.210022520273924,
    "estimated_duration": 3600.0126830222844,
    "input_throughput": 7789.137002833838,
    "output_throughput": 6852.801134936249,
    "total_throughput": 14641.938137770087,
    "itl": 84.57273674281167,
    "ttft": 976532.5236933512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4685582275455818,
    "arrivals": 145872,
    "finished_requests": 113709,
    "scheduler_time": 146.01422654867392
}
#Debug simulation 
Total elapsed time: 29.210189559031278. Arrivals time: 0.3704079305753112 Scheduler time: 28.64416820416227 Scheduler overhead time: 0.07524268282577395 Adapter cache time: 0.01229544123634696 Engine time: 0.07600654661655426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 42.76887895585969,
    "estimated_duration": 3600.062863857026,
    "input_throughput": 7872.501140059415,
    "output_throughput": 6923.541044307132,
    "total_throughput": 14796.042184366548,
    "itl": 87.01964005958891,
    "ttft": 958367.0746316039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 57,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36388349361252065,
    "arrivals": 145872,
    "finished_requests": 114914,
    "scheduler_time": 144.29199441665645
}
#Debug simulation 
Total elapsed time: 42.769022951833904. Arrivals time: 0.39990827860310674 Scheduler time: 42.1650407477282 Scheduler overhead time: 0.08026435831561685 Adapter cache time: 0.013124941848218441 Engine time: 0.07832003245130181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 28.868769723922014,
    "estimated_duration": 3600.00547650071,
    "input_throughput": 7789.225372861588,
    "output_throughput": 6852.861241750152,
    "total_throughput": 14642.086614611739,
    "itl": 84.60470044430996,
    "ttft": 976861.9759529682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4657104560546577,
    "arrivals": 145872,
    "finished_requests": 113701,
    "scheduler_time": 146.00805514484117
}
#Debug simulation 
Total elapsed time: 28.86898912023753. Arrivals time: 0.36912085115909576 Scheduler time: 28.304301666095853 Scheduler overhead time: 0.07487983116880059 Adapter cache time: 0.01234534289687872 Engine time: 0.07598950807005167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 31.43522532004863,
    "estimated_duration": 3600.0231110379605,
    "input_throughput": 7978.8923887542005,
    "output_throughput": 6956.3942306969075,
    "total_throughput": 14935.286619451108,
    "itl": 87.64456384791453,
    "ttft": 935856.6590698733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3636824967572463,
    "arrivals": 144123,
    "finished_requests": 115625,
    "scheduler_time": 141.1539977422747
}
#Debug simulation 
Total elapsed time: 31.43533621309325. Arrivals time: 0.3861701348796487 Scheduler time: 30.857405235990882 Scheduler overhead time: 0.07440906297415495 Adapter cache time: 0.012055989820510149 Engine time: 0.074097846634686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 32.439198879059404,
    "estimated_duration": 3600.0818978256375,
    "input_throughput": 7916.363796393922,
    "output_throughput": 6897.934742817548,
    "total_throughput": 14814.29853921147,
    "itl": 85.71956059583529,
    "ttft": 941568.392819833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3819398439489303,
    "arrivals": 144123,
    "finished_requests": 114636,
    "scheduler_time": 142.44314715205942
}
#Debug simulation 
Total elapsed time: 32.439360483083874. Arrivals time: 0.3862768094986677 Scheduler time: 31.8589539071545 Scheduler overhead time: 0.07496216567233205 Adapter cache time: 0.012330980971455574 Engine time: 0.07494867825880647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 31.8610271057114,
    "estimated_duration": 3600.063218969625,
    "input_throughput": 7822.502352628161,
    "output_throughput": 6826.760949779689,
    "total_throughput": 14649.26330240785,
    "itl": 83.38787748322343,
    "ttft": 959004.4530540808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 53,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4006909864908085,
    "arrivals": 144123,
    "finished_requests": 113362,
    "scheduler_time": 144.23338752265258
}
#Debug simulation 
Total elapsed time: 31.861155061982572. Arrivals time: 0.3754499643109739 Scheduler time: 31.286509986501187 Scheduler overhead time: 0.07697035279124975 Adapter cache time: 0.01283386629074812 Engine time: 0.07657979661598802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 31.25922408234328,
    "estimated_duration": 3600.064831352064,
    "input_throughput": 7942.512521159574,
    "output_throughput": 6923.102268310963,
    "total_throughput": 14865.614789470537,
    "itl": 86.4748585860452,
    "ttft": 943993.6394004821,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 58,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.398031834019348,
    "arrivals": 144123,
    "finished_requests": 115058,
    "scheduler_time": 142.02105503581825
}
#Debug simulation 
Total elapsed time: 31.2593383830972. Arrivals time: 0.38262227177619934 Scheduler time: 30.681971278972924 Scheduler overhead time: 0.0743153290823102 Adapter cache time: 0.012483151163905859 Engine time: 0.07630240079015493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 31.163614590186626,
    "estimated_duration": 3600.0638360401167,
    "input_throughput": 7859.239804792371,
    "output_throughput": 6853.075979657456,
    "total_throughput": 14712.315784449826,
    "itl": 84.23949444171973,
    "ttft": 960695.5394317129,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 58,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43522020343225454,
    "arrivals": 144123,
    "finished_requests": 113841,
    "scheduler_time": 143.71731318872517
}
#Debug simulation 
Total elapsed time: 31.163762780837715. Arrivals time: 0.3898370494134724 Scheduler time: 30.574176685418934 Scheduler overhead time: 0.07704882556572556 Adapter cache time: 0.012514866422861814 Engine time: 0.07735675061121583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 31.24017325323075,
    "estimated_duration": 3600.0494539322954,
    "input_throughput": 7944.5666972045665,
    "output_throughput": 6925.313754445128,
    "total_throughput": 14869.880451649695,
    "itl": 86.53034398471769,
    "ttft": 943949.5822951106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37665133549366175,
    "arrivals": 144123,
    "finished_requests": 115085,
    "scheduler_time": 141.99640369831914
}
#Debug simulation 
Total elapsed time: 31.240301677025855. Arrivals time: 0.37655589263886213 Scheduler time: 30.67032024404034 Scheduler overhead time: 0.07425719127058983 Adapter cache time: 0.012527113314718008 Engine time: 0.07504393626004457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 30.757731081917882,
    "estimated_duration": 3600.006700030803,
    "input_throughput": 7859.799538639162,
    "output_throughput": 6853.470022649928,
    "total_throughput": 14713.26956128909,
    "itl": 84.25606951712014,
    "ttft": 960827.4128498322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43723800970241433,
    "arrivals": 144123,
    "finished_requests": 113840,
    "scheduler_time": 143.7045588628563
}
#Debug simulation 
Total elapsed time: 30.75793262477964. Arrivals time: 0.383899241220206 Scheduler time: 30.17517103254795 Scheduler overhead time: 0.07598574738949537 Adapter cache time: 0.012803526129573584 Engine time: 0.07746409578248858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 33.19175774883479,
    "estimated_duration": 3600.0695922669347,
    "input_throughput": 8037.983782913046,
    "output_throughput": 6965.091189865192,
    "total_throughput": 15003.074972778237,
    "itl": 87.46142037797748,
    "ttft": 893430.9648436918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 54,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35707008772529636,
    "arrivals": 143211,
    "finished_requests": 116312,
    "scheduler_time": 137.97024309164206
}
#Debug simulation 
Total elapsed time: 33.1919192946516. Arrivals time: 0.38125238195061684 Scheduler time: 32.615287847816944 Scheduler overhead time: 0.07508248323574662 Adapter cache time: 0.01236580265685916 Engine time: 0.07604370405897498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 27.532081122975796,
    "estimated_duration": 3600.060340283554,
    "input_throughput": 8016.472856598536,
    "output_throughput": 6930.244674186461,
    "total_throughput": 14946.717530784998,
    "itl": 86.27852854112318,
    "ttft": 913048.7783828459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 56,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41302841160446385,
    "arrivals": 143211,
    "finished_requests": 115865,
    "scheduler_time": 138.87344483591792
}
#Debug simulation 
Total elapsed time: 27.532201217021793. Arrivals time: 0.3809474455192685 Scheduler time: 26.9621527120471 Scheduler overhead time: 0.07269288692623377 Adapter cache time: 0.011876323726028204 Engine time: 0.07315617520362139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 23.025320305023342,
    "estimated_duration": 3600.041363594703,
    "input_throughput": 7924.6314468768505,
    "output_throughput": 6859.340353618245,
    "total_throughput": 14783.971800495096,
    "itl": 84.01999594514315,
    "ttft": 937633.2373079127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 59,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.44743862033821646,
    "arrivals": 143211,
    "finished_requests": 114654,
    "scheduler_time": 140.71953848633203
}
#Debug simulation 
Total elapsed time: 23.02546351822093. Arrivals time: 0.3662925870157778 Scheduler time: 22.470494531095028 Scheduler overhead time: 0.07218601927161217 Adapter cache time: 0.011704850476235151 Engine time: 0.07298847613856196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 27.497219478711486,
    "estimated_duration": 3600.0618895298985,
    "input_throughput": 8016.026081087274,
    "output_throughput": 6931.584168753984,
    "total_throughput": 14947.610249841258,
    "itl": 86.34270502865834,
    "ttft": 912310.1266541627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37888007119763645,
    "arrivals": 143211,
    "finished_requests": 115938,
    "scheduler_time": 138.7895146930334
}
#Debug simulation 
Total elapsed time: 27.497353238984942. Arrivals time: 0.37139512365683913 Scheduler time: 26.93478639656678 Scheduler overhead time: 0.0730902194045484 Adapter cache time: 0.012169180903583765 Engine time: 0.07420278619974852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 23.448117551859468,
    "estimated_duration": 3600.0446396201537,
    "input_throughput": 7925.48728034952,
    "output_throughput": 6857.239970947887,
    "total_throughput": 14782.727251297407,
    "itl": 83.98900139049057,
    "ttft": 937132.2104363088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 58,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4337185142142699,
    "arrivals": 143211,
    "finished_requests": 114641,
    "scheduler_time": 140.7365049170407
}
#Debug simulation 
Total elapsed time: 23.448277866933495. Arrivals time: 0.3693835190497339 Scheduler time: 22.889659758191556 Scheduler overhead time: 0.07243857905268669 Adapter cache time: 0.012321189045906067 Engine time: 0.07265218580141664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 23.98878695210442,
    "estimated_duration": 3600.0682777352536,
    "input_throughput": 8019.660954364593,
    "output_throughput": 6934.437092316688,
    "total_throughput": 14954.098046681282,
    "itl": 86.35276307911853,
    "ttft": 920226.9691051411,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 56,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3574995726719501,
    "arrivals": 143211,
    "finished_requests": 116000,
    "scheduler_time": 138.79224180665932
}
#Debug simulation 
Total elapsed time: 23.98890430899337. Arrivals time: 0.3650777582079172 Scheduler time: 23.43697765469551 Scheduler overhead time: 0.07159861922264099 Adapter cache time: 0.011765115894377232 Engine time: 0.07225660886615515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 23.342743213288486,
    "estimated_duration": 3600.033858162298,
    "input_throughput": 7922.092159032761,
    "output_throughput": 6856.246905577643,
    "total_throughput": 14778.339064610404,
    "itl": 83.90783575082004,
    "ttft": 937733.7443074313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 58,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4338741211593151,
    "arrivals": 143211,
    "finished_requests": 114604,
    "scheduler_time": 140.79109252543276
}
#Debug simulation 
Total elapsed time: 23.342941931914538. Arrivals time: 0.3595924009568989 Scheduler time: 22.794991661328822 Scheduler overhead time: 0.07203779090195894 Adapter cache time: 0.011774254962801933 Engine time: 0.07279181061312556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 42.37408583331853,
    "estimated_duration": 3600.061295459257,
    "input_throughput": 7977.446394099882,
    "output_throughput": 6971.436578498122,
    "total_throughput": 14948.882972598005,
    "itl": 87.87414308085627,
    "ttft": 867499.5857303077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 142732,
    "finished_requests": 116003,
    "scheduler_time": 138.39107931545672
}
#Debug simulation 
Total elapsed time: 42.37424644920975. Arrivals time: 0.4174580993130803 Scheduler time: 41.749173232354224 Scheduler overhead time: 0.08134915819391608 Adapter cache time: 0.013461173512041569 Engine time: 0.0800107242539525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 41.85022002598271,
    "estimated_duration": 3600.085718890121,
    "input_throughput": 7936.821573462121,
    "output_throughput": 6935.928738857374,
    "total_throughput": 14872.750312319495,
    "itl": 86.71861724910379,
    "ttft": 868060.6599934996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 21,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.15488565435167398,
    "arrivals": 142732,
    "finished_requests": 115410,
    "scheduler_time": 139.32401393111488
}
#Debug simulation 
Total elapsed time: 41.85039091994986. Arrivals time: 0.41036650724709034 Scheduler time: 41.23155277315527 Scheduler overhead time: 0.08156914543360472 Adapter cache time: 0.013713269494473934 Engine time: 0.08037701575085521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 33.6932098842226,
    "estimated_duration": 3600.044893704943,
    "input_throughput": 7865.033585973007,
    "output_throughput": 6863.078303052128,
    "total_throughput": 14728.111889025135,
    "itl": 84.33652533477287,
    "ttft": 947367.0885774607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 51,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3879564525326714,
    "arrivals": 142732,
    "finished_requests": 114223,
    "scheduler_time": 141.23653765520834
}
#Debug simulation 
Total elapsed time: 33.69338253419846. Arrivals time: 0.38530457578599453 Scheduler time: 33.10503344936296 Scheduler overhead time: 0.07811715081334114 Adapter cache time: 0.013139537069946527 Engine time: 0.07888707611709833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 42.090733490884304,
    "estimated_duration": 3600.021636220846,
    "input_throughput": 7936.75229963179,
    "output_throughput": 6936.109146891857,
    "total_throughput": 14872.861446523646,
    "itl": 86.71873897673801,
    "ttft": 867993.9782859422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 21,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14516810753848405,
    "arrivals": 142732,
    "finished_requests": 115410,
    "scheduler_time": 139.3185242379163
}
#Debug simulation 
Total elapsed time: 42.09089709864929. Arrivals time: 0.4140697340480983 Scheduler time: 41.46970691997558 Scheduler overhead time: 0.08038265444338322 Adapter cache time: 0.013440697453916073 Engine time: 0.08056003646925092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 33.708531918004155,
    "estimated_duration": 3600.083045822461,
    "input_throughput": 7864.9388471346765,
    "output_throughput": 6863.160845323033,
    "total_throughput": 14728.099692457708,
    "itl": 84.33558486317602,
    "ttft": 947287.64589335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 50,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3761522699799389,
    "arrivals": 142732,
    "finished_requests": 114221,
    "scheduler_time": 141.23390787457313
}
#Debug simulation 
Total elapsed time: 33.70870479475707. Arrivals time: 0.39293312933295965 Scheduler time: 33.11038367636502 Scheduler overhead time: 0.07847108086571097 Adapter cache time: 0.013037619180977345 Engine time: 0.07919631991535425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 41.96134446095675,
    "estimated_duration": 3600.0563098949456,
    "input_throughput": 7936.715023447449,
    "output_throughput": 6935.931232900257,
    "total_throughput": 14872.646256347707,
    "itl": 86.72206821183431,
    "ttft": 868062.6809108065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 21,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13406233975198126,
    "arrivals": 142732,
    "finished_requests": 115409,
    "scheduler_time": 139.32164105781544
}
#Debug simulation 
Total elapsed time: 41.96152434591204. Arrivals time: 0.40585208171978593 Scheduler time: 41.34807542944327 Scheduler overhead time: 0.08137825457379222 Adapter cache time: 0.013005052227526903 Engine time: 0.08052617404609919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 33.61115322075784,
    "estimated_duration": 3600.038461604573,
    "input_throughput": 7865.0956932776435,
    "output_throughput": 6863.126120321396,
    "total_throughput": 14728.221813599039,
    "itl": 84.33829303776596,
    "ttft": 947257.4320635078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 50,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37242416080087426,
    "arrivals": 142732,
    "finished_requests": 114224,
    "scheduler_time": 141.2350240838095
}
#Debug simulation 
Total elapsed time: 33.611365132033825. Arrivals time: 0.39526368025690317 Scheduler time: 33.012957516126335 Scheduler overhead time: 0.07893647905439138 Adapter cache time: 0.012667081318795681 Engine time: 0.07858367497101426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 38.98609383683652,
    "estimated_duration": 3600.0297044951053,
    "input_throughput": 7962.679853504241,
    "output_throughput": 6974.690227874518,
    "total_throughput": 14937.370081378758,
    "itl": 87.92250261658515,
    "ttft": 881782.8981446531,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 48,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3173956335335968,
    "arrivals": 142534,
    "finished_requests": 115862,
    "scheduler_time": 138.14760430373462
}
#Debug simulation 
Total elapsed time: 38.986268661916256. Arrivals time: 0.3978382255882025 Scheduler time: 38.39123735204339 Scheduler overhead time: 0.07641773950308561 Adapter cache time: 0.012863552663475275 Engine time: 0.07617944898083806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 35.562422045040876,
    "estimated_duration": 3600.0780440173476,
    "input_throughput": 7922.300197740537,
    "output_throughput": 6941.4464615644265,
    "total_throughput": 14863.746659304965,
    "itl": 86.74445995587556,
    "ttft": 897353.5810141464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 51,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3755559230083598,
    "arrivals": 142534,
    "finished_requests": 115271,
    "scheduler_time": 139.10052327569153
}
#Debug simulation 
Total elapsed time: 35.56254550721496. Arrivals time: 0.37685870472341776 Scheduler time: 34.98911162605509 Scheduler overhead time: 0.07638600887730718 Adapter cache time: 0.012702627573162317 Engine time: 0.07543659303337336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 23.14520368911326,
    "estimated_duration": 3600.001243969587,
    "input_throughput": 7832.788682291416,
    "output_throughput": 6862.548739777244,
    "total_throughput": 14695.33742206866,
    "itl": 84.25953480711796,
    "ttft": 940891.4985291804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30253725821152333,
    "arrivals": 142534,
    "finished_requests": 113845,
    "scheduler_time": 141.10068797984002
}
#Debug simulation 
Total elapsed time: 23.145351618062705. Arrivals time: 0.3610700620338321 Scheduler time: 22.596884321887046 Scheduler overhead time: 0.07139460789039731 Adapter cache time: 0.011676866561174393 Engine time: 0.0725557510741055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 31.588888277765363,
    "estimated_duration": 3600.065202972115,
    "input_throughput": 7926.643099808611,
    "output_throughput": 6939.784584838675,
    "total_throughput": 14866.427684647286,
    "itl": 86.75843153498592,
    "ttft": 903383.4889736708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 48,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32863974072039126,
    "arrivals": 142534,
    "finished_requests": 115289,
    "scheduler_time": 139.02352013857595
}
#Debug simulation 
Total elapsed time: 31.58905150787905. Arrivals time: 0.3809158653020859 Scheduler time: 31.013770218007267 Scheduler overhead time: 0.0739857442677021 Adapter cache time: 0.012783598154783249 Engine time: 0.07569997664541006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 23.34126377524808,
    "estimated_duration": 3600.054020172324,
    "input_throughput": 7835.881862308747,
    "output_throughput": 6865.79336351649,
    "total_throughput": 14701.675225825236,
    "itl": 84.36233942976601,
    "ttft": 940540.750862079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2992233833856881,
    "arrivals": 142534,
    "finished_requests": 113908,
    "scheduler_time": 141.04092365933988
}
#Debug simulation 
Total elapsed time: 23.341364535037428. Arrivals time: 0.37126164324581623 Scheduler time: 22.78089110739529 Scheduler overhead time: 0.07211186038330197 Adapter cache time: 0.011807645671069622 Engine time: 0.0736420201137662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 35.52065801713616,
    "estimated_duration": 3600.021857709745,
    "input_throughput": 7919.11302953499,
    "output_throughput": 6942.612291776571,
    "total_throughput": 14861.72532131156,
    "itl": 86.76792805011053,
    "ttft": 897844.2315889655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3319638889096679,
    "arrivals": 142534,
    "finished_requests": 115226,
    "scheduler_time": 139.1062392455749
}
#Debug simulation 
Total elapsed time: 35.52080986602232. Arrivals time: 0.3817147817462683 Scheduler time: 34.94217396294698 Scheduler overhead time: 0.07609301246702671 Adapter cache time: 0.01239640824496746 Engine time: 0.07651311857625842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 22.869875171221793,
    "estimated_duration": 3600.026944779056,
    "input_throughput": 7828.673627255225,
    "output_throughput": 6859.009495973496,
    "total_throughput": 14687.683123228722,
    "itl": 84.1665492319948,
    "ttft": 941052.5598432015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2967379772663117,
    "arrivals": 142534,
    "finished_requests": 113784,
    "scheduler_time": 141.16293026939712
}
#Debug simulation 
Total elapsed time: 22.870094120968133. Arrivals time: 0.3428089339286089 Scheduler time: 22.340187090449035 Scheduler overhead time: 0.07148099830374122 Adapter cache time: 0.011601246427744627 Engine time: 0.07250886550173163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 42.36500067310408,
    "estimated_duration": 3600.048272661597,
    "input_throughput": 8022.54186959755,
    "output_throughput": 6966.834081215555,
    "total_throughput": 14989.375950813104,
    "itl": 87.50551848156942,
    "ttft": 849444.2642270931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11241095354314896,
    "arrivals": 142385,
    "finished_requests": 115945,
    "scheduler_time": 137.6562528478581
}
#Debug simulation 
Total elapsed time: 42.36514144204557. Arrivals time: 0.4007131769321859 Scheduler time: 41.75547050638124 Scheduler overhead time: 0.08163513103500009 Adapter cache time: 0.01333916699513793 Engine time: 0.081331723369658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 41.89411675510928,
    "estimated_duration": 3600.0198986217933,
    "input_throughput": 7984.113368652145,
    "output_throughput": 6930.888356909415,
    "total_throughput": 14915.00172556156,
    "itl": 86.35870130133046,
    "ttft": 858100.3484318325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.127961749616079,
    "arrivals": 142385,
    "finished_requests": 115385,
    "scheduler_time": 138.58695293216465
}
#Debug simulation 
Total elapsed time: 41.89429072383791. Arrivals time: 0.41108676604926586 Scheduler time: 41.27639906294644 Scheduler overhead time: 0.08033543732017279 Adapter cache time: 0.01343346294015646 Engine time: 0.07997019262984395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 40.82272068224847,
    "estimated_duration": 3600.0869933378713,
    "input_throughput": 7900.397699453782,
    "output_throughput": 6858.573152730114,
    "total_throughput": 14758.970852183895,
    "itl": 84.0586789457162,
    "ttft": 876354.8333235695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13216682816855607,
    "arrivals": 142385,
    "finished_requests": 114146,
    "scheduler_time": 140.4987565123491
}
#Debug simulation 
Total elapsed time: 40.82286802818999. Arrivals time: 0.3733709533698857 Scheduler time: 40.237178158480674 Scheduler overhead time: 0.08230825653299689 Adapter cache time: 0.013402096927165985 Engine time: 0.08310537878423929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 42.01512497011572,
    "estimated_duration": 3600.069566384444,
    "input_throughput": 7983.902385771464,
    "output_throughput": 6930.7599589133915,
    "total_throughput": 14914.662344684855,
    "itl": 86.35872486264022,
    "ttft": 858064.7983310507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963242377620191,
    "arrivals": 142385,
    "finished_requests": 115384,
    "scheduler_time": 138.5925099947541
}
#Debug simulation 
Total elapsed time: 42.015287136193365. Arrivals time: 0.3898181300610304 Scheduler time: 41.416879412252456 Scheduler overhead time: 0.08049781061708927 Adapter cache time: 0.012979611288756132 Engine time: 0.08202850259840488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 41.04058641102165,
    "estimated_duration": 3600.0897469725983,
    "input_throughput": 7900.551652613145,
    "output_throughput": 6858.569017831748,
    "total_throughput": 14759.120670444892,
    "itl": 84.05809017205455,
    "ttft": 876361.202057033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13092412510886786,
    "arrivals": 142385,
    "finished_requests": 114148,
    "scheduler_time": 140.49729176412606
}
#Debug simulation 
Total elapsed time: 41.04076167801395. Arrivals time: 0.40315692592412233 Scheduler time: 40.42803449416533 Scheduler overhead time: 0.08057790389284492 Adapter cache time: 0.013282338622957468 Engine time: 0.08251406252384186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 41.87762328935787,
    "estimated_duration": 3600.003581257928,
    "input_throughput": 7983.969002041032,
    "output_throughput": 6930.711716481588,
    "total_throughput": 14914.68071852262,
    "itl": 86.35799244953952,
    "ttft": 858028.8467370599,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10852665598969911,
    "arrivals": 142385,
    "finished_requests": 115381,
    "scheduler_time": 138.5883143965059
}
#Debug simulation 
Total elapsed time: 41.8777550351806. Arrivals time: 0.3961204928345978 Scheduler time: 41.272134077735245 Scheduler overhead time: 0.08160908240824938 Adapter cache time: 0.013394713867455721 Engine time: 0.0813366319052875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 40.78303859708831,
    "estimated_duration": 3600.057553463535,
    "input_throughput": 7900.472027908676,
    "output_throughput": 6858.658405681542,
    "total_throughput": 14759.130433590217,
    "itl": 84.05800501230812,
    "ttft": 876334.7884060977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12926718769595025,
    "arrivals": 142385,
    "finished_requests": 114146,
    "scheduler_time": 140.49687659466153
}
#Debug simulation 
Total elapsed time: 40.7832980081439. Arrivals time: 0.3765155537985265 Scheduler time: 40.19409918040037 Scheduler overhead time: 0.08283021207898855 Adapter cache time: 0.013689834158867598 Engine time: 0.08243307890370488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 9.785103497095406,
    "estimated_duration": 3600.0467753778717,
    "input_throughput": 8097.190069687442,
    "output_throughput": 7012.860269669702,
    "total_throughput": 15110.050339357143,
    "itl": 87.37598367580155,
    "ttft": 568372.5219137275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 188,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2431328980065868,
    "arrivals": 132241,
    "finished_requests": 117367,
    "scheduler_time": 118.86680358071102
}
#Debug simulation 
Total elapsed time: 9.785224064718932. Arrivals time: 0.2920581214129925 Scheduler time: 9.319995404686779 Scheduler overhead time: 0.06513265101239085 Adapter cache time: 0.01167164882645011 Engine time: 0.06632479000836611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 9.5510971439071,
    "estimated_duration": 3600.0289247025926,
    "input_throughput": 8054.063066283652,
    "output_throughput": 6977.08505274719,
    "total_throughput": 15031.148119030842,
    "itl": 86.23738603472874,
    "ttft": 591328.7379718601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4092318421881653,
    "arrivals": 132241,
    "finished_requests": 116749,
    "scheduler_time": 119.6014447795674
}
#Debug simulation 
Total elapsed time: 9.551217903848737. Arrivals time: 0.27846650779247284 Scheduler time: 9.099318919237703 Scheduler overhead time: 0.06531270639970899 Adapter cache time: 0.011747765354812145 Engine time: 0.06620687153190374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 9.438540806993842,
    "estimated_duration": 3600.0476471076818,
    "input_throughput": 7969.469243844659,
    "output_throughput": 6904.51389441194,
    "total_throughput": 14873.983138256599,
    "itl": 83.9287590221497,
    "ttft": 637837.9286673076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5516627821279716,
    "arrivals": 132241,
    "finished_requests": 115500,
    "scheduler_time": 121.20134815838385
}
#Debug simulation 
Total elapsed time: 9.438658329192549. Arrivals time: 0.2818012540228665 Scheduler time: 8.978164839558303 Scheduler overhead time: 0.06723511917516589 Adapter cache time: 0.012110920157283545 Engine time: 0.06836759066209197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 9.633491312153637,
    "estimated_duration": 3600.0303119007576,
    "input_throughput": 8053.720521228564,
    "output_throughput": 6976.426536458663,
    "total_throughput": 15030.147057687227,
    "itl": 86.22708367148692,
    "ttft": 591547.0189231988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3028960111690668,
    "arrivals": 132241,
    "finished_requests": 116742,
    "scheduler_time": 119.60953236545879
}
#Debug simulation 
Total elapsed time: 9.633610448334366. Arrivals time: 0.28913818253204226 Scheduler time: 9.170541206374764 Scheduler overhead time: 0.06561395572498441 Adapter cache time: 0.011723133735358715 Engine time: 0.06641846336424351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 9.465864870231599,
    "estimated_duration": 3600.048199171978,
    "input_throughput": 7969.8835717253005,
    "output_throughput": 6904.650055995695,
    "total_throughput": 14874.533627720995,
    "itl": 83.92876695439378,
    "ttft": 637735.3323911485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5353005251754102,
    "arrivals": 132241,
    "finished_requests": 115504,
    "scheduler_time": 121.20161222790239
}
#Debug simulation 
Total elapsed time: 9.465981415007263. Arrivals time: 0.28001143503934145 Scheduler time: 9.00723800715059 Scheduler overhead time: 0.067789644934237 Adapter cache time: 0.012093613855540752 Engine time: 0.06791278580203652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 9.586473505012691,
    "estimated_duration": 3600.0130025466824,
    "input_throughput": 8053.769800133946,
    "output_throughput": 6976.36952484155,
    "total_throughput": 15030.139324975495,
    "itl": 86.22399562029246,
    "ttft": 591555.775914683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2384806624706801,
    "arrivals": 132241,
    "finished_requests": 116741,
    "scheduler_time": 119.60712494689055
}
#Debug simulation 
Total elapsed time: 9.58658620994538. Arrivals time: 0.29042050056159496 Scheduler time: 9.122076258994639 Scheduler overhead time: 0.0658086515031755 Adapter cache time: 0.011743387673050165 Engine time: 0.06636725133284926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 9.384480983018875,
    "estimated_duration": 3600.0517869333326,
    "input_throughput": 7968.498704413565,
    "output_throughput": 6904.890671357537,
    "total_throughput": 14873.389375771103,
    "itl": 83.92624764936689,
    "ttft": 638008.0558045343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5098799744620945,
    "arrivals": 132241,
    "finished_requests": 115492,
    "scheduler_time": 121.2066385096489
}
#Debug simulation 
Total elapsed time: 9.38469080813229. Arrivals time: 0.28901952551677823 Scheduler time: 8.917912742588669 Scheduler overhead time: 0.06687159277498722 Adapter cache time: 0.01205925177782774 Engine time: 0.06786049017682672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 9.328439529053867,
    "estimated_duration": 3599.9705966223864,
    "input_throughput": 8078.359314180392,
    "output_throughput": 7001.044126206703,
    "total_throughput": 15079.403440387096,
    "itl": 87.08003380470561,
    "ttft": 561839.9866778731,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4282803509011848,
    "arrivals": 131339,
    "finished_requests": 116725,
    "scheduler_time": 118.26920365857778
}
#Debug simulation 
Total elapsed time: 9.328577387146652. Arrivals time: 0.28496190533041954 Scheduler time: 8.870237077586353 Scheduler overhead time: 0.06532453466206789 Adapter cache time: 0.011876610107719898 Engine time: 0.06618493190035224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 9.21541480626911,
    "estimated_duration": 3599.9908128312113,
    "input_throughput": 8040.695519785255,
    "output_throughput": 6966.417500459286,
    "total_throughput": 15007.11302024454,
    "itl": 85.93551017774324,
    "ttft": 584165.1288345824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6179653060110297,
    "arrivals": 131339,
    "finished_requests": 116133,
    "scheduler_time": 118.98045951169622
}
#Debug simulation 
Total elapsed time: 9.215535054914653. Arrivals time: 0.2796060903929174 Scheduler time: 8.761436592321843 Scheduler overhead time: 0.06557519314810634 Adapter cache time: 0.011916959658265114 Engine time: 0.06665639160200953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 9.071488269139081,
    "estimated_duration": 3599.9973612617714,
    "input_throughput": 7955.341386684288,
    "output_throughput": 6894.213942229413,
    "total_throughput": 14849.5553289137,
    "itl": 83.64583674098144,
    "ttft": 630937.0630704755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7908323011454226,
    "arrivals": 131339,
    "finished_requests": 114874,
    "scheduler_time": 120.538016009525
}
#Debug simulation 
Total elapsed time: 9.071622115094215. Arrivals time: 0.2870379351079464 Scheduler time: 8.60608032811433 Scheduler overhead time: 0.06688400730490685 Adapter cache time: 0.01236428041011095 Engine time: 0.06819035205990076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 9.192225296981633,
    "estimated_duration": 3600.0153807992997,
    "input_throughput": 8039.612317871248,
    "output_throughput": 6966.478847218618,
    "total_throughput": 15006.091165089865,
    "itl": 85.93313336358402,
    "ttft": 584183.5983719148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5835146794328439,
    "arrivals": 131339,
    "finished_requests": 116130,
    "scheduler_time": 118.97340280097315
}
#Debug simulation 
Total elapsed time: 9.192313149571419. Arrivals time: 0.276797937694937 Scheduler time: 8.741269242949784 Scheduler overhead time: 0.06560556078329682 Adapter cache time: 0.012082681525498629 Engine time: 0.06628491589799523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 9.095109483227134,
    "estimated_duration": 3600.0326801753513,
    "input_throughput": 7955.128618056061,
    "output_throughput": 6894.139916194067,
    "total_throughput": 14849.268534250128,
    "itl": 83.64566483149704,
    "ttft": 631040.5749251802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7804749458003821,
    "arrivals": 131339,
    "finished_requests": 114872,
    "scheduler_time": 120.54591471694101
}
#Debug simulation 
Total elapsed time: 9.095215417910367. Arrivals time: 0.285335723310709 Scheduler time: 8.62997706560418 Scheduler overhead time: 0.06757350033149123 Adapter cache time: 0.0123050631955266 Engine time: 0.06899353489279747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 9.212954848073423,
    "estimated_duration": 3600.000027222781,
    "input_throughput": 8041.483550302349,
    "output_throughput": 6966.7118917629805,
    "total_throughput": 15008.19544206533,
    "itl": 85.93124341636731,
    "ttft": 583868.8998838513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.481069658212359,
    "arrivals": 131339,
    "finished_requests": 116142,
    "scheduler_time": 118.9753171733386
}
#Debug simulation 
Total elapsed time: 9.213053128216416. Arrivals time: 0.28908148128539324 Scheduler time: 8.749389313161373 Scheduler overhead time: 0.0656435564160347 Adapter cache time: 0.012008757796138525 Engine time: 0.06666549993678927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 9.03527008695528,
    "estimated_duration": 3600.0273241156606,
    "input_throughput": 7955.2568415672085,
    "output_throughput": 6894.180728502331,
    "total_throughput": 14849.43757006954,
    "itl": 83.64658258566921,
    "ttft": 630963.7127428476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7676336808502706,
    "arrivals": 131339,
    "finished_requests": 114874,
    "scheduler_time": 120.54552899415434
}
#Debug simulation 
Total elapsed time: 9.035418265033513. Arrivals time: 0.27813122933730483 Scheduler time: 8.578914416022599 Scheduler overhead time: 0.06709284940734506 Adapter cache time: 0.012335381470620632 Engine time: 0.06807725131511688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 9.098115765023977,
    "estimated_duration": 3600.04280650985,
    "input_throughput": 8093.790703630145,
    "output_throughput": 7005.318368547293,
    "total_throughput": 15099.109072177438,
    "itl": 87.12752941386212,
    "ttft": 538324.2189931709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4679548050928843,
    "arrivals": 130909,
    "finished_requests": 116888,
    "scheduler_time": 116.96691399508279
}
#Debug simulation 
Total elapsed time: 9.098214144352823. Arrivals time: 0.2862246041186154 Scheduler time: 8.639610979706049 Scheduler overhead time: 0.06466615339741111 Adapter cache time: 0.011806173715740442 Engine time: 0.06586230173707008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.939024086110294,
    "estimated_duration": 3600.027198735977,
    "input_throughput": 8057.180793018581,
    "output_throughput": 6971.159831462303,
    "total_throughput": 15028.340624480885,
    "itl": 85.98454472570128,
    "ttft": 559882.6603725994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.760083118481563,
    "arrivals": 130909,
    "finished_requests": 116335,
    "scheduler_time": 117.66524339994189
}
#Debug simulation 
Total elapsed time: 8.939122118987143. Arrivals time: 0.28510136902332306 Scheduler time: 8.47944451496005 Scheduler overhead time: 0.06538918614387512 Adapter cache time: 0.01205880893394351 Engine time: 0.06684314738959074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.830664574168622,
    "estimated_duration": 3600.009129793462,
    "input_throughput": 7973.448112240432,
    "output_throughput": 6900.0219456179875,
    "total_throughput": 14873.47005785842,
    "itl": 83.69568498143795,
    "ttft": 605411.9177861284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7993763331184196,
    "arrivals": 130909,
    "finished_requests": 115151,
    "scheduler_time": 119.2136884942395
}
#Debug simulation 
Total elapsed time: 8.8307443712838. Arrivals time: 0.2776552615687251 Scheduler time: 8.374869915656745 Scheduler overhead time: 0.06687297206372023 Adapter cache time: 0.012310890480875969 Engine time: 0.06827787682414055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.981680779717863,
    "estimated_duration": 3600.088838275193,
    "input_throughput": 8057.382276682213,
    "output_throughput": 6971.592404376513,
    "total_throughput": 15028.974681058726,
    "itl": 85.98210294728517,
    "ttft": 559765.7832276253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5488186327833655,
    "arrivals": 130909,
    "finished_requests": 116340,
    "scheduler_time": 117.66980242221405
}
#Debug simulation 
Total elapsed time: 8.981807633768767. Arrivals time: 0.28383195539936423 Scheduler time: 8.524407141376287 Scheduler overhead time: 0.06531117716804147 Adapter cache time: 0.011728783138096333 Engine time: 0.06628442043438554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 8.879950016271323,
    "estimated_duration": 3600.05469137195,
    "input_throughput": 7972.33082841371,
    "output_throughput": 6899.3174074626295,
    "total_throughput": 14871.64823587634,
    "itl": 83.68957655012899,
    "ttft": 605809.1558690752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7724525966728137,
    "arrivals": 130909,
    "finished_requests": 115133,
    "scheduler_time": 119.22300652415284
}
#Debug simulation 
Total elapsed time: 8.880076752975583. Arrivals time: 0.2948637488298118 Scheduler time: 8.407025407068431 Scheduler overhead time: 0.06691661197692156 Adapter cache time: 0.012126218527555466 Engine time: 0.06809298414736986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.923505807295442,
    "estimated_duration": 3600.096943802452,
    "input_throughput": 8056.126391242944,
    "output_throughput": 6970.855060778907,
    "total_throughput": 15026.981452021852,
    "itl": 85.97913594169825,
    "ttft": 560053.0351401346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.481069658212359,
    "arrivals": 130909,
    "finished_requests": 116328,
    "scheduler_time": 117.67193793468958
}
#Debug simulation 
Total elapsed time: 8.923605136107653. Arrivals time: 0.277724077925086 Scheduler time: 8.472374823875725 Scheduler overhead time: 0.06508159497752786 Adapter cache time: 0.011772261001169682 Engine time: 0.06638882961124182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.881086110137403,
    "estimated_duration": 3600.0068319306747,
    "input_throughput": 7972.281537201449,
    "output_throughput": 6899.51162861524,
    "total_throughput": 14871.793165816689,
    "itl": 83.69298238391065,
    "ttft": 605882.2787563584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6867214094474938,
    "arrivals": 130909,
    "finished_requests": 115132,
    "scheduler_time": 119.2227586708352
}
#Debug simulation 
Total elapsed time: 8.88123885774985. Arrivals time: 0.2829782203771174 Scheduler time: 8.420260932296515 Scheduler overhead time: 0.067078887950629 Adapter cache time: 0.012060265522450209 Engine time: 0.06789326434955001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.863881658297032,
    "estimated_duration": 3600.0646474799373,
    "input_throughput": 8110.033807431151,
    "output_throughput": 7008.140817043648,
    "total_throughput": 15118.1746244748,
    "itl": 87.10659010502681,
    "ttft": 526015.3632191713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.494404441220684,
    "arrivals": 130707,
    "finished_requests": 117077,
    "scheduler_time": 116.56909639400678
}
#Debug simulation 
Total elapsed time: 8.863968215882778. Arrivals time: 0.27047170093283057 Scheduler time: 8.422165933996439 Scheduler overhead time: 0.06444764323532581 Adapter cache time: 0.011642761994153261 Engine time: 0.06549451220780611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.722419606987387,
    "estimated_duration": 3600.0496965254806,
    "input_throughput": 8067.509464669534,
    "output_throughput": 6971.764868752654,
    "total_throughput": 15039.274333422189,
    "itl": 85.97199272311734,
    "ttft": 549365.1300706204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6948556881025447,
    "arrivals": 130707,
    "finished_requests": 116460,
    "scheduler_time": 117.28020531208942
}
#Debug simulation 
Total elapsed time: 8.722540844697505. Arrivals time: 0.2735101468861103 Scheduler time: 8.277271065860987 Scheduler overhead time: 0.0650281272828579 Adapter cache time: 0.011697180103510618 Engine time: 0.06518055032938719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.596179693937302,
    "estimated_duration": 3600.0653796841,
    "input_throughput": 7985.900245656856,
    "output_throughput": 6900.029966172369,
    "total_throughput": 14885.930211829225,
    "itl": 83.68537125988324,
    "ttft": 595265.0559070931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8691113392915617,
    "arrivals": 130707,
    "finished_requests": 115269,
    "scheduler_time": 118.81764329175438
}
#Debug simulation 
Total elapsed time: 8.596296532079577. Arrivals time: 0.2653160700574517 Scheduler time: 8.154450800735503 Scheduler overhead time: 0.06642796425148845 Adapter cache time: 0.01204968523234129 Engine time: 0.06734853005036712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.780511294025928,
    "estimated_duration": 3600.066382051706,
    "input_throughput": 8067.399574851192,
    "output_throughput": 6972.191436563209,
    "total_throughput": 15039.5910114144,
    "itl": 85.97202822618523,
    "ttft": 549107.6366924176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.582683642385525,
    "arrivals": 130707,
    "finished_requests": 116467,
    "scheduler_time": 117.26852964527542
}
#Debug simulation 
Total elapsed time: 8.780605452135205. Arrivals time: 0.27565977582708 Scheduler time: 8.332660671323538 Scheduler overhead time: 0.06500356644392014 Adapter cache time: 0.011743210256099701 Engine time: 0.06561732664704323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 8.650627776049078,
    "estimated_duration": 3600.0842648569705,
    "input_throughput": 7986.307787476824,
    "output_throughput": 6900.577089960694,
    "total_throughput": 14886.884877437518,
    "itl": 83.68335943416241,
    "ttft": 595197.0126985967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.744914212254811,
    "arrivals": 130707,
    "finished_requests": 115271,
    "scheduler_time": 118.8173443311272
}
#Debug simulation 
Total elapsed time: 8.650715680792928. Arrivals time: 0.27163160452619195 Scheduler time: 8.20288514206186 Scheduler overhead time: 0.06634431658312678 Adapter cache time: 0.011995741166174412 Engine time: 0.0670013865455985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.680981123819947,
    "estimated_duration": 3600.093474382901,
    "input_throughput": 8067.579968872474,
    "output_throughput": 6972.422849188013,
    "total_throughput": 15040.002818060486,
    "itl": 85.96868285476859,
    "ttft": 549060.7005919412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5002214210340705,
    "arrivals": 130707,
    "finished_requests": 116469,
    "scheduler_time": 117.26899755557963
}
#Debug simulation 
Total elapsed time: 8.681104183662683. Arrivals time: 0.27387447329238057 Scheduler time: 8.234963069669902 Scheduler overhead time: 0.06483580870553851 Adapter cache time: 0.011773237958550453 Engine time: 0.0657279291190207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.59604911506176,
    "estimated_duration": 3600.071318677168,
    "input_throughput": 7985.117086670096,
    "output_throughput": 6900.441630453067,
    "total_throughput": 14885.558717123162,
    "itl": 83.68217738290254,
    "ttft": 595449.0936592389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7684143498539961,
    "arrivals": 130707,
    "finished_requests": 115261,
    "scheduler_time": 118.81714837544303
}
#Debug simulation 
Total elapsed time: 8.596200810279697. Arrivals time: 0.2663748590275645 Scheduler time: 8.153790057171136 Scheduler overhead time: 0.0666612135246396 Adapter cache time: 0.012019190471619368 Engine time: 0.06676202546805143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.650583279784769,
    "estimated_duration": 3600.0466017783865,
    "input_throughput": 8084.807842660172,
    "output_throughput": 7002.432965047444,
    "total_throughput": 15087.240807707616,
    "itl": 87.01475005766368,
    "ttft": 534131.327701128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4811796231567842,
    "arrivals": 130593,
    "finished_requests": 116755,
    "scheduler_time": 116.83866406902952
}
#Debug simulation 
Total elapsed time: 8.650680984836072. Arrivals time: 0.2730937637388706 Scheduler time: 8.207298135850579 Scheduler overhead time: 0.06421125866472721 Adapter cache time: 0.011626613792032003 Engine time: 0.06487880134955049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.633140124380589,
    "estimated_duration": 3600.017841845184,
    "input_throughput": 8044.85235138606,
    "output_throughput": 6968.285742479051,
    "total_throughput": 15013.138093865111,
    "itl": 85.87327789246083,
    "ttft": 555760.3383442326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.669046151218938,
    "arrivals": 130593,
    "finished_requests": 116201,
    "scheduler_time": 117.5450309560483
}
#Debug simulation 
Total elapsed time: 8.633226041216403. Arrivals time: 0.27180337347090244 Scheduler time: 8.18691406166181 Scheduler overhead time: 0.0656185825355351 Adapter cache time: 0.011856136843562126 Engine time: 0.06686965562403202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.432283564005047,
    "estimated_duration": 3600.016074048015,
    "input_throughput": 7961.747228470213,
    "output_throughput": 6898.164199604845,
    "total_throughput": 14859.911428075056,
    "itl": 83.5992364701457,
    "ttft": 601430.476470566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7868048271630006,
    "arrivals": 130593,
    "finished_requests": 115014,
    "scheduler_time": 119.07449750925649
}
#Debug simulation 
Total elapsed time: 8.432404285296798. Arrivals time: 0.2725618174299598 Scheduler time: 7.9842470907606184 Scheduler overhead time: 0.06620787968859076 Adapter cache time: 0.011978934984654188 Engine time: 0.06669829320162535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.574482338968664,
    "estimated_duration": 3600.058724979219,
    "input_throughput": 8045.031821020416,
    "output_throughput": 6968.5510477734815,
    "total_throughput": 15013.582868793897,
    "itl": 85.87380544297585,
    "ttft": 555636.0360182404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5293930168403291,
    "arrivals": 130593,
    "finished_requests": 116207,
    "scheduler_time": 117.54310899898213
}
#Debug simulation 
Total elapsed time: 8.574597112834454. Arrivals time: 0.26487711211666465 Scheduler time: 8.137862525414675 Scheduler overhead time: 0.06480334140360355 Adapter cache time: 0.011713131330907345 Engine time: 0.06536887586116791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 8.5153639158234,
    "estimated_duration": 3600.0214046349083,
    "input_throughput": 7962.352380209525,
    "output_throughput": 6898.730370887522,
    "total_throughput": 14861.082751097047,
    "itl": 83.60155308111632,
    "ttft": 601160.6030508464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7801222155010374,
    "arrivals": 130593,
    "finished_requests": 115026,
    "scheduler_time": 119.07759741613546
}
#Debug simulation 
Total elapsed time: 8.515454515814781. Arrivals time: 0.27399544790387154 Scheduler time: 8.065539211034775 Scheduler overhead time: 0.06654171040281653 Adapter cache time: 0.011989375576376915 Engine time: 0.06658219639211893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.57137402985245,
    "estimated_duration": 3600.062216534619,
    "input_throughput": 8044.99290789451,
    "output_throughput": 6968.902060851021,
    "total_throughput": 15013.894968745532,
    "itl": 85.87084358401836,
    "ttft": 555494.5283601914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4363822116283655,
    "arrivals": 130593,
    "finished_requests": 116211,
    "scheduler_time": 117.54031956683585
}
#Debug simulation 
Total elapsed time: 8.571461969986558. Arrivals time: 0.2631378434598446 Scheduler time: 8.136481432709843 Scheduler overhead time: 0.0648036366328597 Adapter cache time: 0.011714440304785967 Engine time: 0.06532443454489112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.413179277908057,
    "estimated_duration": 3600.066286970856,
    "input_throughput": 7962.319500544257,
    "output_throughput": 6898.5710318397405,
    "total_throughput": 14860.890532383997,
    "itl": 83.59886520307239,
    "ttft": 601153.3100560392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7594638633355544,
    "arrivals": 130593,
    "finished_requests": 115023,
    "scheduler_time": 119.06953617851359
}
#Debug simulation 
Total elapsed time: 8.413336887955666. Arrivals time: 0.2709126314148307 Scheduler time: 7.967929041478783 Scheduler overhead time: 0.06572109647095203 Adapter cache time: 0.011960475239902735 Engine time: 0.0662055155262351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.639075854793191,
    "estimated_duration": 3600.048539455112,
    "input_throughput": 8121.186889448201,
    "output_throughput": 7015.219579184482,
    "total_throughput": 15136.406468632684,
    "itl": 87.11835022576525,
    "ttft": 473891.88730838895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.234994252799084,
    "arrivals": 129421,
    "finished_requests": 117304,
    "scheduler_time": 114.30099613416344
}
#Debug simulation 
Total elapsed time: 8.639172071125358. Arrivals time: 0.2607191475108266 Scheduler time: 8.207637114915997 Scheduler overhead time: 0.0640903296880424 Adapter cache time: 0.012246435973793268 Engine time: 0.06480114068835974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.518978979904205,
    "estimated_duration": 3600.082172994233,
    "input_throughput": 8076.30509606331,
    "output_throughput": 6978.952921817167,
    "total_throughput": 15055.258017880476,
    "itl": 85.97928990416342,
    "ttft": 497740.0707998059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.525624880474063,
    "arrivals": 129421,
    "finished_requests": 116684,
    "scheduler_time": 114.97381045387344
}
#Debug simulation 
Total elapsed time: 8.519073768984526. Arrivals time: 0.27091573690995574 Scheduler time: 8.076537519227713 Scheduler overhead time: 0.06466490123420954 Adapter cache time: 0.01243023807182908 Engine time: 0.06458855327218771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.426435350906104,
    "estimated_duration": 3600.081803648749,
    "input_throughput": 7993.072815966371,
    "output_throughput": 6906.468062697918,
    "total_throughput": 14899.54087866429,
    "itl": 83.6945439803922,
    "ttft": 544624.0446563532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7395973318442857,
    "arrivals": 129421,
    "finished_requests": 115489,
    "scheduler_time": 116.46649936401798
}
#Debug simulation 
Total elapsed time: 8.426519196014851. Arrivals time: 0.2638642010279 Scheduler time: 7.986351918429136 Scheduler overhead time: 0.06620541960000992 Adapter cache time: 0.012817750684916973 Engine time: 0.06663680775091052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.596398244611919,
    "estimated_duration": 3600.094481683509,
    "input_throughput": 8076.374980693104,
    "output_throughput": 6979.507101227504,
    "total_throughput": 15055.882081920607,
    "itl": 85.9752755326669,
    "ttft": 497549.5794440063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.292696565445508,
    "arrivals": 129421,
    "finished_requests": 116689,
    "scheduler_time": 114.96827171670374
}
#Debug simulation 
Total elapsed time: 8.596498494967818. Arrivals time: 0.27128381095826626 Scheduler time: 8.153598330914974 Scheduler overhead time: 0.06458819005638361 Adapter cache time: 0.012335966806858778 Engine time: 0.06464592181146145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 8.448874036315829,
    "estimated_duration": 3600.0027274252684,
    "input_throughput": 7993.2483886145465,
    "output_throughput": 6906.619767419646,
    "total_throughput": 14899.868156034194,
    "itl": 83.69204882582713,
    "ttft": 544551.8883874126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.688702864921666,
    "arrivals": 129421,
    "finished_requests": 115489,
    "scheduler_time": 116.45880159267794
}
#Debug simulation 
Total elapsed time: 8.448981991037726. Arrivals time: 0.26573730353266 Scheduler time: 8.006859987974167 Scheduler overhead time: 0.06622709101065993 Adapter cache time: 0.012764219660311937 Engine time: 0.06674565747380257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.535430528689176,
    "estimated_duration": 3600.038871439782,
    "input_throughput": 8076.903066430231,
    "output_throughput": 6979.387694764308,
    "total_throughput": 15056.29076119454,
    "itl": 85.9712863825884,
    "ttft": 497282.3930269059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2088366454373958,
    "arrivals": 129421,
    "finished_requests": 116694,
    "scheduler_time": 114.96229739486968
}
#Debug simulation 
Total elapsed time: 8.535509499721229. Arrivals time: 0.2618095800280571 Scheduler time: 8.101486927829683 Scheduler overhead time: 0.06489727040752769 Adapter cache time: 0.012469178065657616 Engine time: 0.06499575357884169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.483089057728648,
    "estimated_duration": 3600.04387726279,
    "input_throughput": 7993.157022819108,
    "output_throughput": 6906.5408221926045,
    "total_throughput": 14899.697845011713,
    "itl": 83.69168279717357,
    "ttft": 544507.8459407424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6694409674964987,
    "arrivals": 129421,
    "finished_requests": 115489,
    "scheduler_time": 116.45782354539534
}
#Debug simulation 
Total elapsed time: 8.483234710991383. Arrivals time: 0.27432599011808634 Scheduler time: 8.032654894981533 Scheduler overhead time: 0.06625715224072337 Adapter cache time: 0.01267163036391139 Engine time: 0.06672249268740416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.45106104388833,
    "estimated_duration": 3600.0872614724735,
    "input_throughput": 8065.93893174776,
    "output_throughput": 7028.175753065274,
    "total_throughput": 15094.114684813034,
    "itl": 87.65038915479103,
    "ttft": 444267.8338945014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4928782050451397,
    "arrivals": 128920,
    "finished_requests": 117719,
    "scheduler_time": 113.43746461759655
}
#Debug simulation 
Total elapsed time: 8.451146697625518. Arrivals time: 0.25998754519969225 Scheduler time: 8.021348918788135 Scheduler overhead time: 0.06385655142366886 Adapter cache time: 0.012428420130163431 Engine time: 0.06411070935428143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.474228071048856,
    "estimated_duration": 3600.0069094864443,
    "input_throughput": 8022.132103107494,
    "output_throughput": 6993.691021440748,
    "total_throughput": 15015.823124548242,
    "itl": 86.51961394611375,
    "ttft": 468186.4358352047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.852885877904485,
    "arrivals": 128920,
    "finished_requests": 117097,
    "scheduler_time": 114.08278255608766
}
#Debug simulation 
Total elapsed time: 8.474347863812. Arrivals time: 0.26519092777743936 Scheduler time: 8.03719009226188 Scheduler overhead time: 0.06445613037794828 Adapter cache time: 0.012731007765978575 Engine time: 0.0649221814237535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.321777838747948,
    "estimated_duration": 3600.079090513632,
    "input_throughput": 7939.75167804491,
    "output_throughput": 6919.76630892456,
    "total_throughput": 14859.51798696947,
    "itl": 84.22018236964738,
    "ttft": 515852.2413213056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.105502329249888,
    "arrivals": 128920,
    "finished_requests": 115880,
    "scheduler_time": 115.51835839093587
}
#Debug simulation 
Total elapsed time: 8.321857450064272. Arrivals time: 0.2614426100626588 Scheduler time: 7.885739075485617 Scheduler overhead time: 0.06542029371485114 Adapter cache time: 0.012972183059900999 Engine time: 0.06599201587960124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.419533515814692,
    "estimated_duration": 3600.008811099185,
    "input_throughput": 8022.138698927846,
    "output_throughput": 6993.812604673183,
    "total_throughput": 15015.951303601028,
    "itl": 86.51440176092169,
    "ttft": 468057.1918443329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.632158743147733,
    "arrivals": 128920,
    "finished_requests": 117100,
    "scheduler_time": 114.0825068338356
}
#Debug simulation 
Total elapsed time: 8.41961328079924. Arrivals time: 0.2607005564495921 Scheduler time: 7.98752726521343 Scheduler overhead time: 0.06419000215828419 Adapter cache time: 0.01259902585297823 Engine time: 0.06481638224795461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 8.344129222910851,
    "estimated_duration": 3600.08418125507,
    "input_throughput": 7939.740450745536,
    "output_throughput": 6919.75652394751,
    "total_throughput": 14859.496974693046,
    "itl": 84.21792527975725,
    "ttft": 515845.78099313227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.078368482599056,
    "arrivals": 128920,
    "finished_requests": 115880,
    "scheduler_time": 115.51957203153263
}
#Debug simulation 
Total elapsed time: 8.344222229905427. Arrivals time: 0.2710556201636791 Scheduler time: 7.898080320563167 Scheduler overhead time: 0.06569826183840632 Adapter cache time: 0.01293243421241641 Engine time: 0.06586516695097089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.460411801934242,
    "estimated_duration": 3600.0217371669064,
    "input_throughput": 8022.427670875201,
    "output_throughput": 6994.190268366323,
    "total_throughput": 15016.617939241523,
    "itl": 86.51190533809508,
    "ttft": 467889.0376840848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4897291668224977,
    "arrivals": 128920,
    "finished_requests": 117106,
    "scheduler_time": 114.07993915920532
}
#Debug simulation 
Total elapsed time: 8.460521664936095. Arrivals time: 0.26216824213042855 Scheduler time: 8.025853785220534 Scheduler overhead time: 0.06481262994930148 Adapter cache time: 0.012628129683434963 Engine time: 0.06525922426953912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.313460655044764,
    "estimated_duration": 3600.0940270764822,
    "input_throughput": 7939.3812453312285,
    "output_throughput": 6919.919538943406,
    "total_throughput": 14859.300784274634,
    "itl": 84.2196685426516,
    "ttft": 515824.3446093977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.049373574387303,
    "arrivals": 128920,
    "finished_requests": 115880,
    "scheduler_time": 115.51977353405711
}
#Debug simulation 
Total elapsed time: 8.313570418860763. Arrivals time: 0.26106306817382574 Scheduler time: 7.877320039551705 Scheduler overhead time: 0.06573526421561837 Adapter cache time: 0.012930342927575111 Engine time: 0.06610777322202921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.339782976079732,
    "estimated_duration": 3600.0830252825335,
    "input_throughput": 8122.775445631575,
    "output_throughput": 7015.13182408286,
    "total_throughput": 15137.907269714435,
    "itl": 87.11197825171914,
    "ttft": 442102.8761202271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4862657960131895,
    "arrivals": 128685,
    "finished_requests": 117507,
    "scheduler_time": 113.05056210718281
}
#Debug simulation 
Total elapsed time: 8.339894178789109. Arrivals time: 0.26566593907773495 Scheduler time: 7.904776863288134 Scheduler overhead time: 0.06383110117167234 Adapter cache time: 0.012353395111858845 Engine time: 0.0639030965976417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.258024946786463,
    "estimated_duration": 3600.0844308972287,
    "input_throughput": 8085.001771123992,
    "output_throughput": 6981.109605181217,
    "total_throughput": 15066.111376305209,
    "itl": 85.9751230620676,
    "ttft": 464122.8992154768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8517809877358418,
    "arrivals": 128685,
    "finished_requests": 116965,
    "scheduler_time": 113.705065383827
}
#Debug simulation 
Total elapsed time: 8.258122449740767. Arrivals time: 0.2567636496387422 Scheduler time: 7.830080583691597 Scheduler overhead time: 0.06440465431660414 Adapter cache time: 0.012528919149190187 Engine time: 0.06455711927264929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.242290591355413,
    "estimated_duration": 3600.0894791650953,
    "input_throughput": 8000.132820776263,
    "output_throughput": 6908.335235536369,
    "total_throughput": 14908.468056312631,
    "itl": 83.69525141541637,
    "ttft": 513094.5946109156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 400,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0253725821152546,
    "arrivals": 128685,
    "finished_requests": 115702,
    "scheduler_time": 115.15037858734584
}
#Debug simulation 
Total elapsed time: 8.242380218114704. Arrivals time: 0.2597241341136396 Scheduler time: 7.806630797684193 Scheduler overhead time: 0.06580721959471703 Adapter cache time: 0.012852184008806944 Engine time: 0.06688483897596598 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.34492936078459,
    "estimated_duration": 3600.057332975822,
    "input_throughput": 8085.89428100519,
    "output_throughput": 6981.916584984789,
    "total_throughput": 15067.810865989979,
    "itl": 85.9692843307608,
    "ttft": 463658.6103725331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6235555641865305,
    "arrivals": 128685,
    "finished_requests": 116978,
    "scheduler_time": 113.69744940270935
}
#Debug simulation 
Total elapsed time: 8.345021313987672. Arrivals time: 0.2650702493265271 Scheduler time: 7.908811827655882 Scheduler overhead time: 0.06438943417742848 Adapter cache time: 0.01257635373622179 Engine time: 0.06437622895464301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 8.235724491998553,
    "estimated_duration": 3600.0151871729745,
    "input_throughput": 8000.106528055096,
    "output_throughput": 6908.1605790473195,
    "total_throughput": 14908.267107102414,
    "itl": 83.69467520357219,
    "ttft": 513096.2330214331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.980636768480784,
    "arrivals": 128685,
    "finished_requests": 115699,
    "scheduler_time": 115.14838119862156
}
#Debug simulation 
Total elapsed time: 8.235832526814193. Arrivals time: 0.2588737476617098 Scheduler time: 7.8015364040620625 Scheduler overhead time: 0.0659425025805831 Adapter cache time: 0.012889189645648003 Engine time: 0.0661376747302711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.299422963988036,
    "estimated_duration": 3600.0625891963505,
    "input_throughput": 8086.136082011401,
    "output_throughput": 6982.124720673587,
    "total_throughput": 15068.260802684988,
    "itl": 85.965322465289,
    "ttft": 463538.35437992093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4769613249413567,
    "arrivals": 128685,
    "finished_requests": 116981,
    "scheduler_time": 113.69659445382807
}
#Debug simulation 
Total elapsed time: 8.299527330789715. Arrivals time: 0.25744751561433077 Scheduler time: 7.869917979463935 Scheduler overhead time: 0.0644445214420557 Adapter cache time: 0.0125892860814929 Engine time: 0.06533680437132716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.28717708028853,
    "estimated_duration": 3600.081007269665,
    "input_throughput": 7999.439718674873,
    "output_throughput": 6908.133997479551,
    "total_throughput": 14907.573716154424,
    "itl": 83.69591887926771,
    "ttft": 513266.62537909456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 397,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.942271214853992,
    "arrivals": 128685,
    "finished_requests": 115695,
    "scheduler_time": 115.15419586656589
}
#Debug simulation 
Total elapsed time: 8.287300198338926. Arrivals time: 0.26890504732728004 Scheduler time: 7.842356470879167 Scheduler overhead time: 0.0660082190297544 Adapter cache time: 0.01282023312523961 Engine time: 0.06660247500985861 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.301531428005546,
    "estimated_duration": 3600.085236722713,
    "input_throughput": 8032.278154148395,
    "output_throughput": 7017.371072857692,
    "total_throughput": 15049.649227006086,
    "itl": 87.36115930573885,
    "ttft": 463463.4394998889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.49949061407709,
    "arrivals": 128584,
    "finished_requests": 116862,
    "scheduler_time": 113.91731350945818
}
#Debug simulation 
Total elapsed time: 8.301608441863209. Arrivals time: 0.2576334881596267 Scheduler time: 7.87467075465247 Scheduler overhead time: 0.06341263093054295 Adapter cache time: 0.012429384049028158 Engine time: 0.06406628200784326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.261896483134478,
    "estimated_duration": 3600.0071416793944,
    "input_throughput": 7993.436642621733,
    "output_throughput": 6982.57975907056,
    "total_throughput": 14976.016401692294,
    "itl": 86.23525035115682,
    "ttft": 486681.90383005555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9997350149042963,
    "arrivals": 128584,
    "finished_requests": 116261,
    "scheduler_time": 114.56641965280669
}
#Debug simulation 
Total elapsed time: 8.262011114042252. Arrivals time: 0.2585852784104645 Scheduler time: 7.832350537180901 Scheduler overhead time: 0.0642936653457582 Adapter cache time: 0.012786000967025757 Engine time: 0.06429812079295516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.195800761226565,
    "estimated_duration": 3600.047558015009,
    "input_throughput": 7905.444453542785,
    "output_throughput": 6911.740080933748,
    "total_throughput": 14817.184534476532,
    "itl": 83.963474916984,
    "ttft": 534568.80302735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1564615904912596,
    "arrivals": 128584,
    "finished_requests": 115021,
    "scheduler_time": 115.98868221745404
}
#Debug simulation 
Total elapsed time: 8.195938896387815. Arrivals time: 0.2688365266658366 Scheduler time: 7.752108990214765 Scheduler overhead time: 0.065625861287117 Adapter cache time: 0.01293077040463686 Engine time: 0.066068259999156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.316278491169214,
    "estimated_duration": 3600.0045357822996,
    "input_throughput": 7993.628539622545,
    "output_throughput": 6982.998146289058,
    "total_throughput": 14976.626685911602,
    "itl": 86.2267435244221,
    "ttft": 486389.59397846146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 397,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6940620253374754,
    "arrivals": 128584,
    "finished_requests": 116268,
    "scheduler_time": 114.55915175522964
}
#Debug simulation 
Total elapsed time: 8.316398518159986. Arrivals time: 0.2729824176058173 Scheduler time: 7.871646632440388 Scheduler overhead time: 0.06436039693653584 Adapter cache time: 0.012753967195749283 Engine time: 0.064932513050735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 8.212228877935559,
    "estimated_duration": 3600.0794885121068,
    "input_throughput": 7905.482668040342,
    "output_throughput": 6911.7365545403345,
    "total_throughput": 14817.219222580678,
    "itl": 83.96053314648844,
    "ttft": 534647.3676349182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0690671210456872,
    "arrivals": 128584,
    "finished_requests": 115023,
    "scheduler_time": 115.99271458206064
}
#Debug simulation 
Total elapsed time: 8.212311778217554. Arrivals time: 0.2595806624740362 Scheduler time: 7.777250987943262 Scheduler overhead time: 0.06585584627464414 Adapter cache time: 0.012983809225261211 Engine time: 0.066197051666677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.29185448680073,
    "estimated_duration": 3600.0241285481884,
    "input_throughput": 7994.308919147781,
    "output_throughput": 6983.7945808822,
    "total_throughput": 14978.10350002998,
    "itl": 86.22429990724565,
    "ttft": 486052.5211031716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5791040599904846,
    "arrivals": 128584,
    "finished_requests": 116280,
    "scheduler_time": 114.5566906310312
}
#Debug simulation 
Total elapsed time: 8.291946127079427. Arrivals time: 0.2668564016930759 Scheduler time: 7.85368307121098 Scheduler overhead time: 0.06438902346417308 Adapter cache time: 0.012686282861977816 Engine time: 0.06454248586669564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.22324772272259,
    "estimated_duration": 3600.0679338938803,
    "input_throughput": 7905.503041220924,
    "output_throughput": 6911.956234416296,
    "total_throughput": 14817.45927563722,
    "itl": 83.96197272301775,
    "ttft": 534606.5416994923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 411,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0824703859724343,
    "arrivals": 128584,
    "finished_requests": 115024,
    "scheduler_time": 115.99156859996764
}
#Debug simulation 
Total elapsed time: 8.22338754683733. Arrivals time: 0.27053327625617385 Scheduler time: 7.777782134246081 Scheduler overhead time: 0.06578976102173328 Adapter cache time: 0.012998975813388824 Engine time: 0.06581508181989193 
