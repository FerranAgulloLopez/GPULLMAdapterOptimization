INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 107.83420926705003,
    "estimated_duration": 3600.0560128848333,
    "input_throughput": 6957.086753750246,
    "output_throughput": 6036.764406502925,
    "total_throughput": 12993.851160253173,
    "itl": 99.90027119661691,
    "ttft": 1758661.8027886304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3636824967572463,
    "arrivals": 426676,
    "finished_requests": 101266,
    "scheduler_time": 191.51674433742477
}
#Debug simulation 
Total elapsed time: 107.83449942013249. Arrivals time: 0.7980037690140307 Scheduler time: 106.74322561919689 Scheduler overhead time: 0.11496308771893382 Adapter cache time: 0.02247318672016263 Engine time: 0.11808250797912478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 44.66878208890557,
    "estimated_duration": 3600.0464930240214,
    "input_throughput": 6867.149923731566,
    "output_throughput": 5948.4031779855995,
    "total_throughput": 12815.553101717165,
    "itl": 96.76743358227066,
    "ttft": 1821161.2579348471,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.387124024680819,
    "arrivals": 426676,
    "finished_requests": 99758,
    "scheduler_time": 194.04444097406972
}
#Debug simulation 
Total elapsed time: 44.66899801278487. Arrivals time: 0.5406651315279305 Scheduler time: 43.92072796402499 Scheduler overhead time: 0.07856067409738898 Adapter cache time: 0.018219608813524246 Engine time: 0.08043168671429157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 95.56051577115431,
    "estimated_duration": 3600.0435434260517,
    "input_throughput": 6690.127691366554,
    "output_throughput": 5807.099205279217,
    "total_throughput": 12497.22689664577,
    "itl": 90.90767679913009,
    "ttft": 1786484.7164678746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41342552044894554,
    "arrivals": 426676,
    "finished_requests": 97305,
    "scheduler_time": 198.47472234258424
}
#Debug simulation 
Total elapsed time: 95.56084885122254. Arrivals time: 0.7216183808632195 Scheduler time: 94.54951683385298 Scheduler overhead time: 0.11401689145714045 Adapter cache time: 0.022066717501729727 Engine time: 0.11518232012167573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 52.955756011419,
    "estimated_duration": 3600.025223077327,
    "input_throughput": 6857.39556538372,
    "output_throughput": 5939.147276785828,
    "total_throughput": 12796.542842169547,
    "itl": 96.0848598646954,
    "ttft": 1818043.947030742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5660344683891145,
    "arrivals": 426676,
    "finished_requests": 99609,
    "scheduler_time": 194.37463019080505
}
#Debug simulation 
Total elapsed time: 52.95596672827378. Arrivals time: 0.6115575861185789 Scheduler time: 52.117310021072626 Scheduler overhead time: 0.08692942000925541 Adapter cache time: 0.018817439675331116 Engine time: 0.08866949751973152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 95.92408382426947,
    "estimated_duration": 3600.0144460483425,
    "input_throughput": 6690.181764808557,
    "output_throughput": 5807.146141579474,
    "total_throughput": 12497.32790638803,
    "itl": 90.90680309046252,
    "ttft": 1786448.8058283778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4096974112698809,
    "arrivals": 426676,
    "finished_requests": 97305,
    "scheduler_time": 198.4736625458315
}
#Debug simulation 
Total elapsed time: 95.9243151769042. Arrivals time: 0.7258256380446255 Scheduler time: 94.90016021812335 Scheduler overhead time: 0.11889515444636345 Adapter cache time: 0.022000506054610014 Engine time: 0.11940606124699116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 43.66076199617237,
    "estimated_duration": 3600.0900477127047,
    "input_throughput": 6873.608902011208,
    "output_throughput": 5952.596939516929,
    "total_throughput": 12826.205841528135,
    "itl": 96.85438809193813,
    "ttft": 1820780.6399334727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1386135150911203,
    "arrivals": 426676,
    "finished_requests": 99862,
    "scheduler_time": 193.92799902297452
}
#Debug simulation 
Total elapsed time: 43.661214348860085. Arrivals time: 0.5152161712758243 Scheduler time: 42.94122077105567 Scheduler overhead time: 0.07756121782585979 Adapter cache time: 0.018330371472984552 Engine time: 0.0780583806335926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 96.41725768102333,
    "estimated_duration": 3600.0293358826452,
    "input_throughput": 6690.211315762769,
    "output_throughput": 5807.234066573036,
    "total_throughput": 12497.445382335805,
    "itl": 90.90756106541568,
    "ttft": 1786424.3520374303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4057621849142016,
    "arrivals": 426676,
    "finished_requests": 97306,
    "scheduler_time": 198.47349857326864
}
#Debug simulation 
Total elapsed time: 96.41751859104261. Arrivals time: 0.709396690595895 Scheduler time: 95.41652342537418 Scheduler overhead time: 0.11662683868780732 Adapter cache time: 0.021438015159219503 Engine time: 0.11568677658215165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 31.2585866083391,
    "estimated_duration": 3600.0078489142224,
    "input_throughput": 6940.420145899336,
    "output_throughput": 6030.7157403970705,
    "total_throughput": 12971.135886296406,
    "itl": 99.86192371225825,
    "ttft": 1808141.720025054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3026445792941361,
    "arrivals": 420888,
    "finished_requests": 100851,
    "scheduler_time": 191.81315333977835
}
#Debug simulation 
Total elapsed time: 31.25879169628024. Arrivals time: 0.5575598492287099 Scheduler time: 30.517200173810124 Scheduler overhead time: 0.06960655096918344 Adapter cache time: 0.014017323032021523 Engine time: 0.07138765323907137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 88.56766958767548,
    "estimated_duration": 3600.049713341316,
    "input_throughput": 6880.1405458957615,
    "output_throughput": 5974.079724593223,
    "total_throughput": 12854.220270488984,
    "itl": 97.19366043053064,
    "ttft": 1774920.5262533915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 47,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3361380295129494,
    "arrivals": 420888,
    "finished_requests": 99913,
    "scheduler_time": 193.53986784369448
}
#Debug simulation 
Total elapsed time: 88.56796004157513. Arrivals time: 0.7874929751269519 Scheduler time: 87.50228995271027 Scheduler overhead time: 0.10999780846759677 Adapter cache time: 0.02139872359111905 Engine time: 0.11107590701431036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 32.44940908392891,
    "estimated_duration": 3600.0594482941983,
    "input_throughput": 6652.349313661359,
    "output_throughput": 5783.483939373689,
    "total_throughput": 12435.833253035047,
    "itl": 90.27773401765575,
    "ttft": 1836569.0200758546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3235189906740583,
    "arrivals": 420888,
    "finished_requests": 96637,
    "scheduler_time": 199.1732374927427
}
#Debug simulation 
Total elapsed time: 32.44963330682367. Arrivals time: 0.5012936578132212 Scheduler time: 31.753641469869763 Scheduler overhead time: 0.07373299822211266 Adapter cache time: 0.014534097630530596 Engine time: 0.07556416839361191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 90.15833656815812,
    "estimated_duration": 3600.004299061464,
    "input_throughput": 6883.3440022447385,
    "output_throughput": 5976.613696158437,
    "total_throughput": 12859.957698403176,
    "itl": 97.35929083972732,
    "ttft": 1774178.6394757163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 47,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3194793778331952,
    "arrivals": 420888,
    "finished_requests": 99965,
    "scheduler_time": 193.41348963369313
}
#Debug simulation 
Total elapsed time: 90.1585987880826. Arrivals time: 1.0991021180525422 Scheduler time: 88.77548952307552 Scheduler overhead time: 0.11308096442371607 Adapter cache time: 0.021729665342718363 Engine time: 0.11249261489138007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 32.0714846518822,
    "estimated_duration": 3600.028635286125,
    "input_throughput": 6659.221753133258,
    "output_throughput": 5788.837009721083,
    "total_throughput": 12448.05876285434,
    "itl": 90.4151851980257,
    "ttft": 1836293.6506630448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3306617196137112,
    "arrivals": 420888,
    "finished_requests": 96741,
    "scheduler_time": 199.05515859385437
}
#Debug simulation 
Total elapsed time: 32.07176628988236. Arrivals time: 0.6106666494160891 Scheduler time: 31.26130171166733 Scheduler overhead time: 0.07641916815191507 Adapter cache time: 0.015388534404337406 Engine time: 0.07673862529918551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 88.8750421651639,
    "estimated_duration": 3600.0006006966237,
    "input_throughput": 6882.428851596729,
    "output_throughput": 5976.33344723241,
    "total_throughput": 12858.762298829139,
    "itl": 97.36677389084558,
    "ttft": 1773542.3383129966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 48,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30642820514738567,
    "arrivals": 420888,
    "finished_requests": 99954,
    "scheduler_time": 193.41013433876168
}
#Debug simulation 
Total elapsed time: 88.87523841997609. Arrivals time: 0.7429212424904108 Scheduler time: 87.85241717845201 Scheduler overhead time: 0.11051353253424168 Adapter cache time: 0.020431749057024717 Engine time: 0.11198724620044231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 33.62487459089607,
    "estimated_duration": 3600.064599165566,
    "input_throughput": 6653.79282514879,
    "output_throughput": 5785.581182300919,
    "total_throughput": 12439.37400744971,
    "itl": 90.28994831660698,
    "ttft": 1836539.8335746601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2442573118768652,
    "arrivals": 420888,
    "finished_requests": 96688,
    "scheduler_time": 199.13016193958705
}
#Debug simulation 
Total elapsed time: 33.62513348925859. Arrivals time: 0.5902196788229048 Scheduler time: 32.83994638687 Scheduler overhead time: 0.07411349238827825 Adapter cache time: 0.014902380295097828 Engine time: 0.0750393345952034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 34.85760738980025,
    "estimated_duration": 3600.0579404399805,
    "input_throughput": 6905.788021001014,
    "output_throughput": 6032.443188218755,
    "total_throughput": 12938.23120921977,
    "itl": 100.02790129914905,
    "ttft": 1806356.150105773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0579854451119888,
    "arrivals": 418034,
    "finished_requests": 100883,
    "scheduler_time": 191.47582261469455
}
#Debug simulation 
Total elapsed time: 34.85784436808899. Arrivals time: 0.5593812614679337 Scheduler time: 34.10488500446081 Scheduler overhead time: 0.07405505049973726 Adapter cache time: 0.014786815736442804 Engine time: 0.07532675890251994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 35.84831334790215,
    "estimated_duration": 3600.0397064770677,
    "input_throughput": 6835.877380942094,
    "output_throughput": 5967.471125762387,
    "total_throughput": 12803.34850670448,
    "itl": 97.22511962277078,
    "ttft": 1811594.6448309592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1635830736951915,
    "arrivals": 418034,
    "finished_requests": 99835,
    "scheduler_time": 193.60339988547568
}
#Debug simulation 
Total elapsed time: 35.84847133886069. Arrivals time: 0.5803512316197157 Scheduler time: 35.06907377904281 Scheduler overhead time: 0.07600548164919019 Adapter cache time: 0.014377739746123552 Engine time: 0.0780493994243443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 32.31898828083649,
    "estimated_duration": 3600.0206217186474,
    "input_throughput": 6619.85819087414,
    "output_throughput": 5788.6065636066905,
    "total_throughput": 12408.46475448083,
    "itl": 90.62589376978924,
    "ttft": 1833686.9476993657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2755305090313789,
    "arrivals": 418034,
    "finished_requests": 96678,
    "scheduler_time": 198.99023370626122
}
#Debug simulation 
Total elapsed time: 32.319168163929135. Arrivals time: 0.5565835786983371 Scheduler time: 31.562679724302143 Scheduler overhead time: 0.07580315694212914 Adapter cache time: 0.015013576485216618 Engine time: 0.0775099191814661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 35.589830280747265,
    "estimated_duration": 3600.0983331071866,
    "input_throughput": 6835.748561001125,
    "output_throughput": 5974.433476497938,
    "total_throughput": 12810.182037499064,
    "itl": 97.28499539927775,
    "ttft": 1811423.2678178088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.071686636335216,
    "arrivals": 418034,
    "finished_requests": 99886,
    "scheduler_time": 193.48850923707985
}
#Debug simulation 
Total elapsed time: 35.590123964939266. Arrivals time: 0.5753659172914922 Scheduler time: 34.81318873306736 Scheduler overhead time: 0.07678508199751377 Adapter cache time: 0.014736863318830729 Engine time: 0.07949532475322485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 29.790291047189385,
    "estimated_duration": 3600.082688612424,
    "input_throughput": 6622.640939725031,
    "output_throughput": 5793.615814985373,
    "total_throughput": 12416.256754710403,
    "itl": 90.81403065156408,
    "ttft": 1834808.3157727092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3162183792609738,
    "arrivals": 418034,
    "finished_requests": 96743,
    "scheduler_time": 198.81236313873362
}
#Debug simulation 
Total elapsed time: 29.79048304213211. Arrivals time: 0.5178434317931533 Scheduler time: 29.07487140921876 Scheduler overhead time: 0.07526091672480106 Adapter cache time: 0.01502512488514185 Engine time: 0.07626839354634285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 30.786546282935888,
    "estimated_duration": 3600.004088300074,
    "input_throughput": 6817.032813867845,
    "output_throughput": 5956.618513210025,
    "total_throughput": 12773.65132707787,
    "itl": 97.08291306273823,
    "ttft": 1815410.2454252911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0788826389564177,
    "arrivals": 418034,
    "finished_requests": 99577,
    "scheduler_time": 193.85952388284468
}
#Debug simulation 
Total elapsed time: 30.786698933690786. Arrivals time: 0.5819250158965588 Scheduler time: 30.014539971016347 Scheduler overhead time: 0.07234928663820028 Adapter cache time: 0.014109327923506498 Engine time: 0.07416578428819776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 31.839911109302193,
    "estimated_duration": 3600.051127392924,
    "input_throughput": 6635.835201957291,
    "output_throughput": 5801.629271616842,
    "total_throughput": 12437.464473574133,
    "itl": 90.99741571054872,
    "ttft": 1831306.3784130155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1622038613632326,
    "arrivals": 418034,
    "finished_requests": 96958,
    "scheduler_time": 198.5304158074267
}
#Debug simulation 
Total elapsed time: 31.840137436054647. Arrivals time: 0.5533196544274688 Scheduler time: 31.087807829957455 Scheduler overhead time: 0.07628358388319612 Adapter cache time: 0.014405612833797932 Engine time: 0.07713556382805109 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 69.89183553121984,
    "estimated_duration": 3600.080941795083,
    "input_throughput": 6893.823611539416,
    "output_throughput": 6039.430321574581,
    "total_throughput": 12933.253933113996,
    "itl": 100.27829989809412,
    "ttft": 1808036.415880733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1836212167190374,
    "arrivals": 416602,
    "finished_requests": 100696,
    "scheduler_time": 191.2557284111445
}
#Debug simulation 
Total elapsed time: 69.8920263540931. Arrivals time: 0.6774249961599708 Scheduler time: 68.96504045557231 Scheduler overhead time: 0.09742154320701957 Adapter cache time: 0.018899640534073114 Engine time: 0.09964852733537555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 33.64538102131337,
    "estimated_duration": 3600.009754312882,
    "input_throughput": 6797.80408113669,
    "output_throughput": 5956.619693685498,
    "total_throughput": 12754.423774822188,
    "itl": 97.29062721431687,
    "ttft": 1817033.4512635935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6410078786313558,
    "arrivals": 416602,
    "finished_requests": 99282,
    "scheduler_time": 193.75784173051824
}
#Debug simulation 
Total elapsed time: 33.645555834285915. Arrivals time: 0.5044038589112461 Scheduler time: 32.95419439766556 Scheduler overhead time: 0.07107008807361126 Adapter cache time: 0.014311112929135561 Engine time: 0.0727238948456943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 23.5617745378986,
    "estimated_duration": 3600.0288142263535,
    "input_throughput": 6619.993958332118,
    "output_throughput": 5797.1199890201215,
    "total_throughput": 12417.11394735224,
    "itl": 90.93325634833602,
    "ttft": 1833075.7789599467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2317325293738397,
    "arrivals": 416602,
    "finished_requests": 96657,
    "scheduler_time": 198.64011976098678
}
#Debug simulation 
Total elapsed time: 23.56197641789913. Arrivals time: 0.5298988264985383 Scheduler time: 22.85819647507742 Scheduler overhead time: 0.06579137546941638 Adapter cache time: 0.012403925880789757 Engine time: 0.06647346075624228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 30.442153085023165,
    "estimated_duration": 3600.0543588079067,
    "input_throughput": 6818.533986839112,
    "output_throughput": 5977.648628374021,
    "total_throughput": 12796.182615213133,
    "itl": 97.64112094153357,
    "ttft": 1814511.1458405664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.315947183854877,
    "arrivals": 416602,
    "finished_requests": 99613,
    "scheduler_time": 193.13970287465423
}
#Debug simulation 
Total elapsed time: 30.442320842295885. Arrivals time: 0.5606355885975063 Scheduler time: 29.696870767511427 Scheduler overhead time: 0.07038782024756074 Adapter cache time: 0.013426231686025858 Engine time: 0.07214673701673746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 24.923233028966933,
    "estimated_duration": 3600.0364400988665,
    "input_throughput": 6622.512131945325,
    "output_throughput": 5799.231854282744,
    "total_throughput": 12421.743986228068,
    "itl": 90.97340361034344,
    "ttft": 1832971.5334588352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2222570082312456,
    "arrivals": 416602,
    "finished_requests": 96691,
    "scheduler_time": 198.60943543824973
}
#Debug simulation 
Total elapsed time: 24.92337255505845. Arrivals time: 0.47135301353409886 Scheduler time: 24.27057229541242 Scheduler overhead time: 0.06913536414504051 Adapter cache time: 0.013005698565393686 Engine time: 0.06985723786056042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 35.35129502369091,
    "estimated_duration": 3600.03802635158,
    "input_throughput": 6807.704757729286,
    "output_throughput": 5959.787325286618,
    "total_throughput": 12767.492083015904,
    "itl": 97.17777476558915,
    "ttft": 1813470.3370685831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1618736111838341,
    "arrivals": 416602,
    "finished_requests": 99402,
    "scheduler_time": 193.74934419638157
}
#Debug simulation 
Total elapsed time: 35.35150613775477. Arrivals time: 0.5406987373717129 Scheduler time: 34.62148507870734 Scheduler overhead time: 0.07200034521520138 Adapter cache time: 0.014019900001585484 Engine time: 0.07386558363214135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 31.8628422440961,
    "estimated_duration": 3600.0308020136927,
    "input_throughput": 6620.627242041399,
    "output_throughput": 5799.245658765503,
    "total_throughput": 12419.872900806902,
    "itl": 90.9897825904771,
    "ttft": 1828868.284239193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1607021721452477,
    "arrivals": 416602,
    "finished_requests": 96670,
    "scheduler_time": 198.58112243982038
}
#Debug simulation 
Total elapsed time: 31.86298714624718. Arrivals time: 0.5330520905554295 Scheduler time: 31.138548959512264 Scheduler overhead time: 0.07346785999834538 Adapter cache time: 0.013470727950334549 Engine time: 0.07405310729518533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 84.99230635585263,
    "estimated_duration": 3600.071781562519,
    "input_throughput": 6897.168864011663,
    "output_throughput": 6038.471541410426,
    "total_throughput": 12935.640405422087,
    "itl": 100.0323829520109,
    "ttft": 1765287.5704903172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 42,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2777211793418972,
    "arrivals": 415862,
    "finished_requests": 100719,
    "scheduler_time": 191.35880888218398
}
#Debug simulation 
Total elapsed time: 84.99248756095767. Arrivals time: 0.6575640994124115 Scheduler time: 84.07318078819662 Scheduler overhead time: 0.10521623119711876 Adapter cache time: 0.018501661252230406 Engine time: 0.10395285440608859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 83.43258323520422,
    "estimated_duration": 3600.1017641803724,
    "input_throughput": 6825.09318055181,
    "output_throughput": 5975.688580263742,
    "total_throughput": 12800.781760815551,
    "itl": 97.34502713260693,
    "ttft": 1771295.831092052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 42,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30421842481009675,
    "arrivals": 415862,
    "finished_requests": 99658,
    "scheduler_time": 193.29379346780567
}
#Debug simulation 
Total elapsed time: 83.4328618333675. Arrivals time: 0.7278543603606522 Scheduler time: 82.44556299131364 Scheduler overhead time: 0.10304468497633934 Adapter cache time: 0.017723303753882647 Engine time: 0.10342931421473622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 77.30104247294366,
    "estimated_duration": 3600.021378895333,
    "input_throughput": 6632.762833016007,
    "output_throughput": 5810.628548661233,
    "total_throughput": 12443.39138167724,
    "itl": 91.05080192854551,
    "ttft": 1792630.5722026424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 42,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3118541793804616,
    "arrivals": 415862,
    "finished_requests": 96909,
    "scheduler_time": 198.26594937430062
}
#Debug simulation 
Total elapsed time: 77.30121513316408. Arrivals time: 0.7133805179037154 Scheduler time: 76.32809308962896 Scheduler overhead time: 0.10491784755140543 Adapter cache time: 0.016821276396512985 Engine time: 0.10224444326013327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 83.75697310687974,
    "estimated_duration": 3600.0482664945844,
    "input_throughput": 6825.025994422167,
    "output_throughput": 5975.539050465773,
    "total_throughput": 12800.56504488794,
    "itl": 97.34242022636383,
    "ttft": 1771254.7485613492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 42,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28617155215702955,
    "arrivals": 415862,
    "finished_requests": 99656,
    "scheduler_time": 193.2930017304139
}
#Debug simulation 
Total elapsed time: 83.75715678185225. Arrivals time: 0.699780847877264 Scheduler time: 82.79794044559821 Scheduler overhead time: 0.10414231615141034 Adapter cache time: 0.018063414376229048 Engine time: 0.10333841433748603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 73.9305425430648,
    "estimated_duration": 3600.066931800626,
    "input_throughput": 6632.679461894622,
    "output_throughput": 5810.544469387791,
    "total_throughput": 12443.223931282413,
    "itl": 91.04916056953688,
    "ttft": 1792657.562658206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 42,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3091616560844705,
    "arrivals": 415862,
    "finished_requests": 96909,
    "scheduler_time": 198.26996588212864
}
#Debug simulation 
Total elapsed time: 73.93076631426811. Arrivals time: 0.6926458957605064 Scheduler time: 72.9810867793858 Scheduler overhead time: 0.10253053810447454 Adapter cache time: 0.017731381580233574 Engine time: 0.10145566565915942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 83.64768514502794,
    "estimated_duration": 3600.023698954242,
    "input_throughput": 6825.159791902888,
    "output_throughput": 5975.710383864736,
    "total_throughput": 12800.870175767624,
    "itl": 97.34300465197329,
    "ttft": 1771164.4224823576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 42,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26812467950396235,
    "arrivals": 415862,
    "finished_requests": 99657,
    "scheduler_time": 193.29095631451347
}
#Debug simulation 
Total elapsed time: 83.64783438341692. Arrivals time: 0.6377488193102181 Scheduler time: 82.75442841555923 Scheduler overhead time: 0.10278358357027173 Adapter cache time: 0.01796400547027588 Engine time: 0.10049589304253459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 99.06792931212112,
    "estimated_duration": 3600.0775428550924,
    "input_throughput": 6632.573525364509,
    "output_throughput": 5810.503454714611,
    "total_throughput": 12443.07698007912,
    "itl": 91.0495595592883,
    "ttft": 1792647.6783533087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 42,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30646913278847937,
    "arrivals": 415862,
    "finished_requests": 96908,
    "scheduler_time": 198.27035185348882
}
#Debug simulation 
Total elapsed time: 99.06813354836777. Arrivals time: 0.791759418323636 Scheduler time: 97.94626703066751 Scheduler overhead time: 0.13428747560828924 Adapter cache time: 0.02298586815595627 Engine time: 0.1309707979671657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 28.521200952120125,
    "estimated_duration": 3600.0187815916443,
    "input_throughput": 6894.992916960734,
    "output_throughput": 6041.229593359097,
    "total_throughput": 12936.22251031983,
    "itl": 100.27358543206107,
    "ttft": 1796461.7084473532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0183109909202892,
    "arrivals": 415494,
    "finished_requests": 100914,
    "scheduler_time": 191.19325506401327
}
#Debug simulation 
Total elapsed time: 28.521376034244895. Arrivals time: 0.5608563343994319 Scheduler time: 27.777586351614445 Scheduler overhead time: 0.06995969777926803 Adapter cache time: 0.012997324578464031 Engine time: 0.07097451156005263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 26.47631745180115,
    "estimated_duration": 3600.021285841822,
    "input_throughput": 6817.047470390514,
    "output_throughput": 5976.184942186839,
    "total_throughput": 12793.232412577354,
    "itl": 97.54035107246891,
    "ttft": 1804533.6776520014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0611570078413943,
    "arrivals": 415494,
    "finished_requests": 99821,
    "scheduler_time": 193.26405635617294
}
#Debug simulation 
Total elapsed time: 26.476467958651483. Arrivals time: 0.5646907547488809 Scheduler time: 25.729998014867306 Scheduler overhead time: 0.06963514629751444 Adapter cache time: 0.012795438058674335 Engine time: 0.06980398576706648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 28.61027290765196,
    "estimated_duration": 3600.0978555057427,
    "input_throughput": 6627.702345231971,
    "output_throughput": 5810.224288212166,
    "total_throughput": 12437.926633444136,
    "itl": 91.25204221995882,
    "ttft": 1825831.6820951058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 163,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2258332209941023,
    "arrivals": 415494,
    "finished_requests": 97045,
    "scheduler_time": 198.2081743118158
}
#Debug simulation 
Total elapsed time: 28.61044036783278. Arrivals time: 0.5375673430971801 Scheduler time: 27.879613660275936 Scheduler overhead time: 0.07333487598225474 Adapter cache time: 0.014101140201091766 Engine time: 0.07470238488167524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 33.503717850893736,
    "estimated_duration": 3600.0674582094607,
    "input_throughput": 6808.284923691542,
    "output_throughput": 5963.8757465613035,
    "total_throughput": 12772.160670252844,
    "itl": 97.21093231640394,
    "ttft": 1799654.0281475813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9498349545383821,
    "arrivals": 415494,
    "finished_requests": 99612,
    "scheduler_time": 193.555680872648
}
#Debug simulation 
Total elapsed time: 33.50391663564369. Arrivals time: 0.5184443187899888 Scheduler time: 32.81109092058614 Scheduler overhead time: 0.06707681668922305 Adapter cache time: 0.012319490313529968 Engine time: 0.06686406210064888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 27.032301200088114,
    "estimated_duration": 3600.0010070690914,
    "input_throughput": 6628.504534621656,
    "output_throughput": 5811.506707614228,
    "total_throughput": 12440.011242235883,
    "itl": 91.24816077221269,
    "ttft": 1825741.637293295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2316295089107032,
    "arrivals": 415494,
    "finished_requests": 97060,
    "scheduler_time": 198.22463101973898
}
#Debug simulation 
Total elapsed time: 27.03250238019973. Arrivals time: 0.5405936990864575 Scheduler time: 26.298092724755406 Scheduler overhead time: 0.07358298730105162 Adapter cache time: 0.014134622644633055 Engine time: 0.07472075475379825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 26.856499396730214,
    "estimated_duration": 3600.092517155144,
    "input_throughput": 6817.555349770556,
    "output_throughput": 5976.784179147455,
    "total_throughput": 12794.339528918012,
    "itl": 97.52281028841338,
    "ttft": 1804499.7527539767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9320524573232961,
    "arrivals": 415494,
    "finished_requests": 99833,
    "scheduler_time": 193.28894219020876
}
#Debug simulation 
Total elapsed time: 26.856668999884278. Arrivals time: 0.569851235486567 Scheduler time: 26.103643946815282 Scheduler overhead time: 0.07004145067185163 Adapter cache time: 0.013037308119237423 Engine time: 0.07048076950013638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 28.945072137750685,
    "estimated_duration": 3600.0115700338956,
    "input_throughput": 6626.544536293084,
    "output_throughput": 5811.23826771563,
    "total_throughput": 12437.782804008713,
    "itl": 91.24852849148333,
    "ttft": 1825734.8886101053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2094175985455535,
    "arrivals": 415494,
    "finished_requests": 97044,
    "scheduler_time": 198.20929738928814
}
#Debug simulation 
Total elapsed time: 28.94526445865631. Arrivals time: 0.5286259138956666 Scheduler time: 28.222189779859036 Scheduler overhead time: 0.0738627421669662 Adapter cache time: 0.013997517060488462 Engine time: 0.07534364378079772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 32.304869840852916,
    "estimated_duration": 3600.050265128544,
    "input_throughput": 6877.647026163375,
    "output_throughput": 6038.961514117561,
    "total_throughput": 12916.608540280935,
    "itl": 100.26055365493448,
    "ttft": 1759001.832702766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24465913418214755,
    "arrivals": 386239,
    "finished_requests": 100430,
    "scheduler_time": 190.79910867791526
}
#Debug simulation 
Total elapsed time: 32.30502286879346. Arrivals time: 0.5991123472340405 Scheduler time: 31.510478751268238 Scheduler overhead time: 0.07669274508953094 Adapter cache time: 0.012235794682055712 Engine time: 0.07593016792088747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 29.96344884065911,
    "estimated_duration": 3600.0208926862806,
    "input_throughput": 6805.689114131226,
    "output_throughput": 5977.243922032754,
    "total_throughput": 12782.93303616398,
    "itl": 97.59642193991945,
    "ttft": 1767245.3889863547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2639694942673669,
    "arrivals": 386239,
    "finished_requests": 99426,
    "scheduler_time": 192.70264921648297
}
#Debug simulation 
Total elapsed time: 29.963671068660915. Arrivals time: 0.6319618346169591 Scheduler time: 29.1436696681194 Scheduler overhead time: 0.07250735722482204 Adapter cache time: 0.012176659889519215 Engine time: 0.07332703610882163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 29.245671138633043,
    "estimated_duration": 3600.0753132105233,
    "input_throughput": 6616.1579766392915,
    "output_throughput": 5815.06084697173,
    "total_throughput": 12431.218823611021,
    "itl": 91.31153486315446,
    "ttft": 1789188.8526296373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2697650061175228,
    "arrivals": 386239,
    "finished_requests": 96769,
    "scheduler_time": 197.621300539661
}
#Debug simulation 
Total elapsed time: 29.245926970615983. Arrivals time: 0.5820576809346676 Scheduler time: 28.46490106591955 Scheduler overhead time: 0.07686099642887712 Adapter cache time: 0.012433343101292849 Engine time: 0.07750511402264237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 29.612191529013216,
    "estimated_duration": 3600.0630872898423,
    "input_throughput": 6805.609347930698,
    "output_throughput": 5977.173865638862,
    "total_throughput": 12782.783213569559,
    "itl": 97.59630745860187,
    "ttft": 1767253.2649403692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25147550550755104,
    "arrivals": 386239,
    "finished_requests": 99426,
    "scheduler_time": 192.70539186588815
}
#Debug simulation 
Total elapsed time: 29.612365967128426. Arrivals time: 0.5698416191153228 Scheduler time: 28.856508641969413 Scheduler overhead time: 0.07163534639403224 Adapter cache time: 0.011644127778708935 Engine time: 0.07280589081346989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 28.92113314382732,
    "estimated_duration": 3600.093886967434,
    "input_throughput": 6616.354947360701,
    "output_throughput": 5815.153064698219,
    "total_throughput": 12431.508012058921,
    "itl": 91.30980848411562,
    "ttft": 1789165.2102111252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2679009515279905,
    "arrivals": 386239,
    "finished_requests": 96771,
    "scheduler_time": 197.6237959007915
}
#Debug simulation 
Total elapsed time: 28.921272309031337. Arrivals time: 0.5664541027508676 Scheduler time: 28.157947630621493 Scheduler overhead time: 0.07641464238986373 Adapter cache time: 0.012425565160810947 Engine time: 0.07670567370951176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 29.65981954522431,
    "estimated_duration": 3600.0345420038907,
    "input_throughput": 6805.72719904017,
    "output_throughput": 5977.107371870529,
    "total_throughput": 12782.8345709107,
    "itl": 97.59601215963899,
    "ttft": 1767277.2811148374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23620507480110967,
    "arrivals": 386239,
    "finished_requests": 99426,
    "scheduler_time": 192.70259000703257
}
#Debug simulation 
Total elapsed time: 29.660019940230995. Arrivals time: 0.5981068667024374 Scheduler time: 28.874084880109876 Scheduler overhead time: 0.0725101176649332 Adapter cache time: 0.011710201390087605 Engine time: 0.07364228181540966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 28.920151111204177,
    "estimated_duration": 3600.0744007235103,
    "input_throughput": 6616.159653593031,
    "output_throughput": 5815.062320876686,
    "total_throughput": 12431.221974469718,
    "itl": 91.30940517507021,
    "ttft": 1789166.1073275302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26562266258522876,
    "arrivals": 386239,
    "finished_requests": 96769,
    "scheduler_time": 197.62288196317684
}
#Debug simulation 
Total elapsed time: 28.920331664849073. Arrivals time: 0.5818706140853465 Scheduler time: 28.141809199471027 Scheduler overhead time: 0.07528509991243482 Adapter cache time: 0.01284592691808939 Engine time: 0.07658858131617308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 25.087697186041623,
    "estimated_duration": 3600.0095082117123,
    "input_throughput": 6915.756734311339,
    "output_throughput": 6036.302112655987,
    "total_throughput": 12952.058846967326,
    "itl": 100.01039446432524,
    "ttft": 1756552.8987959423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25788395224604743,
    "arrivals": 383390,
    "finished_requests": 100718,
    "scheduler_time": 190.91962067787895
}
#Debug simulation 
Total elapsed time: 25.08783755870536. Arrivals time: 0.5673008756712079 Scheduler time: 24.34072068752721 Scheduler overhead time: 0.06940279947593808 Adapter cache time: 0.011134371627122164 Engine time: 0.06934029655531049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 24.082276726141572,
    "estimated_duration": 3600.092564043094,
    "input_throughput": 6841.163820615915,
    "output_throughput": 5975.326638779863,
    "total_throughput": 12816.490459395778,
    "itl": 97.3352661362892,
    "ttft": 1765447.6437788743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.276737336148508,
    "arrivals": 383390,
    "finished_requests": 99677,
    "scheduler_time": 192.8393916791065
}
#Debug simulation 
Total elapsed time: 24.082446036860347. Arrivals time: 0.5446705440990627 Scheduler time: 23.358070937450975 Scheduler overhead time: 0.06908697774633765 Adapter cache time: 0.011062894016504288 Engine time: 0.07003012765198946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 22.441724040079862,
    "estimated_duration": 3600.0570353762296,
    "input_throughput": 6651.622950606243,
    "output_throughput": 5810.124615932386,
    "total_throughput": 12461.747566538628,
    "itl": 91.04505672657373,
    "ttft": 1786806.626793998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2824995400756599,
    "arrivals": 383390,
    "finished_requests": 96953,
    "scheduler_time": 197.79132049562128
}
#Debug simulation 
Total elapsed time: 22.44187467219308. Arrivals time: 0.5267920927144587 Scheduler time: 21.7297544721514 Scheduler overhead time: 0.07080434635281563 Adapter cache time: 0.011625140439718962 Engine time: 0.07197177736088634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 25.032061850186437,
    "estimated_duration": 3600.0536928943534,
    "input_throughput": 6841.096300483076,
    "output_throughput": 5975.293658108024,
    "total_throughput": 12816.3899585911,
    "itl": 97.3374150588306,
    "ttft": 1765446.401062784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2628551264153794,
    "arrivals": 383390,
    "finished_requests": 99673,
    "scheduler_time": 192.83566106150528
}
#Debug simulation 
Total elapsed time: 25.032194088213146. Arrivals time: 0.5559341362677515 Scheduler time: 24.29690178576857 Scheduler overhead time: 0.07011468289420009 Adapter cache time: 0.010820316150784492 Engine time: 0.06918075866997242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 23.033983116038144,
    "estimated_duration": 3600.0252302160043,
    "input_throughput": 6651.440884086043,
    "output_throughput": 5809.862337755073,
    "total_throughput": 12461.303221841115,
    "itl": 91.0461970377377,
    "ttft": 1786778.4836661587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28042836830951284,
    "arrivals": 383390,
    "finished_requests": 96949,
    "scheduler_time": 197.78819650505193
}
#Debug simulation 
Total elapsed time: 23.03419092623517. Arrivals time: 0.5401545171625912 Scheduler time: 22.306614086497575 Scheduler overhead time: 0.07204665569588542 Adapter cache time: 0.011603985447436571 Engine time: 0.07279866328462958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 24.71592118218541,
    "estimated_duration": 3600.0746268534726,
    "input_throughput": 6841.135963208024,
    "output_throughput": 5975.322244584,
    "total_throughput": 12816.458207792024,
    "itl": 97.33683403800386,
    "ttft": 1765432.9293257375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24897291668225072,
    "arrivals": 383390,
    "finished_requests": 99675,
    "scheduler_time": 192.83645646068626
}
#Debug simulation 
Total elapsed time: 24.716092242859304. Arrivals time: 0.5471150651574135 Scheduler time: 23.98911912785843 Scheduler overhead time: 0.06904587615281343 Adapter cache time: 0.011090686079114676 Engine time: 0.07018652698025107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 22.666951598133892,
    "estimated_duration": 3600.086030708205,
    "input_throughput": 6651.294662336033,
    "output_throughput": 5809.77699465851,
    "total_throughput": 12461.071656994543,
    "itl": 91.04778142979633,
    "ttft": 1786835.7432001543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2783571965433658,
    "arrivals": 383390,
    "finished_requests": 96948,
    "scheduler_time": 197.7896683055942
}
#Debug simulation 
Total elapsed time: 22.66706407489255. Arrivals time: 0.5326160122640431 Scheduler time: 21.94777909433469 Scheduler overhead time: 0.07132856920361519 Adapter cache time: 0.011754990555346012 Engine time: 0.07252686703577638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 23.656551242806017,
    "estimated_duration": 3600.059829370263,
    "input_throughput": 6950.997257280944,
    "output_throughput": 6038.138539437118,
    "total_throughput": 12989.135796718063,
    "itl": 99.98670563880886,
    "ttft": 1755958.8698497193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23804672515019762,
    "arrivals": 381970,
    "finished_requests": 101104,
    "scheduler_time": 190.94206177642448
}
#Debug simulation 
Total elapsed time: 23.656737936660647. Arrivals time: 0.5886352737434208 Scheduler time: 22.893954412080348 Scheduler overhead time: 0.06711641605943441 Adapter cache time: 0.010687274858355522 Engine time: 0.06786700710654259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 23.010898009873927,
    "estimated_duration": 3600.0677750445084,
    "input_throughput": 6880.175471055895,
    "output_throughput": 5972.887829794859,
    "total_throughput": 12853.063300850754,
    "itl": 97.30830523235336,
    "ttft": 1764281.856608404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25897379430010914,
    "arrivals": 381970,
    "finished_requests": 100039,
    "scheduler_time": 192.85983840797007
}
#Debug simulation 
Total elapsed time: 23.011054386850446. Arrivals time: 0.5447890595532954 Scheduler time: 22.289741463493556 Scheduler overhead time: 0.06748783681541681 Adapter cache time: 0.010810292791575193 Engine time: 0.06906242296099663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 21.866660915780813,
    "estimated_duration": 3600.0363309162435,
    "input_throughput": 6692.980510523792,
    "output_throughput": 5811.612460776236,
    "total_throughput": 12504.592971300028,
    "itl": 91.02052697205801,
    "ttft": 1786248.2663263143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26510654553305363,
    "arrivals": 381970,
    "finished_requests": 97346,
    "scheduler_time": 197.82438448361256
}
#Debug simulation 
Total elapsed time: 21.866881717927754. Arrivals time: 0.529936958104372 Scheduler time: 21.153579691890627 Scheduler overhead time: 0.06983901839703321 Adapter cache time: 0.011719138361513615 Engine time: 0.07097625499591231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 23.0777746620588,
    "estimated_duration": 3600.0579993911556,
    "input_throughput": 6880.252208211368,
    "output_throughput": 5972.94571466254,
    "total_throughput": 12853.19792287391,
    "itl": 97.30380683328568,
    "ttft": 1764272.6487767387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24009588459972292,
    "arrivals": 381970,
    "finished_requests": 100041,
    "scheduler_time": 192.86353085599492
}
#Debug simulation 
Total elapsed time: 23.078105591237545. Arrivals time: 0.5726376939564943 Scheduler time: 22.329950974788517 Scheduler overhead time: 0.06744641857221723 Adapter cache time: 0.010798670817166567 Engine time: 0.06773947086185217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 22.166798260994256,
    "estimated_duration": 3600.009392599633,
    "input_throughput": 6693.137259455953,
    "output_throughput": 5811.656503732571,
    "total_throughput": 12504.793763188525,
    "itl": 91.0196896925876,
    "ttft": 1786234.302010777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2632424909435213,
    "arrivals": 381970,
    "finished_requests": 97347,
    "scheduler_time": 197.82325922590172
}
#Debug simulation 
Total elapsed time: 22.166964204981923. Arrivals time: 0.5325585468672216 Scheduler time: 21.449328794609755 Scheduler overhead time: 0.07168256863951683 Adapter cache time: 0.011303525883704424 Engine time: 0.07109423726797104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 23.274245951790363,
    "estimated_duration": 3600.0548202320256,
    "input_throughput": 6880.199396075757,
    "output_throughput": 5972.94126721496,
    "total_throughput": 12853.140663290716,
    "itl": 97.30786157801705,
    "ttft": 1764323.8061799868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22982115386053914,
    "arrivals": 381970,
    "finished_requests": 100040,
    "scheduler_time": 192.8596161586068
}
#Debug simulation 
Total elapsed time: 23.27437473880127. Arrivals time: 0.5343446754850447 Scheduler time: 22.56307468842715 Scheduler overhead time: 0.06853255303576589 Adapter cache time: 0.010793426539748907 Engine time: 0.0680715567432344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 21.811386779882014,
    "estimated_duration": 3600.082930993849,
    "input_throughput": 6693.0058173265925,
    "output_throughput": 5811.582511024035,
    "total_throughput": 12504.588328350626,
    "itl": 91.01919222292746,
    "ttft": 1786242.399955679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2607570848241449,
    "arrivals": 381970,
    "finished_requests": 97348,
    "scheduler_time": 197.82777955435142
}
#Debug simulation 
Total elapsed time: 21.811618620995432. Arrivals time: 0.5014139581471682 Scheduler time: 21.128057360183448 Scheduler overhead time: 0.0696088457480073 Adapter cache time: 0.011670281179249287 Engine time: 0.06997442059218884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 22.987926584202796,
    "estimated_duration": 3600.0979482817006,
    "input_throughput": 6917.865668595757,
    "output_throughput": 6042.151717117093,
    "total_throughput": 12960.01738571285,
    "itl": 100.19137762175183,
    "ttft": 1759052.4930156707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 38,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2512715432140975,
    "arrivals": 381260,
    "finished_requests": 101025,
    "scheduler_time": 190.75143852658888
}
#Debug simulation 
Total elapsed time: 22.988087818957865. Arrivals time: 0.5436986172571778 Scheduler time: 22.269198949914426 Scheduler overhead time: 0.06820253329351544 Adapter cache time: 0.01053557125851512 Engine time: 0.06773349456489086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 23.13018021499738,
    "estimated_duration": 3600.088203653858,
    "input_throughput": 6840.169353352786,
    "output_throughput": 5979.222114100641,
    "total_throughput": 12819.391467453426,
    "itl": 97.51842742860477,
    "ttft": 1766841.4410283503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 38,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27451807812787593,
    "arrivals": 381260,
    "finished_requests": 99914,
    "scheduler_time": 192.67101912061977
}
#Debug simulation 
Total elapsed time: 23.130324583966285. Arrivals time: 0.5707202591001987 Scheduler time: 22.378318367060274 Scheduler overhead time: 0.06936453934758902 Adapter cache time: 0.011308860033750534 Engine time: 0.07075322838500142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 20.805050953757018,
    "estimated_duration": 3600.0351703635633,
    "input_throughput": 6646.194791920238,
    "output_throughput": 5815.622906230012,
    "total_throughput": 12461.81769815025,
    "itl": 91.23656312855549,
    "ttft": 1788497.8008528568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 38,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28125869228038936,
    "arrivals": 381260,
    "finished_requests": 97144,
    "scheduler_time": 197.60136183556247
}
#Debug simulation 
Total elapsed time: 20.805253498721868. Arrivals time: 0.5163069926202297 Scheduler time: 20.104908832348883 Scheduler overhead time: 0.06999858375638723 Adapter cache time: 0.011510798707604408 Engine time: 0.07162090390920639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 22.92477936577052,
    "estimated_duration": 3600.087793220588,
    "input_throughput": 6839.73319938735,
    "output_throughput": 5978.9497468738855,
    "total_throughput": 12818.682946261237,
    "itl": 97.51748664945666,
    "ttft": 1766819.8241900394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 38,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2592476474214345,
    "arrivals": 381260,
    "finished_requests": 99910,
    "scheduler_time": 192.67204396030274
}
#Debug simulation 
Total elapsed time: 22.924920106772333. Arrivals time: 0.603159103076905 Scheduler time: 22.14057925948873 Scheduler overhead time: 0.06964716780930758 Adapter cache time: 0.011432508006691933 Engine time: 0.06990486709401011 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 21.242227925918996,
    "estimated_duration": 3600.011327416231,
    "input_throughput": 6646.238809801842,
    "output_throughput": 5815.661423217333,
    "total_throughput": 12461.900233019176,
    "itl": 91.23556030997128,
    "ttft": 1788400.8189343933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 38,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27898040333762764,
    "arrivals": 381260,
    "finished_requests": 97144,
    "scheduler_time": 197.6003191431631
}
#Debug simulation 
Total elapsed time: 21.24238137388602. Arrivals time: 0.5218133521266282 Scheduler time: 20.535927094053477 Scheduler overhead time: 0.07170254364609718 Adapter cache time: 0.011102878488600254 Engine time: 0.07113057747483253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 22.094776714220643,
    "estimated_duration": 3600.1050594101603,
    "input_throughput": 6840.137327557487,
    "output_throughput": 5979.194119275721,
    "total_throughput": 12819.331446833208,
    "itl": 97.51525095351352,
    "ttft": 1766776.5170394827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 38,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2425889957416802,
    "arrivals": 381260,
    "finished_requests": 99914,
    "scheduler_time": 192.67347923382306
}
#Debug simulation 
Total elapsed time: 22.094986846204847. Arrivals time: 0.6391678950749338 Scheduler time: 21.282115863170475 Scheduler overhead time: 0.06662304420024157 Adapter cache time: 0.010761523619294167 Engine time: 0.0673323473893106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 19.070729055907577,
    "estimated_duration": 3600.087590812921,
    "input_throughput": 6646.098017464416,
    "output_throughput": 5815.538225633123,
    "total_throughput": 12461.636243097539,
    "itl": 91.23523898517765,
    "ttft": 1788534.716592879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 38,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2764949972182512,
    "arrivals": 381260,
    "finished_requests": 97144,
    "scheduler_time": 197.60498520234196
}
#Debug simulation 
Total elapsed time: 19.07085258467123. Arrivals time: 0.395278986543417 Scheduler time: 18.506693826057017 Scheduler overhead time: 0.06470492389053106 Adapter cache time: 0.010459821671247482 Engine time: 0.06526497937738895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 21.976997144985944,
    "estimated_duration": 3600.0526354549775,
    "input_throughput": 6947.960358595933,
    "output_throughput": 6044.445235520817,
    "total_throughput": 12992.40559411675,
    "itl": 100.12830066621356,
    "ttft": 1757449.971617822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26449636127799736,
    "arrivals": 380865,
    "finished_requests": 101325,
    "scheduler_time": 190.69237845164682
}
#Debug simulation 
Total elapsed time: 21.977143795229495. Arrivals time: 0.5480737644247711 Scheduler time: 21.257743475027382 Scheduler overhead time: 0.06629546592012048 Adapter cache time: 0.010402134619653225 Engine time: 0.06575877405703068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 21.461331912316382,
    "estimated_duration": 3600.0383063318704,
    "input_throughput": 6872.433817297066,
    "output_throughput": 5979.3488758548865,
    "total_throughput": 12851.782693151952,
    "itl": 97.45929476470705,
    "ttft": 1765351.0136192252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28728592000901704,
    "arrivals": 380865,
    "finished_requests": 100205,
    "scheduler_time": 192.59834614098435
}
#Debug simulation 
Total elapsed time: 21.46147813135758. Arrivals time: 0.5239513469859958 Scheduler time: 20.763136265799403 Scheduler overhead time: 0.06696456205099821 Adapter cache time: 0.010968124028295279 Engine time: 0.06713054003193974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 20.387774574104697,
    "estimated_duration": 3600.016654648672,
    "input_throughput": 6684.183799240879,
    "output_throughput": 5813.870881117514,
    "total_throughput": 12498.054680358393,
    "itl": 91.1212368350449,
    "ttft": 1786984.1949461107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2939932262385264,
    "arrivals": 380865,
    "finished_requests": 97438,
    "scheduler_time": 197.5780158548273
}
#Debug simulation 
Total elapsed time: 20.387939685955644. Arrivals time: 0.5130794350989163 Scheduler time: 19.69368226500228 Scheduler overhead time: 0.06939677055925131 Adapter cache time: 0.011372845154255629 Engine time: 0.06952856900170445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 21.59727142425254,
    "estimated_duration": 3600.101911847185,
    "input_throughput": 6872.68016457481,
    "output_throughput": 5979.694610632397,
    "total_throughput": 12852.374775207207,
    "itl": 97.4590479651853,
    "ttft": 1765330.7095831602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27340371027588845,
    "arrivals": 380865,
    "finished_requests": 100209,
    "scheduler_time": 192.60116615733344
}
#Debug simulation 
Total elapsed time: 21.59739979216829. Arrivals time: 0.5290716746822 Scheduler time: 20.89371543424204 Scheduler overhead time: 0.06743240868672729 Adapter cache time: 0.010686216875910759 Engine time: 0.06711260974407196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 20.36220226623118,
    "estimated_duration": 3600.0612125797206,
    "input_throughput": 6683.896905952167,
    "output_throughput": 5813.641981104657,
    "total_throughput": 12497.538887056824,
    "itl": 91.12035278759772,
    "ttft": 1786917.7381682894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2919220544723794,
    "arrivals": 380865,
    "finished_requests": 97437,
    "scheduler_time": 197.58164690646012
}
#Debug simulation 
Total elapsed time: 20.362431533169. Arrivals time: 0.5054141385480762 Scheduler time: 19.675498967058957 Scheduler overhead time: 0.06912287464365363 Adapter cache time: 0.011312732007354498 Engine time: 0.07011723332107067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 21.395052632316947,
    "estimated_duration": 3600.0040549564997,
    "input_throughput": 6871.96837068533,
    "output_throughput": 5979.236320682449,
    "total_throughput": 12851.20469136778,
    "itl": 97.45823669881692,
    "ttft": 1765323.7808901907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25535683762282124,
    "arrivals": 380865,
    "finished_requests": 100201,
    "scheduler_time": 192.59635456968905
}
#Debug simulation 
Total elapsed time: 21.395320686977357. Arrivals time: 0.5294233276508749 Scheduler time: 20.691217495594174 Scheduler overhead time: 0.06666878424584866 Adapter cache time: 0.010888599790632725 Engine time: 0.06743097119033337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 20.161421673372388,
    "estimated_duration": 3600.0217365120548,
    "input_throughput": 6683.94936507057,
    "output_throughput": 5813.727952731187,
    "total_throughput": 12497.677317801757,
    "itl": 91.12142269471852,
    "ttft": 1787129.2657070323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2892295311763883,
    "arrivals": 380865,
    "finished_requests": 97436,
    "scheduler_time": 197.578076728237
}
#Debug simulation 
Total elapsed time: 20.16152979200706. Arrivals time: 0.4994654110632837 Scheduler time: 19.47983187204227 Scheduler overhead time: 0.0699610230512917 Adapter cache time: 0.011479257605969906 Engine time: 0.07015550415962934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 16.898200513795018,
    "estimated_duration": 3600.08114953117,
    "input_throughput": 6877.066924789786,
    "output_throughput": 6040.439394770849,
    "total_throughput": 12917.506319560634,
    "itl": 100.15574288127118,
    "ttft": 1765368.3744458486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25788395224604743,
    "arrivals": 377686,
    "finished_requests": 100478,
    "scheduler_time": 190.75531361127375
}
#Debug simulation 
Total elapsed time: 16.898412723094225. Arrivals time: 0.4716508761048317 Scheduler time: 16.263881327584386 Scheduler overhead time: 0.06225296249613166 Adapter cache time: 0.010207567363977432 Engine time: 0.06225578719750047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 16.772072198800743,
    "estimated_duration": 3600.04917719294,
    "input_throughput": 6805.740642438702,
    "output_throughput": 5975.869200979811,
    "total_throughput": 12781.609843418513,
    "itl": 97.46819350396652,
    "ttft": 1773622.6770307354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28090199906844643,
    "arrivals": 377686,
    "finished_requests": 99407,
    "scheduler_time": 192.66915530069937
}
#Debug simulation 
Total elapsed time: 16.77217246592045. Arrivals time: 0.804670849815011 Scheduler time: 15.802996494807303 Scheduler overhead time: 0.06308291759341955 Adapter cache time: 0.010272570420056581 Engine time: 0.06300250487402081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 14.730719131883234,
    "estimated_duration": 3600.045299413826,
    "input_throughput": 6612.615958992556,
    "output_throughput": 5810.467997001579,
    "total_throughput": 12423.083955994134,
    "itl": 91.14831951998589,
    "ttft": 1794237.8921985498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28762595925945794,
    "arrivals": 377686,
    "finished_requests": 96609,
    "scheduler_time": 197.64071423392525
}
#Debug simulation 
Total elapsed time: 14.730824564117938. Arrivals time: 0.4487423677928746 Scheduler time: 14.11857602884993 Scheduler overhead time: 0.06252172961831093 Adapter cache time: 0.010284480173140764 Engine time: 0.06242971122264862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 15.419706269167364,
    "estimated_duration": 3600.092697238983,
    "input_throughput": 6805.8283662504045,
    "output_throughput": 5975.83779342665,
    "total_throughput": 12781.666159677054,
    "itl": 97.46782449701239,
    "ttft": 1773654.1888647624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.265631568362005,
    "arrivals": 377686,
    "finished_requests": 99408,
    "scheduler_time": 192.67233912868573
}
#Debug simulation 
Total elapsed time: 15.41997635224834. Arrivals time: 0.3945056274533272 Scheduler time: 14.869839851744473 Scheduler overhead time: 0.05919228307902813 Adapter cache time: 0.00968717597424984 Engine time: 0.05972530925646424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 14.790447246283293,
    "estimated_duration": 3600.0121054767337,
    "input_throughput": 6612.5077645669735,
    "output_throughput": 5810.142129294343,
    "total_throughput": 12422.649893861317,
    "itl": 91.15031128284153,
    "ttft": 1794300.0550185628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2853476703166962,
    "arrivals": 377686,
    "finished_requests": 96603,
    "scheduler_time": 197.6368103131096
}
#Debug simulation 
Total elapsed time: 14.79055928112939. Arrivals time: 0.4485843889415264 Scheduler time: 14.177870828192681 Scheduler overhead time: 0.06287948787212372 Adapter cache time: 0.010183981154114008 Engine time: 0.06269231205806136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 15.546601285692304,
    "estimated_duration": 3600.051241604414,
    "input_throughput": 6805.6775739338855,
    "output_throughput": 5975.846330012866,
    "total_throughput": 12781.523903946752,
    "itl": 97.46484206124398,
    "ttft": 1773638.7188498094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24897291668225072,
    "arrivals": 377686,
    "finished_requests": 99406,
    "scheduler_time": 192.6709039693007
}
#Debug simulation 
Total elapsed time: 15.546702656894922. Arrivals time: 0.4010776588693261 Scheduler time: 14.98948786361143 Scheduler overhead time: 0.060037233866751194 Adapter cache time: 0.009701217990368605 Engine time: 0.05945182219147682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 15.071588831022382,
    "estimated_duration": 3600.0959727732543,
    "input_throughput": 6612.892039558815,
    "output_throughput": 5810.647037802606,
    "total_throughput": 12423.53907736142,
    "itl": 91.14991022780205,
    "ttft": 1794249.9901664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2828622641973198,
    "arrivals": 377686,
    "finished_requests": 96612,
    "scheduler_time": 197.64248659950786
}
#Debug simulation 
Total elapsed time: 15.071733602322638. Arrivals time: 0.6799763422459364 Scheduler time: 14.22889042692259 Scheduler overhead time: 0.062026424799114466 Adapter cache time: 0.010175678413361311 Engine time: 0.06211389182135463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 14.315013536717743,
    "estimated_duration": 3600.0942912968435,
    "input_throughput": 6904.571377502963,
    "output_throughput": 6042.956722714957,
    "total_throughput": 12947.52810021792,
    "itl": 100.2538236919987,
    "ttft": 1761829.3576060678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2314343161182477,
    "arrivals": 376204,
    "finished_requests": 100659,
    "scheduler_time": 190.63262769593493
}
#Debug simulation 
Total elapsed time: 14.31512425467372. Arrivals time: 0.4441134980879724 Scheduler time: 13.719421858899295 Scheduler overhead time: 0.057744338642805815 Adapter cache time: 0.00930104311555624 Engine time: 0.05810029758140445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 14.081347102764994,
    "estimated_duration": 3600.0780993766252,
    "input_throughput": 6826.641067663381,
    "output_throughput": 5982.137444109647,
    "total_throughput": 12808.778511773027,
    "itl": 97.58898169933154,
    "ttft": 1769664.549731225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25397809433285146,
    "arrivals": 376204,
    "finished_requests": 99563,
    "scheduler_time": 192.54677180594493
}
#Debug simulation 
Total elapsed time: 14.081475309096277. Arrivals time: 0.3927301410585642 Scheduler time: 13.534503496717662 Scheduler overhead time: 0.05868296045809984 Adapter cache time: 0.00961205642670393 Engine time: 0.05904425261542201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.425468115136027,
    "estimated_duration": 3600.060432122178,
    "input_throughput": 6639.249104468597,
    "output_throughput": 5816.044312249869,
    "total_throughput": 12455.293416718465,
    "itl": 91.26171722582814,
    "ttft": 1791416.4061099482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26044808494858446,
    "arrivals": 376204,
    "finished_requests": 96842,
    "scheduler_time": 197.47932209621706
}
#Debug simulation 
Total elapsed time: 13.42561067827046. Arrivals time: 0.36790823796764016 Scheduler time: 12.896438501309603 Scheduler overhead time: 0.06157886004075408 Adapter cache time: 0.010012710932642221 Engine time: 0.06131533859297633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 14.125395132694393,
    "estimated_duration": 3600.004336824615,
    "input_throughput": 6826.651220558597,
    "output_throughput": 5982.113904616991,
    "total_throughput": 12808.765125175587,
    "itl": 97.58843342966532,
    "ttft": 1769578.1753672585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2400958845997229,
    "arrivals": 376204,
    "finished_requests": 99561,
    "scheduler_time": 192.54384818458905
}
#Debug simulation 
Total elapsed time: 14.125528519041836. Arrivals time: 0.4379477580077946 Scheduler time: 13.533149970695376 Scheduler overhead time: 0.05923755234107375 Adapter cache time: 0.009565757121890783 Engine time: 0.05871914653107524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 13.428622398991138,
    "estimated_duration": 3600.001239456617,
    "input_throughput": 6639.250214149275,
    "output_throughput": 5816.048275350134,
    "total_throughput": 12455.298489499408,
    "itl": 91.26220218365066,
    "ttft": 1791545.427799328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25837691318243744,
    "arrivals": 376204,
    "finished_requests": 96842,
    "scheduler_time": 197.47521876276244
}
#Debug simulation 
Total elapsed time: 13.428737991955131. Arrivals time: 0.3765276144258678 Scheduler time: 12.890988671220839 Scheduler overhead time: 0.061901723966002464 Adapter cache time: 0.010007116012275219 Engine time: 0.061111326329410076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 14.110010833945125,
    "estimated_duration": 3600.0823319411697,
    "input_throughput": 6826.65720779455,
    "output_throughput": 5982.179576539733,
    "total_throughput": 12808.836784334284,
    "itl": 97.58672263375261,
    "ttft": 1769548.3118293323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22343723291996861,
    "arrivals": 376204,
    "finished_requests": 99564,
    "scheduler_time": 192.54895353607967
}
#Debug simulation 
Total elapsed time: 14.11015847325325. Arrivals time: 0.40760382264852524 Scheduler time: 13.548450717236847 Scheduler overhead time: 0.05873649334535003 Adapter cache time: 0.009567126166075468 Engine time: 0.05901402374729514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.507688845973462,
    "estimated_duration": 3600.0670717716725,
    "input_throughput": 6639.391301184056,
    "output_throughput": 5816.534409647819,
    "total_throughput": 12455.925710831876,
    "itl": 91.25992346252728,
    "ttft": 1791412.6856394697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.255891507063061,
    "arrivals": 376204,
    "finished_requests": 96846,
    "scheduler_time": 197.48126144544167
}
#Debug simulation 
Total elapsed time: 13.50778760528192. Arrivals time: 0.38538603903725743 Scheduler time: 12.960077506955713 Scheduler overhead time: 0.061986024491488934 Adapter cache time: 0.010142092127352953 Engine time: 0.06181261548772454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 13.897073765750974,
    "estimated_duration": 3600.1003520896847,
    "input_throughput": 6919.064627057225,
    "output_throughput": 6042.266290542031,
    "total_throughput": 12961.330917599256,
    "itl": 100.13491658838953,
    "ttft": 1758755.0366306575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21820949805434783,
    "arrivals": 375496,
    "finished_requests": 100732,
    "scheduler_time": 190.75634014750918
}
#Debug simulation 
Total elapsed time: 13.897198763675988. Arrivals time: 0.44431396620348096 Scheduler time: 13.302381192333996 Scheduler overhead time: 0.05748619604855776 Adapter cache time: 0.009415804408490658 Engine time: 0.057346158660948277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.92597559094429,
    "estimated_duration": 3600.086304966984,
    "input_throughput": 6843.979536270023,
    "output_throughput": 5976.912822982264,
    "total_throughput": 12820.892359252286,
    "itl": 97.47303078416681,
    "ttft": 1766162.7890600022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23843381050508472,
    "arrivals": 375496,
    "finished_requests": 99636,
    "scheduler_time": 192.6537289613757
}
#Debug simulation 
Total elapsed time: 13.926163435913622. Arrivals time: 0.4341280860826373 Scheduler time: 13.338224617764354 Scheduler overhead time: 0.058769856579601765 Adapter cache time: 0.009620464406907558 Engine time: 0.05861785588786006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 12.961698403116316,
    "estimated_duration": 3600.043351710238,
    "input_throughput": 6654.558475974774,
    "output_throughput": 5812.473894254436,
    "total_throughput": 12467.032370229212,
    "itl": 91.12981109240425,
    "ttft": 1788129.6566643338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24429593820124865,
    "arrivals": 375496,
    "finished_requests": 96887,
    "scheduler_time": 197.62178274578204
}
#Debug simulation 
Total elapsed time: 12.96179897012189. Arrivals time: 0.35684183333069086 Scheduler time: 12.443391410633922 Scheduler overhead time: 0.06149291759356856 Adapter cache time: 0.010092622600495815 Engine time: 0.06176426773890853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 13.568730837199837,
    "estimated_duration": 3600.048208768983,
    "input_throughput": 6844.134181310102,
    "output_throughput": 5977.1188473495295,
    "total_throughput": 12821.25302865963,
    "itl": 97.47054546976794,
    "ttft": 1766165.5264689995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22455160077195607,
    "arrivals": 375496,
    "finished_requests": 99637,
    "scheduler_time": 192.65311866949455
}
#Debug simulation 
Total elapsed time: 13.568873504176736. Arrivals time: 0.44172439351677895 Scheduler time: 12.973192404955626 Scheduler overhead time: 0.05888081481680274 Adapter cache time: 0.009579912293702364 Engine time: 0.05852667614817619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 13.005059044342488,
    "estimated_duration": 3600.0695499113376,
    "input_throughput": 6654.736989897882,
    "output_throughput": 5812.46937313068,
    "total_throughput": 12467.206363028563,
    "itl": 91.12792390775468,
    "ttft": 1788113.8697740473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24222476643510163,
    "arrivals": 375496,
    "finished_requests": 96890,
    "scheduler_time": 197.62463460354027
}
#Debug simulation 
Total elapsed time: 13.00519809499383. Arrivals time: 0.4376964964903891 Scheduler time: 12.405216962564737 Scheduler overhead time: 0.062208457849919796 Adapter cache time: 0.01007414422929287 Engine time: 0.061665262561291456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 13.625076140742749,
    "estimated_duration": 3600.031002125771,
    "input_throughput": 6843.905784547806,
    "output_throughput": 5976.684641686401,
    "total_throughput": 12820.590426234208,
    "itl": 97.46739150644287,
    "ttft": 1766213.6772735233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21066939103882756,
    "arrivals": 375496,
    "finished_requests": 99633,
    "scheduler_time": 192.65406745031183
}
#Debug simulation 
Total elapsed time: 13.625259794760495. Arrivals time: 0.45481505431234837 Scheduler time: 13.01579860271886 Scheduler overhead time: 0.058967513497918844 Adapter cache time: 0.009649313986301422 Engine time: 0.05907119391486049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.091646535787731,
    "estimated_duration": 3600.038759390531,
    "input_throughput": 6654.741407299697,
    "output_throughput": 5812.5954742616705,
    "total_throughput": 12467.336881561369,
    "itl": 91.12821102391077,
    "ttft": 1788045.6323637427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2401535946689546,
    "arrivals": 375496,
    "finished_requests": 96889,
    "scheduler_time": 197.62377190626432
}
#Debug simulation 
Total elapsed time: 13.091792145743966. Arrivals time: 0.4581590839661658 Scheduler time: 12.47113951202482 Scheduler overhead time: 0.06191513920202851 Adapter cache time: 0.010141827166080475 Engine time: 0.06186355836689472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 13.449919928330928,
    "estimated_duration": 3600.063146235359,
    "input_throughput": 6942.195174030454,
    "output_throughput": 6037.232714300585,
    "total_throughput": 12979.427888331038,
    "itl": 100.00568545107956,
    "ttft": 1758948.362523615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23804672515019762,
    "arrivals": 375142,
    "finished_requests": 100812,
    "scheduler_time": 190.75146419405652
}
#Debug simulation 
Total elapsed time: 13.450147834140807. Arrivals time: 0.4680509399622679 Scheduler time: 12.831823927816004 Scheduler overhead time: 0.05730578629299998 Adapter cache time: 0.009396648034453392 Engine time: 0.05747050466015935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 13.197663105092943,
    "estimated_duration": 3600.026281129725,
    "input_throughput": 6869.083464646215,
    "output_throughput": 5973.426114337052,
    "total_throughput": 12842.509578983267,
    "itl": 97.30168194073333,
    "ttft": 1766846.535851048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26036201527342195,
    "arrivals": 375142,
    "finished_requests": 99730,
    "scheduler_time": 192.68671095193764
}
#Debug simulation 
Total elapsed time: 13.197782229166478. Arrivals time: 0.41766760498285294 Scheduler time: 12.626830322667956 Scheduler overhead time: 0.05875924928113818 Adapter cache time: 0.00954322749748826 Engine time: 0.05833369866013527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 12.644854601938277,
    "estimated_duration": 3600.0476987073134,
    "input_throughput": 6678.96706164027,
    "output_throughput": 5809.069698579079,
    "total_throughput": 12488.03676021935,
    "itl": 90.96185437630734,
    "ttft": 1787914.4888136794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26681535192765304,
    "arrivals": 375142,
    "finished_requests": 96943,
    "scheduler_time": 197.68402807828767
}
#Debug simulation 
Total elapsed time: 12.644979159347713. Arrivals time: 0.4375453321263194 Scheduler time: 12.046035180334002 Scheduler overhead time: 0.06158863613381982 Adapter cache time: 0.010004549752920866 Engine time: 0.06152805173769593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 13.167250455822796,
    "estimated_duration": 3600.029680890994,
    "input_throughput": 6869.074755483598,
    "output_throughput": 5973.372973602196,
    "total_throughput": 12842.447729085794,
    "itl": 97.30213441716036,
    "ttft": 1766803.8853238237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24509158456698055,
    "arrivals": 375142,
    "finished_requests": 99730,
    "scheduler_time": 192.68505056125426
}
#Debug simulation 
Total elapsed time: 13.167397325858474. Arrivals time: 0.3999424744397402 Scheduler time: 12.61447400180623 Scheduler overhead time: 0.05827074218541384 Adapter cache time: 0.009497948922216892 Engine time: 0.058625053614377975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 12.695751796010882,
    "estimated_duration": 3600.0821643468003,
    "input_throughput": 6678.911175451648,
    "output_throughput": 5809.01464058514,
    "total_throughput": 12487.925816036788,
    "itl": 90.96058872374434,
    "ttft": 1787993.1932272888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2645370629848913,
    "arrivals": 375142,
    "finished_requests": 96944,
    "scheduler_time": 197.6868481800847
}
#Debug simulation 
Total elapsed time: 12.695850883144885. Arrivals time: 0.44059641379863024 Scheduler time: 12.09337501367554 Scheduler overhead time: 0.06155332084745169 Adapter cache time: 0.010050852783024311 Engine time: 0.06205796869471669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 13.289028063882142,
    "estimated_duration": 3600.090946695272,
    "input_throughput": 6869.037301043825,
    "output_throughput": 5973.397427567893,
    "total_throughput": 12842.434728611717,
    "itl": 97.29968939575198,
    "ttft": 1766835.3207966583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22982115386053914,
    "arrivals": 375142,
    "finished_requests": 99731,
    "scheduler_time": 192.6914320104929
}
#Debug simulation 
Total elapsed time: 13.289178454782814. Arrivals time: 0.4693480562418699 Scheduler time: 12.665617757476866 Scheduler overhead time: 0.058835194911807775 Adapter cache time: 0.009600600693374872 Engine time: 0.058854900766164064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_96_slots_32_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 12.70905159600079,
    "estimated_duration": 3600.0536497753674,
    "input_throughput": 6678.964076410448,
    "output_throughput": 5809.06065144471,
    "total_throughput": 12488.024727855158,
    "itl": 90.96496028033188,
    "ttft": 1787960.5546814527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26225877404212955,
    "arrivals": 375142,
    "finished_requests": 96944,
    "scheduler_time": 197.68149455573953
}
#Debug simulation 
Total elapsed time: 12.709233324043453. Arrivals time: 0.44562927493825555 Scheduler time: 12.101840667426586 Scheduler overhead time: 0.06166944373399019 Adapter cache time: 0.010122112929821014 Engine time: 0.06170123349875212 
