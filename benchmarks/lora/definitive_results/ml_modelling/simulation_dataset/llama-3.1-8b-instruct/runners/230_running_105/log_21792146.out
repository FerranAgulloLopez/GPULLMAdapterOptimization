INFO 05-31 19:31:06 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 135, 17280, 17280, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 17280, 540, 17280, 17280, 135, 17280, 540, 540, 17280, 17280, 540, 135, 135, 540, 17280, 17280, 540, 540, 135, 135, 540, 17280, 17280, 135, 17280, 540, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 540, 135, 17280, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 17280, 540, 135, 135, 540, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 135, 17280, 540, 17280, 17280, 540, 135, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 135, 540, 17280, 17280, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 540, 135, 17280, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 540, 135, 540, 17280, 540, 135, 135, 540, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 540, 17280, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 135, 135, 17280, 540, 17280, 540, 17280, 540, 135, 540, 135, 135, 135, 17280, 17280, 540, 540, 135, 135, 540, 135, 17280, 135, 135, 540, 135, 17280, 540, 135, 17280, 135, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 540, 135, 540, 135, 540, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 17280, 135, 17280, 135, 135, 135, 17280, 540, 17280, 17280, 135, 540, 540, 540, 135, 540, 135, 540, 540, 17280, 135, 135, 540, 135, 540, 540, 135, 17280, 17280, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 17280, 135, 135, 17280, 540, 17280, 540, 540, 540, 540, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1921050 . Total input tokens: 428625514 . Total output tokens: 376917001
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 91.16202727891505,
    "estimated_duration": 3600.0046790839065,
    "input_throughput": 6906.575467653855,
    "output_throughput": 6023.28939347876,
    "total_throughput": 12929.864861132615,
    "itl": 85.47097729320156,
    "ttft": 1935046.2752030643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 475,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.03236244677099,
    "arrivals": 640132,
    "finished_requests": 100673,
    "scheduler_time": 313.97845094175113
}
#Debug simulation 
Total elapsed time: 91.16224713623524. Arrivals time: 0.6310783266089857 Scheduler time: 90.29232394415885 Scheduler overhead time: 0.09204935329034925 Adapter cache time: 0.01973804645240307 Engine time: 0.09167227381840348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 135, 17280, 17280, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 17280, 540, 17280, 17280, 135, 17280, 540, 540, 17280, 17280, 540, 135, 135, 540, 17280, 17280, 540, 540, 135, 135, 540, 17280, 17280, 135, 17280, 540, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 540, 135, 17280, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 17280, 540, 135, 135, 540, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 135, 17280, 540, 17280, 17280, 540, 135, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 135, 135, 540, 17280, 17280, 540, 135, 540, 135, 540, 17280, 17280, 540, 135, 540, 135, 17280, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 540, 135, 540, 17280, 540, 135, 135, 540, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 540, 17280, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 135, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 135, 135, 17280, 540, 17280, 540, 17280, 540, 135, 540, 135, 135, 135, 17280, 17280, 540, 540, 135, 135, 540, 135, 17280, 135, 135, 540, 135, 17280, 540, 135, 17280, 135, 135, 17280, 540, 17280, 540, 135, 540, 17280, 540, 540, 17280, 540, 135, 540, 135, 17280, 540, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 540, 135, 540, 135, 540, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 540, 540, 17280, 135, 17280, 135, 135, 135, 17280, 540, 17280, 17280, 135, 540, 540, 540, 135, 540, 135, 540, 540, 17280, 135, 135, 540, 135, 540, 540, 135, 17280, 17280, 17280, 135, 540, 540, 540, 540, 135, 540, 135, 17280, 135, 135, 17280, 540, 17280, 540, 540, 540, 540, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 540, 17280, 540, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1921050 . Total input tokens: 428625514 . Total output tokens: 376917001
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.13769723661244,
    "estimated_duration": 3600.0406510562784,
    "input_throughput": 6918.504932074067,
    "output_throughput": 6036.396837247903,
    "total_throughput": 12954.90176932197,
    "itl": 84.18170569973755,
    "ttft": 1930089.8254193377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.615518261678547,
    "arrivals": 640132,
    "finished_requests": 100907,
    "scheduler_time": 313.0455270449166
}
#Debug simulation 
Total elapsed time: 89.13791168481112. Arrivals time: 0.6196640753187239 Scheduler time: 88.27835101215169 Scheduler overhead time: 0.09300726512447 Adapter cache time: 0.019614662509411573 Engine time: 0.09174135979264975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 66, 17280, 17280, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 17280, 540, 17280, 17280, 66, 17280, 540, 540, 17280, 17280, 540, 66, 66, 540, 17280, 17280, 540, 540, 66, 66, 540, 17280, 17280, 66, 17280, 540, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 540, 66, 17280, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 17280, 540, 66, 66, 540, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 66, 17280, 540, 17280, 17280, 540, 66, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 66, 540, 17280, 17280, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 540, 66, 17280, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 540, 66, 540, 17280, 540, 66, 66, 540, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 540, 17280, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 66, 66, 17280, 540, 17280, 540, 17280, 540, 66, 540, 66, 66, 66, 17280, 17280, 540, 540, 66, 66, 540, 66, 17280, 66, 66, 540, 66, 17280, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 540, 66, 540, 66, 540, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 17280, 66, 17280, 66, 66, 66, 17280, 540, 17280, 17280, 66, 540, 540, 540, 66, 540, 66, 540, 540, 17280, 66, 66, 540, 66, 540, 540, 66, 17280, 17280, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 540, 540, 540, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1913736 . Total input tokens: 426980670 . Total output tokens: 375473561
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.52908726967871,
    "estimated_duration": 3600.023394154367,
    "input_throughput": 7023.129361063219,
    "output_throughput": 6170.665734025909,
    "total_throughput": 13193.795095089128,
    "itl": 87.34552402346797,
    "ttft": 1936080.350709138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.213630789527706,
    "arrivals": 637737,
    "finished_requests": 102523,
    "scheduler_time": 305.88580928665795
}
#Debug simulation 
Total elapsed time: 90.52926660794765. Arrivals time: 0.6349243177101016 Scheduler time: 89.65780549123883 Scheduler overhead time: 0.09141328558325768 Adapter cache time: 0.019836158026009798 Engine time: 0.09001666586846113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 66, 17280, 17280, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 17280, 540, 17280, 17280, 66, 17280, 540, 540, 17280, 17280, 540, 66, 66, 540, 17280, 17280, 540, 540, 66, 66, 540, 17280, 17280, 66, 17280, 540, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 540, 66, 17280, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 17280, 540, 66, 66, 540, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 66, 17280, 540, 17280, 17280, 540, 66, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 66, 540, 17280, 17280, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 540, 66, 17280, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 540, 66, 540, 17280, 540, 66, 66, 540, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 540, 17280, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 66, 66, 17280, 540, 17280, 540, 17280, 540, 66, 540, 66, 66, 66, 17280, 17280, 540, 540, 66, 66, 540, 66, 17280, 66, 66, 540, 66, 17280, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 540, 66, 540, 66, 540, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 17280, 66, 17280, 66, 66, 66, 17280, 540, 17280, 17280, 66, 540, 540, 540, 66, 540, 66, 540, 540, 17280, 66, 66, 540, 66, 540, 540, 66, 17280, 17280, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 540, 540, 540, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1913736 . Total input tokens: 426980670 . Total output tokens: 375473561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.25352507410571,
    "estimated_duration": 3600.0388353918684,
    "input_throughput": 6969.6012035572485,
    "output_throughput": 6123.485886673886,
    "total_throughput": 13093.087090231134,
    "itl": 86.27858209085598,
    "ttft": 1921820.6956502157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7963558668689856,
    "arrivals": 637737,
    "finished_requests": 101733,
    "scheduler_time": 308.1729650223226
}
#Debug simulation 
Total elapsed time: 90.25370442308486. Arrivals time: 0.6124447220936418 Scheduler time: 89.40276243537664 Scheduler overhead time: 0.09173657326027751 Adapter cache time: 0.020038777962327003 Engine time: 0.09207570971921086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 66, 17280, 17280, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 17280, 540, 17280, 17280, 66, 17280, 540, 540, 17280, 17280, 540, 66, 66, 540, 17280, 17280, 540, 540, 66, 66, 540, 17280, 17280, 66, 17280, 540, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 540, 66, 17280, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 17280, 540, 66, 66, 540, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 66, 17280, 540, 17280, 17280, 540, 66, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 66, 540, 17280, 17280, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 540, 66, 17280, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 540, 66, 540, 17280, 540, 66, 66, 540, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 540, 17280, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 66, 66, 17280, 540, 17280, 540, 17280, 540, 66, 540, 66, 66, 66, 17280, 17280, 540, 540, 66, 66, 540, 66, 17280, 66, 66, 540, 66, 17280, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 540, 66, 540, 66, 540, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 17280, 66, 17280, 66, 66, 66, 17280, 540, 17280, 17280, 66, 540, 540, 540, 66, 540, 66, 540, 540, 17280, 66, 66, 540, 66, 540, 540, 66, 17280, 17280, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 540, 540, 540, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1913736 . Total input tokens: 426980670 . Total output tokens: 375473561
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.30120023991913,
    "estimated_duration": 3600.08864308362,
    "input_throughput": 6989.226514836002,
    "output_throughput": 6111.773953752875,
    "total_throughput": 13101.000468588878,
    "itl": 84.77360625051271,
    "ttft": 1939113.1665228647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7480075583514134,
    "arrivals": 637737,
    "finished_requests": 101962,
    "scheduler_time": 307.36669599209114
}
#Debug simulation 
Total elapsed time: 90.30138536868617. Arrivals time: 0.6373324142768979 Scheduler time: 89.42405878053978 Scheduler overhead time: 0.09292583027854562 Adapter cache time: 0.019965861458331347 Engine time: 0.09197243861854076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 66, 17280, 17280, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 17280, 540, 17280, 17280, 66, 17280, 540, 540, 17280, 17280, 540, 66, 66, 540, 17280, 17280, 540, 540, 66, 66, 540, 17280, 17280, 66, 17280, 540, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 540, 66, 17280, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 17280, 540, 66, 66, 540, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 66, 17280, 540, 17280, 17280, 540, 66, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 66, 540, 17280, 17280, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 540, 66, 17280, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 540, 66, 540, 17280, 540, 66, 66, 540, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 540, 17280, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 66, 66, 17280, 540, 17280, 540, 17280, 540, 66, 540, 66, 66, 66, 17280, 17280, 540, 540, 66, 66, 540, 66, 17280, 66, 66, 540, 66, 17280, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 540, 66, 540, 66, 540, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 17280, 66, 17280, 66, 66, 66, 17280, 540, 17280, 17280, 66, 540, 540, 540, 66, 540, 66, 540, 540, 17280, 66, 66, 540, 66, 540, 540, 66, 17280, 17280, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 540, 540, 540, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1913736 . Total input tokens: 426980670 . Total output tokens: 375473561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 92.50716726807877,
    "estimated_duration": 3600.068264492899,
    "input_throughput": 7039.269296625302,
    "output_throughput": 6161.870656395648,
    "total_throughput": 13201.139953020951,
    "itl": 86.36084187668747,
    "ttft": 1932704.3662743208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2283849348127793,
    "arrivals": 637737,
    "finished_requests": 102766,
    "scheduler_time": 304.92857831851745
}
#Debug simulation 
Total elapsed time: 92.50734305102378. Arrivals time: 0.6378082213923335 Scheduler time: 91.63480615662411 Scheduler overhead time: 0.09080558409914374 Adapter cache time: 0.019727748818695545 Engine time: 0.09001625655218959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 66, 17280, 17280, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 17280, 540, 17280, 17280, 66, 17280, 540, 540, 17280, 17280, 540, 66, 66, 540, 17280, 17280, 540, 540, 66, 66, 540, 17280, 17280, 66, 17280, 540, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 540, 66, 17280, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 17280, 540, 66, 66, 540, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 66, 17280, 540, 17280, 17280, 540, 66, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 66, 540, 17280, 17280, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 540, 66, 17280, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 540, 66, 540, 17280, 540, 66, 66, 540, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 540, 17280, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 66, 66, 17280, 540, 17280, 540, 17280, 540, 66, 540, 66, 66, 66, 17280, 17280, 540, 540, 66, 66, 540, 66, 17280, 66, 66, 540, 66, 17280, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 540, 66, 540, 66, 540, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 17280, 66, 17280, 66, 66, 66, 17280, 540, 17280, 17280, 66, 540, 540, 540, 66, 540, 66, 540, 540, 17280, 66, 66, 540, 66, 540, 540, 66, 17280, 17280, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 540, 540, 540, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1913736 . Total input tokens: 426980670 . Total output tokens: 375473561
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 89.97102563688532,
    "estimated_duration": 3600.0533301488304,
    "input_throughput": 6989.2950721815505,
    "output_throughput": 6111.833904163407,
    "total_throughput": 13101.128976344957,
    "itl": 84.77291318049227,
    "ttft": 1939098.729162036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7134189898567582,
    "arrivals": 637737,
    "finished_requests": 101962,
    "scheduler_time": 307.36627501946117
}
#Debug simulation 
Total elapsed time: 89.9712067199871. Arrivals time: 0.6418118001893163 Scheduler time: 89.08893322618678 Scheduler overhead time: 0.09286125842481852 Adapter cache time: 0.02004616940394044 Engine time: 0.09229157911613584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 66, 17280, 17280, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 17280, 540, 17280, 17280, 66, 17280, 540, 540, 17280, 17280, 540, 66, 66, 540, 17280, 17280, 540, 540, 66, 66, 540, 17280, 17280, 66, 17280, 540, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 540, 66, 17280, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 17280, 540, 66, 66, 540, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 66, 17280, 540, 17280, 17280, 540, 66, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 66, 540, 17280, 17280, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 540, 66, 17280, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 540, 66, 540, 17280, 540, 66, 66, 540, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 540, 17280, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 66, 66, 17280, 540, 17280, 540, 17280, 540, 66, 540, 66, 66, 66, 17280, 17280, 540, 540, 66, 66, 540, 66, 17280, 66, 66, 540, 66, 17280, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 540, 66, 540, 66, 540, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 17280, 66, 17280, 66, 66, 66, 17280, 540, 17280, 17280, 66, 540, 540, 540, 66, 540, 66, 540, 540, 17280, 66, 66, 540, 66, 540, 540, 66, 17280, 17280, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 540, 540, 540, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1913736 . Total input tokens: 426980670 . Total output tokens: 375473561
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 91.43134718900546,
    "estimated_duration": 3600.0475564494504,
    "input_throughput": 6949.550695567678,
    "output_throughput": 6095.750585484839,
    "total_throughput": 13045.301281052518,
    "itl": 85.78823017261831,
    "ttft": 1929197.4042157996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.108969498057836,
    "arrivals": 637737,
    "finished_requests": 101358,
    "scheduler_time": 309.82150100026627
}
#Debug simulation 
Total elapsed time: 91.43153289798647. Arrivals time: 0.6845220932736993 Scheduler time: 90.50952989421785 Scheduler overhead time: 0.09193280106410384 Adapter cache time: 0.01978242676705122 Engine time: 0.09107074933126569 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_320_slots_16_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 66, 17280, 17280, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 17280, 540, 17280, 17280, 66, 17280, 540, 540, 17280, 17280, 540, 66, 66, 540, 17280, 17280, 540, 540, 66, 66, 540, 17280, 17280, 66, 17280, 540, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 540, 66, 17280, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 17280, 540, 66, 66, 540, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 66, 17280, 540, 17280, 17280, 540, 66, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 66, 66, 540, 17280, 17280, 540, 66, 540, 66, 540, 17280, 17280, 540, 66, 540, 66, 17280, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 540, 66, 540, 17280, 540, 66, 66, 540, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 540, 17280, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 66, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 66, 66, 17280, 540, 17280, 540, 17280, 540, 66, 540, 66, 66, 66, 17280, 17280, 540, 540, 66, 66, 540, 66, 17280, 66, 66, 540, 66, 17280, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 66, 540, 17280, 540, 540, 17280, 540, 66, 540, 66, 17280, 540, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 540, 66, 540, 66, 540, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 540, 540, 17280, 66, 17280, 66, 66, 66, 17280, 540, 17280, 17280, 66, 540, 540, 540, 66, 540, 66, 540, 540, 17280, 66, 66, 540, 66, 540, 540, 66, 17280, 17280, 17280, 66, 540, 540, 540, 540, 66, 540, 66, 17280, 66, 66, 17280, 540, 17280, 540, 540, 540, 540, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 540, 17280, 540, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1913736 . Total input tokens: 426980670 . Total output tokens: 375473561
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.3460364593193,
    "estimated_duration": 3600.018398281416,
    "input_throughput": 6989.362891037392,
    "output_throughput": 6111.893208796878,
    "total_throughput": 13101.25609983427,
    "itl": 84.77211449922264,
    "ttft": 1939085.0349134787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6788304213621026,
    "arrivals": 637737,
    "finished_requests": 101962,
    "scheduler_time": 307.3660450473959
}
#Debug simulation 
Total elapsed time: 90.34629661031067. Arrivals time: 0.6203855080530047 Scheduler time: 89.48539491929114 Scheduler overhead time: 0.09351646574214101 Adapter cache time: 0.020191060844808817 Engine time: 0.09115267219021916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 33, 17280, 17280, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 17280, 540, 17280, 17280, 33, 17280, 540, 540, 17280, 17280, 540, 33, 33, 540, 17280, 17280, 540, 540, 33, 33, 540, 17280, 17280, 33, 17280, 540, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 540, 33, 17280, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 17280, 540, 33, 33, 540, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 33, 17280, 540, 17280, 17280, 540, 33, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 33, 540, 17280, 17280, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 540, 33, 17280, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 540, 33, 540, 17280, 540, 33, 33, 540, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 540, 17280, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 33, 33, 17280, 540, 17280, 540, 17280, 540, 33, 540, 33, 33, 33, 17280, 17280, 540, 540, 33, 33, 540, 33, 17280, 33, 33, 540, 33, 17280, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 540, 33, 540, 33, 540, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 17280, 33, 17280, 33, 33, 33, 17280, 540, 17280, 17280, 33, 540, 540, 540, 33, 540, 33, 540, 540, 17280, 33, 33, 540, 33, 540, 540, 33, 17280, 17280, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 540, 540, 540, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1910238 . Total input tokens: 426193471 . Total output tokens: 374786363
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 94.60344323236495,
    "estimated_duration": 3600.0445042229576,
    "input_throughput": 7108.649343079086,
    "output_throughput": 6161.344942814152,
    "total_throughput": 13269.994285893237,
    "itl": 87.77520363732309,
    "ttft": 1922137.2036902711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.816886247610697,
    "arrivals": 636613,
    "finished_requests": 103392,
    "scheduler_time": 304.6946925796078
}
#Debug simulation 
Total elapsed time: 94.60362134315073. Arrivals time: 0.6503696646541357 Scheduler time: 93.71478288713843 Scheduler overhead time: 0.09249368403106928 Adapter cache time: 0.018944636918604374 Engine time: 0.0915017519146204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 33, 17280, 17280, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 17280, 540, 17280, 17280, 33, 17280, 540, 540, 17280, 17280, 540, 33, 33, 540, 17280, 17280, 540, 540, 33, 33, 540, 17280, 17280, 33, 17280, 540, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 540, 33, 17280, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 17280, 540, 33, 33, 540, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 33, 17280, 540, 17280, 17280, 540, 33, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 33, 540, 17280, 17280, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 540, 33, 17280, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 540, 33, 540, 17280, 540, 33, 33, 540, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 540, 17280, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 33, 33, 17280, 540, 17280, 540, 17280, 540, 33, 540, 33, 33, 33, 17280, 17280, 540, 540, 33, 33, 540, 33, 17280, 33, 33, 540, 33, 17280, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 540, 33, 540, 33, 540, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 17280, 33, 17280, 33, 33, 33, 17280, 540, 17280, 17280, 33, 540, 540, 540, 33, 540, 33, 540, 540, 17280, 33, 33, 540, 33, 540, 540, 33, 17280, 17280, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 540, 540, 540, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1910238 . Total input tokens: 426193471 . Total output tokens: 374786363
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 93.5030927369371,
    "estimated_duration": 3600.002106052748,
    "input_throughput": 7037.716993943552,
    "output_throughput": 6120.159752950204,
    "total_throughput": 13157.876746893757,
    "itl": 85.61378957529179,
    "ttft": 1923386.9211115951,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.204568191245201,
    "arrivals": 636613,
    "finished_requests": 102430,
    "scheduler_time": 308.2121202209945
}
#Debug simulation 
Total elapsed time: 93.50327724777162. Arrivals time: 0.6421959386207163 Scheduler time: 92.62105702795088 Scheduler overhead time: 0.09319194359704852 Adapter cache time: 0.019665540661662817 Engine time: 0.09147483948618174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 33, 17280, 17280, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 17280, 540, 17280, 17280, 33, 17280, 540, 540, 17280, 17280, 540, 33, 33, 540, 17280, 17280, 540, 540, 33, 33, 540, 17280, 17280, 33, 17280, 540, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 540, 33, 17280, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 17280, 540, 33, 33, 540, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 33, 17280, 540, 17280, 17280, 540, 33, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 33, 540, 17280, 17280, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 540, 33, 17280, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 540, 33, 540, 17280, 540, 33, 33, 540, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 540, 17280, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 33, 33, 17280, 540, 17280, 540, 17280, 540, 33, 540, 33, 33, 33, 17280, 17280, 540, 540, 33, 33, 540, 33, 17280, 33, 33, 540, 33, 17280, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 540, 33, 540, 33, 540, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 17280, 33, 17280, 33, 33, 33, 17280, 540, 17280, 17280, 33, 540, 540, 540, 33, 540, 33, 540, 540, 17280, 33, 33, 540, 33, 540, 540, 33, 17280, 17280, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 540, 540, 540, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1910238 . Total input tokens: 426193471 . Total output tokens: 374786363
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.79916682466865,
    "estimated_duration": 3600.0696691569233,
    "input_throughput": 6970.659266679359,
    "output_throughput": 6047.505187614469,
    "total_throughput": 13018.164454293828,
    "itl": 84.49613407242371,
    "ttft": 1933102.8561276936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2584797647502515,
    "arrivals": 636613,
    "finished_requests": 101486,
    "scheduler_time": 310.60114161098613
}
#Debug simulation 
Total elapsed time: 88.79933713190258. Arrivals time: 0.631925311870873 Scheduler time: 87.92688849754632 Scheduler overhead time: 0.09273153590038419 Adapter cache time: 0.0195716661401093 Engine time: 0.09248966258019209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 33, 17280, 17280, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 17280, 540, 17280, 17280, 33, 17280, 540, 540, 17280, 17280, 540, 33, 33, 540, 17280, 17280, 540, 540, 33, 33, 540, 17280, 17280, 33, 17280, 540, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 540, 33, 17280, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 17280, 540, 33, 33, 540, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 33, 17280, 540, 17280, 17280, 540, 33, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 33, 540, 17280, 17280, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 540, 33, 17280, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 540, 33, 540, 17280, 540, 33, 33, 540, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 540, 17280, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 33, 33, 17280, 540, 17280, 540, 17280, 540, 33, 540, 33, 33, 33, 17280, 17280, 540, 540, 33, 33, 540, 33, 17280, 33, 33, 540, 33, 17280, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 540, 33, 540, 33, 540, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 17280, 33, 17280, 33, 33, 33, 17280, 540, 17280, 17280, 33, 540, 540, 540, 33, 540, 33, 540, 540, 17280, 33, 33, 540, 33, 540, 540, 33, 17280, 17280, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 540, 540, 540, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1910238 . Total input tokens: 426193471 . Total output tokens: 374786363
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 93.62105856696144,
    "estimated_duration": 3600.0017098578005,
    "input_throughput": 7038.823601281267,
    "output_throughput": 6119.369315763512,
    "total_throughput": 13158.192917044778,
    "itl": 86.31567345226806,
    "ttft": 1919776.3454130145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0851622321736025,
    "arrivals": 636613,
    "finished_requests": 102366,
    "scheduler_time": 308.24391305374957
}
#Debug simulation 
Total elapsed time: 93.62123498506844. Arrivals time: 0.6371844164095819 Scheduler time: 92.74283625697717 Scheduler overhead time: 0.09345730487257242 Adapter cache time: 0.019305641762912273 Engine time: 0.09294641017913818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 33, 17280, 17280, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 17280, 540, 17280, 17280, 33, 17280, 540, 540, 17280, 17280, 540, 33, 33, 540, 17280, 17280, 540, 540, 33, 33, 540, 17280, 17280, 33, 17280, 540, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 540, 33, 17280, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 17280, 540, 33, 33, 540, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 33, 17280, 540, 17280, 17280, 540, 33, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 33, 540, 17280, 17280, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 540, 33, 17280, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 540, 33, 540, 17280, 540, 33, 33, 540, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 540, 17280, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 33, 33, 17280, 540, 17280, 540, 17280, 540, 33, 540, 33, 33, 33, 17280, 17280, 540, 540, 33, 33, 540, 33, 17280, 33, 33, 540, 33, 17280, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 540, 33, 540, 33, 540, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 17280, 33, 17280, 33, 33, 33, 17280, 540, 17280, 17280, 33, 540, 540, 540, 33, 540, 33, 540, 540, 17280, 33, 33, 540, 33, 540, 540, 33, 17280, 17280, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 540, 540, 540, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1910238 . Total input tokens: 426193471 . Total output tokens: 374786363
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 91.6790286200121,
    "estimated_duration": 3600.0300955059015,
    "input_throughput": 6980.597754271969,
    "output_throughput": 6044.430024950142,
    "total_throughput": 13025.027779222111,
    "itl": 84.51918400537845,
    "ttft": 1932260.2758754792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1723313501430903,
    "arrivals": 636613,
    "finished_requests": 101518,
    "scheduler_time": 310.573029478745
}
#Debug simulation 
Total elapsed time: 91.679203517735. Arrivals time: 0.6305255629122257 Scheduler time: 90.80606692377478 Scheduler overhead time: 0.09329871693626046 Adapter cache time: 0.019940616097301245 Engine time: 0.09288986213505268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 33, 17280, 17280, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 17280, 540, 17280, 17280, 33, 17280, 540, 540, 17280, 17280, 540, 33, 33, 540, 17280, 17280, 540, 540, 33, 33, 540, 17280, 17280, 33, 17280, 540, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 540, 33, 17280, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 17280, 540, 33, 33, 540, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 33, 17280, 540, 17280, 17280, 540, 33, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 33, 540, 17280, 17280, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 540, 33, 17280, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 540, 33, 540, 17280, 540, 33, 33, 540, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 540, 17280, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 33, 33, 17280, 540, 17280, 540, 17280, 540, 33, 540, 33, 33, 33, 17280, 17280, 540, 540, 33, 33, 540, 33, 17280, 33, 33, 540, 33, 17280, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 540, 33, 540, 33, 540, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 17280, 33, 17280, 33, 33, 33, 17280, 540, 17280, 17280, 33, 540, 540, 540, 33, 540, 33, 540, 540, 17280, 33, 33, 540, 33, 540, 540, 33, 17280, 17280, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 540, 540, 540, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1910238 . Total input tokens: 426193471 . Total output tokens: 374786363
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 91.81319819716737,
    "estimated_duration": 3600.0408577082044,
    "input_throughput": 7088.733436277145,
    "output_throughput": 6139.4800430321875,
    "total_throughput": 13228.213479309332,
    "itl": 86.46650827403418,
    "ttft": 1923929.1840709255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.802541292910452,
    "arrivals": 636613,
    "finished_requests": 103131,
    "scheduler_time": 305.72945324576045
}
#Debug simulation 
Total elapsed time: 91.8133783210069. Arrivals time: 0.6507009076885879 Scheduler time: 90.92474570963532 Scheduler overhead time: 0.0917957229539752 Adapter cache time: 0.01939292484894395 Engine time: 0.09199347300454974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [106 107 107]
Adapter prompts. [540, 17280, 17280, 33, 17280, 17280, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 17280, 540, 17280, 17280, 33, 17280, 540, 540, 17280, 17280, 540, 33, 33, 540, 17280, 17280, 540, 540, 33, 33, 540, 17280, 17280, 33, 17280, 540, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 540, 33, 17280, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 17280, 540, 33, 33, 540, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 33, 17280, 540, 17280, 17280, 540, 33, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 33, 33, 540, 17280, 17280, 540, 33, 540, 33, 540, 17280, 17280, 540, 33, 540, 33, 17280, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 540, 33, 540, 17280, 540, 33, 33, 540, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 540, 17280, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 33, 540, 17280, 17280, 17280, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 33, 33, 17280, 540, 17280, 540, 17280, 540, 33, 540, 33, 33, 33, 17280, 17280, 540, 540, 33, 33, 540, 33, 17280, 33, 33, 540, 33, 17280, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 33, 540, 17280, 540, 540, 17280, 540, 33, 540, 33, 17280, 540, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 540, 33, 540, 33, 540, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 540, 540, 17280, 33, 17280, 33, 33, 33, 17280, 540, 17280, 17280, 33, 540, 540, 540, 33, 540, 33, 540, 540, 17280, 33, 33, 540, 33, 540, 540, 33, 17280, 17280, 17280, 33, 540, 540, 540, 540, 33, 540, 33, 17280, 33, 33, 17280, 540, 17280, 540, 540, 540, 540, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 540, 17280, 540, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1910238 . Total input tokens: 426193471 . Total output tokens: 374786363
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.69584975298494,
    "estimated_duration": 3600.008904473508,
    "input_throughput": 7027.7059505495945,
    "output_throughput": 6091.869098642499,
    "total_throughput": 13119.575049192094,
    "itl": 84.64015322959676,
    "ttft": 1933717.171446267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.252464746292705,
    "arrivals": 636613,
    "finished_requests": 102283,
    "scheduler_time": 308.1839973648903
}
#Debug simulation 
Total elapsed time: 89.69602632429451. Arrivals time: 0.6433991212397814 Scheduler time: 88.80994418542832 Scheduler overhead time: 0.09243657998740673 Adapter cache time: 0.021834978833794594 Engine time: 0.09287647437304258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 135, 17280, 17280, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 17280, 270, 17280, 17280, 135, 17280, 270, 270, 17280, 17280, 270, 135, 135, 270, 17280, 17280, 270, 270, 135, 135, 270, 17280, 17280, 135, 17280, 270, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 270, 135, 17280, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 17280, 270, 135, 135, 270, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 135, 17280, 270, 17280, 17280, 270, 135, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 135, 270, 17280, 17280, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 270, 135, 17280, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 270, 135, 270, 17280, 270, 135, 135, 270, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 270, 17280, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 135, 135, 17280, 270, 17280, 270, 17280, 270, 135, 270, 135, 135, 135, 17280, 17280, 270, 270, 135, 135, 270, 135, 17280, 135, 135, 270, 135, 17280, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 270, 135, 270, 135, 270, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 17280, 135, 17280, 135, 135, 135, 17280, 270, 17280, 17280, 135, 270, 270, 270, 135, 270, 135, 270, 270, 17280, 135, 135, 270, 135, 270, 270, 135, 17280, 17280, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 270, 270, 270, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1892160 . Total input tokens: 422125854 . Total output tokens: 371209282
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 91.96345494594425,
    "estimated_duration": 3600.063021437735,
    "input_throughput": 7101.143465480345,
    "output_throughput": 6162.73905981224,
    "total_throughput": 13263.882525292585,
    "itl": 88.15865396395454,
    "ttft": 1915593.845044545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 532,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.517801604997413,
    "arrivals": 630527,
    "finished_requests": 103578,
    "scheduler_time": 304.6785165447079
}
#Debug simulation 
Total elapsed time: 91.96363493381068. Arrivals time: 0.6407663570716977 Scheduler time: 91.08183994004503 Scheduler overhead time: 0.09289922285825014 Adapter cache time: 0.020410462748259306 Engine time: 0.09064945671707392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 135, 17280, 17280, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 17280, 270, 17280, 17280, 135, 17280, 270, 270, 17280, 17280, 270, 135, 135, 270, 17280, 17280, 270, 270, 135, 135, 270, 17280, 17280, 135, 17280, 270, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 270, 135, 17280, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 17280, 270, 135, 135, 270, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 135, 17280, 270, 17280, 17280, 270, 135, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 135, 270, 17280, 17280, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 270, 135, 17280, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 270, 135, 270, 17280, 270, 135, 135, 270, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 270, 17280, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 135, 135, 17280, 270, 17280, 270, 17280, 270, 135, 270, 135, 135, 135, 17280, 17280, 270, 270, 135, 135, 270, 135, 17280, 135, 135, 270, 135, 17280, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 270, 135, 270, 135, 270, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 17280, 135, 17280, 135, 135, 135, 17280, 270, 17280, 17280, 135, 270, 270, 270, 135, 270, 135, 270, 270, 17280, 135, 135, 270, 135, 270, 270, 135, 17280, 17280, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 270, 270, 270, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1892160 . Total input tokens: 422125854 . Total output tokens: 371209282
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 94.1515134703368,
    "estimated_duration": 3600.078093076707,
    "input_throughput": 6958.0801728087035,
    "output_throughput": 6033.681614233021,
    "total_throughput": 12991.761787041723,
    "itl": 86.18042714535798,
    "ttft": 1936671.7771456162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 464,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3952642600983434,
    "arrivals": 630527,
    "finished_requests": 101557,
    "scheduler_time": 311.31329038287066
}
#Debug simulation 
Total elapsed time: 94.1516907592304. Arrivals time: 0.6410890952683985 Scheduler time: 93.26734836399555 Scheduler overhead time: 0.0937590361572802 Adapter cache time: 0.02006264217197895 Engine time: 0.0937365097925067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 135, 17280, 17280, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 17280, 270, 17280, 17280, 135, 17280, 270, 270, 17280, 17280, 270, 135, 135, 270, 17280, 17280, 270, 270, 135, 135, 270, 17280, 17280, 135, 17280, 270, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 270, 135, 17280, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 17280, 270, 135, 135, 270, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 135, 17280, 270, 17280, 17280, 270, 135, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 135, 270, 17280, 17280, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 270, 135, 17280, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 270, 135, 270, 17280, 270, 135, 135, 270, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 270, 17280, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 135, 135, 17280, 270, 17280, 270, 17280, 270, 135, 270, 135, 135, 135, 17280, 17280, 270, 270, 135, 135, 270, 135, 17280, 135, 135, 270, 135, 17280, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 270, 135, 270, 135, 270, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 17280, 135, 17280, 135, 135, 135, 17280, 270, 17280, 17280, 135, 270, 270, 270, 135, 270, 135, 270, 270, 17280, 135, 135, 270, 135, 270, 270, 135, 17280, 17280, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 270, 270, 270, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1892160 . Total input tokens: 422125854 . Total output tokens: 371209282
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 92.3378121140413,
    "estimated_duration": 3600.0684346911044,
    "input_throughput": 6897.757209476682,
    "output_throughput": 5999.448452666207,
    "total_throughput": 12897.20566214289,
    "itl": 83.94089421941118,
    "ttft": 1940421.880986553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3681271791924043,
    "arrivals": 630527,
    "finished_requests": 100677,
    "scheduler_time": 314.4867115224576
}
#Debug simulation 
Total elapsed time: 92.33799295406789. Arrivals time: 0.6503941463306546 Scheduler time: 91.44251241069287 Scheduler overhead time: 0.09479279397055507 Adapter cache time: 0.02008536458015442 Engine time: 0.09441543277353048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 135, 17280, 17280, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 17280, 270, 17280, 17280, 135, 17280, 270, 270, 17280, 17280, 270, 135, 135, 270, 17280, 17280, 270, 270, 135, 135, 270, 17280, 17280, 135, 17280, 270, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 270, 135, 17280, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 17280, 270, 135, 135, 270, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 135, 17280, 270, 17280, 17280, 270, 135, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 135, 270, 17280, 17280, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 270, 135, 17280, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 270, 135, 270, 17280, 270, 135, 135, 270, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 270, 17280, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 135, 135, 17280, 270, 17280, 270, 17280, 270, 135, 270, 135, 135, 135, 17280, 17280, 270, 270, 135, 135, 270, 135, 17280, 135, 135, 270, 135, 17280, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 270, 135, 270, 135, 270, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 17280, 135, 17280, 135, 135, 135, 17280, 270, 17280, 17280, 135, 270, 270, 270, 135, 270, 135, 270, 270, 17280, 135, 135, 270, 135, 270, 270, 135, 17280, 17280, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 270, 270, 270, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1892160 . Total input tokens: 422125854 . Total output tokens: 371209282
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 91.23422478977591,
    "estimated_duration": 3600.034716577247,
    "input_throughput": 7076.418425270308,
    "output_throughput": 6165.893872574744,
    "total_throughput": 13242.312297845052,
    "itl": 87.01930026598036,
    "ttft": 1917628.0218863443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.394650210072282,
    "arrivals": 630527,
    "finished_requests": 103262,
    "scheduler_time": 305.9002865795909
}
#Debug simulation 
Total elapsed time: 91.23440102487803. Arrivals time: 0.6448097540996969 Scheduler time: 90.35098973987624 Scheduler overhead time: 0.0921414508484304 Adapter cache time: 0.020044079050421715 Engine time: 0.09088017465546727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 135, 17280, 17280, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 17280, 270, 17280, 17280, 135, 17280, 270, 270, 17280, 17280, 270, 135, 135, 270, 17280, 17280, 270, 270, 135, 135, 270, 17280, 17280, 135, 17280, 270, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 270, 135, 17280, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 17280, 270, 135, 135, 270, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 135, 17280, 270, 17280, 17280, 270, 135, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 135, 270, 17280, 17280, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 270, 135, 17280, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 270, 135, 270, 17280, 270, 135, 135, 270, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 270, 17280, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 135, 135, 17280, 270, 17280, 270, 17280, 270, 135, 270, 135, 135, 135, 17280, 17280, 270, 270, 135, 135, 270, 135, 17280, 135, 135, 270, 135, 17280, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 270, 135, 270, 135, 270, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 17280, 135, 17280, 135, 135, 135, 17280, 270, 17280, 17280, 135, 270, 270, 270, 135, 270, 135, 270, 270, 17280, 135, 135, 270, 135, 270, 270, 135, 17280, 17280, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 270, 270, 270, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1892160 . Total input tokens: 422125854 . Total output tokens: 371209282
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 92.00508093368262,
    "estimated_duration": 3600.029271532399,
    "input_throughput": 6897.83224718886,
    "output_throughput": 5999.513718066618,
    "total_throughput": 12897.345965255477,
    "itl": 83.94012418166663,
    "ttft": 1940402.3893809281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.337888071406658,
    "arrivals": 630527,
    "finished_requests": 100677,
    "scheduler_time": 314.4860827678629
}
#Debug simulation 
Total elapsed time: 92.00525409867987. Arrivals time: 0.6326025519520044 Scheduler time: 91.12938526971266 Scheduler overhead time: 0.09418812114745378 Adapter cache time: 0.019912535790354013 Engine time: 0.09351448388770223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 135, 17280, 17280, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 17280, 270, 17280, 17280, 135, 17280, 270, 270, 17280, 17280, 270, 135, 135, 270, 17280, 17280, 270, 270, 135, 135, 270, 17280, 17280, 135, 17280, 270, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 270, 135, 17280, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 17280, 270, 135, 135, 270, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 135, 17280, 270, 17280, 17280, 270, 135, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 135, 270, 17280, 17280, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 270, 135, 17280, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 270, 135, 270, 17280, 270, 135, 135, 270, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 270, 17280, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 135, 135, 17280, 270, 17280, 270, 17280, 270, 135, 270, 135, 135, 135, 17280, 17280, 270, 270, 135, 135, 270, 135, 17280, 135, 135, 270, 135, 17280, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 270, 135, 270, 135, 270, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 17280, 135, 17280, 135, 135, 135, 17280, 270, 17280, 17280, 135, 270, 270, 270, 135, 270, 135, 270, 270, 17280, 135, 135, 270, 135, 270, 270, 135, 17280, 17280, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 270, 270, 270, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1892160 . Total input tokens: 422125854 . Total output tokens: 371209282
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 91.53698906768113,
    "estimated_duration": 3600.0139683008624,
    "input_throughput": 7006.066149211158,
    "output_throughput": 6092.062195622883,
    "total_throughput": 13098.12834483404,
    "itl": 86.74585986129675,
    "ttft": 1920709.838692053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2813353634532394,
    "arrivals": 630527,
    "finished_requests": 102180,
    "scheduler_time": 309.6416858291832
}
#Debug simulation 
Total elapsed time: 91.5372314429842. Arrivals time: 0.9525322653353214 Scheduler time: 90.34540720330551 Scheduler overhead time: 0.092265241779387 Adapter cache time: 0.02043590135872364 Engine time: 0.0912702833302319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 135, 17280, 17280, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 17280, 270, 17280, 17280, 135, 17280, 270, 270, 17280, 17280, 270, 135, 135, 270, 17280, 17280, 270, 270, 135, 135, 270, 17280, 17280, 135, 17280, 270, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 270, 135, 17280, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 17280, 270, 135, 135, 270, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 135, 17280, 270, 17280, 17280, 270, 135, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 135, 135, 270, 17280, 17280, 270, 135, 270, 135, 270, 17280, 17280, 270, 135, 270, 135, 17280, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 270, 135, 270, 17280, 270, 135, 135, 270, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 270, 17280, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 135, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 135, 135, 17280, 270, 17280, 270, 17280, 270, 135, 270, 135, 135, 135, 17280, 17280, 270, 270, 135, 135, 270, 135, 17280, 135, 135, 270, 135, 17280, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 135, 270, 17280, 270, 270, 17280, 270, 135, 270, 135, 17280, 270, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 270, 135, 270, 135, 270, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 270, 270, 17280, 135, 17280, 135, 135, 135, 17280, 270, 17280, 17280, 135, 270, 270, 270, 135, 270, 135, 270, 270, 17280, 135, 135, 270, 135, 270, 270, 135, 17280, 17280, 17280, 135, 270, 270, 270, 270, 135, 270, 135, 17280, 135, 135, 17280, 270, 17280, 270, 270, 270, 270, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 270, 17280, 270, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 1892160 . Total input tokens: 422125854 . Total output tokens: 371209282
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 92.7689261212945,
    "estimated_duration": 3600.0351858187855,
    "input_throughput": 6897.820915145351,
    "output_throughput": 5999.503861817865,
    "total_throughput": 12897.324776963216,
    "itl": 83.93945036305372,
    "ttft": 1940416.3605282728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.306406260561223,
    "arrivals": 630527,
    "finished_requests": 100677,
    "scheduler_time": 314.49127574246785
}
#Debug simulation 
Total elapsed time: 92.76910632336512. Arrivals time: 0.6457350319251418 Scheduler time: 91.87910028873011 Scheduler overhead time: 0.09393478790298104 Adapter cache time: 0.0199271934106946 Engine time: 0.09430564008653164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 66, 17280, 17280, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 17280, 270, 17280, 17280, 66, 17280, 270, 270, 17280, 17280, 270, 66, 66, 270, 17280, 17280, 270, 270, 66, 66, 270, 17280, 17280, 66, 17280, 270, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 270, 66, 17280, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 17280, 270, 66, 66, 270, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 66, 17280, 270, 17280, 17280, 270, 66, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 66, 270, 17280, 17280, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 270, 66, 17280, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 270, 66, 270, 17280, 270, 66, 66, 270, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 270, 17280, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 66, 66, 17280, 270, 17280, 270, 17280, 270, 66, 270, 66, 66, 66, 17280, 17280, 270, 270, 66, 66, 270, 66, 17280, 66, 66, 270, 66, 17280, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 270, 66, 270, 66, 270, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 17280, 66, 17280, 66, 66, 66, 17280, 270, 17280, 17280, 66, 270, 270, 270, 66, 270, 66, 270, 270, 17280, 66, 66, 270, 66, 270, 270, 66, 17280, 17280, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 270, 270, 270, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1884846 . Total input tokens: 420481262 . Total output tokens: 369791205
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 93.65635036025196,
    "estimated_duration": 3600.0134261855724,
    "input_throughput": 7102.697677184145,
    "output_throughput": 6174.869470849056,
    "total_throughput": 13277.5671480332,
    "itl": 87.65805065439488,
    "ttft": 1916224.0652440682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.902847565026049,
    "arrivals": 628242,
    "finished_requests": 103405,
    "scheduler_time": 305.224779292419
}
#Debug simulation 
Total elapsed time: 93.65652923099697. Arrivals time: 0.6520044035278261 Scheduler time: 92.7672404134646 Scheduler overhead time: 0.0922539266757667 Adapter cache time: 0.019434943329542875 Engine time: 0.09105648100376129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 66, 17280, 17280, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 17280, 270, 17280, 17280, 66, 17280, 270, 270, 17280, 17280, 270, 66, 66, 270, 17280, 17280, 270, 270, 66, 66, 270, 17280, 17280, 66, 17280, 270, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 270, 66, 17280, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 17280, 270, 66, 66, 270, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 66, 17280, 270, 17280, 17280, 270, 66, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 66, 270, 17280, 17280, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 270, 66, 17280, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 270, 66, 270, 17280, 270, 66, 66, 270, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 270, 17280, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 66, 66, 17280, 270, 17280, 270, 17280, 270, 66, 270, 66, 66, 66, 17280, 17280, 270, 270, 66, 66, 270, 66, 17280, 66, 66, 270, 66, 17280, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 270, 66, 270, 66, 270, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 17280, 66, 17280, 66, 66, 66, 17280, 270, 17280, 17280, 66, 270, 270, 270, 66, 270, 66, 270, 270, 17280, 66, 66, 270, 66, 270, 270, 66, 17280, 17280, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 270, 270, 270, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1884846 . Total input tokens: 420481262 . Total output tokens: 369791205
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 91.62598287919536,
    "estimated_duration": 3600.07406761968,
    "input_throughput": 7067.337649756335,
    "output_throughput": 6125.469805842238,
    "total_throughput": 13192.807455598573,
    "itl": 86.84321290832119,
    "ttft": 1925428.6606901046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3075514410249935,
    "arrivals": 628242,
    "finished_requests": 102974,
    "scheduler_time": 306.6792172929903
}
#Debug simulation 
Total elapsed time: 91.6261673020199. Arrivals time: 0.6481643305160105 Scheduler time: 90.74015836510807 Scheduler overhead time: 0.09189096931368113 Adapter cache time: 0.019455331843346357 Engine time: 0.09154238412156701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 66, 17280, 17280, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 17280, 270, 17280, 17280, 66, 17280, 270, 270, 17280, 17280, 270, 66, 66, 270, 17280, 17280, 270, 270, 66, 66, 270, 17280, 17280, 66, 17280, 270, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 270, 66, 17280, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 17280, 270, 66, 66, 270, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 66, 17280, 270, 17280, 17280, 270, 66, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 66, 270, 17280, 17280, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 270, 66, 17280, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 270, 66, 270, 17280, 270, 66, 66, 270, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 270, 17280, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 66, 66, 17280, 270, 17280, 270, 17280, 270, 66, 270, 66, 66, 66, 17280, 17280, 270, 270, 66, 66, 270, 66, 17280, 66, 66, 270, 66, 17280, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 270, 66, 270, 66, 270, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 17280, 66, 17280, 66, 66, 66, 17280, 270, 17280, 17280, 66, 270, 270, 270, 66, 270, 66, 270, 270, 17280, 66, 66, 270, 66, 270, 270, 66, 17280, 17280, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 270, 270, 270, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1884846 . Total input tokens: 420481262 . Total output tokens: 369791205
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 91.13598412275314,
    "estimated_duration": 3600.0373331081337,
    "input_throughput": 7011.657009180744,
    "output_throughput": 6091.994046368754,
    "total_throughput": 13103.651055549499,
    "itl": 84.8194957682187,
    "ttft": 1931100.8269940857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3316323837125923,
    "arrivals": 628242,
    "finished_requests": 102183,
    "scheduler_time": 308.6677103371637
}
#Debug simulation 
Total elapsed time: 91.13616828806698. Arrivals time: 0.6386845759116113 Scheduler time: 90.2583852619864 Scheduler overhead time: 0.09230374963954091 Adapter cache time: 0.01951279677450657 Engine time: 0.09175055241212249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 66, 17280, 17280, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 17280, 270, 17280, 17280, 66, 17280, 270, 270, 17280, 17280, 270, 66, 66, 270, 17280, 17280, 270, 270, 66, 66, 270, 17280, 17280, 66, 17280, 270, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 270, 66, 17280, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 17280, 270, 66, 66, 270, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 66, 17280, 270, 17280, 17280, 270, 66, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 66, 270, 17280, 17280, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 270, 66, 17280, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 270, 66, 270, 17280, 270, 66, 66, 270, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 270, 17280, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 66, 66, 17280, 270, 17280, 270, 17280, 270, 66, 270, 66, 66, 66, 17280, 17280, 270, 270, 66, 66, 270, 66, 17280, 66, 66, 270, 66, 17280, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 270, 66, 270, 66, 270, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 17280, 66, 17280, 66, 66, 66, 17280, 270, 17280, 17280, 66, 270, 270, 270, 66, 270, 66, 270, 270, 17280, 66, 66, 270, 66, 270, 270, 66, 17280, 17280, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 270, 270, 270, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1884846 . Total input tokens: 420481262 . Total output tokens: 369791205
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 91.68902679858729,
    "estimated_duration": 3600.0671176261867,
    "input_throughput": 7067.8479507842485,
    "output_throughput": 6125.947178046462,
    "total_throughput": 13193.79512883071,
    "itl": 86.84068263267974,
    "ttft": 1925419.793613085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.102094736974682,
    "arrivals": 628242,
    "finished_requests": 102979,
    "scheduler_time": 306.69423505774193
}
#Debug simulation 
Total elapsed time: 91.68920932384208. Arrivals time: 0.6609183582477272 Scheduler time: 90.79051633970812 Scheduler overhead time: 0.09172906680032611 Adapter cache time: 0.019711580593138933 Engine time: 0.09118195436894894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 66, 17280, 17280, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 17280, 270, 17280, 17280, 66, 17280, 270, 270, 17280, 17280, 270, 66, 66, 270, 17280, 17280, 270, 270, 66, 66, 270, 17280, 17280, 66, 17280, 270, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 270, 66, 17280, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 17280, 270, 66, 66, 270, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 66, 17280, 270, 17280, 17280, 270, 66, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 66, 270, 17280, 17280, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 270, 66, 17280, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 270, 66, 270, 17280, 270, 66, 66, 270, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 270, 17280, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 66, 66, 17280, 270, 17280, 270, 17280, 270, 66, 270, 66, 66, 66, 17280, 17280, 270, 270, 66, 66, 270, 66, 17280, 66, 66, 270, 66, 17280, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 270, 66, 270, 66, 270, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 17280, 66, 17280, 66, 66, 66, 17280, 270, 17280, 17280, 66, 270, 270, 270, 66, 270, 66, 270, 270, 17280, 66, 66, 270, 66, 270, 270, 66, 17280, 17280, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 270, 270, 270, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1884846 . Total input tokens: 420481262 . Total output tokens: 369791205
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 91.04765151301399,
    "estimated_duration": 3600.007284143104,
    "input_throughput": 7011.715534905734,
    "output_throughput": 6092.0448957425515,
    "total_throughput": 13103.760430648284,
    "itl": 84.81896636049622,
    "ttft": 1931088.720854616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3020146274566895,
    "arrivals": 628242,
    "finished_requests": 102183,
    "scheduler_time": 308.6675825220548
}
#Debug simulation 
Total elapsed time: 91.04783347807825. Arrivals time: 0.6428334936499596 Scheduler time: 90.16656398121268 Scheduler overhead time: 0.0921637280844152 Adapter cache time: 0.019801361486315727 Engine time: 0.09142153710126877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 66, 17280, 17280, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 17280, 270, 17280, 17280, 66, 17280, 270, 270, 17280, 17280, 270, 66, 66, 270, 17280, 17280, 270, 270, 66, 66, 270, 17280, 17280, 66, 17280, 270, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 270, 66, 17280, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 17280, 270, 66, 66, 270, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 66, 17280, 270, 17280, 17280, 270, 66, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 66, 270, 17280, 17280, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 270, 66, 17280, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 270, 66, 270, 17280, 270, 66, 66, 270, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 270, 17280, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 66, 66, 17280, 270, 17280, 270, 17280, 270, 66, 270, 66, 66, 66, 17280, 17280, 270, 270, 66, 66, 270, 66, 17280, 66, 66, 270, 66, 17280, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 270, 66, 270, 66, 270, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 17280, 66, 17280, 66, 66, 66, 17280, 270, 17280, 17280, 66, 270, 270, 270, 66, 270, 66, 270, 270, 17280, 66, 66, 270, 66, 270, 270, 66, 17280, 17280, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 270, 270, 270, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1884846 . Total input tokens: 420481262 . Total output tokens: 369791205
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 94.43958676001057,
    "estimated_duration": 3600.0126296343515,
    "input_throughput": 6988.037151012763,
    "output_throughput": 6059.215687311504,
    "total_throughput": 13047.252838324266,
    "itl": 86.52355487551726,
    "ttft": 1931300.2618856844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8089252138510226,
    "arrivals": 628242,
    "finished_requests": 101761,
    "scheduler_time": 310.3581148412164
}
#Debug simulation 
Total elapsed time: 94.43977171974257. Arrivals time: 0.6428659437224269 Scheduler time: 93.55533180432394 Scheduler overhead time: 0.09332875534892082 Adapter cache time: 0.019754311069846153 Engine time: 0.0930523807182908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_320_slots_16_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 66, 17280, 17280, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 17280, 270, 17280, 17280, 66, 17280, 270, 270, 17280, 17280, 270, 66, 66, 270, 17280, 17280, 270, 270, 66, 66, 270, 17280, 17280, 66, 17280, 270, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 270, 66, 17280, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 17280, 270, 66, 66, 270, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 66, 17280, 270, 17280, 17280, 270, 66, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 66, 66, 270, 17280, 17280, 270, 66, 270, 66, 270, 17280, 17280, 270, 66, 270, 66, 17280, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 270, 66, 270, 17280, 270, 66, 66, 270, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 270, 17280, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 66, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 66, 66, 17280, 270, 17280, 270, 17280, 270, 66, 270, 66, 66, 66, 17280, 17280, 270, 270, 66, 66, 270, 66, 17280, 66, 66, 270, 66, 17280, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 66, 270, 17280, 270, 270, 17280, 270, 66, 270, 66, 17280, 270, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 270, 66, 270, 66, 270, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 270, 270, 17280, 66, 17280, 66, 66, 66, 17280, 270, 17280, 17280, 66, 270, 270, 270, 66, 270, 66, 270, 270, 17280, 66, 66, 270, 66, 270, 270, 66, 17280, 17280, 17280, 66, 270, 270, 270, 270, 66, 270, 66, 17280, 66, 66, 17280, 270, 17280, 270, 270, 270, 270, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 270, 17280, 270, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1884846 . Total input tokens: 420481262 . Total output tokens: 369791205
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.87256013415754,
    "estimated_duration": 3600.000904654458,
    "input_throughput": 6985.820188957396,
    "output_throughput": 6059.822088320815,
    "total_throughput": 13045.642277278212,
    "itl": 84.77968031540067,
    "ttft": 1929738.185001289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.339023264460291,
    "arrivals": 628242,
    "finished_requests": 101785,
    "scheduler_time": 310.2672500836398
}
#Debug simulation 
Total elapsed time: 88.87272942904383. Arrivals time: 0.6690722089260817 Scheduler time: 87.96379361115396 Scheduler overhead time: 0.09326128056272864 Adapter cache time: 0.019394191913306713 Engine time: 0.0916533563286066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 33, 17280, 17280, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 17280, 270, 17280, 17280, 33, 17280, 270, 270, 17280, 17280, 270, 33, 33, 270, 17280, 17280, 270, 270, 33, 33, 270, 17280, 17280, 33, 17280, 270, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 270, 33, 17280, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 17280, 270, 33, 33, 270, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 33, 17280, 270, 17280, 17280, 270, 33, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 33, 270, 17280, 17280, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 270, 33, 17280, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 270, 33, 270, 17280, 270, 33, 33, 270, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 270, 17280, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 270, 33, 270, 33, 33, 33, 17280, 17280, 270, 270, 33, 33, 270, 33, 17280, 33, 33, 270, 33, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 270, 33, 270, 33, 270, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 17280, 33, 17280, 33, 33, 33, 17280, 270, 17280, 17280, 33, 270, 270, 270, 33, 270, 33, 270, 270, 17280, 33, 33, 270, 33, 270, 270, 33, 17280, 17280, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 270, 270, 270, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1881348 . Total input tokens: 419699521 . Total output tokens: 369102416
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 95.86890594335273,
    "estimated_duration": 3600.0053467327966,
    "input_throughput": 6989.876840832292,
    "output_throughput": 6076.651808268468,
    "total_throughput": 13066.52864910076,
    "itl": 87.18647912305438,
    "ttft": 1935897.250898826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7110877030994947,
    "arrivals": 627073,
    "finished_requests": 101833,
    "scheduler_time": 309.4515445084389
}
#Debug simulation 
Total elapsed time: 95.8691654172726. Arrivals time: 0.6991064073517919 Scheduler time: 94.93173817591742 Scheduler overhead time: 0.09222047217190266 Adapter cache time: 0.01945043494924903 Engine time: 0.09145821630954742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 33, 17280, 17280, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 17280, 270, 17280, 17280, 33, 17280, 270, 270, 17280, 17280, 270, 33, 33, 270, 17280, 17280, 270, 270, 33, 33, 270, 17280, 17280, 33, 17280, 270, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 270, 33, 17280, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 17280, 270, 33, 33, 270, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 33, 17280, 270, 17280, 17280, 270, 33, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 33, 270, 17280, 17280, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 270, 33, 17280, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 270, 33, 270, 17280, 270, 33, 33, 270, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 270, 17280, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 270, 33, 270, 33, 33, 33, 17280, 17280, 270, 270, 33, 33, 270, 33, 17280, 33, 33, 270, 33, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 270, 33, 270, 33, 270, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 17280, 33, 17280, 33, 33, 33, 17280, 270, 17280, 17280, 33, 270, 270, 270, 33, 270, 33, 270, 270, 17280, 33, 33, 270, 33, 270, 270, 33, 17280, 17280, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 270, 270, 270, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1881348 . Total input tokens: 419699521 . Total output tokens: 369102416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 93.14381737494841,
    "estimated_duration": 3600.0242056259904,
    "input_throughput": 6966.233160545988,
    "output_throughput": 6056.777608862914,
    "total_throughput": 13023.010769408902,
    "itl": 85.85838128744982,
    "ttft": 1940327.5454220339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0557926047127735,
    "arrivals": 627073,
    "finished_requests": 101450,
    "scheduler_time": 310.61948976862163
}
#Debug simulation 
Total elapsed time: 93.144002456218. Arrivals time: 0.6457261024042964 Scheduler time: 92.25827186414972 Scheduler overhead time: 0.09301498346030712 Adapter cache time: 0.01923306565731764 Engine time: 0.09230533335357904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 33, 17280, 17280, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 17280, 270, 17280, 17280, 33, 17280, 270, 270, 17280, 17280, 270, 33, 33, 270, 17280, 17280, 270, 270, 33, 33, 270, 17280, 17280, 33, 17280, 270, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 270, 33, 17280, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 17280, 270, 33, 33, 270, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 33, 17280, 270, 17280, 17280, 270, 33, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 33, 270, 17280, 17280, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 270, 33, 17280, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 270, 33, 270, 17280, 270, 33, 33, 270, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 270, 17280, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 270, 33, 270, 33, 33, 33, 17280, 17280, 270, 270, 33, 33, 270, 33, 17280, 33, 33, 270, 33, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 270, 33, 270, 33, 270, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 17280, 33, 17280, 33, 33, 33, 17280, 270, 17280, 17280, 33, 270, 270, 270, 33, 270, 33, 270, 270, 17280, 33, 33, 270, 33, 270, 270, 33, 17280, 17280, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 270, 270, 270, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1881348 . Total input tokens: 419699521 . Total output tokens: 369102416
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.75327081698924,
    "estimated_duration": 3600.0212214576286,
    "input_throughput": 7022.603047257498,
    "output_throughput": 6125.056949267633,
    "total_throughput": 13147.65999652513,
    "itl": 84.84353088428486,
    "ttft": 1929648.9845433617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.221517010671111,
    "arrivals": 627073,
    "finished_requests": 102267,
    "scheduler_time": 307.89555785756744
}
#Debug simulation 
Total elapsed time: 88.75345196807757. Arrivals time: 0.6383670782670379 Scheduler time: 87.87724404828623 Scheduler overhead time: 0.09220502804964781 Adapter cache time: 0.019515031948685646 Engine time: 0.09100902453064919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 33, 17280, 17280, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 17280, 270, 17280, 17280, 33, 17280, 270, 270, 17280, 17280, 270, 33, 33, 270, 17280, 17280, 270, 270, 33, 33, 270, 17280, 17280, 33, 17280, 270, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 270, 33, 17280, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 17280, 270, 33, 33, 270, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 33, 17280, 270, 17280, 17280, 270, 33, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 33, 270, 17280, 17280, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 270, 33, 17280, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 270, 33, 270, 17280, 270, 33, 33, 270, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 270, 17280, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 270, 33, 270, 33, 33, 33, 17280, 17280, 270, 270, 33, 33, 270, 33, 17280, 33, 33, 270, 33, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 270, 33, 270, 33, 270, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 17280, 33, 17280, 33, 33, 33, 17280, 270, 17280, 17280, 33, 270, 270, 270, 33, 270, 33, 270, 270, 17280, 33, 33, 270, 33, 270, 270, 33, 17280, 17280, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 270, 270, 270, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1881348 . Total input tokens: 419699521 . Total output tokens: 369102416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 91.80910744192079,
    "estimated_duration": 3600.0285100696096,
    "input_throughput": 7084.529172105597,
    "output_throughput": 6154.955978271282,
    "total_throughput": 13239.48515037688,
    "itl": 86.93902238472825,
    "ttft": 1929633.4741997037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 431,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9499855245696343,
    "arrivals": 627073,
    "finished_requests": 103078,
    "scheduler_time": 305.3376466596464
}
#Debug simulation 
Total elapsed time: 91.80928563978523. Arrivals time: 0.6295284857042134 Scheduler time: 90.94160776259378 Scheduler overhead time: 0.09120830846950412 Adapter cache time: 0.01967777218669653 Engine time: 0.09196713799610734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 33, 17280, 17280, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 17280, 270, 17280, 17280, 33, 17280, 270, 270, 17280, 17280, 270, 33, 33, 270, 17280, 17280, 270, 270, 33, 33, 270, 17280, 17280, 33, 17280, 270, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 270, 33, 17280, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 17280, 270, 33, 33, 270, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 33, 17280, 270, 17280, 17280, 270, 33, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 33, 270, 17280, 17280, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 270, 33, 17280, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 270, 33, 270, 17280, 270, 33, 33, 270, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 270, 17280, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 270, 33, 270, 33, 33, 33, 17280, 17280, 270, 270, 33, 33, 270, 33, 17280, 33, 33, 270, 33, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 270, 33, 270, 33, 270, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 17280, 33, 17280, 33, 33, 33, 17280, 270, 17280, 17280, 33, 270, 270, 270, 33, 270, 33, 270, 270, 17280, 33, 33, 270, 33, 270, 270, 33, 17280, 17280, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 270, 270, 270, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1881348 . Total input tokens: 419699521 . Total output tokens: 369102416
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 88.66734332684427,
    "estimated_duration": 3600.0347881050534,
    "input_throughput": 7022.576582741137,
    "output_throughput": 6125.033867132882,
    "total_throughput": 13147.610449874019,
    "itl": 84.84305054432362,
    "ttft": 1929636.6194163468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.192520605945053,
    "arrivals": 627073,
    "finished_requests": 102267,
    "scheduler_time": 307.90093673096095
}
#Debug simulation 
Total elapsed time: 88.6675210040994. Arrivals time: 0.6375994002446532 Scheduler time: 87.790821576491 Scheduler overhead time: 0.09290065197274089 Adapter cache time: 0.01947152568027377 Engine time: 0.09142782306298614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 33, 17280, 17280, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 17280, 270, 17280, 17280, 33, 17280, 270, 270, 17280, 17280, 270, 33, 33, 270, 17280, 17280, 270, 270, 33, 33, 270, 17280, 17280, 33, 17280, 270, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 270, 33, 17280, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 17280, 270, 33, 33, 270, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 33, 17280, 270, 17280, 17280, 270, 33, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 33, 270, 17280, 17280, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 270, 33, 17280, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 270, 33, 270, 17280, 270, 33, 33, 270, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 270, 17280, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 270, 33, 270, 33, 33, 33, 17280, 17280, 270, 270, 33, 33, 270, 33, 17280, 33, 33, 270, 33, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 270, 33, 270, 33, 270, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 17280, 33, 17280, 33, 33, 33, 17280, 270, 17280, 17280, 33, 270, 270, 270, 33, 270, 33, 270, 270, 17280, 33, 33, 270, 33, 270, 270, 33, 17280, 17280, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 270, 270, 270, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1881348 . Total input tokens: 419699521 . Total output tokens: 369102416
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 94.86552611598745,
    "estimated_duration": 3600.067830765385,
    "input_throughput": 7027.959524479544,
    "output_throughput": 6112.390386633817,
    "total_throughput": 13140.34991111336,
    "itl": 86.57946234263915,
    "ttft": 1927759.1286449218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7833895300887406,
    "arrivals": 627073,
    "finished_requests": 102305,
    "scheduler_time": 308.01262533658377
}
#Debug simulation 
Total elapsed time: 94.86570232408121. Arrivals time: 0.6506400797516108 Scheduler time: 93.9751432929188 Scheduler overhead time: 0.0922494069673121 Adapter cache time: 0.019673339556902647 Engine time: 0.09232117887586355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [106 107 107]
Adapter prompts. [270, 17280, 17280, 33, 17280, 17280, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 17280, 270, 17280, 17280, 33, 17280, 270, 270, 17280, 17280, 270, 33, 33, 270, 17280, 17280, 270, 270, 33, 33, 270, 17280, 17280, 33, 17280, 270, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 270, 33, 17280, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 17280, 270, 33, 33, 270, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 33, 17280, 270, 17280, 17280, 270, 33, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 33, 33, 270, 17280, 17280, 270, 33, 270, 33, 270, 17280, 17280, 270, 33, 270, 33, 17280, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 270, 33, 270, 17280, 270, 33, 33, 270, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 270, 17280, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 33, 270, 17280, 17280, 17280, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 270, 33, 270, 33, 33, 33, 17280, 17280, 270, 270, 33, 33, 270, 33, 17280, 33, 33, 270, 33, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 33, 270, 17280, 270, 270, 17280, 270, 33, 270, 33, 17280, 270, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 270, 33, 270, 33, 270, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 270, 270, 17280, 33, 17280, 33, 33, 33, 17280, 270, 17280, 17280, 33, 270, 270, 270, 33, 270, 33, 270, 270, 17280, 33, 33, 270, 33, 270, 270, 33, 17280, 17280, 17280, 33, 270, 270, 270, 270, 33, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 270, 270, 270, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 270, 17280, 270, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1881348 . Total input tokens: 419699521 . Total output tokens: 369102416
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.53536383714527,
    "estimated_duration": 3600.005269269616,
    "input_throughput": 7022.634165513103,
    "output_throughput": 6125.084090355696,
    "total_throughput": 13147.718255868798,
    "itl": 84.84253994671394,
    "ttft": 1929624.6360753141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.16290284968915,
    "arrivals": 627073,
    "finished_requests": 102267,
    "scheduler_time": 307.90073225811386
}
#Debug simulation 
Total elapsed time: 88.53553253319114. Arrivals time: 0.6250884830951691 Scheduler time: 87.67387534631416 Scheduler overhead time: 0.09096118481829762 Adapter cache time: 0.01933239307254553 Engine time: 0.09140908485278487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 66, 17280, 17280, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 17280, 135, 17280, 17280, 66, 17280, 135, 135, 17280, 17280, 135, 66, 66, 135, 17280, 17280, 135, 135, 66, 66, 135, 17280, 17280, 66, 17280, 135, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 135, 66, 17280, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 17280, 135, 66, 66, 135, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 66, 17280, 135, 17280, 17280, 135, 66, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 66, 135, 17280, 17280, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 135, 66, 17280, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 135, 66, 135, 17280, 135, 66, 66, 135, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 135, 17280, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 135, 66, 135, 66, 66, 66, 17280, 17280, 135, 135, 66, 66, 135, 66, 17280, 66, 66, 135, 66, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 135, 66, 135, 66, 135, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 17280, 66, 17280, 66, 66, 66, 17280, 135, 17280, 17280, 66, 135, 135, 135, 66, 135, 66, 135, 135, 17280, 66, 66, 135, 66, 135, 135, 66, 17280, 17280, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 135, 135, 135, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1870401 . Total input tokens: 417241423 . Total output tokens: 366917622
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 94.01709455717355,
    "estimated_duration": 3600.0920314351683,
    "input_throughput": 7073.290565254977,
    "output_throughput": 6145.390675243469,
    "total_throughput": 13218.681240498445,
    "itl": 87.93612575606754,
    "ttft": 1925882.3159877753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0152585185692016,
    "arrivals": 623634,
    "finished_requests": 103378,
    "scheduler_time": 305.4263889690351
}
#Debug simulation 
Total elapsed time: 94.01727329101413. Arrivals time: 0.6309148236177862 Scheduler time: 93.14891724521294 Scheduler overhead time: 0.0919578424654901 Adapter cache time: 0.01986009068787098 Engine time: 0.09085190063342452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 66, 17280, 17280, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 17280, 135, 17280, 17280, 66, 17280, 135, 135, 17280, 17280, 135, 66, 66, 135, 17280, 17280, 135, 135, 66, 66, 135, 17280, 17280, 66, 17280, 135, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 135, 66, 17280, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 17280, 135, 66, 66, 135, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 66, 17280, 135, 17280, 17280, 135, 66, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 66, 135, 17280, 17280, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 135, 66, 17280, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 135, 66, 135, 17280, 135, 66, 66, 135, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 135, 17280, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 135, 66, 135, 66, 66, 66, 17280, 17280, 135, 135, 66, 66, 135, 66, 17280, 66, 66, 135, 66, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 135, 66, 135, 66, 135, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 17280, 66, 17280, 66, 66, 66, 17280, 135, 17280, 17280, 66, 135, 135, 135, 66, 135, 66, 135, 135, 17280, 66, 66, 135, 66, 135, 135, 66, 17280, 17280, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 135, 135, 135, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1870401 . Total input tokens: 417241423 . Total output tokens: 366917622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 93.98855968611315,
    "estimated_duration": 3600.0775700715544,
    "input_throughput": 7054.447440557572,
    "output_throughput": 6126.842983430937,
    "total_throughput": 13181.290423988508,
    "itl": 86.70877133705804,
    "ttft": 1928626.8919103933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3372517877072148,
    "arrivals": 623634,
    "finished_requests": 103071,
    "scheduler_time": 306.4460600488272
}
#Debug simulation 
Total elapsed time: 93.9887413312681. Arrivals time: 0.6449127541854978 Scheduler time: 93.10109967598692 Scheduler overhead time: 0.0927326693199575 Adapter cache time: 0.019965534564107656 Engine time: 0.09465103782713413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 66, 17280, 17280, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 17280, 135, 17280, 17280, 66, 17280, 135, 135, 17280, 17280, 135, 66, 66, 135, 17280, 17280, 135, 135, 66, 66, 135, 17280, 17280, 66, 17280, 135, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 135, 66, 17280, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 17280, 135, 66, 66, 135, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 66, 17280, 135, 17280, 17280, 135, 66, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 66, 135, 17280, 17280, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 135, 66, 17280, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 135, 66, 135, 17280, 135, 66, 66, 135, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 135, 17280, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 135, 66, 135, 66, 66, 66, 17280, 17280, 135, 135, 66, 66, 135, 66, 17280, 66, 66, 135, 66, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 135, 66, 135, 66, 135, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 17280, 66, 17280, 66, 66, 66, 17280, 135, 17280, 17280, 66, 135, 135, 135, 66, 135, 66, 135, 135, 17280, 66, 66, 135, 66, 135, 135, 66, 17280, 17280, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 135, 135, 135, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1870401 . Total input tokens: 417241423 . Total output tokens: 366917622
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.54191202903166,
    "estimated_duration": 3600.0312474111056,
    "input_throughput": 6950.575781250317,
    "output_throughput": 6039.613410476901,
    "total_throughput": 12990.189191727219,
    "itl": 84.46672539328932,
    "ttft": 1930507.3946180972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4684576724656186,
    "arrivals": 623634,
    "finished_requests": 101549,
    "scheduler_time": 311.05522791268197
}
#Debug simulation 
Total elapsed time: 90.54208296677098. Arrivals time: 0.6335861342959106 Scheduler time: 89.6682457816787 Scheduler overhead time: 0.09218736505135894 Adapter cache time: 0.02013166109099984 Engine time: 0.09277120651677251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 66, 17280, 17280, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 17280, 135, 17280, 17280, 66, 17280, 135, 135, 17280, 17280, 135, 66, 66, 135, 17280, 17280, 135, 135, 66, 66, 135, 17280, 17280, 66, 17280, 135, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 135, 66, 17280, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 17280, 135, 66, 66, 135, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 66, 17280, 135, 17280, 17280, 135, 66, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 66, 135, 17280, 17280, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 135, 66, 17280, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 135, 66, 135, 17280, 135, 66, 66, 135, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 135, 17280, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 135, 66, 135, 66, 66, 66, 17280, 17280, 135, 135, 66, 66, 135, 66, 17280, 66, 66, 135, 66, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 135, 66, 135, 66, 135, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 17280, 66, 17280, 66, 66, 66, 17280, 135, 17280, 17280, 66, 135, 135, 135, 66, 135, 66, 135, 135, 17280, 66, 66, 135, 66, 135, 135, 66, 17280, 17280, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 135, 135, 135, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1870401 . Total input tokens: 417241423 . Total output tokens: 366917622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 94.06381299067289,
    "estimated_duration": 3600.0668474504973,
    "input_throughput": 7055.209827002873,
    "output_throughput": 6127.158448633032,
    "total_throughput": 13182.368275635905,
    "itl": 86.70606271491462,
    "ttft": 1928526.048882722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1290186417102777,
    "arrivals": 623634,
    "finished_requests": 103078,
    "scheduler_time": 306.46048615284894
}
#Debug simulation 
Total elapsed time: 94.0640076706186. Arrivals time: 0.6362982331775129 Scheduler time: 93.19026639033109 Scheduler overhead time: 0.09135528234764934 Adapter cache time: 0.01993390079587698 Engine time: 0.09117351938039064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 66, 17280, 17280, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 17280, 135, 17280, 17280, 66, 17280, 135, 135, 17280, 17280, 135, 66, 66, 135, 17280, 17280, 135, 135, 66, 66, 135, 17280, 17280, 66, 17280, 135, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 135, 66, 17280, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 17280, 135, 66, 66, 135, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 66, 17280, 135, 17280, 17280, 135, 66, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 66, 135, 17280, 17280, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 135, 66, 17280, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 135, 66, 135, 17280, 135, 66, 66, 135, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 135, 17280, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 135, 66, 135, 66, 66, 66, 17280, 17280, 135, 135, 66, 66, 135, 66, 17280, 66, 66, 135, 66, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 135, 66, 135, 66, 135, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 17280, 66, 17280, 66, 66, 66, 17280, 135, 17280, 17280, 66, 135, 135, 135, 66, 135, 66, 135, 135, 17280, 66, 66, 135, 66, 135, 135, 66, 17280, 17280, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 135, 135, 135, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1870401 . Total input tokens: 417241423 . Total output tokens: 366917622
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 90.59098152024671,
    "estimated_duration": 3600.1003424585347,
    "input_throughput": 6950.618488292374,
    "output_throughput": 6039.723044261353,
    "total_throughput": 12990.341532553728,
    "itl": 84.4665735410613,
    "ttft": 1930535.8663416416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4371829787967982,
    "arrivals": 623634,
    "finished_requests": 101555,
    "scheduler_time": 311.06299356549755
}
#Debug simulation 
Total elapsed time: 90.59123437805101. Arrivals time: 0.6075954874977469 Scheduler time: 89.74402952985838 Scheduler overhead time: 0.09190501319244504 Adapter cache time: 0.019891020841896534 Engine time: 0.09209331730380654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 66, 17280, 17280, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 17280, 135, 17280, 17280, 66, 17280, 135, 135, 17280, 17280, 135, 66, 66, 135, 17280, 17280, 135, 135, 66, 66, 135, 17280, 17280, 66, 17280, 135, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 135, 66, 17280, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 17280, 135, 66, 66, 135, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 66, 17280, 135, 17280, 17280, 135, 66, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 66, 135, 17280, 17280, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 135, 66, 17280, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 135, 66, 135, 17280, 135, 66, 66, 135, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 135, 17280, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 135, 66, 135, 66, 66, 66, 17280, 17280, 135, 135, 66, 66, 135, 66, 17280, 66, 66, 135, 66, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 135, 66, 135, 66, 135, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 17280, 66, 17280, 66, 66, 66, 17280, 135, 17280, 17280, 66, 135, 135, 135, 66, 135, 66, 135, 135, 17280, 66, 66, 135, 66, 135, 135, 66, 17280, 17280, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 135, 135, 135, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1870401 . Total input tokens: 417241423 . Total output tokens: 366917622
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 93.65189371816814,
    "estimated_duration": 3600.097325923189,
    "input_throughput": 7014.6864692120835,
    "output_throughput": 6101.966700127123,
    "total_throughput": 13116.653169339206,
    "itl": 86.36815334733873,
    "ttft": 1926781.1784611784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8855322651378685,
    "arrivals": 623634,
    "finished_requests": 102577,
    "scheduler_time": 307.75148775511263
}
#Debug simulation 
Total elapsed time: 93.65207203291357. Arrivals time: 0.6233846102841198 Scheduler time: 92.79127651359886 Scheduler overhead time: 0.09111141227185726 Adapter cache time: 0.01979996683076024 Engine time: 0.09151513548567891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 66, 17280, 17280, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 17280, 135, 17280, 17280, 66, 17280, 135, 135, 17280, 17280, 135, 66, 66, 135, 17280, 17280, 135, 135, 66, 66, 135, 17280, 17280, 66, 17280, 135, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 66, 17280, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 135, 66, 17280, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 17280, 135, 66, 66, 135, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 66, 17280, 135, 17280, 17280, 135, 66, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 66, 66, 135, 17280, 17280, 135, 66, 135, 66, 135, 17280, 17280, 135, 66, 135, 66, 17280, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 135, 66, 135, 17280, 135, 66, 66, 135, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 135, 17280, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 66, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 135, 66, 135, 66, 66, 66, 17280, 17280, 135, 135, 66, 66, 135, 66, 17280, 66, 66, 135, 66, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 66, 135, 17280, 135, 135, 17280, 135, 66, 135, 66, 17280, 135, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 66, 66, 17280, 135, 66, 135, 66, 135, 17280, 17280, 17280, 66, 17280, 17280, 17280, 17280, 66, 135, 135, 17280, 66, 17280, 66, 66, 66, 17280, 135, 17280, 17280, 66, 135, 135, 135, 66, 135, 66, 135, 135, 17280, 66, 66, 135, 66, 135, 135, 66, 17280, 17280, 17280, 66, 135, 135, 135, 135, 66, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 135, 135, 135, 66, 66, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 17280, 135, 17280, 135, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 1870401 . Total input tokens: 417241423 . Total output tokens: 366917622
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 92.01681543095037,
    "estimated_duration": 3600.0715604054644,
    "input_throughput": 6960.831633351652,
    "output_throughput": 6051.529708355664,
    "total_throughput": 13012.361341707316,
    "itl": 84.36239290624977,
    "ttft": 1933723.678055676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3487544199824577,
    "arrivals": 623634,
    "finished_requests": 101743,
    "scheduler_time": 310.54269878582375
}
#Debug simulation 
Total elapsed time: 92.01700130198151. Arrivals time: 0.6291661839932203 Scheduler time: 91.14878224954009 Scheduler overhead time: 0.09224896691739559 Adapter cache time: 0.019518994260579348 Engine time: 0.0918430513702333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 33, 17280, 17280, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 17280, 135, 17280, 17280, 33, 17280, 135, 135, 17280, 17280, 135, 33, 33, 135, 17280, 17280, 135, 135, 33, 33, 135, 17280, 17280, 33, 17280, 135, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 135, 33, 17280, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 17280, 135, 33, 33, 135, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 33, 17280, 135, 17280, 17280, 135, 33, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 33, 135, 17280, 17280, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 135, 33, 17280, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 135, 33, 135, 17280, 135, 33, 33, 135, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 135, 17280, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 135, 33, 135, 33, 33, 33, 17280, 17280, 135, 135, 33, 33, 135, 33, 17280, 33, 33, 135, 33, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 135, 33, 135, 33, 135, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 17280, 33, 17280, 33, 33, 33, 17280, 135, 17280, 17280, 33, 135, 135, 135, 33, 135, 33, 135, 135, 17280, 33, 33, 135, 33, 135, 135, 33, 17280, 17280, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 135, 135, 135, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1866903 . Total input tokens: 416447510 . Total output tokens: 366257866
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.96036948403344,
    "estimated_duration": 3600.073245991519,
    "input_throughput": 7177.1517506677055,
    "output_throughput": 6216.11491513879,
    "total_throughput": 13393.266665806495,
    "itl": 87.18672530270166,
    "ttft": 1916688.9004995057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.38555342435841,
    "arrivals": 622407,
    "finished_requests": 104409,
    "scheduler_time": 302.0298731372004
}
#Debug simulation 
Total elapsed time: 90.9605433200486. Arrivals time: 0.6412894339300692 Scheduler time: 90.08588604675606 Scheduler overhead time: 0.09008777095004916 Adapter cache time: 0.01940019801259041 Engine time: 0.08995776670053601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 33, 17280, 17280, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 17280, 135, 17280, 17280, 33, 17280, 135, 135, 17280, 17280, 135, 33, 33, 135, 17280, 17280, 135, 135, 33, 33, 135, 17280, 17280, 33, 17280, 135, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 135, 33, 17280, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 17280, 135, 33, 33, 135, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 33, 17280, 135, 17280, 17280, 135, 33, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 33, 135, 17280, 17280, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 135, 33, 17280, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 135, 33, 135, 17280, 135, 33, 33, 135, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 135, 17280, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 135, 33, 135, 33, 33, 33, 17280, 17280, 135, 135, 33, 33, 135, 33, 17280, 33, 33, 135, 33, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 135, 33, 135, 33, 135, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 17280, 33, 17280, 33, 33, 33, 17280, 135, 17280, 17280, 33, 135, 135, 135, 33, 135, 33, 135, 135, 17280, 33, 33, 135, 33, 135, 135, 33, 17280, 17280, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 135, 135, 135, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1866903 . Total input tokens: 416447510 . Total output tokens: 366257866
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.5498965671286,
    "estimated_duration": 3600.0114400318357,
    "input_throughput": 7080.050278888725,
    "output_throughput": 6131.81412551422,
    "total_throughput": 13211.864404402946,
    "itl": 86.42699440995332,
    "ttft": 1927683.8608099387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.181808949429546,
    "arrivals": 622407,
    "finished_requests": 102986,
    "scheduler_time": 306.33917196349336
}
#Debug simulation 
Total elapsed time: 90.55007376940921. Arrivals time: 0.6306636156514287 Scheduler time: 89.68265191325918 Scheduler overhead time: 0.09155913162976503 Adapter cache time: 0.019493158906698227 Engine time: 0.09103040723130107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 33, 17280, 17280, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 17280, 135, 17280, 17280, 33, 17280, 135, 135, 17280, 17280, 135, 33, 33, 135, 17280, 17280, 135, 135, 33, 33, 135, 17280, 17280, 33, 17280, 135, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 135, 33, 17280, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 17280, 135, 33, 33, 135, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 33, 17280, 135, 17280, 17280, 135, 33, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 33, 135, 17280, 17280, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 135, 33, 17280, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 135, 33, 135, 17280, 135, 33, 33, 135, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 135, 17280, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 135, 33, 135, 33, 33, 33, 17280, 17280, 135, 135, 33, 33, 135, 33, 17280, 33, 33, 135, 33, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 135, 33, 135, 33, 135, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 17280, 33, 17280, 33, 33, 33, 17280, 135, 17280, 17280, 33, 135, 135, 135, 33, 135, 33, 135, 135, 17280, 33, 33, 135, 33, 135, 135, 33, 17280, 17280, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 135, 135, 135, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1866903 . Total input tokens: 416447510 . Total output tokens: 366257866
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.18080935906619,
    "estimated_duration": 3600.075505271481,
    "input_throughput": 7039.440968082953,
    "output_throughput": 6090.117823333444,
    "total_throughput": 13129.558791416397,
    "itl": 84.21440493918853,
    "ttft": 1932299.4745760723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2440364243975157,
    "arrivals": 622407,
    "finished_requests": 102286,
    "scheduler_time": 308.5368927683755
}
#Debug simulation 
Total elapsed time: 90.1809866190888. Arrivals time: 0.6405863054096699 Scheduler time: 89.30147818662226 Scheduler overhead time: 0.092096496373415 Adapter cache time: 0.01969479164108634 Engine time: 0.09192606341093779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 33, 17280, 17280, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 17280, 135, 17280, 17280, 33, 17280, 135, 135, 17280, 17280, 135, 33, 33, 135, 17280, 17280, 135, 135, 33, 33, 135, 17280, 17280, 33, 17280, 135, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 135, 33, 17280, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 17280, 135, 33, 33, 135, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 33, 17280, 135, 17280, 17280, 135, 33, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 33, 135, 17280, 17280, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 135, 33, 17280, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 135, 33, 135, 17280, 135, 33, 33, 135, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 135, 17280, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 135, 33, 135, 33, 33, 33, 17280, 17280, 135, 135, 33, 33, 135, 33, 17280, 33, 33, 135, 33, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 135, 33, 135, 33, 135, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 17280, 33, 17280, 33, 33, 33, 17280, 135, 17280, 17280, 33, 135, 135, 135, 33, 135, 33, 135, 135, 17280, 33, 33, 135, 33, 135, 135, 33, 17280, 17280, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 135, 135, 135, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1866903 . Total input tokens: 416447510 . Total output tokens: 366257866
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 90.5080979061313,
    "estimated_duration": 3600.0204321836864,
    "input_throughput": 7080.339537000561,
    "output_throughput": 6132.23852915999,
    "total_throughput": 13212.578066160551,
    "itl": 86.4248038181488,
    "ttft": 1927624.3092019474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9902344551123643,
    "arrivals": 622407,
    "finished_requests": 102995,
    "scheduler_time": 306.3542270394224
}
#Debug simulation 
Total elapsed time: 90.50827278522775. Arrivals time: 0.6298496858216822 Scheduler time: 89.63896835036576 Scheduler overhead time: 0.09250757656991482 Adapter cache time: 0.019558708183467388 Engine time: 0.0922814798541367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 33, 17280, 17280, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 17280, 135, 17280, 17280, 33, 17280, 135, 135, 17280, 17280, 135, 33, 33, 135, 17280, 17280, 135, 135, 33, 33, 135, 17280, 17280, 33, 17280, 135, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 135, 33, 17280, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 17280, 135, 33, 33, 135, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 33, 17280, 135, 17280, 17280, 135, 33, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 33, 135, 17280, 17280, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 135, 33, 17280, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 135, 33, 135, 17280, 135, 33, 33, 135, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 135, 17280, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 135, 33, 135, 33, 33, 33, 17280, 17280, 135, 135, 33, 33, 135, 33, 17280, 33, 33, 135, 33, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 135, 33, 135, 33, 135, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 17280, 33, 17280, 33, 33, 33, 17280, 135, 17280, 17280, 33, 135, 135, 135, 33, 135, 33, 135, 135, 17280, 33, 33, 135, 33, 135, 135, 33, 17280, 17280, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 135, 135, 135, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1866903 . Total input tokens: 416447510 . Total output tokens: 366257866
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 90.25346071878448,
    "estimated_duration": 3600.0468200278874,
    "input_throughput": 7039.497058486502,
    "output_throughput": 6090.166349511577,
    "total_throughput": 13129.66340799808,
    "itl": 84.21389799059024,
    "ttft": 1932288.2658183177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2152471368480717,
    "arrivals": 622407,
    "finished_requests": 102286,
    "scheduler_time": 308.5367945498879
}
#Debug simulation 
Total elapsed time: 90.253635707777. Arrivals time: 0.6387888616882265 Scheduler time: 89.37601254926994 Scheduler overhead time: 0.09238892886787653 Adapter cache time: 0.019232552498579025 Engine time: 0.09239884838461876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 33, 17280, 17280, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 17280, 135, 17280, 17280, 33, 17280, 135, 135, 17280, 17280, 135, 33, 33, 135, 17280, 17280, 135, 135, 33, 33, 135, 17280, 17280, 33, 17280, 135, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 135, 33, 17280, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 17280, 135, 33, 33, 135, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 33, 17280, 135, 17280, 17280, 135, 33, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 33, 135, 17280, 17280, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 135, 33, 17280, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 135, 33, 135, 17280, 135, 33, 33, 135, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 135, 17280, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 135, 33, 135, 33, 33, 33, 17280, 17280, 135, 135, 33, 33, 135, 33, 17280, 33, 33, 135, 33, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 135, 33, 135, 33, 135, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 17280, 33, 17280, 33, 33, 33, 17280, 135, 17280, 17280, 33, 135, 135, 135, 33, 135, 33, 135, 135, 17280, 33, 33, 135, 33, 135, 135, 33, 17280, 17280, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 135, 135, 135, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1866903 . Total input tokens: 416447510 . Total output tokens: 366257866
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.87528906902298,
    "estimated_duration": 3600.0735580086634,
    "input_throughput": 7196.497122222871,
    "output_throughput": 6235.570645511232,
    "total_throughput": 13432.067767734103,
    "itl": 87.10762042163856,
    "ttft": 1920303.1605575704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 480,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0642820514738425,
    "arrivals": 622407,
    "finished_requests": 104806,
    "scheduler_time": 300.88976728182934
}
#Debug simulation 
Total elapsed time: 90.8754662447609. Arrivals time: 0.6496471334248781 Scheduler time: 89.9903887459077 Scheduler overhead time: 0.0907111307606101 Adapter cache time: 0.019242487847805023 Engine time: 0.09051355440169573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [106 107 107]
Adapter prompts. [135, 17280, 17280, 33, 17280, 17280, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 17280, 135, 17280, 17280, 33, 17280, 135, 135, 17280, 17280, 135, 33, 33, 135, 17280, 17280, 135, 135, 33, 33, 135, 17280, 17280, 33, 17280, 135, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 135, 33, 17280, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 17280, 135, 33, 33, 135, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 33, 17280, 135, 17280, 17280, 135, 33, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 33, 33, 135, 17280, 17280, 135, 33, 135, 33, 135, 17280, 17280, 135, 33, 135, 33, 17280, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 135, 33, 135, 17280, 135, 33, 33, 135, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 135, 17280, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 33, 135, 17280, 17280, 17280, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 135, 33, 135, 33, 33, 33, 17280, 17280, 135, 135, 33, 33, 135, 33, 17280, 33, 33, 135, 33, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 33, 135, 17280, 135, 135, 17280, 135, 33, 135, 33, 17280, 135, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 135, 33, 135, 33, 135, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 135, 135, 17280, 33, 17280, 33, 33, 33, 17280, 135, 17280, 17280, 33, 135, 135, 135, 33, 135, 33, 135, 135, 17280, 33, 33, 135, 33, 135, 135, 33, 17280, 17280, 17280, 33, 135, 135, 135, 135, 33, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 135, 135, 135, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 135, 17280, 135, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1866903 . Total input tokens: 416447510 . Total output tokens: 366257866
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.43037751410156,
    "estimated_duration": 3600.0160931437154,
    "input_throughput": 7039.557142054228,
    "output_throughput": 6090.218330344765,
    "total_throughput": 13129.775472398993,
    "itl": 84.21338905973593,
    "ttft": 1932275.6336295507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1850080290623253,
    "arrivals": 622407,
    "finished_requests": 102286,
    "scheduler_time": 308.5363067735002
}
#Debug simulation 
Total elapsed time: 90.4305532281287. Arrivals time: 0.6416802708990872 Scheduler time: 89.54805860668421 Scheduler overhead time: 0.09344707429409027 Adapter cache time: 0.01947224186733365 Engine time: 0.09264838602393866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [106 107 107]
Adapter prompts. [66, 17280, 17280, 33, 17280, 17280, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 17280, 66, 17280, 17280, 33, 17280, 66, 66, 17280, 17280, 66, 33, 33, 66, 17280, 17280, 66, 66, 33, 33, 66, 17280, 17280, 33, 17280, 66, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 66, 33, 17280, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 17280, 66, 33, 33, 66, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 33, 17280, 66, 17280, 17280, 66, 33, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 33, 66, 17280, 17280, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 66, 33, 17280, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 66, 33, 66, 17280, 66, 33, 33, 66, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 66, 17280, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 66, 33, 66, 33, 33, 33, 17280, 17280, 66, 66, 33, 33, 66, 33, 17280, 33, 33, 66, 33, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 66, 33, 66, 33, 66, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 17280, 33, 17280, 33, 33, 33, 17280, 66, 17280, 17280, 33, 66, 66, 66, 33, 66, 33, 66, 66, 17280, 33, 33, 66, 33, 66, 66, 33, 17280, 17280, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 66, 66, 66, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1859520 . Total input tokens: 414790763 . Total output tokens: 364817911
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.15388754894957,
    "estimated_duration": 3600.0291914470195,
    "input_throughput": 7237.277148168768,
    "output_throughput": 6300.346689934277,
    "total_throughput": 13537.623838103045,
    "itl": 88.26969833069691,
    "ttft": 1911547.4760095612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.035095745665052,
    "arrivals": 619978,
    "finished_requests": 105330,
    "scheduler_time": 297.31898643208416
}
#Debug simulation 
Total elapsed time: 90.1540659032762. Arrivals time: 0.6420027050189674 Scheduler time: 89.27933044871315 Scheduler overhead time: 0.08896518778055906 Adapter cache time: 0.01931459316983819 Engine time: 0.09026753343641758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [106 107 107]
Adapter prompts. [66, 17280, 17280, 33, 17280, 17280, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 17280, 66, 17280, 17280, 33, 17280, 66, 66, 17280, 17280, 66, 33, 33, 66, 17280, 17280, 66, 66, 33, 33, 66, 17280, 17280, 33, 17280, 66, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 66, 33, 17280, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 17280, 66, 33, 33, 66, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 33, 17280, 66, 17280, 17280, 66, 33, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 33, 66, 17280, 17280, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 66, 33, 17280, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 66, 33, 66, 17280, 66, 33, 33, 66, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 66, 17280, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 66, 33, 66, 33, 33, 33, 17280, 17280, 66, 66, 33, 33, 66, 33, 17280, 33, 33, 66, 33, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 66, 33, 66, 33, 66, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 17280, 33, 17280, 33, 33, 33, 17280, 66, 17280, 17280, 33, 66, 66, 66, 33, 66, 33, 66, 66, 17280, 33, 33, 66, 33, 66, 66, 33, 17280, 17280, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 66, 66, 66, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1859520 . Total input tokens: 414790763 . Total output tokens: 364817911
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 91.62551664700732,
    "estimated_duration": 3600.045199617729,
    "input_throughput": 7198.04434754086,
    "output_throughput": 6266.89853849492,
    "total_throughput": 13464.94288603578,
    "itl": 87.24715843859579,
    "ttft": 1914919.4081909435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.264252215414313,
    "arrivals": 619978,
    "finished_requests": 104784,
    "scheduler_time": 299.0581790615486
}
#Debug simulation 
Total elapsed time: 91.6258141240105. Arrivals time: 0.639275748282671 Scheduler time: 90.74853833625093 Scheduler overhead time: 0.09247902361676097 Adapter cache time: 0.019363828469067812 Engine time: 0.09117776015773416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [106 107 107]
Adapter prompts. [66, 17280, 17280, 33, 17280, 17280, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 17280, 66, 17280, 17280, 33, 17280, 66, 66, 17280, 17280, 66, 33, 33, 66, 17280, 17280, 66, 66, 33, 33, 66, 17280, 17280, 33, 17280, 66, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 66, 33, 17280, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 17280, 66, 33, 33, 66, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 33, 17280, 66, 17280, 17280, 66, 33, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 33, 66, 17280, 17280, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 66, 33, 17280, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 66, 33, 66, 17280, 66, 33, 33, 66, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 66, 17280, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 66, 33, 66, 33, 33, 33, 17280, 17280, 66, 66, 33, 33, 66, 33, 17280, 33, 33, 66, 33, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 66, 33, 66, 33, 66, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 17280, 33, 17280, 33, 33, 33, 17280, 66, 17280, 17280, 33, 66, 66, 66, 33, 66, 33, 66, 66, 17280, 33, 33, 66, 33, 66, 66, 33, 17280, 17280, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 66, 66, 66, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1859520 . Total input tokens: 414790763 . Total output tokens: 364817911
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.5823727240786,
    "estimated_duration": 3600.0085072943843,
    "input_throughput": 7099.447111920404,
    "output_throughput": 6173.512077809824,
    "total_throughput": 13272.959189730229,
    "itl": 84.83804345400718,
    "ttft": 1918485.5773298102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.251339608575242,
    "arrivals": 619978,
    "finished_requests": 103272,
    "scheduler_time": 304.1678158317846
}
#Debug simulation 
Total elapsed time: 90.58253991184756. Arrivals time: 0.6373534398153424 Scheduler time: 89.70677088852972 Scheduler overhead time: 0.09250717656686902 Adapter cache time: 0.019754990935325623 Engine time: 0.09110184339806437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [106 107 107]
Adapter prompts. [66, 17280, 17280, 33, 17280, 17280, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 17280, 66, 17280, 17280, 33, 17280, 66, 66, 17280, 17280, 66, 33, 33, 66, 17280, 17280, 66, 66, 33, 33, 66, 17280, 17280, 33, 17280, 66, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 66, 33, 17280, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 17280, 66, 33, 33, 66, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 33, 17280, 66, 17280, 17280, 66, 33, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 33, 66, 17280, 17280, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 66, 33, 17280, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 66, 33, 66, 17280, 66, 33, 33, 66, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 66, 17280, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 66, 33, 66, 33, 33, 33, 17280, 17280, 66, 66, 33, 33, 66, 33, 17280, 33, 33, 66, 33, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 66, 33, 66, 33, 66, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 17280, 33, 17280, 33, 33, 33, 17280, 66, 17280, 17280, 33, 66, 66, 66, 33, 66, 33, 66, 66, 17280, 33, 33, 66, 33, 66, 66, 33, 17280, 17280, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 66, 66, 66, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1859520 . Total input tokens: 414790763 . Total output tokens: 364817911
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 92.76635727612302,
    "estimated_duration": 3600.022383989846,
    "input_throughput": 7184.694493853168,
    "output_throughput": 6250.785578466412,
    "total_throughput": 13435.48007231958,
    "itl": 87.1374620718954,
    "ttft": 1911160.0571965892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8839270571432976,
    "arrivals": 619978,
    "finished_requests": 104521,
    "scheduler_time": 300.18791191702564
}
#Debug simulation 
Total elapsed time: 92.76653298689052. Arrivals time: 0.6458436450920999 Scheduler time: 91.88149824226275 Scheduler overhead time: 0.09396925196051598 Adapter cache time: 0.019029132556170225 Engine time: 0.09139360301196575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [106 107 107]
Adapter prompts. [66, 17280, 17280, 33, 17280, 17280, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 17280, 66, 17280, 17280, 33, 17280, 66, 66, 17280, 17280, 66, 33, 33, 66, 17280, 17280, 66, 66, 33, 33, 66, 17280, 17280, 33, 17280, 66, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 66, 33, 17280, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 17280, 66, 33, 33, 66, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 33, 17280, 66, 17280, 17280, 66, 33, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 33, 66, 17280, 17280, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 66, 33, 17280, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 66, 33, 66, 17280, 66, 33, 33, 66, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 66, 17280, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 66, 33, 66, 33, 33, 33, 17280, 17280, 66, 66, 33, 33, 66, 33, 17280, 33, 33, 66, 33, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 66, 33, 66, 33, 66, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 17280, 33, 17280, 33, 33, 33, 17280, 66, 17280, 17280, 33, 66, 66, 66, 33, 66, 33, 66, 66, 17280, 33, 33, 66, 33, 66, 66, 33, 17280, 17280, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 66, 66, 66, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1859520 . Total input tokens: 414790763 . Total output tokens: 364817911
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 90.99541390500963,
    "estimated_duration": 3600.075791186231,
    "input_throughput": 7099.322203874648,
    "output_throughput": 6173.428363483672,
    "total_throughput": 13272.75056735832,
    "itl": 84.83763300797268,
    "ttft": 1918456.9054079372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.221928969495954,
    "arrivals": 619978,
    "finished_requests": 103273,
    "scheduler_time": 304.17524360474664
}
#Debug simulation 
Total elapsed time: 90.99558345135301. Arrivals time: 0.6176273585297167 Scheduler time: 90.13663715869188 Scheduler overhead time: 0.09256249945610762 Adapter cache time: 0.019686680752784014 Engine time: 0.09335329383611679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [106 107 107]
Adapter prompts. [66, 17280, 17280, 33, 17280, 17280, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 17280, 66, 17280, 17280, 33, 17280, 66, 66, 17280, 17280, 66, 33, 33, 66, 17280, 17280, 66, 66, 33, 33, 66, 17280, 17280, 33, 17280, 66, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 66, 33, 17280, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 17280, 66, 33, 33, 66, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 33, 17280, 66, 17280, 17280, 66, 33, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 33, 66, 17280, 17280, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 66, 33, 17280, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 66, 33, 66, 17280, 66, 33, 33, 66, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 66, 17280, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 66, 33, 66, 33, 33, 33, 17280, 17280, 66, 66, 33, 33, 66, 33, 17280, 33, 33, 66, 33, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 66, 33, 66, 33, 66, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 17280, 33, 17280, 33, 33, 33, 17280, 66, 17280, 17280, 33, 66, 66, 66, 33, 66, 33, 66, 66, 17280, 33, 33, 66, 33, 66, 66, 33, 17280, 17280, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 66, 66, 66, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1859520 . Total input tokens: 414790763 . Total output tokens: 364817911
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 92.62261537602171,
    "estimated_duration": 3600.034264324943,
    "input_throughput": 7185.170779159348,
    "output_throughput": 6251.279112261442,
    "total_throughput": 13436.449891420789,
    "itl": 87.1336336579684,
    "ttft": 1911043.9177843407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6812467950396126,
    "arrivals": 619978,
    "finished_requests": 104528,
    "scheduler_time": 300.2063742765503
}
#Debug simulation 
Total elapsed time: 92.62278304807842. Arrivals time: 0.6479119965806603 Scheduler time: 91.74040012760088 Scheduler overhead time: 0.09062480693683028 Adapter cache time: 0.019060326740145683 Engine time: 0.09006709745153785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_320_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [106 107 107]
Adapter prompts. [66, 17280, 17280, 33, 17280, 17280, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 17280, 66, 17280, 17280, 33, 17280, 66, 66, 17280, 17280, 66, 33, 33, 66, 17280, 17280, 66, 66, 33, 33, 66, 17280, 17280, 33, 17280, 66, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 33, 17280, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 66, 33, 17280, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 17280, 66, 33, 33, 66, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 33, 17280, 66, 17280, 17280, 66, 33, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 33, 33, 66, 17280, 17280, 66, 33, 66, 33, 66, 17280, 17280, 66, 33, 66, 33, 17280, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 66, 33, 66, 17280, 66, 33, 33, 66, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 66, 17280, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 33, 66, 17280, 17280, 17280, 17280, 66, 66, 17280, 17280, 17280, 66, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 66, 33, 66, 33, 33, 33, 17280, 17280, 66, 66, 33, 33, 66, 33, 17280, 33, 33, 66, 33, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 33, 66, 17280, 66, 66, 17280, 66, 33, 66, 33, 17280, 66, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 33, 33, 17280, 66, 33, 66, 33, 66, 17280, 17280, 17280, 33, 17280, 17280, 17280, 17280, 33, 66, 66, 17280, 33, 17280, 33, 33, 33, 17280, 66, 17280, 17280, 33, 66, 66, 66, 33, 66, 33, 66, 66, 17280, 33, 33, 66, 33, 66, 66, 33, 17280, 17280, 17280, 33, 66, 66, 66, 66, 33, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 66, 66, 66, 33, 33, 17280, 33, 33, 17280, 17280, 17280, 33, 17280, 17280, 66, 17280, 66, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 1859520 . Total input tokens: 414790763 . Total output tokens: 364817911
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 90.80041271494702,
    "estimated_duration": 3600.0436839479294,
    "input_throughput": 7099.385519670174,
    "output_throughput": 6173.483421631018,
    "total_throughput": 13272.868941301193,
    "itl": 84.83701496142129,
    "ttft": 1918444.4198938233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.190654275827134,
    "arrivals": 619978,
    "finished_requests": 103273,
    "scheduler_time": 304.1749167162226
}
#Debug simulation 
Total elapsed time: 90.80058571184054. Arrivals time: 0.6362837459892035 Scheduler time: 89.92737748241052 Scheduler overhead time: 0.09148232499137521 Adapter cache time: 0.019402443431317806 Engine time: 0.09135706955567002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 1080, 8640, 8640, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 8640, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 4320, 1080, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1501200 . Total input tokens: 335039895 . Total output tokens: 294375402
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 91.21399558708072,
    "estimated_duration": 3600.002804726566,
    "input_throughput": 6736.039474236427,
    "output_throughput": 5852.61238472849,
    "total_throughput": 12588.651858964917,
    "itl": 81.29121105671892,
    "ttft": 1918602.007344299,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.073243963681225,
    "arrivals": 500641,
    "finished_requests": 97985,
    "scheduler_time": 316.662802612947
}
#Debug simulation 
Total elapsed time: 91.21416882611811. Arrivals time: 0.6516576306894422 Scheduler time: 90.32049686275423 Scheduler overhead time: 0.09342284128069878 Adapter cache time: 0.02204620884731412 Engine time: 0.09092655079439282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 1080, 8640, 8640, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 8640, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 4320, 1080, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1501200 . Total input tokens: 335039895 . Total output tokens: 294375402
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.88100890209898,
    "estimated_duration": 3600.0274117141685,
    "input_throughput": 6737.127867715715,
    "output_throughput": 5855.636801932697,
    "total_throughput": 12592.764669648412,
    "itl": 80.63464070032283,
    "ttft": 1916692.4909615966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.495281977509154,
    "arrivals": 500641,
    "finished_requests": 98005,
    "scheduler_time": 316.20794406545565
}
#Debug simulation 
Total elapsed time: 89.88118286803365. Arrivals time: 0.6535635814070702 Scheduler time: 88.98355349339545 Scheduler overhead time: 0.0937800775282085 Adapter cache time: 0.02218341687694192 Engine time: 0.0921424082480371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 1080, 8640, 8640, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 8640, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 4320, 1080, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1501200 . Total input tokens: 335039895 . Total output tokens: 294375402
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.54878029227257,
    "estimated_duration": 3600.0021545741492,
    "input_throughput": 6712.491260399959,
    "output_throughput": 5833.44595316893,
    "total_throughput": 12545.93721356889,
    "itl": 79.61277714612632,
    "ttft": 1919891.8399500574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.585884326812841,
    "arrivals": 500641,
    "finished_requests": 97655,
    "scheduler_time": 317.433768510484
}
#Debug simulation 
Total elapsed time: 88.54896300425753. Arrivals time: 0.6660460424609482 Scheduler time: 87.63858769414946 Scheduler overhead time: 0.09421285940334201 Adapter cache time: 0.022116522304713726 Engine time: 0.09259590925648808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 1080, 8640, 8640, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 8640, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 4320, 1080, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1501200 . Total input tokens: 335039895 . Total output tokens: 294375402
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 90.15566226607189,
    "estimated_duration": 3600.043615569003,
    "input_throughput": 6691.663094251824,
    "output_throughput": 5816.952580639814,
    "total_throughput": 12508.615674891638,
    "itl": 79.56569135346425,
    "ttft": 1919747.5702792096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.139916363707737,
    "arrivals": 500641,
    "finished_requests": 97396,
    "scheduler_time": 318.31870082987064
}
#Debug simulation 
Total elapsed time: 90.15583913773298. Arrivals time: 0.6508903047069907 Scheduler time: 89.26213782466948 Scheduler overhead time: 0.09401662228628993 Adapter cache time: 0.022043107077479362 Engine time: 0.09145295852795243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 1080, 8640, 8640, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 8640, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 4320, 1080, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1501200 . Total input tokens: 335039895 . Total output tokens: 294375402
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 89.69831113191321,
    "estimated_duration": 3600.0042246482144,
    "input_throughput": 6697.057418691204,
    "output_throughput": 5823.117055383036,
    "total_throughput": 12520.17447407424,
    "itl": 78.85301105090942,
    "ttft": 1919416.2155930356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.613572690868782,
    "arrivals": 500641,
    "finished_requests": 97453,
    "scheduler_time": 318.03915132155186
}
#Debug simulation 
Total elapsed time: 89.69848831184208. Arrivals time: 0.6477189678698778 Scheduler time: 88.80316991405562 Scheduler overhead time: 0.0949458833783865 Adapter cache time: 0.022599488962441683 Engine time: 0.09381374809890985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 1080, 8640, 8640, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 8640, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 4320, 1080, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1501200 . Total input tokens: 335039895 . Total output tokens: 294375402
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 91.54960224963725,
    "estimated_duration": 3600.0022967932755,
    "input_throughput": 6741.689309925924,
    "output_throughput": 5856.608485716948,
    "total_throughput": 12598.297795642873,
    "itl": 80.64515584842121,
    "ttft": 1913379.7203459106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9133435365697187,
    "arrivals": 500641,
    "finished_requests": 98030,
    "scheduler_time": 316.41655971584623
}
#Debug simulation 
Total elapsed time: 91.54978146683425. Arrivals time: 0.6524438448250294 Scheduler time: 90.65583768254146 Scheduler overhead time: 0.09348948718979955 Adapter cache time: 0.021701494231820107 Engine time: 0.09062647772952914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 1080, 8640, 8640, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 1080, 8640, 4320, 4320, 8640, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 8640, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 1080, 8640, 4320, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 1080, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 4320, 1080, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 4320, 8640, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 1080, 4320, 8640, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 4320, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 8640, 4320, 1080, 4320, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 1080]
Prompts retrieved: 1501200 . Total input tokens: 335039895 . Total output tokens: 294375402
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.38855716818944,
    "estimated_duration": 3600.0312785200367,
    "input_throughput": 6697.15570080034,
    "output_throughput": 5823.116628202742,
    "total_throughput": 12520.27232900308,
    "itl": 78.85170413662689,
    "ttft": 1919453.4272559776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.570906552486154,
    "arrivals": 500641,
    "finished_requests": 97455,
    "scheduler_time": 318.0453930353417
}
#Debug simulation 
Total elapsed time: 89.38874058192596. Arrivals time: 0.649875667411834 Scheduler time: 88.495422730688 Scheduler overhead time: 0.09356942493468523 Adapter cache time: 0.021945804357528687 Engine time: 0.09221529448404908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 540, 8640, 8640, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 4320, 4320, 8640, 4320, 8640, 8640, 540, 8640, 4320, 4320, 8640, 8640, 4320, 540, 540, 4320, 8640, 8640, 4320, 4320, 540, 540, 4320, 8640, 8640, 540, 8640, 4320, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 8640, 4320, 540, 540, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 540, 8640, 4320, 8640, 8640, 4320, 540, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 8640, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 4320, 540, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 4320, 8640, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 4320, 540, 4320, 540, 540, 540, 8640, 8640, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 540, 4320, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 8640, 540, 8640, 540, 540, 540, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1443960 . Total input tokens: 322181269 . Total output tokens: 283168020
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.67162536503747,
    "estimated_duration": 3600.0086361523963,
    "input_throughput": 6707.050576913635,
    "output_throughput": 5892.749197033306,
    "total_throughput": 12599.799773946941,
    "itl": 81.72735123051677,
    "ttft": 1917393.631738747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8351972385310202,
    "arrivals": 481654,
    "finished_requests": 97882,
    "scheduler_time": 314.0130813242916
}
#Debug simulation 
Total elapsed time: 90.67180784605443. Arrivals time: 0.6523742419667542 Scheduler time: 89.77927402872592 Scheduler overhead time: 0.09231715509667993 Adapter cache time: 0.021170133724808693 Engine time: 0.09130036598071456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 540, 8640, 8640, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 4320, 4320, 8640, 4320, 8640, 8640, 540, 8640, 4320, 4320, 8640, 8640, 4320, 540, 540, 4320, 8640, 8640, 4320, 4320, 540, 540, 4320, 8640, 8640, 540, 8640, 4320, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 8640, 4320, 540, 540, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 540, 8640, 4320, 8640, 8640, 4320, 540, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 8640, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 4320, 540, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 4320, 8640, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 4320, 540, 4320, 540, 540, 540, 8640, 8640, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 540, 4320, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 8640, 540, 8640, 540, 540, 540, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1443960 . Total input tokens: 322181269 . Total output tokens: 283168020
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.023332035169,
    "estimated_duration": 3600.0192786321472,
    "input_throughput": 6485.778600850046,
    "output_throughput": 5676.737377852304,
    "total_throughput": 12162.515978702351,
    "itl": 77.66635942653402,
    "ttft": 1958514.794884716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.002870072671217,
    "arrivals": 481654,
    "finished_requests": 94640,
    "scheduler_time": 324.31332915430704
}
#Debug simulation 
Total elapsed time: 88.02350274519995. Arrivals time: 0.6368353650905192 Scheduler time: 87.14325416320935 Scheduler overhead time: 0.09382758988067508 Adapter cache time: 0.0212707556784153 Engine time: 0.09229171695187688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 540, 8640, 8640, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 4320, 4320, 8640, 4320, 8640, 8640, 540, 8640, 4320, 4320, 8640, 8640, 4320, 540, 540, 4320, 8640, 8640, 4320, 4320, 540, 540, 4320, 8640, 8640, 540, 8640, 4320, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 8640, 4320, 540, 540, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 540, 8640, 4320, 8640, 8640, 4320, 540, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 8640, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 4320, 540, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 4320, 8640, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 4320, 540, 4320, 540, 540, 540, 8640, 8640, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 540, 4320, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 8640, 540, 8640, 540, 540, 540, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1443960 . Total input tokens: 322181269 . Total output tokens: 283168020
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.16406487394124,
    "estimated_duration": 3600.0643448556984,
    "input_throughput": 6473.7023473768395,
    "output_throughput": 5664.679585280419,
    "total_throughput": 12138.38193265726,
    "itl": 76.62831325297387,
    "ttft": 1958653.980199373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.110026984368486,
    "arrivals": 481654,
    "finished_requests": 94421,
    "scheduler_time": 325.26950776876
}
#Debug simulation 
Total elapsed time: 88.16424540802836. Arrivals time: 0.6375866131857038 Scheduler time: 87.28173110168427 Scheduler overhead time: 0.09490534476935863 Adapter cache time: 0.02134996885433793 Engine time: 0.09294967819005251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 540, 8640, 8640, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 4320, 4320, 8640, 4320, 8640, 8640, 540, 8640, 4320, 4320, 8640, 8640, 4320, 540, 540, 4320, 8640, 8640, 4320, 4320, 540, 540, 4320, 8640, 8640, 540, 8640, 4320, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 8640, 4320, 540, 540, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 540, 8640, 4320, 8640, 8640, 4320, 540, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 8640, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 4320, 540, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 4320, 8640, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 4320, 540, 4320, 540, 540, 540, 8640, 8640, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 540, 4320, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 8640, 540, 8640, 540, 540, 540, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1443960 . Total input tokens: 322181269 . Total output tokens: 283168020
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 88.26769063668326,
    "estimated_duration": 3600.077345178264,
    "input_throughput": 6485.91815152899,
    "output_throughput": 5676.804424041626,
    "total_throughput": 12162.722575570617,
    "itl": 77.66172331191085,
    "ttft": 1958408.7458612022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.744660971635015,
    "arrivals": 481654,
    "finished_requests": 94642,
    "scheduler_time": 324.33561154111953
}
#Debug simulation 
Total elapsed time: 88.26787018403411. Arrivals time: 0.6454453580081463 Scheduler time: 87.37828621640801 Scheduler overhead time: 0.09492752887308598 Adapter cache time: 0.021313003730028868 Engine time: 0.0916914064437151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 540, 8640, 8640, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 4320, 4320, 8640, 4320, 8640, 8640, 540, 8640, 4320, 4320, 8640, 8640, 4320, 540, 540, 4320, 8640, 8640, 4320, 4320, 540, 540, 4320, 8640, 8640, 540, 8640, 4320, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 8640, 4320, 540, 540, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 540, 8640, 4320, 8640, 8640, 4320, 540, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 8640, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 4320, 540, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 4320, 8640, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 4320, 540, 4320, 540, 540, 540, 8640, 8640, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 540, 4320, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 8640, 540, 8640, 540, 540, 540, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1443960 . Total input tokens: 322181269 . Total output tokens: 283168020
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 88.27984662307426,
    "estimated_duration": 3600.0551747071077,
    "input_throughput": 6473.718837349792,
    "output_throughput": 5664.694014490805,
    "total_throughput": 12138.412851840596,
    "itl": 76.62732271007809,
    "ttft": 1958660.3264873743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.071710306694766,
    "arrivals": 481654,
    "finished_requests": 94421,
    "scheduler_time": 325.27441491546904
}
#Debug simulation 
Total elapsed time: 88.28011014824733. Arrivals time: 0.6395749961957335 Scheduler time: 87.39571194630116 Scheduler overhead time: 0.09482281282544136 Adapter cache time: 0.021321043372154236 Engine time: 0.09260752610862255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 540, 8640, 8640, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 4320, 4320, 8640, 4320, 8640, 8640, 540, 8640, 4320, 4320, 8640, 8640, 4320, 540, 540, 4320, 8640, 8640, 4320, 4320, 540, 540, 4320, 8640, 8640, 540, 8640, 4320, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 8640, 4320, 540, 540, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 540, 8640, 4320, 8640, 8640, 4320, 540, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 8640, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 4320, 540, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 4320, 8640, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 4320, 540, 4320, 540, 540, 540, 8640, 8640, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 540, 4320, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 8640, 540, 8640, 540, 540, 540, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1443960 . Total input tokens: 322181269 . Total output tokens: 283168020
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.22561762202531,
    "estimated_duration": 3600.049239642028,
    "input_throughput": 6687.767971305313,
    "output_throughput": 5850.980805499896,
    "total_throughput": 12538.748776805209,
    "itl": 80.57187798976041,
    "ttft": 1930961.615456165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7154419874120332,
    "arrivals": 481654,
    "finished_requests": 97516,
    "scheduler_time": 314.94885626353437
}
#Debug simulation 
Total elapsed time: 90.22579139005393. Arrivals time: 0.6596229779534042 Scheduler time: 89.32217952935025 Scheduler overhead time: 0.09426356013864279 Adapter cache time: 0.02140757953748107 Engine time: 0.09261781768873334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 540, 8640, 8640, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 4320, 4320, 8640, 4320, 8640, 8640, 540, 8640, 4320, 4320, 8640, 8640, 4320, 540, 540, 4320, 8640, 8640, 4320, 4320, 540, 540, 4320, 8640, 8640, 540, 8640, 4320, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 8640, 4320, 540, 540, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 540, 8640, 4320, 8640, 8640, 4320, 540, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 540, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 4320, 540, 4320, 540, 8640, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 4320, 540, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 4320, 8640, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 540, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 4320, 540, 4320, 540, 540, 540, 8640, 8640, 4320, 4320, 540, 540, 4320, 540, 8640, 540, 540, 4320, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 540, 4320, 8640, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 4320, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 4320, 540, 4320, 540, 4320, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 4320, 4320, 8640, 540, 8640, 540, 540, 540, 8640, 4320, 8640, 8640, 540, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 540, 540, 4320, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 4320, 8640, 4320, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1443960 . Total input tokens: 322181269 . Total output tokens: 283168020
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 87.98211017763242,
    "estimated_duration": 3600.0171032404523,
    "input_throughput": 6473.787299238662,
    "output_throughput": 5664.75392065322,
    "total_throughput": 12138.541219891882,
    "itl": 76.62666643460032,
    "ttft": 1958644.1694710823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.034014980550892,
    "arrivals": 481654,
    "finished_requests": 94421,
    "scheduler_time": 325.27403877495806
}
#Debug simulation 
Total elapsed time: 87.9822993897833. Arrivals time: 0.6555620580911636 Scheduler time: 87.07765355333686 Scheduler overhead time: 0.096755834762007 Adapter cache time: 0.021656298078596592 Engine time: 0.09383538225665689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 270, 8640, 8640, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 4320, 4320, 8640, 4320, 8640, 8640, 270, 8640, 4320, 4320, 8640, 8640, 4320, 270, 270, 4320, 8640, 8640, 4320, 4320, 270, 270, 4320, 8640, 8640, 270, 8640, 4320, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 8640, 4320, 270, 270, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 270, 8640, 4320, 8640, 8640, 4320, 270, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 8640, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 4320, 270, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 4320, 8640, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 4320, 270, 4320, 270, 270, 270, 8640, 8640, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 270, 4320, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 270, 270, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 8640, 270, 8640, 270, 270, 270, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1415340 . Total input tokens: 315827385 . Total output tokens: 277487323
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 89.64716099901125,
    "estimated_duration": 3600.012388091587,
    "input_throughput": 6752.767596137931,
    "output_throughput": 5904.9882912400635,
    "total_throughput": 12657.755887377994,
    "itl": 82.1266545492181,
    "ttft": 1910036.614843652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7492359211156683,
    "arrivals": 472156,
    "finished_requests": 98433,
    "scheduler_time": 313.36507256985567
}
#Debug simulation 
Total elapsed time: 89.64732972998172. Arrivals time: 0.6488715852610767 Scheduler time: 88.75775101734325 Scheduler overhead time: 0.09319935971871018 Adapter cache time: 0.02076731389388442 Engine time: 0.09109536744654179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 270, 8640, 8640, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 4320, 4320, 8640, 4320, 8640, 8640, 270, 8640, 4320, 4320, 8640, 8640, 4320, 270, 270, 4320, 8640, 8640, 4320, 4320, 270, 270, 4320, 8640, 8640, 270, 8640, 4320, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 8640, 4320, 270, 270, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 270, 8640, 4320, 8640, 8640, 4320, 270, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 8640, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 4320, 270, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 4320, 8640, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 4320, 270, 4320, 270, 270, 270, 8640, 8640, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 270, 4320, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 270, 270, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 8640, 270, 8640, 270, 270, 270, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1415340 . Total input tokens: 315827385 . Total output tokens: 277487323
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.33263565599918,
    "estimated_duration": 3600.012723867529,
    "input_throughput": 6565.880682389996,
    "output_throughput": 5727.108369175496,
    "total_throughput": 12292.989051565492,
    "itl": 80.39191675326573,
    "ttft": 1925226.2217265146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9498533002473466,
    "arrivals": 472156,
    "finished_requests": 95713,
    "scheduler_time": 321.7956779414218
}
#Debug simulation 
Total elapsed time: 89.33282211283222. Arrivals time: 0.6419768328778446 Scheduler time: 88.44687088718638 Scheduler overhead time: 0.09437142964452505 Adapter cache time: 0.021125111263245344 Engine time: 0.09251911798492074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 270, 8640, 8640, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 4320, 4320, 8640, 4320, 8640, 8640, 270, 8640, 4320, 4320, 8640, 8640, 4320, 270, 270, 4320, 8640, 8640, 4320, 4320, 270, 270, 4320, 8640, 8640, 270, 8640, 4320, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 8640, 4320, 270, 270, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 270, 8640, 4320, 8640, 8640, 4320, 270, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 8640, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 4320, 270, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 4320, 8640, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 4320, 270, 4320, 270, 270, 270, 8640, 8640, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 270, 4320, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 270, 270, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 8640, 270, 8640, 270, 270, 270, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1415340 . Total input tokens: 315827385 . Total output tokens: 277487323
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.10388632584363,
    "estimated_duration": 3600.0466864048076,
    "input_throughput": 6598.155821062876,
    "output_throughput": 5755.712579575709,
    "total_throughput": 12353.868400638585,
    "itl": 79.2144832527286,
    "ttft": 1928114.405695623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.050076857963611,
    "arrivals": 472156,
    "finished_requests": 96187,
    "scheduler_time": 320.44141502342865
}
#Debug simulation 
Total elapsed time: 89.10406425595284. Arrivals time: 0.6400925675407052 Scheduler time: 88.21797650167719 Scheduler overhead time: 0.09532822947949171 Adapter cache time: 0.021164884325116873 Engine time: 0.09311847109347582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 270, 8640, 8640, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 4320, 4320, 8640, 4320, 8640, 8640, 270, 8640, 4320, 4320, 8640, 8640, 4320, 270, 270, 4320, 8640, 8640, 4320, 4320, 270, 270, 4320, 8640, 8640, 270, 8640, 4320, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 8640, 4320, 270, 270, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 270, 8640, 4320, 8640, 8640, 4320, 270, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 8640, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 4320, 270, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 4320, 8640, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 4320, 270, 4320, 270, 270, 270, 8640, 8640, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 270, 4320, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 270, 270, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 8640, 270, 8640, 270, 270, 270, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1415340 . Total input tokens: 315827385 . Total output tokens: 277487323
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 88.12709784600884,
    "estimated_duration": 3600.0269946584035,
    "input_throughput": 6488.333847123386,
    "output_throughput": 5653.737605356844,
    "total_throughput": 12142.07145248023,
    "itl": 77.72747961549022,
    "ttft": 1936890.3260139541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.615037147924298,
    "arrivals": 472156,
    "finished_requests": 94578,
    "scheduler_time": 325.7448912014002
}
#Debug simulation 
Total elapsed time: 88.12728343205526. Arrivals time: 0.631707280408591 Scheduler time: 87.25071595003828 Scheduler overhead time: 0.0949643487110734 Adapter cache time: 0.02152358926832676 Engine time: 0.09255706192925572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 270, 8640, 8640, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 4320, 4320, 8640, 4320, 8640, 8640, 270, 8640, 4320, 4320, 8640, 8640, 4320, 270, 270, 4320, 8640, 8640, 4320, 4320, 270, 270, 4320, 8640, 8640, 270, 8640, 4320, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 8640, 4320, 270, 270, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 270, 8640, 4320, 8640, 8640, 4320, 270, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 8640, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 4320, 270, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 4320, 8640, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 4320, 270, 4320, 270, 270, 270, 8640, 8640, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 270, 4320, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 270, 270, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 8640, 270, 8640, 270, 270, 270, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1415340 . Total input tokens: 315827385 . Total output tokens: 277487323
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 89.14042117772624,
    "estimated_duration": 3600.0094396671184,
    "input_throughput": 6598.224087489179,
    "output_throughput": 5755.772129840857,
    "total_throughput": 12353.996217330036,
    "itl": 79.21366371992045,
    "ttft": 1928099.4220935616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.012795766172966,
    "arrivals": 472156,
    "finished_requests": 96187,
    "scheduler_time": 320.4412471150864
}
#Debug simulation 
Total elapsed time: 89.14061514381319. Arrivals time: 0.646732535213232 Scheduler time: 88.2465779306367 Scheduler overhead time: 0.09580628713592887 Adapter cache time: 0.02153656305745244 Engine time: 0.09351401356980205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 270, 8640, 8640, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 4320, 4320, 8640, 4320, 8640, 8640, 270, 8640, 4320, 4320, 8640, 8640, 4320, 270, 270, 4320, 8640, 8640, 4320, 4320, 270, 270, 4320, 8640, 8640, 270, 8640, 4320, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 8640, 4320, 270, 270, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 270, 8640, 4320, 8640, 8640, 4320, 270, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 8640, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 4320, 270, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 4320, 8640, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 4320, 270, 4320, 270, 270, 270, 8640, 8640, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 270, 4320, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 270, 270, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 8640, 270, 8640, 270, 270, 270, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1415340 . Total input tokens: 315827385 . Total output tokens: 277487323
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 89.99745062692091,
    "estimated_duration": 3600.09846396821,
    "input_throughput": 6675.10715068381,
    "output_throughput": 5821.1577293639975,
    "total_throughput": 12496.264880047807,
    "itl": 80.18546960299213,
    "ttft": 1919397.482111117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4983886754326363,
    "arrivals": 472156,
    "finished_requests": 97276,
    "scheduler_time": 316.9081360595295
}
#Debug simulation 
Total elapsed time: 89.99763960810378. Arrivals time: 0.6361797428689897 Scheduler time: 89.12028775224462 Scheduler overhead time: 0.09285979252308607 Adapter cache time: 0.021053442265838385 Engine time: 0.09156437451019883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 270, 8640, 8640, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 4320, 4320, 8640, 4320, 8640, 8640, 270, 8640, 4320, 4320, 8640, 8640, 4320, 270, 270, 4320, 8640, 8640, 4320, 4320, 270, 270, 4320, 8640, 8640, 270, 8640, 4320, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 8640, 4320, 270, 270, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 270, 8640, 4320, 8640, 8640, 4320, 270, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 270, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 4320, 270, 4320, 270, 8640, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 4320, 270, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 4320, 8640, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 270, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 4320, 270, 4320, 270, 270, 270, 8640, 8640, 4320, 4320, 270, 270, 4320, 270, 8640, 270, 270, 4320, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 270, 4320, 8640, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 4320, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 270, 270, 8640, 4320, 270, 4320, 270, 4320, 8640, 8640, 8640, 270, 8640, 8640, 8640, 8640, 270, 4320, 4320, 8640, 270, 8640, 270, 270, 270, 8640, 4320, 8640, 8640, 270, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 270, 270, 4320, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 270, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 8640, 8640, 4320, 8640, 4320, 270, 8640, 270, 8640, 8640, 270, 270, 270]
Prompts retrieved: 1415340 . Total input tokens: 315827385 . Total output tokens: 277487323
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.90483017126098,
    "estimated_duration": 3600.003759283803,
    "input_throughput": 6509.317647118778,
    "output_throughput": 5680.05212418668,
    "total_throughput": 12189.369771305459,
    "itl": 77.79943941347082,
    "ttft": 1934083.7295531516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.836930565517429,
    "arrivals": 472156,
    "finished_requests": 94869,
    "scheduler_time": 324.7583711321232
}
#Debug simulation 
Total elapsed time: 88.90500216325745. Arrivals time: 0.605766586959362 Scheduler time: 88.05425985576585 Scheduler overhead time: 0.09440522501245141 Adapter cache time: 0.020475606434047222 Engine time: 0.09412368899211287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 135, 8640, 8640, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 4320, 4320, 8640, 4320, 8640, 8640, 135, 8640, 4320, 4320, 8640, 8640, 4320, 135, 135, 4320, 8640, 8640, 4320, 4320, 135, 135, 4320, 8640, 8640, 135, 8640, 4320, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 8640, 4320, 135, 135, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 135, 8640, 4320, 8640, 8640, 4320, 135, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 8640, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 4320, 135, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 4320, 8640, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 4320, 135, 4320, 135, 135, 135, 8640, 8640, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 135, 4320, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 135, 135, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 8640, 135, 8640, 135, 135, 135, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1401030 . Total input tokens: 312633146 . Total output tokens: 274692781
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.72378538688645,
    "estimated_duration": 3600.0595968107223,
    "input_throughput": 6767.296580751827,
    "output_throughput": 5914.544031121908,
    "total_throughput": 12681.840611873735,
    "itl": 82.92804842995935,
    "ttft": 1914674.0522421834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8550344656268707,
    "arrivals": 467317,
    "finished_requests": 98989,
    "scheduler_time": 312.04638566172383
}
#Debug simulation 
Total elapsed time: 90.72397191403434. Arrivals time: 0.6548362988978624 Scheduler time: 89.8252653288655 Scheduler overhead time: 0.0939853466115892 Adapter cache time: 0.021267217583954334 Engine time: 0.09288119105622172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 135, 8640, 8640, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 4320, 4320, 8640, 4320, 8640, 8640, 135, 8640, 4320, 4320, 8640, 8640, 4320, 135, 135, 4320, 8640, 8640, 4320, 4320, 135, 135, 4320, 8640, 8640, 135, 8640, 4320, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 8640, 4320, 135, 135, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 135, 8640, 4320, 8640, 8640, 4320, 135, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 8640, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 4320, 135, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 4320, 8640, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 4320, 135, 4320, 135, 135, 135, 8640, 8640, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 135, 4320, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 135, 135, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 8640, 135, 8640, 135, 135, 135, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1401030 . Total input tokens: 312633146 . Total output tokens: 274692781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 91.14456446189433,
    "estimated_duration": 3600.012346992625,
    "input_throughput": 6737.17606003802,
    "output_throughput": 5884.750094731661,
    "total_throughput": 12621.92615476968,
    "itl": 82.10202819952889,
    "ttft": 1915229.0845786072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.068097503050236,
    "arrivals": 467317,
    "finished_requests": 98598,
    "scheduler_time": 313.09488690259974
}
#Debug simulation 
Total elapsed time: 91.14474253216758. Arrivals time: 0.66051693726331 Scheduler time: 90.23990937555209 Scheduler overhead time: 0.09427034482359886 Adapter cache time: 0.021239245776087046 Engine time: 0.09344027377665043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 135, 8640, 8640, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 4320, 4320, 8640, 4320, 8640, 8640, 135, 8640, 4320, 4320, 8640, 8640, 4320, 135, 135, 4320, 8640, 8640, 4320, 4320, 135, 135, 4320, 8640, 8640, 135, 8640, 4320, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 8640, 4320, 135, 135, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 135, 8640, 4320, 8640, 8640, 4320, 135, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 8640, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 4320, 135, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 4320, 8640, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 4320, 135, 4320, 135, 135, 135, 8640, 8640, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 135, 4320, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 135, 135, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 8640, 135, 8640, 135, 135, 135, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1401030 . Total input tokens: 312633146 . Total output tokens: 274692781
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.24316738918424,
    "estimated_duration": 3600.0405873688424,
    "input_throughput": 6728.970246889628,
    "output_throughput": 5881.7139657516245,
    "total_throughput": 12610.684212641252,
    "itl": 81.18398230056167,
    "ttft": 1913102.2755990587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.196687026484905,
    "arrivals": 467317,
    "finished_requests": 98452,
    "scheduler_time": 314.0362632172701
}
#Debug simulation 
Total elapsed time: 89.24334550090134. Arrivals time: 0.6590654430910945 Scheduler time: 88.3400901351124 Scheduler overhead time: 0.0934034283272922 Adapter cache time: 0.0213138060644269 Engine time: 0.0937961763702333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 135, 8640, 8640, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 4320, 4320, 8640, 4320, 8640, 8640, 135, 8640, 4320, 4320, 8640, 8640, 4320, 135, 135, 4320, 8640, 8640, 4320, 4320, 135, 135, 4320, 8640, 8640, 135, 8640, 4320, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 8640, 4320, 135, 135, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 135, 8640, 4320, 8640, 8640, 4320, 135, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 8640, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 4320, 135, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 4320, 8640, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 4320, 135, 4320, 135, 135, 135, 8640, 8640, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 135, 4320, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 135, 135, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 8640, 135, 8640, 135, 135, 135, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1401030 . Total input tokens: 312633146 . Total output tokens: 274692781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 91.69430440710858,
    "estimated_duration": 3600.0617683904356,
    "input_throughput": 6737.86800353846,
    "output_throughput": 5885.324298053032,
    "total_throughput": 12623.192301591493,
    "itl": 82.0985316931787,
    "ttft": 1915269.0482966208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8126648439606594,
    "arrivals": 467317,
    "finished_requests": 98609,
    "scheduler_time": 313.1174789208824
}
#Debug simulation 
Total elapsed time: 91.69448677822948. Arrivals time: 0.6406101118773222 Scheduler time: 90.80799871683121 Scheduler overhead time: 0.0941355936229229 Adapter cache time: 0.0213815625756979 Engine time: 0.09439485659822822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 135, 8640, 8640, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 4320, 4320, 8640, 4320, 8640, 8640, 135, 8640, 4320, 4320, 8640, 8640, 4320, 135, 135, 4320, 8640, 8640, 4320, 4320, 135, 135, 4320, 8640, 8640, 135, 8640, 4320, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 8640, 4320, 135, 135, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 135, 8640, 4320, 8640, 8640, 4320, 135, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 8640, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 4320, 135, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 4320, 8640, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 4320, 135, 4320, 135, 135, 135, 8640, 8640, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 135, 4320, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 135, 135, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 8640, 135, 8640, 135, 135, 135, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1401030 . Total input tokens: 312633146 . Total output tokens: 274692781
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 88.95367387589067,
    "estimated_duration": 3600.0321786658274,
    "input_throughput": 6728.9859639470305,
    "output_throughput": 5881.7277038471475,
    "total_throughput": 12610.713667794178,
    "itl": 81.1827330203737,
    "ttft": 1913109.2911243422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1585774659878005,
    "arrivals": 467317,
    "finished_requests": 98452,
    "scheduler_time": 314.0413201674925
}
#Debug simulation 
Total elapsed time: 88.95384577894583. Arrivals time: 0.6506499643437564 Scheduler time: 88.05855890642852 Scheduler overhead time: 0.09409823035821319 Adapter cache time: 0.021264738868921995 Engine time: 0.09335966687649488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 135, 8640, 8640, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 4320, 4320, 8640, 4320, 8640, 8640, 135, 8640, 4320, 4320, 8640, 8640, 4320, 135, 135, 4320, 8640, 8640, 4320, 4320, 135, 135, 4320, 8640, 8640, 135, 8640, 4320, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 8640, 4320, 135, 135, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 135, 8640, 4320, 8640, 8640, 4320, 135, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 8640, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 4320, 135, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 4320, 8640, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 4320, 135, 4320, 135, 135, 135, 8640, 8640, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 135, 4320, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 135, 135, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 8640, 135, 8640, 135, 135, 135, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1401030 . Total input tokens: 312633146 . Total output tokens: 274692781
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 92.04591399710625,
    "estimated_duration": 3600.035230587716,
    "input_throughput": 6738.527110480433,
    "output_throughput": 5886.212951460639,
    "total_throughput": 12624.740061941071,
    "itl": 82.09486359183217,
    "ttft": 1915185.3451048674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.555843963897771,
    "arrivals": 467317,
    "finished_requests": 98619,
    "scheduler_time": 313.1352805026229
}
#Debug simulation 
Total elapsed time: 92.04609385784715. Arrivals time: 0.6388587607070804 Scheduler time: 91.16451650811359 Scheduler overhead time: 0.09318336844444275 Adapter cache time: 0.02071951050311327 Engine time: 0.09344895370304585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 135, 8640, 8640, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 4320, 4320, 8640, 4320, 8640, 8640, 135, 8640, 4320, 4320, 8640, 8640, 4320, 135, 135, 4320, 8640, 8640, 4320, 4320, 135, 135, 4320, 8640, 8640, 135, 8640, 4320, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 8640, 4320, 135, 135, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 135, 8640, 4320, 8640, 8640, 4320, 135, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 135, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 4320, 135, 4320, 135, 8640, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 4320, 135, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 4320, 8640, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 135, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 4320, 135, 4320, 135, 135, 135, 8640, 8640, 4320, 4320, 135, 135, 4320, 135, 8640, 135, 135, 4320, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 135, 4320, 8640, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 4320, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 135, 135, 8640, 4320, 135, 4320, 135, 4320, 8640, 8640, 8640, 135, 8640, 8640, 8640, 8640, 135, 4320, 4320, 8640, 135, 8640, 135, 135, 135, 8640, 4320, 8640, 8640, 135, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 135, 135, 4320, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 135, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 8640, 8640, 4320, 8640, 4320, 135, 8640, 135, 8640, 8640, 135, 135, 135]
Prompts retrieved: 1401030 . Total input tokens: 312633146 . Total output tokens: 274692781
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 89.12453406397253,
    "estimated_duration": 3600.050191654212,
    "input_throughput": 6728.968683869615,
    "output_throughput": 5881.757995787922,
    "total_throughput": 12610.726679657537,
    "itl": 81.18184432356938,
    "ttft": 1913084.9871852624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.119432319607623,
    "arrivals": 467317,
    "finished_requests": 98453,
    "scheduler_time": 314.0471357524533
}
#Debug simulation 
Total elapsed time: 89.12471369188279. Arrivals time: 0.6468806420452893 Scheduler time: 88.23299195431173 Scheduler overhead time: 0.09370178077369928 Adapter cache time: 0.021268253680318594 Engine time: 0.09335294738411903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 66, 8640, 8640, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 4320, 4320, 8640, 4320, 8640, 8640, 66, 8640, 4320, 4320, 8640, 8640, 4320, 66, 66, 4320, 8640, 8640, 4320, 4320, 66, 66, 4320, 8640, 8640, 66, 8640, 4320, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 8640, 4320, 66, 66, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 66, 8640, 4320, 8640, 8640, 4320, 66, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 8640, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 4320, 66, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 4320, 8640, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 4320, 66, 4320, 66, 66, 66, 8640, 8640, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 66, 4320, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 66, 66, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 8640, 66, 8640, 66, 66, 66, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1393716 . Total input tokens: 310988146 . Total output tokens: 273253532
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 90.10518609406427,
    "estimated_duration": 3600.010095152352,
    "input_throughput": 6794.436224758657,
    "output_throughput": 5931.524755653864,
    "total_throughput": 12725.960980412521,
    "itl": 84.12816160354245,
    "ttft": 1901456.7385270526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6831118307961668,
    "arrivals": 464889,
    "finished_requests": 99300,
    "scheduler_time": 310.43371978334187
}
#Debug simulation 
Total elapsed time: 90.10545199038461. Arrivals time: 0.6415697908960283 Scheduler time: 89.22328414907679 Scheduler overhead time: 0.09297625301405787 Adapter cache time: 0.020728119648993015 Engine time: 0.09188134456053376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 66, 8640, 8640, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 4320, 4320, 8640, 4320, 8640, 8640, 66, 8640, 4320, 4320, 8640, 8640, 4320, 66, 66, 4320, 8640, 8640, 4320, 4320, 66, 66, 4320, 8640, 8640, 66, 8640, 4320, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 8640, 4320, 66, 66, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 66, 8640, 4320, 8640, 8640, 4320, 66, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 8640, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 4320, 66, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 4320, 8640, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 4320, 66, 4320, 66, 66, 66, 8640, 8640, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 66, 4320, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 66, 66, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 8640, 66, 8640, 66, 66, 66, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1393716 . Total input tokens: 310988146 . Total output tokens: 273253532
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.5308206146583,
    "estimated_duration": 3600.048404640016,
    "input_throughput": 6653.196098454971,
    "output_throughput": 5804.075848832753,
    "total_throughput": 12457.271947287725,
    "itl": 80.4145820706422,
    "ttft": 1926003.3967689923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.856587597280746,
    "arrivals": 464889,
    "finished_requests": 97166,
    "scheduler_time": 317.5388597855221
}
#Debug simulation 
Total elapsed time: 88.53100680466741. Arrivals time: 0.6486256108619273 Scheduler time: 87.6418148232624 Scheduler overhead time: 0.09255776787176728 Adapter cache time: 0.020764296408742666 Engine time: 0.09192601311951876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 66, 8640, 8640, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 4320, 4320, 8640, 4320, 8640, 8640, 66, 8640, 4320, 4320, 8640, 8640, 4320, 66, 66, 4320, 8640, 8640, 4320, 4320, 66, 66, 4320, 8640, 8640, 66, 8640, 4320, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 8640, 4320, 66, 66, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 66, 8640, 4320, 8640, 8640, 4320, 66, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 8640, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 4320, 66, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 4320, 8640, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 4320, 66, 4320, 66, 66, 66, 8640, 8640, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 66, 4320, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 66, 66, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 8640, 66, 8640, 66, 66, 66, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1393716 . Total input tokens: 310988146 . Total output tokens: 273253532
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 87.02900419989601,
    "estimated_duration": 3600.0359518657615,
    "input_throughput": 6616.030039271154,
    "output_throughput": 5772.734294286549,
    "total_throughput": 12388.764333557703,
    "itl": 79.32485090744042,
    "ttft": 1928969.473573436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.935770982936991,
    "arrivals": 464889,
    "finished_requests": 96559,
    "scheduler_time": 319.0387966312869
}
#Debug simulation 
Total elapsed time: 87.02918542781845. Arrivals time: 0.6139516443945467 Scheduler time: 86.1706103850156 Scheduler overhead time: 0.09478532988578081 Adapter cache time: 0.020661527756601572 Engine time: 0.09328973339870572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 66, 8640, 8640, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 4320, 4320, 8640, 4320, 8640, 8640, 66, 8640, 4320, 4320, 8640, 8640, 4320, 66, 66, 4320, 8640, 8640, 4320, 4320, 66, 66, 4320, 8640, 8640, 66, 8640, 4320, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 8640, 4320, 66, 66, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 66, 8640, 4320, 8640, 8640, 4320, 66, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 8640, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 4320, 66, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 4320, 8640, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 4320, 66, 4320, 66, 66, 66, 8640, 8640, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 66, 4320, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 66, 66, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 8640, 66, 8640, 66, 66, 66, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1393716 . Total input tokens: 310988146 . Total output tokens: 273253532
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 88.9880285738036,
    "estimated_duration": 3600.048457903111,
    "input_throughput": 6786.443095332783,
    "output_throughput": 5936.134540935434,
    "total_throughput": 12722.577636268217,
    "itl": 82.89452081901477,
    "ttft": 1894846.1376765957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.809057364966715,
    "arrivals": 464889,
    "finished_requests": 99137,
    "scheduler_time": 311.4023826761164
}
#Debug simulation 
Total elapsed time: 88.98821389209479. Arrivals time: 0.6459070201963186 Scheduler time: 88.10145826498047 Scheduler overhead time: 0.09307820862159133 Adapter cache time: 0.02094737533479929 Engine time: 0.09128582337871194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 66, 8640, 8640, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 4320, 4320, 8640, 4320, 8640, 8640, 66, 8640, 4320, 4320, 8640, 8640, 4320, 66, 66, 4320, 8640, 8640, 4320, 4320, 66, 66, 4320, 8640, 8640, 66, 8640, 4320, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 8640, 4320, 66, 66, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 66, 8640, 4320, 8640, 8640, 4320, 66, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 8640, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 4320, 66, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 4320, 8640, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 4320, 66, 4320, 66, 66, 66, 8640, 8640, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 66, 4320, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 66, 66, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 8640, 66, 8640, 66, 66, 66, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1393716 . Total input tokens: 310988146 . Total output tokens: 273253532
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 87.07782436581329,
    "estimated_duration": 3600.0699006508275,
    "input_throughput": 6616.064592438631,
    "output_throughput": 5772.803743683671,
    "total_throughput": 12388.868336122303,
    "itl": 79.32449818400698,
    "ttft": 1928940.104189866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.899318359852804,
    "arrivals": 464889,
    "finished_requests": 96561,
    "scheduler_time": 319.04522036488584
}
#Debug simulation 
Total elapsed time: 87.07801754586399. Arrivals time: 0.6206282274797559 Scheduler time: 86.2136953426525 Scheduler overhead time: 0.09468567278236151 Adapter cache time: 0.020585632417351007 Engine time: 0.09281460242345929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 66, 8640, 8640, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 4320, 4320, 8640, 4320, 8640, 8640, 66, 8640, 4320, 4320, 8640, 8640, 4320, 66, 66, 4320, 8640, 8640, 4320, 4320, 66, 66, 4320, 8640, 8640, 66, 8640, 4320, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 8640, 4320, 66, 66, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 66, 8640, 4320, 8640, 8640, 4320, 66, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 8640, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 4320, 66, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 4320, 8640, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 4320, 66, 4320, 66, 66, 66, 8640, 8640, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 66, 4320, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 66, 66, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 8640, 66, 8640, 66, 66, 66, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1393716 . Total input tokens: 310988146 . Total output tokens: 273253532
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 89.30712801218033,
    "estimated_duration": 3600.002006971324,
    "input_throughput": 6786.946494109169,
    "output_throughput": 5936.2900238989305,
    "total_throughput": 12723.2365180081,
    "itl": 82.88770154419632,
    "ttft": 1895025.4634300359,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5494600429572003,
    "arrivals": 464889,
    "finished_requests": 99141,
    "scheduler_time": 311.4198301565179
}
#Debug simulation 
Total elapsed time: 89.30730622122064. Arrivals time: 0.6367269554175436 Scheduler time: 88.43127969140187 Scheduler overhead time: 0.09183404874056578 Adapter cache time: 0.02042242418974638 Engine time: 0.09116067690774798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 66, 8640, 8640, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 4320, 4320, 8640, 4320, 8640, 8640, 66, 8640, 4320, 4320, 8640, 8640, 4320, 66, 66, 4320, 8640, 8640, 4320, 4320, 66, 66, 4320, 8640, 8640, 66, 8640, 4320, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 8640, 4320, 66, 66, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 66, 8640, 4320, 8640, 8640, 4320, 66, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 66, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 4320, 66, 4320, 66, 8640, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 4320, 66, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 4320, 8640, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 66, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 4320, 66, 4320, 66, 66, 66, 8640, 8640, 4320, 4320, 66, 66, 4320, 66, 8640, 66, 66, 4320, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 66, 4320, 8640, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 4320, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 66, 66, 8640, 4320, 66, 4320, 66, 4320, 8640, 8640, 8640, 66, 8640, 8640, 8640, 8640, 66, 4320, 4320, 8640, 66, 8640, 66, 66, 66, 8640, 4320, 8640, 8640, 66, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 66, 66, 4320, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 66, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 8640, 8640, 4320, 8640, 4320, 66, 8640, 66, 8640, 8640, 66, 66, 66]
Prompts retrieved: 1393716 . Total input tokens: 310988146 . Total output tokens: 273253532
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 85.79398297378793,
    "estimated_duration": 3600.033888327021,
    "input_throughput": 6616.130775110189,
    "output_throughput": 5772.86149093943,
    "total_throughput": 12388.99226604962,
    "itl": 79.32376212721574,
    "ttft": 1928925.0915931654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.86390132265169,
    "arrivals": 464889,
    "finished_requests": 96561,
    "scheduler_time": 319.04472620950213
}
#Debug simulation 
Total elapsed time: 85.794163598679. Arrivals time: 0.5676635005511343 Scheduler time: 84.99070917395875 Scheduler overhead time: 0.0912437024526298 Adapter cache time: 0.020011323504149914 Engine time: 0.08947536209598184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 33, 8640, 8640, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 4320, 4320, 8640, 4320, 8640, 8640, 33, 8640, 4320, 4320, 8640, 8640, 4320, 33, 33, 4320, 8640, 8640, 4320, 4320, 33, 33, 4320, 8640, 8640, 33, 8640, 4320, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 8640, 4320, 33, 33, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 33, 8640, 4320, 8640, 8640, 4320, 33, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 8640, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 4320, 33, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 4320, 8640, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 4320, 33, 4320, 33, 33, 33, 8640, 8640, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 33, 4320, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 33, 33, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 8640, 33, 8640, 33, 33, 33, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1390218 . Total input tokens: 310210586 . Total output tokens: 272569261
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 88.26225782791153,
    "estimated_duration": 3600.064434301132,
    "input_throughput": 6840.60867504464,
    "output_throughput": 5966.972922852734,
    "total_throughput": 12807.581597897373,
    "itl": 83.8027768734579,
    "ttft": 1905192.730228887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.742623512083718,
    "arrivals": 463720,
    "finished_requests": 99798,
    "scheduler_time": 308.3830888012933
}
#Debug simulation 
Total elapsed time: 88.26243209186941. Arrivals time: 0.5210627960041165 Scheduler time: 87.51900480408221 Scheduler overhead time: 0.08650777395814657 Adapter cache time: 0.01876601856201887 Engine time: 0.08308954862877727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 33, 8640, 8640, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 4320, 4320, 8640, 4320, 8640, 8640, 33, 8640, 4320, 4320, 8640, 8640, 4320, 33, 33, 4320, 8640, 8640, 4320, 4320, 33, 33, 4320, 8640, 8640, 33, 8640, 4320, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 8640, 4320, 33, 33, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 33, 8640, 4320, 8640, 8640, 4320, 33, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 8640, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 4320, 33, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 4320, 8640, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 4320, 33, 4320, 33, 33, 33, 8640, 8640, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 33, 4320, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 33, 33, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 8640, 33, 8640, 33, 33, 33, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1390218 . Total input tokens: 310210586 . Total output tokens: 272569261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.53484177822247,
    "estimated_duration": 3600.04952487767,
    "input_throughput": 6849.842989545523,
    "output_throughput": 5973.89559543095,
    "total_throughput": 12823.738584976474,
    "itl": 83.3542257122025,
    "ttft": 1908885.3142466946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.101131475605076,
    "arrivals": 463720,
    "finished_requests": 99955,
    "scheduler_time": 307.92410540597
}
#Debug simulation 
Total elapsed time: 88.53500481508672. Arrivals time: 0.5274858153425157 Scheduler time: 87.7832543659024 Scheduler overhead time: 0.08725610002875328 Adapter cache time: 0.018925081472843885 Engine time: 0.08419797709211707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 33, 8640, 8640, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 4320, 4320, 8640, 4320, 8640, 8640, 33, 8640, 4320, 4320, 8640, 8640, 4320, 33, 33, 4320, 8640, 8640, 4320, 4320, 33, 33, 4320, 8640, 8640, 33, 8640, 4320, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 8640, 4320, 33, 33, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 33, 8640, 4320, 8640, 8640, 4320, 33, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 8640, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 4320, 33, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 4320, 8640, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 4320, 33, 4320, 33, 33, 33, 8640, 8640, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 33, 4320, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 33, 33, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 8640, 33, 8640, 33, 33, 33, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1390218 . Total input tokens: 310210586 . Total output tokens: 272569261
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 87.07831308990717,
    "estimated_duration": 3600.0216962437476,
    "input_throughput": 6778.784979396882,
    "output_throughput": 5910.106048027391,
    "total_throughput": 12688.891027424273,
    "itl": 81.44767286827931,
    "ttft": 1915651.6012389476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.195446178689634,
    "arrivals": 463720,
    "finished_requests": 98815,
    "scheduler_time": 311.3712151225447
}
#Debug simulation 
Total elapsed time: 87.078542875126. Arrivals time: 0.5392829435877502 Scheduler time: 86.31227444065735 Scheduler overhead time: 0.08811113610863686 Adapter cache time: 0.01950256573036313 Engine time: 0.08536485861986876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 33, 8640, 8640, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 4320, 4320, 8640, 4320, 8640, 8640, 33, 8640, 4320, 4320, 8640, 8640, 4320, 33, 33, 4320, 8640, 8640, 4320, 4320, 33, 33, 4320, 8640, 8640, 33, 8640, 4320, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 8640, 4320, 33, 33, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 33, 8640, 4320, 8640, 8640, 4320, 33, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 8640, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 4320, 33, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 4320, 8640, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 4320, 33, 4320, 33, 33, 33, 8640, 8640, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 33, 4320, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 33, 33, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 8640, 33, 8640, 33, 33, 33, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1390218 . Total input tokens: 310210586 . Total output tokens: 272569261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 88.61307842796668,
    "estimated_duration": 3600.016075014327,
    "input_throughput": 6850.02135716904,
    "output_throughput": 5974.3036008288955,
    "total_throughput": 12824.324957997937,
    "itl": 83.34860829987514,
    "ttft": 1908861.7284851244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8373694906756226,
    "arrivals": 463720,
    "finished_requests": 99960,
    "scheduler_time": 307.94173376951
}
#Debug simulation 
Total elapsed time: 88.6132467831485. Arrivals time: 0.545661264564842 Scheduler time: 87.84166330704466 Scheduler overhead time: 0.08739850716665387 Adapter cache time: 0.019262448884546757 Engine time: 0.08536227978765965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 33, 8640, 8640, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 4320, 4320, 8640, 4320, 8640, 8640, 33, 8640, 4320, 4320, 8640, 8640, 4320, 33, 33, 4320, 8640, 8640, 4320, 4320, 33, 33, 4320, 8640, 8640, 33, 8640, 4320, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 8640, 4320, 33, 33, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 33, 8640, 4320, 8640, 8640, 4320, 33, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 8640, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 4320, 33, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 4320, 8640, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 4320, 33, 4320, 33, 33, 33, 8640, 8640, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 33, 4320, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 33, 33, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 8640, 33, 8640, 33, 33, 33, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1390218 . Total input tokens: 310210586 . Total output tokens: 272569261
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 86.60260243108496,
    "estimated_duration": 3600.0810814914207,
    "input_throughput": 6778.673715284808,
    "output_throughput": 5910.0377237019475,
    "total_throughput": 12688.711438986755,
    "itl": 81.44708063792164,
    "ttft": 1915690.633080218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.156093915132841,
    "arrivals": 463720,
    "finished_requests": 98817,
    "scheduler_time": 311.37856212015265
}
#Debug simulation 
Total elapsed time: 86.60276968311518. Arrivals time: 0.5337968547828496 Scheduler time: 85.84351523965597 Scheduler overhead time: 0.08765819342806935 Adapter cache time: 0.019388760905712843 Engine time: 0.0841451333835721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 33, 8640, 8640, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 4320, 4320, 8640, 4320, 8640, 8640, 33, 8640, 4320, 4320, 8640, 8640, 4320, 33, 33, 4320, 8640, 8640, 4320, 4320, 33, 33, 4320, 8640, 8640, 33, 8640, 4320, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 8640, 4320, 33, 33, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 33, 8640, 4320, 8640, 8640, 4320, 33, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 8640, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 4320, 33, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 4320, 8640, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 4320, 33, 4320, 33, 33, 33, 8640, 8640, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 33, 4320, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 33, 33, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 8640, 33, 8640, 33, 33, 33, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1390218 . Total input tokens: 310210586 . Total output tokens: 272569261
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 88.73658457631245,
    "estimated_duration": 3600.0840220581686,
    "input_throughput": 6850.495946453085,
    "output_throughput": 5974.4400042373445,
    "total_throughput": 12824.93595069043,
    "itl": 83.34404045001506,
    "ttft": 1908762.7825241925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5749957267194823,
    "arrivals": 463720,
    "finished_requests": 99966,
    "scheduler_time": 307.9662741158839
}
#Debug simulation 
Total elapsed time: 88.73674791306257. Arrivals time: 0.5354237793944776 Scheduler time: 87.98033084860072 Scheduler overhead time: 0.08675271272659302 Adapter cache time: 0.018853470217436552 Engine time: 0.08165750885382295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_320_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [106 107 107]
Adapter prompts. [4320, 8640, 8640, 33, 8640, 8640, 4320, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 4320, 4320, 8640, 4320, 8640, 8640, 33, 8640, 4320, 4320, 8640, 8640, 4320, 33, 33, 4320, 8640, 8640, 4320, 4320, 33, 33, 4320, 8640, 8640, 33, 8640, 4320, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 8640, 4320, 33, 33, 4320, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 33, 8640, 4320, 8640, 8640, 4320, 33, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 33, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 4320, 33, 4320, 33, 8640, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 4320, 33, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 4320, 8640, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 33, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 4320, 33, 4320, 33, 33, 33, 8640, 8640, 4320, 4320, 33, 33, 4320, 33, 8640, 33, 33, 4320, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 33, 4320, 8640, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 4320, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 33, 33, 8640, 4320, 33, 4320, 33, 4320, 8640, 8640, 8640, 33, 8640, 8640, 8640, 8640, 33, 4320, 4320, 8640, 33, 8640, 33, 33, 33, 8640, 4320, 8640, 8640, 33, 4320, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 33, 33, 4320, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 4320, 4320, 4320, 4320, 33, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 4320, 4320, 4320, 33, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 8640, 8640, 4320, 8640, 4320, 33, 8640, 33, 8640, 8640, 33, 33, 33]
Prompts retrieved: 1390218 . Total input tokens: 310210586 . Total output tokens: 272569261
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 87.16381251718849,
    "estimated_duration": 3600.041986090606,
    "input_throughput": 6778.747329694562,
    "output_throughput": 5910.101904979424,
    "total_throughput": 12688.849234673986,
    "itl": 81.44635033233203,
    "ttft": 1915673.994869227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.117570120282507,
    "arrivals": 463720,
    "finished_requests": 98817,
    "scheduler_time": 311.37788938296575
}
#Debug simulation 
Total elapsed time: 87.16397116985172. Arrivals time: 0.540662370622158 Scheduler time: 86.3990935892798 Scheduler overhead time: 0.08766955276951194 Adapter cache time: 0.019323978573083878 Engine time: 0.08318248903378844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [106 107 107]
Adapter prompts. [1080, 8640, 8640, 540, 8640, 8640, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 540, 1080, 1080, 8640, 1080, 8640, 8640, 540, 8640, 1080, 1080, 8640, 8640, 1080, 540, 540, 1080, 8640, 8640, 1080, 1080, 540, 540, 1080, 8640, 8640, 540, 8640, 1080, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 8640, 1080, 540, 540, 1080, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 540, 8640, 1080, 8640, 8640, 1080, 540, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 8640, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 1080, 540, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 1080, 8640, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 1080, 540, 1080, 540, 540, 540, 8640, 8640, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 540, 1080, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 8640, 540, 8640, 540, 540, 540, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1097280 . Total input tokens: 244858941 . Total output tokens: 215076744
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 89.1310811736621,
    "estimated_duration": 3600.028138591809,
    "input_throughput": 6642.725023075577,
    "output_throughput": 5776.498738183312,
    "total_throughput": 12419.223761258889,
    "itl": 80.66739339879234,
    "ttft": 1829241.104209917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8087476024032196,
    "arrivals": 366022,
    "finished_requests": 96490,
    "scheduler_time": 315.4690587906797
}
#Debug simulation 
Total elapsed time: 89.13124745991081. Arrivals time: 0.5309232859872282 Scheduler time: 88.37200103700161 Scheduler overhead time: 0.08927345415577292 Adapter cache time: 0.019860494416207075 Engine time: 0.08437838591635227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [106 107 107]
Adapter prompts. [1080, 8640, 8640, 540, 8640, 8640, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 540, 1080, 1080, 8640, 1080, 8640, 8640, 540, 8640, 1080, 1080, 8640, 8640, 1080, 540, 540, 1080, 8640, 8640, 1080, 1080, 540, 540, 1080, 8640, 8640, 540, 8640, 1080, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 8640, 1080, 540, 540, 1080, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 540, 8640, 1080, 8640, 8640, 1080, 540, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 8640, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 1080, 540, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 1080, 8640, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 1080, 540, 1080, 540, 540, 540, 8640, 8640, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 540, 1080, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 8640, 540, 8640, 540, 540, 540, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1097280 . Total input tokens: 244858941 . Total output tokens: 215076744
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.87399848876521,
    "estimated_duration": 3600.000632221597,
    "input_throughput": 6664.897718418812,
    "output_throughput": 5791.913427285345,
    "total_throughput": 12456.811145704158,
    "itl": 80.92291391389816,
    "ttft": 1835074.3927138094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1047389545990205,
    "arrivals": 366022,
    "finished_requests": 96832,
    "scheduler_time": 314.29521195813044
}
#Debug simulation 
Total elapsed time: 88.87416327884421. Arrivals time: 0.5267983819358051 Scheduler time: 88.11774598993361 Scheduler overhead time: 0.08996352553367615 Adapter cache time: 0.02008029632270336 Engine time: 0.08463096572086215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [106 107 107]
Adapter prompts. [1080, 8640, 8640, 540, 8640, 8640, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 540, 1080, 1080, 8640, 1080, 8640, 8640, 540, 8640, 1080, 1080, 8640, 8640, 1080, 540, 540, 1080, 8640, 8640, 1080, 1080, 540, 540, 1080, 8640, 8640, 540, 8640, 1080, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 8640, 1080, 540, 540, 1080, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 540, 8640, 1080, 8640, 8640, 1080, 540, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 8640, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 1080, 540, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 1080, 8640, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 1080, 540, 1080, 540, 540, 540, 8640, 8640, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 540, 1080, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 8640, 540, 8640, 540, 540, 540, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1097280 . Total input tokens: 244858941 . Total output tokens: 215076744
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 88.62775606801733,
    "estimated_duration": 3600.020241077737,
    "input_throughput": 6664.768638305526,
    "output_throughput": 5793.671036070605,
    "total_throughput": 12458.43967437613,
    "itl": 80.33535242014369,
    "ttft": 1835857.6861917272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.166559497984159,
    "arrivals": 366022,
    "finished_requests": 96829,
    "scheduler_time": 314.45029126096915
}
#Debug simulation 
Total elapsed time: 88.6278963801451. Arrivals time: 0.5274006742984056 Scheduler time: 87.87037902651355 Scheduler overhead time: 0.09036846505478024 Adapter cache time: 0.019364493433386087 Engine time: 0.08488987712189555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [106 107 107]
Adapter prompts. [1080, 8640, 8640, 540, 8640, 8640, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 540, 1080, 1080, 8640, 1080, 8640, 8640, 540, 8640, 1080, 1080, 8640, 8640, 1080, 540, 540, 1080, 8640, 8640, 1080, 1080, 540, 540, 1080, 8640, 8640, 540, 8640, 1080, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 8640, 1080, 540, 540, 1080, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 540, 8640, 1080, 8640, 8640, 1080, 540, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 8640, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 1080, 540, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 1080, 8640, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 1080, 540, 1080, 540, 540, 540, 8640, 8640, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 540, 1080, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 8640, 540, 8640, 540, 540, 540, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1097280 . Total input tokens: 244858941 . Total output tokens: 215076744
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 89.58904675813392,
    "estimated_duration": 3600.0393769301195,
    "input_throughput": 6665.204040201744,
    "output_throughput": 5792.172478341411,
    "total_throughput": 12457.376518543155,
    "itl": 80.91691246294255,
    "ttft": 1834953.9381395502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8437534116161935,
    "arrivals": 366022,
    "finished_requests": 96836,
    "scheduler_time": 314.32745096489765
}
#Debug simulation 
Total elapsed time: 89.58919904800132. Arrivals time: 0.5238213585689664 Scheduler time: 88.83553535444662 Scheduler overhead time: 0.08992257807403803 Adapter cache time: 0.019600261002779007 Engine time: 0.0849375557154417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_320_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [106 107 107]
Adapter prompts. [1080, 8640, 8640, 540, 8640, 8640, 1080, 1080, 1080, 1080, 540, 1080, 540, 1080, 540, 1080, 1080, 8640, 1080, 8640, 8640, 540, 8640, 1080, 1080, 8640, 8640, 1080, 540, 540, 1080, 8640, 8640, 1080, 1080, 540, 540, 1080, 8640, 8640, 540, 8640, 1080, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 8640, 1080, 540, 540, 1080, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 540, 8640, 1080, 8640, 8640, 1080, 540, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 540, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 1080, 540, 1080, 540, 8640, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 1080, 540, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 1080, 8640, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 540, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 1080, 540, 1080, 540, 540, 540, 8640, 8640, 1080, 1080, 540, 540, 1080, 540, 8640, 540, 540, 1080, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 540, 1080, 8640, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 1080, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 540, 540, 8640, 1080, 540, 1080, 540, 1080, 8640, 8640, 8640, 540, 8640, 8640, 8640, 8640, 540, 1080, 1080, 8640, 540, 8640, 540, 540, 540, 8640, 1080, 8640, 8640, 540, 1080, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 540, 540, 1080, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 1080, 1080, 1080, 1080, 540, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 1080, 1080, 1080, 540, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 8640, 8640, 1080, 8640, 1080, 540, 8640, 540, 8640, 8640, 540, 540, 540]
Prompts retrieved: 1097280 . Total input tokens: 244858941 . Total output tokens: 215076744
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 87.51510641304776,
    "estimated_duration": 3600.052888977236,
    "input_throughput": 6618.565819673064,
    "output_throughput": 5749.956636298432,
    "total_throughput": 12368.522455971495,
    "itl": 79.47769997180937,
    "ttft": 1837514.379953319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 579,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3096908468986275,
    "arrivals": 366022,
    "finished_requests": 96122,
    "scheduler_time": 316.6981227994269
}
#Debug simulation 
Total elapsed time: 87.51525674993172. Arrivals time: 0.524315589107573 Scheduler time: 86.76053992519155 Scheduler overhead time: 0.09043439012020826 Adapter cache time: 0.0197604987770319 Engine time: 0.08481279108673334 
