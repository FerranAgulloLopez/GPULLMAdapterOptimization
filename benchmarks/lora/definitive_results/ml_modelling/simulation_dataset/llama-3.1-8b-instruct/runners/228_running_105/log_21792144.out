INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [1080, 34560, 34560, 33, 34560, 34560, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 1080, 1080, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 34560, 1080, 33, 33, 1080, 34560, 34560, 1080, 1080, 33, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 34560, 1080, 33, 33, 1080, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 33, 34560, 1080, 34560, 34560, 1080, 33, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 33, 1080, 34560, 34560, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 1080, 33, 34560, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 1080, 33, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 1080, 34560, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 34560, 34560, 34560, 1080, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 1080, 33, 1080, 33, 33, 33, 34560, 34560, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 33, 1080, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 1080, 33, 1080, 33, 1080, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 34560, 33, 34560, 33, 33, 33, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3816978 . Total input tokens: 851432403 . Total output tokens: 748532292
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 106.44376424932852,
    "estimated_duration": 3600.0002394464113,
    "input_throughput": 7137.391469715896,
    "output_throughput": 6218.326253067804,
    "total_throughput": 13355.7177227837,
    "itl": 86.95888855625034,
    "ttft": 2021332.3555619197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7067824788018946,
    "arrivals": 1271499,
    "finished_requests": 104154,
    "scheduler_time": 308.1087941929541
}
#Debug simulation 
Total elapsed time: 106.44396446831524. Arrivals time: 0.8070351565256715 Scheduler time: 105.34561260836199 Scheduler overhead time: 0.11481804680079222 Adapter cache time: 0.02470536809414625 Engine time: 0.11226374888792634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [1080, 34560, 34560, 33, 34560, 34560, 1080, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 1080, 1080, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 34560, 1080, 33, 33, 1080, 34560, 34560, 1080, 1080, 33, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 1080, 33, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 34560, 1080, 33, 33, 1080, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 33, 34560, 1080, 34560, 34560, 1080, 33, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 33, 33, 1080, 34560, 34560, 1080, 33, 1080, 33, 1080, 34560, 34560, 1080, 33, 1080, 33, 34560, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 1080, 33, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 1080, 34560, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 33, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 34560, 34560, 34560, 1080, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 1080, 33, 1080, 33, 33, 33, 34560, 34560, 1080, 1080, 33, 33, 1080, 33, 34560, 33, 33, 1080, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 1080, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 1080, 33, 1080, 33, 1080, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 1080, 1080, 34560, 33, 34560, 33, 33, 33, 34560, 1080, 34560, 34560, 33, 1080, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 33, 33, 1080, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 1080, 1080, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 1080, 1080, 1080, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3816978 . Total input tokens: 851432403 . Total output tokens: 748532292
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 103.68333578994498,
    "estimated_duration": 3600.0065402992677,
    "input_throughput": 7126.613441615368,
    "output_throughput": 6199.382348384491,
    "total_throughput": 13325.995789999859,
    "itl": 85.26613170788985,
    "ttft": 2031940.5279789497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2269956783764306,
    "arrivals": 1271499,
    "finished_requests": 103916,
    "scheduler_time": 308.780448250373
}
#Debug simulation 
Total elapsed time: 103.68354940926656. Arrivals time: 0.8225072347559035 Scheduler time: 102.57059445092455 Scheduler overhead time: 0.11739074159413576 Adapter cache time: 0.024598595686256886 Engine time: 0.10972672002390027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 270, 34560, 34560, 540, 540, 540, 540, 270, 540, 270, 540, 270, 540, 540, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 34560, 540, 270, 270, 540, 34560, 34560, 540, 540, 270, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 270, 34560, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 540, 270, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 34560, 540, 270, 270, 540, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 270, 34560, 540, 34560, 34560, 540, 270, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 270, 540, 34560, 34560, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 540, 270, 34560, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 540, 270, 540, 34560, 540, 270, 270, 540, 34560, 270, 270, 270, 34560, 34560, 34560, 270, 540, 34560, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 540, 270, 540, 270, 270, 270, 34560, 34560, 540, 540, 270, 270, 540, 270, 34560, 270, 270, 540, 270, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 270, 270, 34560, 540, 270, 540, 270, 540, 34560, 34560, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 34560, 270, 34560, 270, 270, 270, 34560, 540, 34560, 34560, 270, 540, 540, 540, 270, 540, 270, 540, 540, 34560, 270, 270, 540, 270, 540, 540, 270, 34560, 34560, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 540, 540, 540, 270, 270, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 3784320 . Total input tokens: 844099310 . Total output tokens: 742135439
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 108.8184701250866,
    "estimated_duration": 3600.0555582329507,
    "input_throughput": 6987.282721921901,
    "output_throughput": 6116.15422146639,
    "total_throughput": 13103.43694338829,
    "itl": 88.66329319870285,
    "ttft": 2039232.9159105397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0086461095372514,
    "arrivals": 1260602,
    "finished_requests": 102267,
    "scheduler_time": 311.6812202483773
}
#Debug simulation 
Total elapsed time: 108.81864317925647. Arrivals time: 0.7880486533977091 Scheduler time: 107.73712748894468 Scheduler overhead time: 0.11748571367934346 Adapter cache time: 0.025384950917214155 Engine time: 0.11147113190963864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 270, 34560, 34560, 540, 540, 540, 540, 270, 540, 270, 540, 270, 540, 540, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 34560, 540, 270, 270, 540, 34560, 34560, 540, 540, 270, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 270, 34560, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 540, 270, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 34560, 540, 270, 270, 540, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 270, 34560, 540, 34560, 34560, 540, 270, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 270, 540, 34560, 34560, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 540, 270, 34560, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 540, 270, 540, 34560, 540, 270, 270, 540, 34560, 270, 270, 270, 34560, 34560, 34560, 270, 540, 34560, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 540, 270, 540, 270, 270, 270, 34560, 34560, 540, 540, 270, 270, 540, 270, 34560, 270, 270, 540, 270, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 270, 270, 34560, 540, 270, 540, 270, 540, 34560, 34560, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 34560, 270, 34560, 270, 270, 270, 34560, 540, 34560, 34560, 270, 540, 540, 540, 270, 540, 270, 540, 540, 34560, 270, 270, 540, 270, 540, 540, 270, 34560, 34560, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 540, 540, 540, 270, 270, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 3784320 . Total input tokens: 844099310 . Total output tokens: 742135439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 105.69511332595721,
    "estimated_duration": 3600.040999077204,
    "input_throughput": 7025.834985346948,
    "output_throughput": 6147.193047432684,
    "total_throughput": 13173.028032779632,
    "itl": 88.07810704251841,
    "ttft": 2036752.03202911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.492694625984883,
    "arrivals": 1260602,
    "finished_requests": 102717,
    "scheduler_time": 310.1279481335383
}
#Debug simulation 
Total elapsed time: 105.69528929004446. Arrivals time: 0.9080935739912093 Scheduler time: 104.49601415265352 Scheduler overhead time: 0.11610729433596134 Adapter cache time: 0.02552950754761696 Engine time: 0.11071765143424273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 270, 34560, 34560, 540, 540, 540, 540, 270, 540, 270, 540, 270, 540, 540, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 34560, 540, 270, 270, 540, 34560, 34560, 540, 540, 270, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 270, 34560, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 540, 270, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 34560, 540, 270, 270, 540, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 270, 34560, 540, 34560, 34560, 540, 270, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 270, 540, 34560, 34560, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 540, 270, 34560, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 540, 270, 540, 34560, 540, 270, 270, 540, 34560, 270, 270, 270, 34560, 34560, 34560, 270, 540, 34560, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 540, 270, 540, 270, 270, 270, 34560, 34560, 540, 540, 270, 270, 540, 270, 34560, 270, 270, 540, 270, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 270, 270, 34560, 540, 270, 540, 270, 540, 34560, 34560, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 34560, 270, 34560, 270, 270, 270, 34560, 540, 34560, 34560, 270, 540, 540, 540, 270, 540, 270, 540, 540, 34560, 270, 270, 540, 270, 540, 540, 270, 34560, 34560, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 540, 540, 540, 270, 270, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 3784320 . Total input tokens: 844099310 . Total output tokens: 742135439
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 103.18607610603794,
    "estimated_duration": 3600.0160998645747,
    "input_throughput": 6992.748449360267,
    "output_throughput": 6118.027639050985,
    "total_throughput": 13110.776088411252,
    "itl": 86.12584687295222,
    "ttft": 2039146.5926094311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.575928321913845,
    "arrivals": 1260602,
    "finished_requests": 102207,
    "scheduler_time": 311.7162450481378
}
#Debug simulation 
Total elapsed time: 103.18624151032418. Arrivals time: 0.78352828649804 Scheduler time: 102.11325951991603 Scheduler overhead time: 0.11473282240331173 Adapter cache time: 0.02544541982933879 Engine time: 0.11031891778111458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 270, 34560, 34560, 540, 540, 540, 540, 270, 540, 270, 540, 270, 540, 540, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 34560, 540, 270, 270, 540, 34560, 34560, 540, 540, 270, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 270, 34560, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 540, 270, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 34560, 540, 270, 270, 540, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 270, 34560, 540, 34560, 34560, 540, 270, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 270, 540, 34560, 34560, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 540, 270, 34560, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 540, 270, 540, 34560, 540, 270, 270, 540, 34560, 270, 270, 270, 34560, 34560, 34560, 270, 540, 34560, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 540, 270, 540, 270, 270, 270, 34560, 34560, 540, 540, 270, 270, 540, 270, 34560, 270, 270, 540, 270, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 270, 270, 34560, 540, 270, 540, 270, 540, 34560, 34560, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 34560, 270, 34560, 270, 270, 270, 34560, 540, 34560, 34560, 270, 540, 540, 540, 270, 540, 270, 540, 540, 34560, 270, 270, 540, 270, 540, 540, 270, 34560, 34560, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 540, 540, 540, 270, 270, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 3784320 . Total input tokens: 844099310 . Total output tokens: 742135439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 107.71869735140353,
    "estimated_duration": 3600.041710183304,
    "input_throughput": 7049.173049361102,
    "output_throughput": 6171.104056144064,
    "total_throughput": 13220.277105505167,
    "itl": 88.03419883658505,
    "ttft": 2037658.2530098916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1778707512142095,
    "arrivals": 1260602,
    "finished_requests": 103131,
    "scheduler_time": 308.8824378380863
}
#Debug simulation 
Total elapsed time: 107.71887628128752. Arrivals time: 0.778248312883079 Scheduler time: 106.65056401723996 Scheduler overhead time: 0.11631024396046996 Adapter cache time: 0.025785000063478947 Engine time: 0.10965565219521523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 270, 34560, 34560, 540, 540, 540, 540, 270, 540, 270, 540, 270, 540, 540, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 34560, 540, 270, 270, 540, 34560, 34560, 540, 540, 270, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 270, 34560, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 540, 270, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 34560, 540, 270, 270, 540, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 270, 34560, 540, 34560, 34560, 540, 270, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 270, 540, 34560, 34560, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 540, 270, 34560, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 540, 270, 540, 34560, 540, 270, 270, 540, 34560, 270, 270, 270, 34560, 34560, 34560, 270, 540, 34560, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 540, 270, 540, 270, 270, 270, 34560, 34560, 540, 540, 270, 270, 540, 270, 34560, 270, 270, 540, 270, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 270, 270, 34560, 540, 270, 540, 270, 540, 34560, 34560, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 34560, 270, 34560, 270, 270, 270, 34560, 540, 34560, 34560, 270, 540, 540, 540, 270, 540, 270, 540, 540, 34560, 270, 270, 540, 270, 540, 540, 270, 34560, 34560, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 540, 540, 540, 270, 270, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 3784320 . Total input tokens: 844099310 . Total output tokens: 742135439
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 104.86508416384459,
    "estimated_duration": 3600.034991765206,
    "input_throughput": 6884.994744966789,
    "output_throughput": 6034.988284751922,
    "total_throughput": 12919.98302971871,
    "itl": 85.0530389687283,
    "ttft": 2041145.4841556405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 458,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4155439027585355,
    "arrivals": 1260602,
    "finished_requests": 100750,
    "scheduler_time": 316.21055006027706
}
#Debug simulation 
Total elapsed time: 104.86526018660516. Arrivals time: 0.8436719956807792 Scheduler time: 103.7349168159999 Scheduler overhead time: 0.11384513741359115 Adapter cache time: 0.02549526235088706 Engine time: 0.10880625527352095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 270, 34560, 34560, 540, 540, 540, 540, 270, 540, 270, 540, 270, 540, 540, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 34560, 540, 270, 270, 540, 34560, 34560, 540, 540, 270, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 270, 34560, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 540, 270, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 34560, 540, 270, 270, 540, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 270, 34560, 540, 34560, 34560, 540, 270, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 270, 540, 34560, 34560, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 540, 270, 34560, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 540, 270, 540, 34560, 540, 270, 270, 540, 34560, 270, 270, 270, 34560, 34560, 34560, 270, 540, 34560, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 540, 270, 540, 270, 270, 270, 34560, 34560, 540, 540, 270, 270, 540, 270, 34560, 270, 270, 540, 270, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 270, 270, 34560, 540, 270, 540, 270, 540, 34560, 34560, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 34560, 270, 34560, 270, 270, 270, 34560, 540, 34560, 34560, 270, 540, 540, 540, 270, 540, 270, 540, 540, 34560, 270, 270, 540, 270, 540, 540, 270, 34560, 34560, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 540, 540, 540, 270, 270, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 3784320 . Total input tokens: 844099310 . Total output tokens: 742135439
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 105.5621336242184,
    "estimated_duration": 3600.022057590551,
    "input_throughput": 6908.610725748149,
    "output_throughput": 6048.771271855541,
    "total_throughput": 12957.38199760369,
    "itl": 87.265306157039,
    "ttft": 2044384.8000029968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.853612660435016,
    "arrivals": 1260602,
    "finished_requests": 101107,
    "scheduler_time": 315.37578507885513
}
#Debug simulation 
Total elapsed time: 105.56230261083692. Arrivals time: 0.7744894362986088 Scheduler time: 104.49940860411152 Scheduler overhead time: 0.11562947602942586 Adapter cache time: 0.024872941430658102 Engine time: 0.10930390004068613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 270, 34560, 34560, 540, 540, 540, 540, 270, 540, 270, 540, 270, 540, 540, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 34560, 540, 270, 270, 540, 34560, 34560, 540, 540, 270, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 270, 34560, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 540, 270, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 34560, 540, 270, 270, 540, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 270, 34560, 540, 34560, 34560, 540, 270, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 270, 270, 540, 34560, 34560, 540, 270, 540, 270, 540, 34560, 34560, 540, 270, 540, 270, 34560, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 540, 270, 540, 34560, 540, 270, 270, 540, 34560, 270, 270, 270, 34560, 34560, 34560, 270, 540, 34560, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 270, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 540, 270, 540, 270, 270, 270, 34560, 34560, 540, 540, 270, 270, 540, 270, 34560, 270, 270, 540, 270, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 270, 540, 270, 34560, 540, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 270, 270, 34560, 540, 270, 540, 270, 540, 34560, 34560, 34560, 270, 34560, 34560, 34560, 34560, 270, 540, 540, 34560, 270, 34560, 270, 270, 270, 34560, 540, 34560, 34560, 270, 540, 540, 540, 270, 540, 270, 540, 540, 34560, 270, 270, 540, 270, 540, 540, 270, 34560, 34560, 34560, 270, 540, 540, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 540, 540, 540, 270, 270, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 3784320 . Total input tokens: 844099310 . Total output tokens: 742135439
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 104.67745020706207,
    "estimated_duration": 3600.0886793063005,
    "input_throughput": 6971.919648611661,
    "output_throughput": 6097.313415132236,
    "total_throughput": 13069.233063743897,
    "itl": 85.9995065875355,
    "ttft": 2037617.507639901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 470,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.467149273045388,
    "arrivals": 1260602,
    "finished_requests": 101988,
    "scheduler_time": 312.698989319832
}
#Debug simulation 
Total elapsed time: 104.6776545792818. Arrivals time: 0.7719677570275962 Scheduler time: 103.61654378147796 Scheduler overhead time: 0.11468892591074109 Adapter cache time: 0.025835799518972635 Engine time: 0.11071450868621469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 135, 34560, 34560, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 34560, 540, 135, 135, 540, 34560, 34560, 540, 540, 135, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 540, 135, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 34560, 540, 135, 135, 540, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 135, 34560, 540, 34560, 34560, 540, 135, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 135, 540, 34560, 34560, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 540, 135, 34560, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 540, 135, 540, 34560, 540, 135, 135, 540, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 540, 34560, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 540, 135, 540, 135, 135, 135, 34560, 34560, 540, 540, 135, 135, 540, 135, 34560, 135, 135, 540, 135, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 540, 135, 540, 135, 540, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 34560, 135, 34560, 135, 135, 135, 34560, 540, 34560, 34560, 135, 540, 540, 540, 135, 540, 135, 540, 540, 34560, 135, 135, 540, 135, 540, 540, 135, 34560, 34560, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 540, 540, 540, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3770010 . Total input tokens: 840907653 . Total output tokens: 739373113
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 104.69499440398067,
    "estimated_duration": 3600.0766073862505,
    "input_throughput": 7054.9898710183215,
    "output_throughput": 6155.010411316679,
    "total_throughput": 13210.000282335,
    "itl": 88.73314295558339,
    "ttft": 2014437.3845801305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7227862849878677,
    "arrivals": 1256018,
    "finished_requests": 102901,
    "scheduler_time": 309.6738507085002
}
#Debug simulation 
Total elapsed time: 104.69515981478617. Arrivals time: 0.7857783399522305 Scheduler time: 103.6232694927603 Scheduler overhead time: 0.11375478189438581 Adapter cache time: 0.02626817813143134 Engine time: 0.1088677910156548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 135, 34560, 34560, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 34560, 540, 135, 135, 540, 34560, 34560, 540, 540, 135, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 540, 135, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 34560, 540, 135, 135, 540, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 135, 34560, 540, 34560, 34560, 540, 135, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 135, 540, 34560, 34560, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 540, 135, 34560, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 540, 135, 540, 34560, 540, 135, 135, 540, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 540, 34560, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 540, 135, 540, 135, 135, 135, 34560, 34560, 540, 540, 135, 135, 540, 135, 34560, 135, 135, 540, 135, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 540, 135, 540, 135, 540, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 34560, 135, 34560, 135, 135, 135, 34560, 540, 34560, 34560, 135, 540, 540, 540, 135, 540, 135, 540, 540, 34560, 135, 135, 540, 135, 540, 540, 135, 34560, 34560, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 540, 540, 540, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3770010 . Total input tokens: 840907653 . Total output tokens: 739373113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 103.26312519889325,
    "estimated_duration": 3600.029265714586,
    "input_throughput": 7078.865231097376,
    "output_throughput": 6179.636430145552,
    "total_throughput": 13258.501661242928,
    "itl": 88.12915752086711,
    "ttft": 2028705.2884452748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3019985571317414,
    "arrivals": 1256018,
    "finished_requests": 103252,
    "scheduler_time": 308.51638356590524
}
#Debug simulation 
Total elapsed time: 103.26330677513033. Arrivals time: 0.8031198792159557 Scheduler time: 102.17136685131118 Scheduler overhead time: 0.1141507439315319 Adapter cache time: 0.024944535456597805 Engine time: 0.11104542901739478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 135, 34560, 34560, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 34560, 540, 135, 135, 540, 34560, 34560, 540, 540, 135, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 540, 135, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 34560, 540, 135, 135, 540, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 135, 34560, 540, 34560, 34560, 540, 135, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 135, 540, 34560, 34560, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 540, 135, 34560, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 540, 135, 540, 34560, 540, 135, 135, 540, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 540, 34560, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 540, 135, 540, 135, 135, 135, 34560, 34560, 540, 540, 135, 135, 540, 135, 34560, 135, 135, 540, 135, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 540, 135, 540, 135, 540, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 34560, 135, 34560, 135, 135, 135, 34560, 540, 34560, 34560, 135, 540, 540, 540, 135, 540, 135, 540, 540, 34560, 135, 135, 540, 135, 540, 540, 135, 34560, 34560, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 540, 540, 540, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3770010 . Total input tokens: 840907653 . Total output tokens: 739373113
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 100.97220504283905,
    "estimated_duration": 3600.0044869723956,
    "input_throughput": 7040.1873363538225,
    "output_throughput": 6147.82261524712,
    "total_throughput": 13188.009951600943,
    "itl": 86.13359840343651,
    "ttft": 2032715.4864220181,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.479483400029157,
    "arrivals": 1256018,
    "finished_requests": 102678,
    "scheduler_time": 310.0643507594817
}
#Debug simulation 
Total elapsed time: 100.97237473865971. Arrivals time: 0.7882170416414738 Scheduler time: 99.89648462785408 Scheduler overhead time: 0.11432327283546329 Adapter cache time: 0.026060338597744703 Engine time: 0.10843410482630134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 135, 34560, 34560, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 34560, 540, 135, 135, 540, 34560, 34560, 540, 540, 135, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 540, 135, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 34560, 540, 135, 135, 540, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 135, 34560, 540, 34560, 34560, 540, 135, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 135, 540, 34560, 34560, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 540, 135, 34560, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 540, 135, 540, 34560, 540, 135, 135, 540, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 540, 34560, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 540, 135, 540, 135, 135, 135, 34560, 34560, 540, 540, 135, 135, 540, 135, 34560, 135, 135, 540, 135, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 540, 135, 540, 135, 540, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 34560, 135, 34560, 135, 135, 135, 34560, 540, 34560, 34560, 135, 540, 540, 540, 135, 540, 135, 540, 540, 34560, 135, 135, 540, 135, 540, 540, 135, 34560, 34560, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 540, 540, 540, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3770010 . Total input tokens: 840907653 . Total output tokens: 739373113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 106.92871720110998,
    "estimated_duration": 3600.0619034247916,
    "input_throughput": 7062.321893913274,
    "output_throughput": 6166.644795435469,
    "total_throughput": 13228.966689348743,
    "itl": 87.7966627095142,
    "ttft": 2037874.849755476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0263187227351542,
    "arrivals": 1256018,
    "finished_requests": 103073,
    "scheduler_time": 309.1978695233806
}
#Debug simulation 
Total elapsed time: 106.92888954887167. Arrivals time: 0.8437520838342607 Scheduler time: 105.78887391882017 Scheduler overhead time: 0.11797282937914133 Adapter cache time: 0.02590193133801222 Engine time: 0.1131648370064795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 135, 34560, 34560, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 34560, 540, 135, 135, 540, 34560, 34560, 540, 540, 135, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 540, 135, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 34560, 540, 135, 135, 540, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 135, 34560, 540, 34560, 34560, 540, 135, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 135, 540, 34560, 34560, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 540, 135, 34560, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 540, 135, 540, 34560, 540, 135, 135, 540, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 540, 34560, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 540, 135, 540, 135, 135, 135, 34560, 34560, 540, 540, 135, 135, 540, 135, 34560, 135, 135, 540, 135, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 540, 135, 540, 135, 540, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 34560, 135, 34560, 135, 135, 135, 34560, 540, 34560, 34560, 135, 540, 540, 540, 135, 540, 135, 540, 540, 34560, 135, 135, 540, 135, 540, 540, 135, 34560, 34560, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 540, 540, 540, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3770010 . Total input tokens: 840907653 . Total output tokens: 739373113
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 105.0065235812217,
    "estimated_duration": 3600.0761061143703,
    "input_throughput": 6997.029856457081,
    "output_throughput": 6107.164224294741,
    "total_throughput": 13104.194080751822,
    "itl": 85.79214959732218,
    "ttft": 2041629.0971227572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.242064501051814,
    "arrivals": 1256018,
    "finished_requests": 102098,
    "scheduler_time": 312.24074694224174
}
#Debug simulation 
Total elapsed time: 105.00670587690547. Arrivals time: 0.7711012810468674 Scheduler time: 103.94577281177044 Scheduler overhead time: 0.11502029933035374 Adapter cache time: 0.025892557576298714 Engine time: 0.1098691145889461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 135, 34560, 34560, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 34560, 540, 135, 135, 540, 34560, 34560, 540, 540, 135, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 540, 135, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 34560, 540, 135, 135, 540, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 135, 34560, 540, 34560, 34560, 540, 135, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 135, 540, 34560, 34560, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 540, 135, 34560, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 540, 135, 540, 34560, 540, 135, 135, 540, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 540, 34560, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 540, 135, 540, 135, 135, 135, 34560, 34560, 540, 540, 135, 135, 540, 135, 34560, 135, 135, 540, 135, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 540, 135, 540, 135, 540, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 34560, 135, 34560, 135, 135, 135, 34560, 540, 34560, 34560, 135, 540, 540, 540, 135, 540, 135, 540, 540, 34560, 135, 135, 540, 135, 540, 540, 135, 34560, 34560, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 540, 540, 540, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3770010 . Total input tokens: 840907653 . Total output tokens: 739373113
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 108.0172774521634,
    "estimated_duration": 3600.084599644194,
    "input_throughput": 7114.513920737137,
    "output_throughput": 6207.469958402826,
    "total_throughput": 13321.983879139963,
    "itl": 88.137825357051,
    "ttft": 2030017.3240518607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.994058921127567,
    "arrivals": 1256018,
    "finished_requests": 103745,
    "scheduler_time": 307.0903569918714
}
#Debug simulation 
Total elapsed time: 108.0174575378187. Arrivals time: 0.8041048822924495 Scheduler time: 106.91817832179368 Scheduler overhead time: 0.11664273776113987 Adapter cache time: 0.02695224853232503 Engine time: 0.11242995550855994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 135, 34560, 34560, 540, 540, 540, 540, 135, 540, 135, 540, 135, 540, 540, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 34560, 540, 135, 135, 540, 34560, 34560, 540, 540, 135, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 540, 135, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 34560, 540, 135, 135, 540, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 135, 34560, 540, 34560, 34560, 540, 135, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 135, 135, 540, 34560, 34560, 540, 135, 540, 135, 540, 34560, 34560, 540, 135, 540, 135, 34560, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 540, 135, 540, 34560, 540, 135, 135, 540, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 540, 34560, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 135, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 540, 135, 540, 135, 135, 135, 34560, 34560, 540, 540, 135, 135, 540, 135, 34560, 135, 135, 540, 135, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 135, 540, 135, 34560, 540, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 540, 135, 540, 135, 540, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 540, 540, 34560, 135, 34560, 135, 135, 135, 34560, 540, 34560, 34560, 135, 540, 540, 540, 135, 540, 135, 540, 540, 34560, 135, 135, 540, 135, 540, 540, 135, 34560, 34560, 34560, 135, 540, 540, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 540, 540, 540, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3770010 . Total input tokens: 840907653 . Total output tokens: 739373113
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 105.21259267767891,
    "estimated_duration": 3600.0156814323336,
    "input_throughput": 7022.091356541542,
    "output_throughput": 6131.024682430919,
    "total_throughput": 13153.116038972461,
    "itl": 85.8508282678195,
    "ttft": 2031741.6861158372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5716684130393257,
    "arrivals": 1256018,
    "finished_requests": 102502,
    "scheduler_time": 311.02463687807926
}
#Debug simulation 
Total elapsed time: 105.21276669297367. Arrivals time: 0.7827694215811789 Scheduler time: 104.1425774069503 Scheduler overhead time: 0.11342709325253963 Adapter cache time: 0.02661660173907876 Engine time: 0.10951409954577684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 66, 34560, 34560, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 34560, 540, 66, 66, 540, 34560, 34560, 540, 540, 66, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 540, 66, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 34560, 540, 66, 66, 540, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 66, 34560, 540, 34560, 34560, 540, 66, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 66, 540, 34560, 34560, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 540, 66, 34560, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 540, 66, 540, 34560, 540, 66, 66, 540, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 540, 34560, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 540, 66, 540, 66, 66, 66, 34560, 34560, 540, 540, 66, 66, 540, 66, 34560, 66, 66, 540, 66, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 540, 66, 540, 66, 540, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 34560, 66, 34560, 66, 66, 66, 34560, 540, 34560, 34560, 66, 540, 540, 540, 66, 540, 66, 540, 540, 34560, 66, 66, 540, 66, 540, 540, 66, 34560, 34560, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 540, 540, 540, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3762696 . Total input tokens: 839302351 . Total output tokens: 737952086
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 109.33801661571488,
    "estimated_duration": 3600.0864505679547,
    "input_throughput": 7212.097919455199,
    "output_throughput": 6257.4083454151705,
    "total_throughput": 13469.50626487037,
    "itl": 89.22117117428667,
    "ttft": 2025198.150804979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9226847921218995,
    "arrivals": 1253655,
    "finished_requests": 105070,
    "scheduler_time": 304.4400861115943
}
#Debug simulation 
Total elapsed time: 109.33827630896121. Arrivals time: 0.8250840734690428 Scheduler time: 108.22211508266628 Scheduler overhead time: 0.11670990334823728 Adapter cache time: 0.025277414824813604 Engine time: 0.11078344285488129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 66, 34560, 34560, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 34560, 540, 66, 66, 540, 34560, 34560, 540, 540, 66, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 540, 66, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 34560, 540, 66, 66, 540, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 66, 34560, 540, 34560, 34560, 540, 66, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 66, 540, 34560, 34560, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 540, 66, 34560, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 540, 66, 540, 34560, 540, 66, 66, 540, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 540, 34560, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 540, 66, 540, 66, 66, 66, 34560, 34560, 540, 540, 66, 66, 540, 66, 34560, 66, 66, 540, 66, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 540, 66, 540, 66, 540, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 34560, 66, 34560, 66, 66, 66, 34560, 540, 34560, 34560, 66, 540, 540, 540, 66, 540, 66, 540, 540, 34560, 66, 66, 540, 66, 540, 540, 66, 34560, 34560, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 540, 540, 540, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3762696 . Total input tokens: 839302351 . Total output tokens: 737952086
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 106.64368164213374,
    "estimated_duration": 3600.0412402664633,
    "input_throughput": 7180.311356123995,
    "output_throughput": 6230.1191856179685,
    "total_throughput": 13410.430541741964,
    "itl": 88.29317664370971,
    "ttft": 2017870.3585207977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 470,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.439120669635017,
    "arrivals": 1253655,
    "finished_requests": 104518,
    "scheduler_time": 306.3215740098262
}
#Debug simulation 
Total elapsed time: 106.6438382002525. Arrivals time: 0.8054967639036477 Scheduler time: 105.54652817174792 Scheduler overhead time: 0.11520599806681275 Adapter cache time: 0.026533150114119053 Engine time: 0.11146712768822908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 66, 34560, 34560, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 34560, 540, 66, 66, 540, 34560, 34560, 540, 540, 66, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 540, 66, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 34560, 540, 66, 66, 540, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 66, 34560, 540, 34560, 34560, 540, 66, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 66, 540, 34560, 34560, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 540, 66, 34560, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 540, 66, 540, 34560, 540, 66, 66, 540, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 540, 34560, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 540, 66, 540, 66, 66, 66, 34560, 34560, 540, 540, 66, 66, 540, 66, 34560, 66, 66, 540, 66, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 540, 66, 540, 66, 540, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 34560, 66, 34560, 66, 66, 66, 34560, 540, 34560, 34560, 66, 540, 540, 540, 66, 540, 66, 540, 540, 34560, 66, 66, 540, 66, 540, 540, 66, 34560, 34560, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 540, 540, 540, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3762696 . Total input tokens: 839302351 . Total output tokens: 737952086
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 104.15963524999097,
    "estimated_duration": 3600.0603476702418,
    "input_throughput": 7177.327740275642,
    "output_throughput": 6218.261317337929,
    "total_throughput": 13395.589057613572,
    "itl": 86.6357596205245,
    "ttft": 2030475.8675822637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 470,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5360159136448335,
    "arrivals": 1253655,
    "finished_requests": 104495,
    "scheduler_time": 306.40645226708614
}
#Debug simulation 
Total elapsed time: 104.15982001787052. Arrivals time: 0.8042167481034994 Scheduler time: 103.06763592036441 Scheduler overhead time: 0.11395131796598434 Adapter cache time: 0.025378251913934946 Engine time: 0.11058052303269506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 66, 34560, 34560, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 34560, 540, 66, 66, 540, 34560, 34560, 540, 540, 66, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 540, 66, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 34560, 540, 66, 66, 540, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 66, 34560, 540, 34560, 34560, 540, 66, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 66, 540, 34560, 34560, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 540, 66, 34560, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 540, 66, 540, 34560, 540, 66, 66, 540, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 540, 34560, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 540, 66, 540, 66, 66, 66, 34560, 34560, 540, 540, 66, 66, 540, 66, 34560, 66, 66, 540, 66, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 540, 66, 540, 66, 540, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 34560, 66, 34560, 66, 66, 66, 34560, 540, 34560, 34560, 66, 540, 540, 540, 66, 540, 66, 540, 540, 34560, 66, 66, 540, 66, 540, 540, 66, 34560, 34560, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 540, 540, 540, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3762696 . Total input tokens: 839302351 . Total output tokens: 737952086
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 107.42610127525404,
    "estimated_duration": 3600.0442431769934,
    "input_throughput": 7128.182118493581,
    "output_throughput": 6193.371662655929,
    "total_throughput": 13321.553781149509,
    "itl": 87.99018229484648,
    "ttft": 2019381.3311341542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2161742768576325,
    "arrivals": 1253655,
    "finished_requests": 103911,
    "scheduler_time": 307.90454051652176
}
#Debug simulation 
Total elapsed time: 107.42628181213513. Arrivals time: 0.9784774933941662 Scheduler time: 106.15549497678876 Scheduler overhead time: 0.11665110196918249 Adapter cache time: 0.026041791774332523 Engine time: 0.11093851737678051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 66, 34560, 34560, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 34560, 540, 66, 66, 540, 34560, 34560, 540, 540, 66, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 540, 66, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 34560, 540, 66, 66, 540, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 66, 34560, 540, 34560, 34560, 540, 66, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 66, 540, 34560, 34560, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 540, 66, 34560, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 540, 66, 540, 34560, 540, 66, 66, 540, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 540, 34560, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 540, 66, 540, 66, 66, 66, 34560, 34560, 540, 540, 66, 66, 540, 66, 34560, 66, 66, 540, 66, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 540, 66, 540, 66, 540, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 34560, 66, 34560, 66, 66, 66, 34560, 540, 34560, 34560, 66, 540, 540, 540, 66, 540, 66, 540, 540, 34560, 66, 66, 540, 66, 540, 540, 66, 34560, 34560, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 540, 540, 540, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3762696 . Total input tokens: 839302351 . Total output tokens: 737952086
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 103.97211463889107,
    "estimated_duration": 3600.012078190505,
    "input_throughput": 7115.684737612269,
    "output_throughput": 6175.191226351101,
    "total_throughput": 13290.87596396337,
    "itl": 86.03668880646666,
    "ttft": 2034575.1307797262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2963205809891467,
    "arrivals": 1253655,
    "finished_requests": 103700,
    "scheduler_time": 308.7175001903434
}
#Debug simulation 
Total elapsed time: 103.97228649212047. Arrivals time: 0.9106397135183215 Scheduler time: 102.77077420754358 Scheduler overhead time: 0.11632948508486152 Adapter cache time: 0.02484780829399824 Engine time: 0.11161940591409802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 66, 34560, 34560, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 34560, 540, 66, 66, 540, 34560, 34560, 540, 540, 66, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 540, 66, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 34560, 540, 66, 66, 540, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 66, 34560, 540, 34560, 34560, 540, 66, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 66, 540, 34560, 34560, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 540, 66, 34560, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 540, 66, 540, 34560, 540, 66, 66, 540, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 540, 34560, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 540, 66, 540, 66, 66, 66, 34560, 34560, 540, 540, 66, 66, 540, 66, 34560, 66, 66, 540, 66, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 540, 66, 540, 66, 540, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 34560, 66, 34560, 66, 66, 66, 34560, 540, 34560, 34560, 66, 540, 540, 540, 66, 540, 66, 540, 540, 34560, 66, 66, 540, 66, 540, 540, 66, 34560, 34560, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 540, 540, 540, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3762696 . Total input tokens: 839302351 . Total output tokens: 737952086
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 109.48843283113092,
    "estimated_duration": 3600.005379521117,
    "input_throughput": 7164.000961418207,
    "output_throughput": 6229.643746527754,
    "total_throughput": 13393.644707945961,
    "itl": 88.22687786082473,
    "ttft": 2030275.30879108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9493714745435735,
    "arrivals": 1253655,
    "finished_requests": 104327,
    "scheduler_time": 307.1837969217659
}
#Debug simulation 
Total elapsed time: 109.48859742702916. Arrivals time: 0.8028120505623519 Scheduler time: 108.39436228293926 Scheduler overhead time: 0.11688229674473405 Adapter cache time: 0.02483817469328642 Engine time: 0.11118159210309386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 66, 34560, 34560, 540, 540, 540, 540, 66, 540, 66, 540, 66, 540, 540, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 34560, 540, 66, 66, 540, 34560, 34560, 540, 540, 66, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 540, 66, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 34560, 540, 66, 66, 540, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 66, 34560, 540, 34560, 34560, 540, 66, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 66, 66, 540, 34560, 34560, 540, 66, 540, 66, 540, 34560, 34560, 540, 66, 540, 66, 34560, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 540, 66, 540, 34560, 540, 66, 66, 540, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 540, 34560, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 66, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 540, 66, 540, 66, 66, 66, 34560, 34560, 540, 540, 66, 66, 540, 66, 34560, 66, 66, 540, 66, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 66, 540, 66, 34560, 540, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 540, 66, 540, 66, 540, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 540, 540, 34560, 66, 34560, 66, 66, 66, 34560, 540, 34560, 34560, 66, 540, 540, 540, 66, 540, 66, 540, 540, 34560, 66, 66, 540, 66, 540, 540, 66, 34560, 34560, 34560, 66, 540, 540, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 540, 540, 540, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3762696 . Total input tokens: 839302351 . Total output tokens: 737952086
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 103.10648841504008,
    "estimated_duration": 3600.025243837354,
    "input_throughput": 7111.860685929322,
    "output_throughput": 6175.844193887124,
    "total_throughput": 13287.704879816445,
    "itl": 85.93249424683844,
    "ttft": 2020939.7063849545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 507,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7357753140665904,
    "arrivals": 1253655,
    "finished_requests": 103451,
    "scheduler_time": 310.07973421678247
}
#Debug simulation 
Total elapsed time: 103.10668103629723. Arrivals time: 0.9666784820146859 Scheduler time: 101.84999206056818 Scheduler overhead time: 0.11564199021086097 Adapter cache time: 0.025927374605089426 Engine time: 0.10961567983031273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 33, 34560, 34560, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 34560, 540, 33, 33, 540, 34560, 34560, 540, 540, 33, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 540, 33, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 34560, 540, 33, 33, 540, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 33, 34560, 540, 34560, 34560, 540, 33, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 33, 540, 34560, 34560, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 540, 33, 34560, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 540, 33, 540, 34560, 540, 33, 33, 540, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 540, 34560, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 540, 33, 540, 33, 33, 33, 34560, 34560, 540, 540, 33, 33, 540, 33, 34560, 33, 33, 540, 33, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 540, 33, 540, 33, 540, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 34560, 33, 34560, 33, 33, 33, 34560, 540, 34560, 34560, 33, 540, 540, 540, 33, 540, 33, 540, 540, 34560, 33, 33, 540, 33, 540, 540, 33, 34560, 34560, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 540, 540, 540, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3759198 . Total input tokens: 838525391 . Total output tokens: 737269949
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 107.51389490999281,
    "estimated_duration": 3600.047186003726,
    "input_throughput": 7188.85994067446,
    "output_throughput": 6236.614644188378,
    "total_throughput": 13425.474584862837,
    "itl": 89.21364859419081,
    "ttft": 2012584.5641636688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1871811533999055,
    "arrivals": 1252442,
    "finished_requests": 104446,
    "scheduler_time": 305.403823474365
}
#Debug simulation 
Total elapsed time: 107.51405370282009. Arrivals time: 0.8300356175750494 Scheduler time: 106.39182125031948 Scheduler overhead time: 0.11603558156639338 Adapter cache time: 0.02641906077042222 Engine time: 0.11113673774525523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 33, 34560, 34560, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 34560, 540, 33, 33, 540, 34560, 34560, 540, 540, 33, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 540, 33, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 34560, 540, 33, 33, 540, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 33, 34560, 540, 34560, 34560, 540, 33, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 33, 540, 34560, 34560, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 540, 33, 34560, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 540, 33, 540, 34560, 540, 33, 33, 540, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 540, 34560, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 540, 33, 540, 33, 33, 33, 34560, 34560, 540, 540, 33, 33, 540, 33, 34560, 33, 33, 540, 33, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 540, 33, 540, 33, 540, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 34560, 33, 34560, 33, 33, 33, 34560, 540, 34560, 34560, 33, 540, 540, 540, 33, 540, 33, 540, 540, 34560, 33, 33, 540, 33, 540, 540, 33, 34560, 34560, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 540, 540, 540, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3759198 . Total input tokens: 838525391 . Total output tokens: 737269949
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 107.53653536504135,
    "estimated_duration": 3600.0113382678496,
    "input_throughput": 7230.189728381182,
    "output_throughput": 6297.149055899255,
    "total_throughput": 13527.338784280437,
    "itl": 88.15268261709554,
    "ttft": 2012549.2760165876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.103813677169387,
    "arrivals": 1252442,
    "finished_requests": 105061,
    "scheduler_time": 303.8700939629237
}
#Debug simulation 
Total elapsed time: 107.53670304594561. Arrivals time: 0.815882689319551 Scheduler time: 106.43282181024551 Scheduler overhead time: 0.11453875992447138 Adapter cache time: 0.024382320698350668 Engine time: 0.1111868629232049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 33, 34560, 34560, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 34560, 540, 33, 33, 540, 34560, 34560, 540, 540, 33, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 540, 33, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 34560, 540, 33, 33, 540, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 33, 34560, 540, 34560, 34560, 540, 33, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 33, 540, 34560, 34560, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 540, 33, 34560, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 540, 33, 540, 34560, 540, 33, 33, 540, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 540, 34560, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 540, 33, 540, 33, 33, 33, 34560, 34560, 540, 540, 33, 33, 540, 33, 34560, 33, 33, 540, 33, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 540, 33, 540, 33, 540, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 34560, 33, 34560, 33, 33, 33, 34560, 540, 34560, 34560, 33, 540, 540, 540, 33, 540, 33, 540, 540, 34560, 33, 33, 540, 33, 540, 540, 33, 34560, 34560, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 540, 540, 540, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3759198 . Total input tokens: 838525391 . Total output tokens: 737269949
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 107.31727804383263,
    "estimated_duration": 3600.0739362337827,
    "input_throughput": 7079.494880225409,
    "output_throughput": 6138.540316513354,
    "total_throughput": 13218.035196738763,
    "itl": 85.6654505050785,
    "ttft": 2035099.1589491367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.979397837463786,
    "arrivals": 1252442,
    "finished_requests": 102931,
    "scheduler_time": 310.6618852420313
}
#Debug simulation 
Total elapsed time: 107.31752535887063. Arrivals time: 0.8372185016050935 Scheduler time: 106.18786738766357 Scheduler overhead time: 0.11655599996447563 Adapter cache time: 0.02486618096008897 Engine time: 0.11207525385543704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 33, 34560, 34560, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 34560, 540, 33, 33, 540, 34560, 34560, 540, 540, 33, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 540, 33, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 34560, 540, 33, 33, 540, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 33, 34560, 540, 34560, 34560, 540, 33, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 33, 540, 34560, 34560, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 540, 33, 34560, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 540, 33, 540, 34560, 540, 33, 33, 540, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 540, 34560, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 540, 33, 540, 33, 33, 33, 34560, 34560, 540, 540, 33, 33, 540, 33, 34560, 33, 33, 540, 33, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 540, 33, 540, 33, 540, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 34560, 33, 34560, 33, 33, 33, 34560, 540, 34560, 34560, 33, 540, 540, 540, 33, 540, 33, 540, 540, 34560, 33, 33, 540, 33, 540, 540, 33, 34560, 34560, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 540, 540, 540, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3759198 . Total input tokens: 838525391 . Total output tokens: 737269949
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 114.01518760016188,
    "estimated_duration": 3600.0268279698694,
    "input_throughput": 7248.223206912833,
    "output_throughput": 6287.708420429986,
    "total_throughput": 13535.93162734282,
    "itl": 88.26939142558244,
    "ttft": 2020084.0165834345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.652434351504777,
    "arrivals": 1252442,
    "finished_requests": 105366,
    "scheduler_time": 302.9258858271334
}
#Debug simulation 
Total elapsed time: 114.01534430123866. Arrivals time: 0.7624224429018795 Scheduler time: 112.95840534335002 Scheduler overhead time: 0.11749330582097173 Adapter cache time: 0.025351517368108034 Engine time: 0.113152374047786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 33, 34560, 34560, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 34560, 540, 33, 33, 540, 34560, 34560, 540, 540, 33, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 540, 33, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 34560, 540, 33, 33, 540, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 33, 34560, 540, 34560, 34560, 540, 33, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 33, 540, 34560, 34560, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 540, 33, 34560, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 540, 33, 540, 34560, 540, 33, 33, 540, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 540, 34560, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 540, 33, 540, 33, 33, 33, 34560, 34560, 540, 540, 33, 33, 540, 33, 34560, 33, 33, 540, 33, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 540, 33, 540, 33, 540, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 34560, 33, 34560, 33, 33, 33, 34560, 540, 34560, 34560, 33, 540, 540, 540, 33, 540, 33, 540, 540, 34560, 33, 33, 540, 33, 540, 540, 33, 34560, 34560, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 540, 540, 540, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3759198 . Total input tokens: 838525391 . Total output tokens: 737269949
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 106.74520812090486,
    "estimated_duration": 3600.05762444235,
    "input_throughput": 7133.051100530021,
    "output_throughput": 6183.906571064527,
    "total_throughput": 13316.957671594548,
    "itl": 86.00600273452247,
    "ttft": 2028227.4674842781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.970953053911236,
    "arrivals": 1252442,
    "finished_requests": 103580,
    "scheduler_time": 308.25541934555866
}
#Debug simulation 
Total elapsed time: 106.74538624472916. Arrivals time: 0.8186013493686914 Scheduler time: 105.6386799220927 Scheduler overhead time: 0.11435200925916433 Adapter cache time: 0.02426842972636223 Engine time: 0.11098278453573585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 33, 34560, 34560, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 34560, 540, 33, 33, 540, 34560, 34560, 540, 540, 33, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 540, 33, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 34560, 540, 33, 33, 540, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 33, 34560, 540, 34560, 34560, 540, 33, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 33, 540, 34560, 34560, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 540, 33, 34560, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 540, 33, 540, 34560, 540, 33, 33, 540, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 540, 34560, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 540, 33, 540, 33, 33, 33, 34560, 34560, 540, 540, 33, 33, 540, 33, 34560, 33, 33, 540, 33, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 540, 33, 540, 33, 540, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 34560, 33, 34560, 33, 33, 33, 34560, 540, 34560, 34560, 33, 540, 540, 540, 33, 540, 33, 540, 540, 34560, 33, 33, 540, 33, 540, 540, 33, 34560, 34560, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 540, 540, 540, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3759198 . Total input tokens: 838525391 . Total output tokens: 737269949
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 108.00068800803274,
    "estimated_duration": 3600.0281778297294,
    "input_throughput": 7209.752456895244,
    "output_throughput": 6282.237216719274,
    "total_throughput": 13491.989673614518,
    "itl": 88.38064317202836,
    "ttft": 2017720.650351268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.713166399742465,
    "arrivals": 1252442,
    "finished_requests": 104817,
    "scheduler_time": 304.7643226025602
}
#Debug simulation 
Total elapsed time: 108.00085964612663. Arrivals time: 0.8488553548231721 Scheduler time: 106.85834463173524 Scheduler overhead time: 0.11680409545078874 Adapter cache time: 0.02518852101638913 Engine time: 0.11271104821935296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [540, 34560, 34560, 33, 34560, 34560, 540, 540, 540, 540, 33, 540, 33, 540, 33, 540, 540, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 34560, 540, 33, 33, 540, 34560, 34560, 540, 540, 33, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 540, 33, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 34560, 540, 33, 33, 540, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 33, 34560, 540, 34560, 34560, 540, 33, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 33, 33, 540, 34560, 34560, 540, 33, 540, 33, 540, 34560, 34560, 540, 33, 540, 33, 34560, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 540, 33, 540, 34560, 540, 33, 33, 540, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 540, 34560, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 33, 540, 34560, 34560, 34560, 34560, 540, 540, 34560, 34560, 34560, 540, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 540, 33, 540, 33, 33, 33, 34560, 34560, 540, 540, 33, 33, 540, 33, 34560, 33, 33, 540, 33, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 33, 540, 33, 34560, 540, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 540, 33, 540, 33, 540, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 540, 540, 34560, 33, 34560, 33, 33, 33, 34560, 540, 34560, 34560, 33, 540, 540, 540, 33, 540, 33, 540, 540, 34560, 33, 33, 540, 33, 540, 540, 33, 34560, 34560, 34560, 33, 540, 540, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 540, 540, 540, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3759198 . Total input tokens: 838525391 . Total output tokens: 737269949
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.01004347857088,
    "estimated_duration": 3600.0123558936684,
    "input_throughput": 7177.608976174082,
    "output_throughput": 6233.580549594876,
    "total_throughput": 13411.18952576896,
    "itl": 86.03674278012095,
    "ttft": 2024206.9023767733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9036474540829853,
    "arrivals": 1252442,
    "finished_requests": 104360,
    "scheduler_time": 306.21735214885496
}
#Debug simulation 
Total elapsed time: 123.01025455258787. Arrivals time: 0.9274005377665162 Scheduler time: 121.75061607500538 Scheduler overhead time: 0.13295507710427046 Adapter cache time: 0.028531287796795368 Engine time: 0.12899444438517094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 135, 34560, 34560, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 34560, 270, 135, 135, 270, 34560, 34560, 270, 270, 135, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 270, 135, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 34560, 270, 135, 135, 270, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 135, 34560, 270, 34560, 34560, 270, 135, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 135, 270, 34560, 34560, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 270, 135, 34560, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 270, 135, 270, 34560, 270, 135, 135, 270, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 270, 34560, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 270, 135, 270, 135, 135, 135, 34560, 34560, 270, 270, 135, 135, 270, 135, 34560, 135, 135, 270, 135, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 270, 135, 270, 135, 270, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 34560, 135, 34560, 135, 135, 135, 34560, 270, 34560, 34560, 135, 270, 270, 270, 135, 270, 135, 270, 270, 34560, 135, 135, 270, 135, 270, 270, 135, 34560, 34560, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 270, 270, 270, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3741120 . Total input tokens: 834499646 . Total output tokens: 733741164
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 130.5160589851439,
    "estimated_duration": 3600.07583457862,
    "input_throughput": 7120.006404809589,
    "output_throughput": 6191.23346956075,
    "total_throughput": 13311.239874370338,
    "itl": 88.3969022132994,
    "ttft": 2030301.78728041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9292972011538496,
    "arrivals": 1246365,
    "finished_requests": 103556,
    "scheduler_time": 309.0016520398625
}
#Debug simulation 
Total elapsed time: 130.51621459284797. Arrivals time: 0.9761237716302276 Scheduler time: 129.19592642365023 Scheduler overhead time: 0.1346072517335415 Adapter cache time: 0.031052141916006804 Engine time: 0.13565941993147135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 135, 34560, 34560, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 34560, 270, 135, 135, 270, 34560, 34560, 270, 270, 135, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 270, 135, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 34560, 270, 135, 135, 270, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 135, 34560, 270, 34560, 34560, 270, 135, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 135, 270, 34560, 34560, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 270, 135, 34560, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 270, 135, 270, 34560, 270, 135, 135, 270, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 270, 34560, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 270, 135, 270, 135, 135, 135, 34560, 34560, 270, 270, 135, 135, 270, 135, 34560, 135, 135, 270, 135, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 270, 135, 270, 135, 270, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 34560, 135, 34560, 135, 135, 135, 34560, 270, 34560, 34560, 135, 270, 270, 270, 135, 270, 135, 270, 270, 34560, 135, 135, 270, 135, 270, 270, 135, 34560, 34560, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 270, 270, 270, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3741120 . Total input tokens: 834499646 . Total output tokens: 733741164
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.54899176117033,
    "estimated_duration": 3600.0734217705144,
    "input_throughput": 7098.965217056929,
    "output_throughput": 6160.091309774863,
    "total_throughput": 13259.056526831791,
    "itl": 87.73470838257201,
    "ttft": 2030946.2862597841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.630647775535477,
    "arrivals": 1246365,
    "finished_requests": 103246,
    "scheduler_time": 309.43205798918274
}
#Debug simulation 
Total elapsed time: 123.549175451044. Arrivals time: 0.9576752614229918 Scheduler time: 122.25319953681901 Scheduler overhead time: 0.1341997990384698 Adapter cache time: 0.03081669146195054 Engine time: 0.1310618780553341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 135, 34560, 34560, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 34560, 270, 135, 135, 270, 34560, 34560, 270, 270, 135, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 270, 135, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 34560, 270, 135, 135, 270, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 135, 34560, 270, 34560, 34560, 270, 135, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 135, 270, 34560, 34560, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 270, 135, 34560, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 270, 135, 270, 34560, 270, 135, 135, 270, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 270, 34560, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 270, 135, 270, 135, 135, 135, 34560, 34560, 270, 270, 135, 135, 270, 135, 34560, 135, 135, 270, 135, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 270, 135, 270, 135, 270, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 34560, 135, 34560, 135, 135, 135, 34560, 270, 34560, 34560, 135, 270, 270, 270, 135, 270, 135, 270, 270, 34560, 135, 135, 270, 135, 270, 270, 135, 34560, 34560, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 270, 270, 270, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3741120 . Total input tokens: 834499646 . Total output tokens: 733741164
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 117.91237385803834,
    "estimated_duration": 3600.078352655234,
    "input_throughput": 7082.967230752926,
    "output_throughput": 6154.202444971553,
    "total_throughput": 13237.169675724479,
    "itl": 85.88751281983045,
    "ttft": 2032606.0630800927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 464,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4875594734028263,
    "arrivals": 1246365,
    "finished_requests": 103122,
    "scheduler_time": 309.8236908582958
}
#Debug simulation 
Total elapsed time: 117.91257369006053. Arrivals time: 0.930588680319488 Scheduler time: 116.65032680612057 Scheduler overhead time: 0.13171975687146187 Adapter cache time: 0.02985103102400899 Engine time: 0.12848690804094076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 135, 34560, 34560, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 34560, 270, 135, 135, 270, 34560, 34560, 270, 270, 135, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 270, 135, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 34560, 270, 135, 135, 270, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 135, 34560, 270, 34560, 34560, 270, 135, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 135, 270, 34560, 34560, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 270, 135, 34560, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 270, 135, 270, 34560, 270, 135, 135, 270, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 270, 34560, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 270, 135, 270, 135, 135, 135, 34560, 34560, 270, 270, 135, 135, 270, 135, 34560, 135, 135, 270, 135, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 270, 135, 270, 135, 270, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 34560, 135, 34560, 135, 135, 135, 34560, 270, 34560, 34560, 135, 270, 270, 270, 135, 270, 135, 270, 270, 34560, 135, 135, 270, 135, 270, 270, 135, 34560, 34560, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 270, 270, 270, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3741120 . Total input tokens: 834499646 . Total output tokens: 733741164
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 124.61183438031003,
    "estimated_duration": 3600.063012318528,
    "input_throughput": 7116.238774804334,
    "output_throughput": 6186.853097789422,
    "total_throughput": 13303.091872593755,
    "itl": 87.90146831898468,
    "ttft": 2030414.1511500452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.360785200470122,
    "arrivals": 1246365,
    "finished_requests": 103558,
    "scheduler_time": 308.0475420423003
}
#Debug simulation 
Total elapsed time: 124.61200901307166. Arrivals time: 0.9534530858509243 Scheduler time: 123.32299955608323 Scheduler overhead time: 0.13339099008589983 Adapter cache time: 0.030559435952454805 Engine time: 0.12976922281086445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 135, 34560, 34560, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 34560, 270, 135, 135, 270, 34560, 34560, 270, 270, 135, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 270, 135, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 34560, 270, 135, 135, 270, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 135, 34560, 270, 34560, 34560, 270, 135, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 135, 270, 34560, 34560, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 270, 135, 34560, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 270, 135, 270, 34560, 270, 135, 135, 270, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 270, 34560, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 270, 135, 270, 135, 135, 135, 34560, 34560, 270, 270, 135, 135, 270, 135, 34560, 135, 135, 270, 135, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 270, 135, 270, 135, 270, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 34560, 135, 34560, 135, 135, 135, 34560, 270, 34560, 34560, 135, 270, 270, 270, 135, 270, 135, 270, 270, 34560, 135, 135, 270, 135, 270, 270, 135, 34560, 34560, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 270, 270, 270, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3741120 . Total input tokens: 834499646 . Total output tokens: 733741164
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 125.71681778831407,
    "estimated_duration": 3600.0443928121804,
    "input_throughput": 7016.765140573056,
    "output_throughput": 6098.315910724211,
    "total_throughput": 13115.081051297266,
    "itl": 85.43134592777825,
    "ttft": 2033485.7264215625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1798397962330145,
    "arrivals": 1246365,
    "finished_requests": 102134,
    "scheduler_time": 312.78762100990457
}
#Debug simulation 
Total elapsed time: 125.7169956592843. Arrivals time: 0.9446474649012089 Scheduler time: 124.43358089681715 Scheduler overhead time: 0.1361841238103807 Adapter cache time: 0.03041358618065715 Engine time: 0.13015989074483514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 135, 34560, 34560, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 34560, 270, 135, 135, 270, 34560, 34560, 270, 270, 135, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 270, 135, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 34560, 270, 135, 135, 270, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 135, 34560, 270, 34560, 34560, 270, 135, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 135, 270, 34560, 34560, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 270, 135, 34560, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 270, 135, 270, 34560, 270, 135, 135, 270, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 270, 34560, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 270, 135, 270, 135, 135, 135, 34560, 34560, 270, 270, 135, 135, 270, 135, 34560, 135, 135, 270, 135, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 270, 135, 270, 135, 270, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 34560, 135, 34560, 135, 135, 135, 34560, 270, 34560, 34560, 135, 270, 270, 270, 135, 270, 135, 270, 270, 34560, 135, 135, 270, 135, 270, 270, 135, 34560, 34560, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 270, 270, 270, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3741120 . Total input tokens: 834499646 . Total output tokens: 733741164
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 126.2806081911549,
    "estimated_duration": 3600.0083196998294,
    "input_throughput": 7121.891874443719,
    "output_throughput": 6188.9215305640555,
    "total_throughput": 13310.813405007773,
    "itl": 87.29526305304069,
    "ttft": 2028312.4241768124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.994058921127567,
    "arrivals": 1246365,
    "finished_requests": 103604,
    "scheduler_time": 308.09822466498673
}
#Debug simulation 
Total elapsed time: 126.2808353593573. Arrivals time: 0.9628251045942307 Scheduler time: 124.98835629690439 Scheduler overhead time: 0.13059434993192554 Adapter cache time: 0.03051105933263898 Engine time: 0.12739457841962576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 135, 34560, 34560, 270, 270, 270, 270, 135, 270, 135, 270, 135, 270, 270, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 34560, 270, 135, 135, 270, 34560, 34560, 270, 270, 135, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 135, 34560, 135, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 270, 135, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 34560, 270, 135, 135, 270, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 135, 34560, 270, 34560, 34560, 270, 135, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 135, 135, 270, 34560, 34560, 270, 135, 270, 135, 270, 34560, 34560, 270, 135, 270, 135, 34560, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 270, 135, 270, 34560, 270, 135, 135, 270, 34560, 135, 135, 135, 34560, 34560, 34560, 135, 270, 34560, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 135, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 270, 135, 270, 135, 135, 135, 34560, 34560, 270, 270, 135, 135, 270, 135, 34560, 135, 135, 270, 135, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 135, 270, 135, 34560, 270, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 135, 135, 34560, 270, 135, 270, 135, 270, 34560, 34560, 34560, 135, 34560, 34560, 34560, 34560, 135, 270, 270, 34560, 135, 34560, 135, 135, 135, 34560, 270, 34560, 34560, 135, 270, 270, 270, 135, 270, 135, 270, 270, 34560, 135, 135, 270, 135, 270, 270, 135, 34560, 34560, 34560, 135, 270, 270, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 270, 270, 270, 135, 135, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 3741120 . Total input tokens: 834499646 . Total output tokens: 733741164
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 119.3095189994201,
    "estimated_duration": 3600.046048464958,
    "input_throughput": 7121.75807054807,
    "output_throughput": 6174.993791948544,
    "total_throughput": 13296.751862496612,
    "itl": 85.88691775364275,
    "ttft": 2034187.528372121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 466,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4446835835650838,
    "arrivals": 1246365,
    "finished_requests": 103608,
    "scheduler_time": 308.6920185809233
}
#Debug simulation 
Total elapsed time: 119.30969994422048. Arrivals time: 0.9614215395413339 Scheduler time: 118.01438425201923 Scheduler overhead time: 0.13278618268668652 Adapter cache time: 0.028967063408344984 Engine time: 0.12992043513804674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 66, 34560, 34560, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 34560, 270, 66, 66, 270, 34560, 34560, 270, 270, 66, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 270, 66, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 34560, 270, 66, 66, 270, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 66, 34560, 270, 34560, 34560, 270, 66, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 66, 270, 34560, 34560, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 270, 66, 34560, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 270, 66, 270, 34560, 270, 66, 66, 270, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 270, 34560, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 270, 66, 270, 66, 66, 66, 34560, 34560, 270, 270, 66, 66, 270, 66, 34560, 66, 66, 270, 66, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 270, 66, 270, 66, 270, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 34560, 66, 34560, 66, 66, 66, 34560, 270, 34560, 34560, 66, 270, 270, 270, 66, 270, 66, 270, 270, 34560, 66, 66, 270, 66, 270, 270, 66, 34560, 34560, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 270, 270, 270, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3733806 . Total input tokens: 832836264 . Total output tokens: 732313195
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 124.4404017040506,
    "estimated_duration": 3600.016918157836,
    "input_throughput": 7266.968626743829,
    "output_throughput": 6357.90984329938,
    "total_throughput": 13624.878470043208,
    "itl": 89.55389375532098,
    "ttft": 2009346.2397892599,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1012198359845535,
    "arrivals": 1243986,
    "finished_requests": 105771,
    "scheduler_time": 300.7589316656892
}
#Debug simulation 
Total elapsed time: 124.44058698508888. Arrivals time: 0.983582706656307 Scheduler time: 123.11786728119478 Scheduler overhead time: 0.13527514412999153 Adapter cache time: 0.03023927239701152 Engine time: 0.13201868254691362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 66, 34560, 34560, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 34560, 270, 66, 66, 270, 34560, 34560, 270, 270, 66, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 270, 66, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 34560, 270, 66, 66, 270, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 66, 34560, 270, 34560, 34560, 270, 66, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 66, 270, 34560, 34560, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 270, 66, 34560, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 270, 66, 270, 34560, 270, 66, 66, 270, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 270, 34560, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 270, 66, 270, 66, 66, 66, 34560, 34560, 270, 270, 66, 66, 270, 66, 34560, 66, 66, 270, 66, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 270, 66, 270, 66, 270, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 34560, 66, 34560, 66, 66, 66, 34560, 270, 34560, 34560, 66, 270, 270, 270, 66, 270, 66, 270, 270, 34560, 66, 66, 270, 66, 270, 270, 66, 34560, 34560, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 270, 270, 270, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3733806 . Total input tokens: 832836264 . Total output tokens: 732313195
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.3820445057936,
    "estimated_duration": 3600.0571364967627,
    "input_throughput": 7198.720191763075,
    "output_throughput": 6284.013875961533,
    "total_throughput": 13482.734067724607,
    "itl": 88.04437861030522,
    "ttft": 2024738.3095840989,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.376386350397956,
    "arrivals": 1243986,
    "finished_requests": 104831,
    "scheduler_time": 303.98282359029736
}
#Debug simulation 
Total elapsed time: 123.38239829568192. Arrivals time: 0.9739626781083643 Scheduler time: 122.07173949666321 Scheduler overhead time: 0.13498303992673755 Adapter cache time: 0.030489148106426 Engine time: 0.1292058415710926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 66, 34560, 34560, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 34560, 270, 66, 66, 270, 34560, 34560, 270, 270, 66, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 270, 66, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 34560, 270, 66, 66, 270, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 66, 34560, 270, 34560, 34560, 270, 66, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 66, 270, 34560, 34560, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 270, 66, 34560, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 270, 66, 270, 34560, 270, 66, 66, 270, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 270, 34560, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 270, 66, 270, 66, 66, 66, 34560, 34560, 270, 270, 66, 66, 270, 66, 34560, 66, 66, 270, 66, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 270, 66, 270, 66, 270, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 34560, 66, 34560, 66, 66, 66, 34560, 270, 34560, 34560, 66, 270, 270, 270, 66, 270, 66, 270, 270, 34560, 66, 66, 270, 66, 270, 270, 66, 34560, 34560, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 270, 270, 270, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3733806 . Total input tokens: 832836264 . Total output tokens: 732313195
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 127.05581921059638,
    "estimated_duration": 3600.0447967403966,
    "input_throughput": 7082.959640693261,
    "output_throughput": 6188.042165522652,
    "total_throughput": 13271.001806215912,
    "itl": 85.9573280485372,
    "ttft": 2031523.26900959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2440364243975157,
    "arrivals": 1243986,
    "finished_requests": 103124,
    "scheduler_time": 308.5564535887343
}
#Debug simulation 
Total elapsed time: 127.0560605819337. Arrivals time: 0.9716163952834904 Scheduler time: 125.74097802536562 Scheduler overhead time: 0.13600816437974572 Adapter cache time: 0.03098761336877942 Engine time: 0.13255970273166895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 66, 34560, 34560, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 34560, 270, 66, 66, 270, 34560, 34560, 270, 270, 66, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 270, 66, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 34560, 270, 66, 66, 270, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 66, 34560, 270, 34560, 34560, 270, 66, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 66, 270, 34560, 34560, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 270, 66, 34560, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 270, 66, 270, 34560, 270, 66, 66, 270, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 270, 34560, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 270, 66, 270, 66, 66, 66, 34560, 34560, 270, 270, 66, 66, 270, 66, 34560, 66, 66, 270, 66, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 270, 66, 270, 66, 270, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 34560, 66, 34560, 66, 66, 66, 34560, 270, 34560, 34560, 66, 270, 270, 270, 66, 270, 66, 270, 270, 34560, 66, 66, 270, 66, 270, 270, 66, 34560, 34560, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 270, 270, 270, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3733806 . Total input tokens: 832836264 . Total output tokens: 732313195
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 96.11443294771016,
    "estimated_duration": 3600.0028426282224,
    "input_throughput": 7232.507622408254,
    "output_throughput": 6305.488632177794,
    "total_throughput": 13537.996254586047,
    "itl": 88.29920772808575,
    "ttft": 2026875.2311503328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3754984472505702,
    "arrivals": 1243986,
    "finished_requests": 105228,
    "scheduler_time": 301.70110816821074
}
#Debug simulation 
Total elapsed time: 96.11460862588137. Arrivals time: 0.8253366216085851 Scheduler time: 95.03238949505612 Scheduler overhead time: 0.1016598273999989 Adapter cache time: 0.023234234657138586 Engine time: 0.09709555376321077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 66, 34560, 34560, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 34560, 270, 66, 66, 270, 34560, 34560, 270, 270, 66, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 270, 66, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 34560, 270, 66, 66, 270, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 66, 34560, 270, 34560, 34560, 270, 66, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 66, 270, 34560, 34560, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 270, 66, 34560, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 270, 66, 270, 34560, 270, 66, 66, 270, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 270, 34560, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 270, 66, 270, 66, 66, 66, 34560, 34560, 270, 270, 66, 66, 270, 66, 34560, 66, 66, 270, 66, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 270, 66, 270, 66, 270, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 34560, 66, 34560, 66, 66, 66, 34560, 270, 34560, 34560, 66, 270, 270, 270, 66, 270, 66, 270, 270, 34560, 66, 66, 270, 66, 270, 270, 66, 34560, 34560, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 270, 270, 270, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3733806 . Total input tokens: 832836264 . Total output tokens: 732313195
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 123.7492010188289,
    "estimated_duration": 3600.033091408714,
    "input_throughput": 6977.938080610832,
    "output_throughput": 6078.722457364084,
    "total_throughput": 13056.660537974916,
    "itl": 85.63245766216873,
    "ttft": 2034246.8828773818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1302959011262517,
    "arrivals": 1243986,
    "finished_requests": 101503,
    "scheduler_time": 313.64296872136794
}
#Debug simulation 
Total elapsed time: 123.74939072178677. Arrivals time: 0.9270873288623989 Scheduler time: 122.48639338929206 Scheduler overhead time: 0.1341782510280609 Adapter cache time: 0.029510541819036007 Engine time: 0.12941734259948134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 66, 34560, 34560, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 34560, 270, 66, 66, 270, 34560, 34560, 270, 270, 66, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 270, 66, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 34560, 270, 66, 66, 270, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 66, 34560, 270, 34560, 34560, 270, 66, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 66, 270, 34560, 34560, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 270, 66, 34560, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 270, 66, 270, 34560, 270, 66, 66, 270, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 270, 34560, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 270, 66, 270, 66, 66, 66, 34560, 34560, 270, 270, 66, 66, 270, 66, 34560, 66, 66, 270, 66, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 270, 66, 270, 66, 270, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 34560, 66, 34560, 66, 66, 66, 34560, 270, 34560, 34560, 66, 270, 270, 270, 66, 270, 66, 270, 270, 34560, 66, 66, 270, 66, 270, 270, 66, 34560, 34560, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 270, 270, 270, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3733806 . Total input tokens: 832836264 . Total output tokens: 732313195
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 121.03486881498247,
    "estimated_duration": 3600.032897605801,
    "input_throughput": 7184.944064595128,
    "output_throughput": 6263.694149849723,
    "total_throughput": 13448.63821444485,
    "itl": 88.38172219741057,
    "ttft": 2028851.5319901104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.90468402795958,
    "arrivals": 1243986,
    "finished_requests": 104566,
    "scheduler_time": 304.00463299076506
}
#Debug simulation 
Total elapsed time: 121.0350431445986. Arrivals time: 0.9123805519193411 Scheduler time: 119.80042615858838 Scheduler overhead time: 0.12742628902196884 Adapter cache time: 0.028621531557291746 Engine time: 0.12391877779737115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_320_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 66, 34560, 34560, 270, 270, 270, 270, 66, 270, 66, 270, 66, 270, 270, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 34560, 270, 66, 66, 270, 34560, 34560, 270, 270, 66, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 270, 66, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 34560, 270, 66, 66, 270, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 66, 34560, 270, 34560, 34560, 270, 66, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 66, 66, 270, 34560, 34560, 270, 66, 270, 66, 270, 34560, 34560, 270, 66, 270, 66, 34560, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 270, 66, 270, 34560, 270, 66, 66, 270, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 270, 34560, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 66, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 270, 66, 270, 66, 66, 66, 34560, 34560, 270, 270, 66, 66, 270, 66, 34560, 66, 66, 270, 66, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 66, 270, 66, 34560, 270, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 270, 66, 270, 66, 270, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 270, 270, 34560, 66, 34560, 66, 66, 66, 34560, 270, 34560, 34560, 66, 270, 270, 270, 66, 270, 66, 270, 270, 34560, 66, 66, 270, 66, 270, 270, 66, 34560, 34560, 34560, 66, 270, 270, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 270, 270, 270, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3733806 . Total input tokens: 832836264 . Total output tokens: 732313195
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 118.67219537729397,
    "estimated_duration": 3600.0031245114906,
    "input_throughput": 7139.078526074198,
    "output_throughput": 6237.8048638630635,
    "total_throughput": 13376.88338993726,
    "itl": 86.41201397395952,
    "ttft": 2021206.8061293208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.439397846907405,
    "arrivals": 1243986,
    "finished_requests": 103886,
    "scheduler_time": 306.36744444231374
}
#Debug simulation 
Total elapsed time: 118.67239462118596. Arrivals time: 0.8920607380568981 Scheduler time: 117.45408226782456 Scheduler overhead time: 0.1305891158990562 Adapter cache time: 0.030179957393556833 Engine time: 0.12409172672778368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 33, 34560, 34560, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 34560, 270, 33, 33, 270, 34560, 34560, 270, 270, 33, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 270, 33, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 34560, 270, 33, 33, 270, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 33, 34560, 270, 34560, 34560, 270, 33, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 33, 270, 34560, 34560, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 270, 33, 34560, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 270, 33, 270, 34560, 270, 33, 33, 270, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 270, 34560, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 270, 33, 270, 33, 33, 33, 34560, 34560, 270, 270, 33, 33, 270, 33, 34560, 33, 33, 270, 33, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 270, 33, 270, 33, 270, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 34560, 33, 34560, 33, 33, 33, 34560, 270, 34560, 34560, 33, 270, 270, 270, 33, 270, 33, 270, 270, 34560, 33, 33, 270, 33, 270, 270, 33, 34560, 34560, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 270, 270, 270, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3730308 . Total input tokens: 832057723 . Total output tokens: 731636072
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 126.99654913181439,
    "estimated_duration": 3600.053022147046,
    "input_throughput": 7249.466838250916,
    "output_throughput": 6312.716468394209,
    "total_throughput": 13562.183306645125,
    "itl": 89.36565344619773,
    "ttft": 2020331.1617717312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5788395224604916,
    "arrivals": 1242821,
    "finished_requests": 105435,
    "scheduler_time": 301.620835634591
}
#Debug simulation 
Total elapsed time: 126.99671554518864. Arrivals time: 0.948255461640656 Scheduler time: 125.71433352399617 Scheduler overhead time: 0.13511216640472412 Adapter cache time: 0.029215462040156126 Engine time: 0.12772178230807185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 33, 34560, 34560, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 34560, 270, 33, 33, 270, 34560, 34560, 270, 270, 33, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 270, 33, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 34560, 270, 33, 33, 270, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 33, 34560, 270, 34560, 34560, 270, 33, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 33, 270, 34560, 34560, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 270, 33, 34560, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 270, 33, 270, 34560, 270, 33, 33, 270, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 270, 34560, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 270, 33, 270, 33, 33, 33, 34560, 34560, 270, 270, 33, 33, 270, 33, 34560, 33, 33, 270, 33, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 270, 33, 270, 33, 270, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 34560, 33, 34560, 33, 33, 33, 34560, 270, 34560, 34560, 33, 270, 270, 270, 33, 270, 33, 270, 270, 34560, 33, 33, 270, 33, 270, 270, 33, 34560, 34560, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 270, 270, 270, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3730308 . Total input tokens: 832057723 . Total output tokens: 731636072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.47880275780335,
    "estimated_duration": 3600.0480079988506,
    "input_throughput": 7251.413576151492,
    "output_throughput": 6321.112648897561,
    "total_throughput": 13572.526225049052,
    "itl": 88.74456905668326,
    "ttft": 2021716.5483316516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0471894257515713,
    "arrivals": 1242821,
    "finished_requests": 105573,
    "scheduler_time": 301.074883187528
}
#Debug simulation 
Total elapsed time: 123.47899145213887. Arrivals time: 0.9569315873086452 Scheduler time: 122.18171526817605 Scheduler overhead time: 0.13533139787614346 Adapter cache time: 0.029907261952757835 Engine time: 0.13172396551817656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 33, 34560, 34560, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 34560, 270, 33, 33, 270, 34560, 34560, 270, 270, 33, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 270, 33, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 34560, 270, 33, 33, 270, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 33, 34560, 270, 34560, 34560, 270, 33, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 33, 270, 34560, 34560, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 270, 33, 34560, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 270, 33, 270, 34560, 270, 33, 33, 270, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 270, 34560, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 270, 33, 270, 33, 33, 33, 34560, 34560, 270, 270, 33, 33, 270, 33, 34560, 33, 33, 270, 33, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 270, 33, 270, 33, 270, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 34560, 33, 34560, 33, 33, 33, 34560, 270, 34560, 34560, 33, 270, 270, 270, 33, 270, 33, 270, 270, 34560, 33, 33, 270, 33, 270, 270, 33, 34560, 34560, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 270, 270, 270, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3730308 . Total input tokens: 832057723 . Total output tokens: 731636072
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 121.41114885406569,
    "estimated_duration": 3600.084598334882,
    "input_throughput": 7162.095860726644,
    "output_throughput": 6242.049148065506,
    "total_throughput": 13404.145008792151,
    "itl": 86.65262128624937,
    "ttft": 2023264.658675168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.045247272248401,
    "arrivals": 1242821,
    "finished_requests": 104299,
    "scheduler_time": 305.092831351981
}
#Debug simulation 
Total elapsed time: 121.41132079204544. Arrivals time: 0.9026307221502066 Scheduler time: 120.17703495454043 Scheduler overhead time: 0.13334535295143723 Adapter cache time: 0.02841418469324708 Engine time: 0.128890011459589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 33, 34560, 34560, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 34560, 270, 33, 33, 270, 34560, 34560, 270, 270, 33, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 270, 33, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 34560, 270, 33, 33, 270, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 33, 34560, 270, 34560, 34560, 270, 33, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 33, 270, 34560, 34560, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 270, 33, 34560, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 270, 33, 270, 34560, 270, 33, 33, 270, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 270, 34560, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 270, 33, 270, 33, 33, 33, 34560, 34560, 270, 270, 33, 33, 270, 33, 34560, 33, 33, 270, 33, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 270, 33, 270, 33, 270, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 34560, 33, 34560, 33, 33, 33, 34560, 270, 34560, 34560, 33, 270, 270, 270, 33, 270, 33, 270, 270, 34560, 33, 33, 270, 33, 270, 270, 33, 34560, 34560, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 270, 270, 270, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3730308 . Total input tokens: 832057723 . Total output tokens: 731636072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 123.54749259678647,
    "estimated_duration": 3600.0204302820025,
    "input_throughput": 7200.85330126011,
    "output_throughput": 6275.471052876858,
    "total_throughput": 13476.324354136968,
    "itl": 88.47037900912407,
    "ttft": 2019818.9846900392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7781768431002245,
    "arrivals": 1242821,
    "finished_requests": 104866,
    "scheduler_time": 303.43962062733004
}
#Debug simulation 
Total elapsed time: 123.54779613483697. Arrivals time: 0.9373234692029655 Scheduler time: 122.26393068302423 Scheduler overhead time: 0.13747320650145411 Adapter cache time: 0.0306714354082942 Engine time: 0.13652047095820308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 33, 34560, 34560, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 34560, 270, 33, 33, 270, 34560, 34560, 270, 270, 33, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 270, 33, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 34560, 270, 33, 33, 270, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 33, 34560, 270, 34560, 34560, 270, 33, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 33, 270, 34560, 34560, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 270, 33, 34560, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 270, 33, 270, 34560, 270, 33, 33, 270, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 270, 34560, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 270, 33, 270, 33, 33, 33, 34560, 34560, 270, 270, 33, 33, 270, 33, 34560, 33, 33, 270, 33, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 270, 33, 270, 33, 270, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 34560, 33, 34560, 33, 33, 33, 34560, 270, 34560, 34560, 33, 270, 270, 270, 33, 270, 33, 270, 270, 34560, 33, 33, 270, 33, 270, 270, 33, 34560, 34560, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 270, 270, 270, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3730308 . Total input tokens: 832057723 . Total output tokens: 731636072
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 120.25312011921778,
    "estimated_duration": 3600.0582820903924,
    "input_throughput": 7183.882307867212,
    "output_throughput": 6259.972265480714,
    "total_throughput": 13443.854573347926,
    "itl": 86.7533067099118,
    "ttft": 2025551.9695018316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1496603987505885,
    "arrivals": 1242821,
    "finished_requests": 104631,
    "scheduler_time": 304.20022120735234
}
#Debug simulation 
Total elapsed time: 120.2533009070903. Arrivals time: 0.935722628608346 Scheduler time: 118.98318860819563 Scheduler overhead time: 0.1331807584501803 Adapter cache time: 0.02961369976401329 Engine time: 0.1291366615332663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 33, 34560, 34560, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 34560, 270, 33, 33, 270, 34560, 34560, 270, 270, 33, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 270, 33, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 34560, 270, 33, 33, 270, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 33, 34560, 270, 34560, 34560, 270, 33, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 33, 270, 34560, 34560, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 270, 33, 34560, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 270, 33, 270, 34560, 270, 33, 33, 270, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 270, 34560, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 270, 33, 270, 33, 33, 33, 34560, 34560, 270, 270, 33, 33, 270, 33, 34560, 33, 33, 270, 33, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 270, 33, 270, 33, 270, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 34560, 33, 34560, 33, 33, 33, 34560, 270, 34560, 34560, 33, 270, 270, 270, 33, 270, 33, 270, 270, 34560, 33, 33, 270, 33, 270, 270, 33, 34560, 34560, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 270, 270, 270, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3730308 . Total input tokens: 832057723 . Total output tokens: 731636072
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 124.65390799287707,
    "estimated_duration": 3600.023126788019,
    "input_throughput": 7224.851642329889,
    "output_throughput": 6305.113661936918,
    "total_throughput": 13529.965304266807,
    "itl": 88.79330755141125,
    "ttft": 2015393.1878726159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.636559348455619,
    "arrivals": 1242821,
    "finished_requests": 105139,
    "scheduler_time": 302.3239579317701
}
#Debug simulation 
Total elapsed time: 124.65408826386556. Arrivals time: 0.9451968907378614 Scheduler time: 123.37855826737359 Scheduler overhead time: 0.13207258004695177 Adapter cache time: 0.028798526152968407 Engine time: 0.12829281948506832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [270, 34560, 34560, 33, 34560, 34560, 270, 270, 270, 270, 33, 270, 33, 270, 33, 270, 270, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 34560, 270, 33, 33, 270, 34560, 34560, 270, 270, 33, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 270, 33, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 34560, 270, 33, 33, 270, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 33, 34560, 270, 34560, 34560, 270, 33, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 33, 270, 34560, 34560, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 270, 33, 34560, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 270, 33, 270, 34560, 270, 33, 33, 270, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 270, 34560, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 34560, 34560, 34560, 270, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 270, 33, 270, 33, 33, 33, 34560, 34560, 270, 270, 33, 33, 270, 33, 34560, 33, 33, 270, 33, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 270, 33, 270, 33, 270, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 34560, 33, 34560, 33, 33, 33, 34560, 270, 34560, 34560, 33, 270, 270, 270, 33, 270, 33, 270, 270, 34560, 33, 33, 270, 33, 270, 270, 33, 34560, 34560, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 270, 270, 270, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3730308 . Total input tokens: 832057723 . Total output tokens: 731636072
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 119.19775094790384,
    "estimated_duration": 3600.061110872848,
    "input_throughput": 7144.94176843669,
    "output_throughput": 6225.2415472376,
    "total_throughput": 13370.183315674289,
    "itl": 86.668168555371,
    "ttft": 2023424.6588318888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.969602482095381,
    "arrivals": 1242821,
    "finished_requests": 104020,
    "scheduler_time": 306.18679575158774
}
#Debug simulation 
Total elapsed time: 119.19792515505105. Arrivals time: 0.9432416381314397 Scheduler time: 117.91434438107535 Scheduler overhead time: 0.13430765876546502 Adapter cache time: 0.029641843400895596 Engine time: 0.13477473193779588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 66, 34560, 34560, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 34560, 135, 66, 66, 135, 34560, 34560, 135, 135, 66, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 135, 66, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 34560, 135, 66, 66, 135, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 66, 34560, 135, 34560, 34560, 135, 66, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 66, 135, 34560, 34560, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 135, 66, 34560, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 135, 66, 135, 34560, 135, 66, 66, 135, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 135, 34560, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 135, 66, 135, 66, 66, 66, 34560, 34560, 135, 135, 66, 66, 135, 66, 34560, 66, 66, 135, 66, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 135, 66, 135, 66, 135, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 34560, 66, 34560, 66, 66, 66, 34560, 135, 34560, 34560, 66, 135, 135, 135, 66, 135, 66, 135, 135, 34560, 66, 66, 135, 66, 135, 135, 66, 34560, 34560, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 135, 135, 135, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3719361 . Total input tokens: 829649856 . Total output tokens: 729485100
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 131.24559092102572,
    "estimated_duration": 3600.101280249412,
    "input_throughput": 6873.489403133033,
    "output_throughput": 6020.735616220873,
    "total_throughput": 12894.225019353908,
    "itl": 87.5114906118081,
    "ttft": 2033535.4558592418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.559002295364641,
    "arrivals": 1239193,
    "finished_requests": 100576,
    "scheduler_time": 316.8039415135258
}
#Debug simulation 
Total elapsed time: 131.24578880285844. Arrivals time: 0.9332858817651868 Scheduler time: 129.97380021354184 Scheduler overhead time: 0.13504815520718694 Adapter cache time: 0.029315748251974583 Engine time: 0.1316880416125059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 66, 34560, 34560, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 34560, 135, 66, 66, 135, 34560, 34560, 135, 135, 66, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 135, 66, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 34560, 135, 66, 66, 135, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 66, 34560, 135, 34560, 34560, 135, 66, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 66, 135, 34560, 34560, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 135, 66, 34560, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 135, 66, 135, 34560, 135, 66, 66, 135, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 135, 34560, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 135, 66, 135, 66, 66, 66, 34560, 34560, 135, 135, 66, 66, 135, 66, 34560, 66, 66, 135, 66, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 135, 66, 135, 66, 135, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 34560, 66, 34560, 66, 66, 66, 34560, 135, 34560, 34560, 66, 135, 135, 135, 66, 135, 66, 135, 135, 34560, 66, 66, 135, 66, 135, 135, 66, 34560, 34560, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 135, 135, 135, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3719361 . Total input tokens: 829649856 . Total output tokens: 729485100
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.75427861139178,
    "estimated_duration": 3600.061321225808,
    "input_throughput": 7124.269758623726,
    "output_throughput": 6243.086990625808,
    "total_throughput": 13367.356749249533,
    "itl": 88.22599998141884,
    "ttft": 2022885.5680269233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2056825590971902,
    "arrivals": 1239193,
    "finished_requests": 104275,
    "scheduler_time": 305.07927305320106
}
#Debug simulation 
Total elapsed time: 123.75447058025748. Arrivals time: 0.960936083458364 Scheduler time: 122.45950983883813 Scheduler overhead time: 0.13374269055202603 Adapter cache time: 0.02916840324178338 Engine time: 0.12945092050358653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 66, 34560, 34560, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 34560, 135, 66, 66, 135, 34560, 34560, 135, 135, 66, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 135, 66, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 34560, 135, 66, 66, 135, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 66, 34560, 135, 34560, 34560, 135, 66, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 66, 135, 34560, 34560, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 135, 66, 34560, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 135, 66, 135, 34560, 135, 66, 66, 135, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 135, 34560, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 135, 66, 135, 66, 66, 66, 34560, 34560, 135, 135, 66, 66, 135, 66, 34560, 66, 66, 135, 66, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 135, 66, 135, 66, 135, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 34560, 66, 34560, 66, 66, 66, 34560, 135, 34560, 34560, 66, 135, 135, 135, 66, 135, 66, 135, 135, 34560, 66, 66, 135, 66, 135, 135, 66, 34560, 34560, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 135, 135, 135, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3719361 . Total input tokens: 829649856 . Total output tokens: 729485100
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.78669534809887,
    "estimated_duration": 3600.0589661637355,
    "input_throughput": 7120.853641827283,
    "output_throughput": 6248.205990906246,
    "total_throughput": 13369.05963273353,
    "itl": 86.60489803407408,
    "ttft": 2021255.5932930377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3333411901071908,
    "arrivals": 1239193,
    "finished_requests": 104313,
    "scheduler_time": 304.8104342281824
}
#Debug simulation 
Total elapsed time: 123.78691875608638. Arrivals time: 0.928317476529628 Scheduler time: 122.5315582123585 Scheduler overhead time: 0.13045298168435693 Adapter cache time: 0.029307777993381023 Engine time: 0.12516726134344935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 66, 34560, 34560, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 34560, 135, 66, 66, 135, 34560, 34560, 135, 135, 66, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 135, 66, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 34560, 135, 66, 66, 135, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 66, 34560, 135, 34560, 34560, 135, 66, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 66, 135, 34560, 34560, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 135, 66, 34560, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 135, 66, 135, 34560, 135, 66, 66, 135, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 135, 34560, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 135, 66, 135, 66, 66, 66, 34560, 34560, 135, 135, 66, 66, 135, 66, 34560, 66, 66, 135, 66, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 135, 66, 135, 66, 135, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 34560, 66, 34560, 66, 66, 66, 34560, 135, 34560, 34560, 66, 135, 135, 135, 66, 135, 66, 135, 135, 34560, 66, 66, 135, 66, 135, 135, 66, 34560, 34560, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 135, 135, 135, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3719361 . Total input tokens: 829649856 . Total output tokens: 729485100
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 123.97995049366727,
    "estimated_duration": 3600.070371668237,
    "input_throughput": 7119.418331848642,
    "output_throughput": 6240.615510409919,
    "total_throughput": 13360.033842258561,
    "itl": 88.35487696666732,
    "ttft": 2022085.863821574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9469352295016833,
    "arrivals": 1239193,
    "finished_requests": 104225,
    "scheduler_time": 305.28478955793986
}
#Debug simulation 
Total elapsed time: 123.98013892676681. Arrivals time: 0.9542231168597937 Scheduler time: 122.69138258788735 Scheduler overhead time: 0.13401755457744002 Adapter cache time: 0.03047583717852831 Engine time: 0.12795050395652652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 66, 34560, 34560, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 34560, 135, 66, 66, 135, 34560, 34560, 135, 135, 66, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 135, 66, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 34560, 135, 66, 66, 135, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 66, 34560, 135, 34560, 34560, 135, 66, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 66, 135, 34560, 34560, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 135, 66, 34560, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 135, 66, 135, 34560, 135, 66, 66, 135, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 135, 34560, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 135, 66, 135, 66, 66, 66, 34560, 34560, 135, 135, 66, 66, 135, 66, 34560, 66, 66, 135, 66, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 135, 66, 135, 66, 135, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 34560, 66, 34560, 66, 66, 66, 34560, 135, 34560, 34560, 66, 135, 135, 135, 66, 135, 66, 135, 135, 34560, 66, 66, 135, 66, 135, 135, 66, 34560, 34560, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 135, 135, 135, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3719361 . Total input tokens: 829649856 . Total output tokens: 729485100
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 123.14973596809432,
    "estimated_duration": 3600.020236744186,
    "input_throughput": 7077.821602204122,
    "output_throughput": 6206.7098323335795,
    "total_throughput": 13284.531434537701,
    "itl": 85.80764877090762,
    "ttft": 2019294.6025390718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.309469349300513,
    "arrivals": 1239193,
    "finished_requests": 103595,
    "scheduler_time": 307.092102396212
}
#Debug simulation 
Total elapsed time: 123.14993166737258. Arrivals time: 0.9370492259040475 Scheduler time: 121.88355436781421 Scheduler overhead time: 0.13079568091779947 Adapter cache time: 0.02991552371531725 Engine time: 0.12717269640415907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 66, 34560, 34560, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 34560, 135, 66, 66, 135, 34560, 34560, 135, 135, 66, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 135, 66, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 34560, 135, 66, 66, 135, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 66, 34560, 135, 34560, 34560, 135, 66, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 66, 135, 34560, 34560, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 135, 66, 34560, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 135, 66, 135, 34560, 135, 66, 66, 135, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 135, 34560, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 135, 66, 135, 66, 66, 66, 34560, 34560, 135, 135, 66, 66, 135, 66, 34560, 66, 66, 135, 66, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 135, 66, 135, 66, 135, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 34560, 66, 34560, 66, 66, 66, 34560, 135, 34560, 34560, 66, 135, 135, 135, 66, 135, 66, 135, 135, 34560, 66, 66, 135, 66, 135, 135, 66, 34560, 34560, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 135, 135, 135, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3719361 . Total input tokens: 829649856 . Total output tokens: 729485100
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 123.94041993282735,
    "estimated_duration": 3600.061378677822,
    "input_throughput": 7280.935307171538,
    "output_throughput": 6383.655883233949,
    "total_throughput": 13664.591190405486,
    "itl": 89.23488477777366,
    "ttft": 2016781.7281022596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0898177352361245,
    "arrivals": 1239193,
    "finished_requests": 106525,
    "scheduler_time": 297.6348208805746
}
#Debug simulation 
Total elapsed time: 123.94059137580916. Arrivals time: 0.9627721616998315 Scheduler time: 122.64631604822353 Scheduler overhead time: 0.13249834021553397 Adapter cache time: 0.030230877455323935 Engine time: 0.12781607592478395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 66, 34560, 34560, 135, 135, 135, 135, 66, 135, 66, 135, 66, 135, 135, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 34560, 135, 66, 66, 135, 34560, 34560, 135, 135, 66, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 66, 34560, 66, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 135, 66, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 34560, 135, 66, 66, 135, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 66, 34560, 135, 34560, 34560, 135, 66, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 66, 135, 34560, 34560, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 135, 66, 34560, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 135, 66, 135, 34560, 135, 66, 66, 135, 34560, 66, 66, 66, 34560, 34560, 34560, 66, 135, 34560, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 135, 66, 135, 66, 66, 66, 34560, 34560, 135, 135, 66, 66, 135, 66, 34560, 66, 66, 135, 66, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 66, 66, 34560, 135, 66, 135, 66, 135, 34560, 34560, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 34560, 66, 34560, 66, 66, 66, 34560, 135, 34560, 34560, 66, 135, 135, 135, 66, 135, 66, 135, 135, 34560, 66, 66, 135, 66, 135, 135, 66, 34560, 34560, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 135, 135, 135, 66, 66, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 3719361 . Total input tokens: 829649856 . Total output tokens: 729485100
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.82888084603474,
    "estimated_duration": 3600.039231616267,
    "input_throughput": 7178.517048659383,
    "output_throughput": 6294.856956274288,
    "total_throughput": 13473.374004933672,
    "itl": 86.75944349216118,
    "ttft": 2020473.2204151317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4180733365379536,
    "arrivals": 1239193,
    "finished_requests": 105104,
    "scheduler_time": 302.2537951234696
}
#Debug simulation 
Total elapsed time: 123.82906605117023. Arrivals time: 0.9558658837340772 Scheduler time: 122.5407376815565 Scheduler overhead time: 0.13184097176417708 Adapter cache time: 0.030265859328210354 Engine time: 0.12742449529469013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 33, 34560, 34560, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 34560, 135, 33, 33, 135, 34560, 34560, 135, 135, 33, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 135, 33, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 34560, 135, 33, 33, 135, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 33, 34560, 135, 34560, 34560, 135, 33, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 33, 135, 34560, 34560, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 135, 33, 34560, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 135, 33, 135, 34560, 135, 33, 33, 135, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 135, 34560, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 135, 33, 135, 33, 33, 33, 34560, 34560, 135, 135, 33, 33, 135, 33, 34560, 33, 33, 135, 33, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 135, 33, 135, 33, 135, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 34560, 33, 34560, 33, 33, 33, 34560, 135, 34560, 34560, 33, 135, 135, 135, 33, 135, 33, 135, 135, 34560, 33, 33, 135, 33, 135, 135, 33, 34560, 34560, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 135, 135, 135, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3715863 . Total input tokens: 828890404 . Total output tokens: 728807028
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 107.63706327369437,
    "estimated_duration": 3600.067131582564,
    "input_throughput": 7296.759765824814,
    "output_throughput": 6348.267175216108,
    "total_throughput": 13645.026941040922,
    "itl": 88.68052989526751,
    "ttft": 2023143.6266697457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9160723830899493,
    "arrivals": 1238058,
    "finished_requests": 106129,
    "scheduler_time": 299.5357875003742
}
#Debug simulation 
Total elapsed time: 107.6373267727904. Arrivals time: 0.8324126983061433 Scheduler time: 106.51153069222346 Scheduler overhead time: 0.11690473649650812 Adapter cache time: 0.02603561384603381 Engine time: 0.111868882086128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 33, 34560, 34560, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 34560, 135, 33, 33, 135, 34560, 34560, 135, 135, 33, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 135, 33, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 34560, 135, 33, 33, 135, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 33, 34560, 135, 34560, 34560, 135, 33, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 33, 135, 34560, 34560, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 135, 33, 34560, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 135, 33, 135, 34560, 135, 33, 33, 135, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 135, 34560, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 135, 33, 135, 33, 33, 33, 34560, 34560, 135, 135, 33, 33, 135, 33, 34560, 33, 33, 135, 33, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 135, 33, 135, 33, 135, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 34560, 33, 34560, 33, 33, 33, 34560, 135, 34560, 34560, 33, 135, 135, 135, 33, 135, 33, 135, 135, 34560, 33, 33, 135, 33, 135, 135, 33, 34560, 34560, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 135, 135, 135, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3715863 . Total input tokens: 828890404 . Total output tokens: 728807028
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 119.55948993004858,
    "estimated_duration": 3600.0590198512086,
    "input_throughput": 6569.413409499456,
    "output_throughput": 5728.273866146872,
    "total_throughput": 12297.687275646327,
    "itl": 75.17316298886075,
    "ttft": 2059026.2547498255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6666378027759525,
    "arrivals": 1238058,
    "finished_requests": 95562,
    "scheduler_time": 332.49364417414955
}
#Debug simulation 
Total elapsed time: 119.5596544300206. Arrivals time: 0.8876083986833692 Scheduler time: 118.33165830839425 Scheduler overhead time: 0.13580233743414283 Adapter cache time: 0.029589185491204262 Engine time: 0.1306341872550547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 33, 34560, 34560, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 34560, 135, 33, 33, 135, 34560, 34560, 135, 135, 33, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 135, 33, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 34560, 135, 33, 33, 135, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 33, 34560, 135, 34560, 34560, 135, 33, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 33, 135, 34560, 34560, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 135, 33, 34560, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 135, 33, 135, 34560, 135, 33, 33, 135, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 135, 34560, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 135, 33, 135, 33, 33, 33, 34560, 34560, 135, 135, 33, 33, 135, 33, 34560, 33, 33, 135, 33, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 135, 33, 135, 33, 135, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 34560, 33, 34560, 33, 33, 33, 34560, 135, 34560, 34560, 33, 135, 135, 135, 33, 135, 33, 135, 135, 34560, 33, 33, 135, 33, 135, 135, 33, 34560, 34560, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 135, 135, 135, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3715863 . Total input tokens: 828890404 . Total output tokens: 728807028
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 128.25717948796228,
    "estimated_duration": 3600.074020607657,
    "input_throughput": 7153.397361439026,
    "output_throughput": 6225.116170310649,
    "total_throughput": 13378.513531749675,
    "itl": 85.38327217272476,
    "ttft": 2030689.3459408497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.014651785148327,
    "arrivals": 1238058,
    "finished_requests": 103970,
    "scheduler_time": 306.0707780831488
}
#Debug simulation 
Total elapsed time: 128.25736710289493. Arrivals time: 0.9965490493923426 Scheduler time: 126.92457927390933 Scheduler overhead time: 0.13448472041636705 Adapter cache time: 0.02961815893650055 Engine time: 0.12956245336681604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 33, 34560, 34560, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 34560, 135, 33, 33, 135, 34560, 34560, 135, 135, 33, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 135, 33, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 34560, 135, 33, 33, 135, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 33, 34560, 135, 34560, 34560, 135, 33, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 33, 135, 34560, 34560, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 135, 33, 34560, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 135, 33, 135, 34560, 135, 33, 33, 135, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 135, 34560, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 135, 33, 135, 33, 33, 33, 34560, 34560, 135, 135, 33, 33, 135, 33, 34560, 33, 33, 135, 33, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 135, 33, 135, 33, 135, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 34560, 33, 34560, 33, 33, 33, 34560, 135, 34560, 34560, 33, 135, 135, 135, 33, 135, 33, 135, 135, 34560, 33, 33, 135, 33, 135, 135, 33, 34560, 34560, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 135, 135, 135, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3715863 . Total input tokens: 828890404 . Total output tokens: 728807028
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 125.48500723065808,
    "estimated_duration": 3600.0271309689274,
    "input_throughput": 7296.641398624705,
    "output_throughput": 6352.757400982319,
    "total_throughput": 13649.398799607025,
    "itl": 88.2173090954298,
    "ttft": 2024397.5050178953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0782211273070383,
    "arrivals": 1238058,
    "finished_requests": 106184,
    "scheduler_time": 299.1687442548336
}
#Debug simulation 
Total elapsed time: 125.48521393584087. Arrivals time: 0.9764193259179592 Scheduler time: 124.17469303682446 Scheduler overhead time: 0.13256243290379643 Adapter cache time: 0.030588143039494753 Engine time: 0.12890316685661674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 33, 34560, 34560, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 34560, 135, 33, 33, 135, 34560, 34560, 135, 135, 33, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 135, 33, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 34560, 135, 33, 33, 135, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 33, 34560, 135, 34560, 34560, 135, 33, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 33, 135, 34560, 34560, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 135, 33, 34560, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 135, 33, 135, 34560, 135, 33, 33, 135, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 135, 34560, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 135, 33, 135, 33, 33, 33, 34560, 34560, 135, 135, 33, 33, 135, 33, 34560, 33, 33, 135, 33, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 135, 33, 135, 33, 135, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 34560, 33, 34560, 33, 33, 33, 34560, 135, 34560, 34560, 33, 135, 135, 135, 33, 135, 33, 135, 135, 34560, 33, 33, 135, 33, 135, 135, 33, 34560, 34560, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 135, 135, 135, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3715863 . Total input tokens: 828890404 . Total output tokens: 728807028
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 128.66121958335862,
    "estimated_duration": 3600.0737121621464,
    "input_throughput": 7153.397974324616,
    "output_throughput": 6225.116703663378,
    "total_throughput": 13378.514677987994,
    "itl": 85.38140600249484,
    "ttft": 2030700.181900074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.985862497598883,
    "arrivals": 1238058,
    "finished_requests": 103970,
    "scheduler_time": 306.0757274613663
}
#Debug simulation 
Total elapsed time: 128.66139681823552. Arrivals time: 0.9902169168926775 Scheduler time: 127.32777477940544 Scheduler overhead time: 0.13843123149126768 Adapter cache time: 0.02892983704805374 Engine time: 0.133001203648746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 33, 34560, 34560, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 34560, 135, 33, 33, 135, 34560, 34560, 135, 135, 33, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 135, 33, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 34560, 135, 33, 33, 135, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 33, 34560, 135, 34560, 34560, 135, 33, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 33, 135, 34560, 34560, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 135, 33, 34560, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 135, 33, 135, 34560, 135, 33, 33, 135, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 135, 34560, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 135, 33, 135, 33, 33, 33, 34560, 34560, 135, 135, 33, 33, 135, 33, 34560, 33, 33, 135, 33, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 135, 33, 135, 33, 135, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 34560, 33, 34560, 33, 33, 33, 34560, 135, 34560, 34560, 33, 135, 135, 135, 33, 135, 33, 135, 135, 34560, 33, 33, 135, 33, 135, 135, 33, 34560, 34560, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 135, 135, 135, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3715863 . Total input tokens: 828890404 . Total output tokens: 728807028
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 125.40702226292342,
    "estimated_duration": 3600.0207020340663,
    "input_throughput": 7290.826962514403,
    "output_throughput": 6359.399540970573,
    "total_throughput": 13650.226503484975,
    "itl": 87.87941656746563,
    "ttft": 2017336.0707870808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8599965813755865,
    "arrivals": 1238058,
    "finished_requests": 106057,
    "scheduler_time": 300.021787298671
}
#Debug simulation 
Total elapsed time: 125.40720487991348. Arrivals time: 0.9744797958992422 Scheduler time: 124.10564695065841 Scheduler overhead time: 0.13020560517907143 Adapter cache time: 0.028277933597564697 Engine time: 0.12637055711820722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [135, 34560, 34560, 33, 34560, 34560, 135, 135, 135, 135, 33, 135, 33, 135, 33, 135, 135, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 34560, 135, 33, 33, 135, 34560, 34560, 135, 135, 33, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 135, 33, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 34560, 135, 33, 33, 135, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 33, 34560, 135, 34560, 34560, 135, 33, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 33, 135, 34560, 34560, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 135, 33, 34560, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 135, 33, 135, 34560, 135, 33, 33, 135, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 135, 34560, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 34560, 34560, 34560, 135, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 135, 33, 135, 33, 33, 33, 34560, 34560, 135, 135, 33, 33, 135, 33, 34560, 33, 33, 135, 33, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 135, 33, 135, 33, 135, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 34560, 33, 34560, 33, 33, 33, 34560, 135, 34560, 34560, 33, 135, 135, 135, 33, 135, 33, 135, 135, 34560, 33, 33, 135, 33, 135, 135, 33, 34560, 34560, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 135, 135, 135, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3715863 . Total input tokens: 828890404 . Total output tokens: 728807028
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 125.89841090235859,
    "estimated_duration": 3600.014803701626,
    "input_throughput": 7153.515028193873,
    "output_throughput": 6225.218567700491,
    "total_throughput": 13378.733595894364,
    "itl": 85.38171885514349,
    "ttft": 2030664.9491631156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.958730147462357,
    "arrivals": 1238058,
    "finished_requests": 103970,
    "scheduler_time": 306.07011222656985
}
#Debug simulation 
Total elapsed time: 125.89859478827566. Arrivals time: 0.7821713616140187 Scheduler time: 124.80462707160041 Scheduler overhead time: 0.12408692995086312 Adapter cache time: 0.02577291103079915 Engine time: 0.12203645939007401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [66, 34560, 34560, 33, 34560, 34560, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 34560, 66, 34560, 34560, 33, 34560, 66, 66, 34560, 34560, 66, 33, 33, 66, 34560, 34560, 66, 66, 33, 33, 66, 34560, 34560, 33, 34560, 66, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 66, 33, 34560, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 34560, 66, 33, 33, 66, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 33, 34560, 66, 34560, 34560, 66, 33, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 33, 66, 34560, 34560, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 66, 33, 34560, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 66, 33, 66, 34560, 66, 33, 33, 66, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 66, 34560, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 66, 33, 66, 33, 33, 33, 34560, 34560, 66, 66, 33, 33, 66, 33, 34560, 33, 33, 66, 33, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 66, 33, 66, 33, 66, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 34560, 33, 34560, 33, 33, 33, 34560, 66, 34560, 34560, 33, 66, 66, 66, 33, 66, 33, 66, 66, 34560, 33, 33, 66, 33, 66, 66, 33, 34560, 34560, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 66, 66, 66, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3708480 . Total input tokens: 827225876 . Total output tokens: 727380884
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 122.07340023387223,
    "estimated_duration": 3600.071036675383,
    "input_throughput": 7393.139115549052,
    "output_throughput": 6414.959250728364,
    "total_throughput": 13808.098366277416,
    "itl": 89.30935206129458,
    "ttft": 2004861.5976065397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.684638066971694,
    "arrivals": 1235689,
    "finished_requests": 107600,
    "scheduler_time": 296.1386561593148
}
#Debug simulation 
Total elapsed time: 122.07357228174806. Arrivals time: 0.7694434314034879 Scheduler time: 121.00553022883832 Scheduler overhead time: 0.11929235141724348 Adapter cache time: 0.024590580724179745 Engine time: 0.1161525622010231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [66, 34560, 34560, 33, 34560, 34560, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 34560, 66, 34560, 34560, 33, 34560, 66, 66, 34560, 34560, 66, 33, 33, 66, 34560, 34560, 66, 66, 33, 33, 66, 34560, 34560, 33, 34560, 66, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 66, 33, 34560, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 34560, 66, 33, 33, 66, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 33, 34560, 66, 34560, 34560, 66, 33, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 33, 66, 34560, 34560, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 66, 33, 34560, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 66, 33, 66, 34560, 66, 33, 33, 66, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 66, 34560, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 66, 33, 66, 33, 33, 33, 34560, 34560, 66, 66, 33, 33, 66, 33, 34560, 33, 33, 66, 33, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 66, 33, 66, 33, 66, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 34560, 33, 34560, 33, 33, 33, 34560, 66, 34560, 34560, 33, 66, 66, 66, 33, 66, 33, 66, 66, 34560, 33, 33, 66, 33, 66, 66, 33, 34560, 34560, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 66, 66, 66, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3708480 . Total input tokens: 827225876 . Total output tokens: 727380884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 118.28360586706549,
    "estimated_duration": 3600.084116138502,
    "input_throughput": 7390.648979763,
    "output_throughput": 6405.6067180828095,
    "total_throughput": 13796.255697845809,
    "itl": 88.59747375229372,
    "ttft": 2007303.139377424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0446868369262696,
    "arrivals": 1235689,
    "finished_requests": 107478,
    "scheduler_time": 296.51127255139374
}
#Debug simulation 
Total elapsed time: 118.28380172234029. Arrivals time: 0.9038950754329562 Scheduler time: 117.07719171745703 Scheduler overhead time: 0.1211838903836906 Adapter cache time: 0.02475693030282855 Engine time: 0.11816660547628999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [66, 34560, 34560, 33, 34560, 34560, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 34560, 66, 34560, 34560, 33, 34560, 66, 66, 34560, 34560, 66, 33, 33, 66, 34560, 34560, 66, 66, 33, 33, 66, 34560, 34560, 33, 34560, 66, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 66, 33, 34560, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 34560, 66, 33, 33, 66, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 33, 34560, 66, 34560, 34560, 66, 33, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 33, 66, 34560, 34560, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 66, 33, 34560, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 66, 33, 66, 34560, 66, 33, 33, 66, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 66, 34560, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 66, 33, 66, 33, 33, 33, 34560, 34560, 66, 66, 33, 33, 66, 33, 34560, 33, 33, 66, 33, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 66, 33, 66, 33, 66, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 34560, 33, 34560, 33, 33, 33, 34560, 66, 34560, 34560, 33, 66, 66, 66, 33, 66, 33, 66, 66, 34560, 33, 33, 66, 33, 66, 66, 33, 34560, 34560, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 66, 66, 66, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3708480 . Total input tokens: 827225876 . Total output tokens: 727380884
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 115.37634310824797,
    "estimated_duration": 3600.025649645383,
    "input_throughput": 7336.389395614892,
    "output_throughput": 6355.349718759286,
    "total_throughput": 13691.739114374177,
    "itl": 86.84130403647161,
    "ttft": 2016313.5478073673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 423,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1581492714770367,
    "arrivals": 1235689,
    "finished_requests": 106716,
    "scheduler_time": 299.12664738299367
}
#Debug simulation 
Total elapsed time: 115.37654926208779. Arrivals time: 0.7544769058004022 Scheduler time: 114.32227001572028 Scheduler overhead time: 0.11865673633292317 Adapter cache time: 0.024893261026591063 Engine time: 0.1172780473716557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [66, 34560, 34560, 33, 34560, 34560, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 34560, 66, 34560, 34560, 33, 34560, 66, 66, 34560, 34560, 66, 33, 33, 66, 34560, 34560, 66, 66, 33, 33, 66, 34560, 34560, 33, 34560, 66, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 66, 33, 34560, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 34560, 66, 33, 33, 66, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 33, 34560, 66, 34560, 34560, 66, 33, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 33, 66, 34560, 34560, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 66, 33, 34560, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 66, 33, 66, 34560, 66, 33, 33, 66, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 66, 34560, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 66, 33, 66, 33, 33, 33, 34560, 34560, 66, 66, 33, 33, 66, 33, 34560, 33, 33, 66, 33, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 66, 33, 66, 33, 66, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 34560, 33, 34560, 33, 33, 33, 34560, 66, 34560, 34560, 33, 66, 66, 66, 33, 66, 33, 66, 66, 34560, 33, 33, 66, 33, 66, 66, 33, 34560, 34560, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 66, 66, 66, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3708480 . Total input tokens: 827225876 . Total output tokens: 727380884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 114.97426953725517,
    "estimated_duration": 3600.067414755021,
    "input_throughput": 7427.266470180676,
    "output_throughput": 6432.166215858418,
    "total_throughput": 13859.432686039094,
    "itl": 88.95496519139029,
    "ttft": 2009100.3531590952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9769094293052287,
    "arrivals": 1235689,
    "finished_requests": 107922,
    "scheduler_time": 294.9835097997284
}
#Debug simulation 
Total elapsed time: 114.9744482873939. Arrivals time: 0.7803052789531648 Scheduler time: 113.89954748330638 Scheduler overhead time: 0.11650899238884449 Adapter cache time: 0.024957588873803616 Engine time: 0.1157608306966722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [66, 34560, 34560, 33, 34560, 34560, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 34560, 66, 34560, 34560, 33, 34560, 66, 66, 34560, 34560, 66, 33, 33, 66, 34560, 34560, 66, 66, 33, 33, 66, 34560, 34560, 33, 34560, 66, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 66, 33, 34560, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 34560, 66, 33, 33, 66, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 33, 34560, 66, 34560, 34560, 66, 33, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 33, 66, 34560, 34560, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 66, 33, 34560, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 66, 33, 66, 34560, 66, 33, 33, 66, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 66, 34560, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 66, 33, 66, 33, 33, 33, 34560, 34560, 66, 66, 33, 33, 66, 33, 34560, 33, 33, 66, 33, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 66, 33, 66, 33, 66, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 34560, 33, 34560, 33, 33, 33, 34560, 66, 34560, 34560, 33, 66, 66, 66, 33, 66, 33, 66, 66, 34560, 33, 33, 66, 33, 66, 66, 33, 34560, 34560, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 66, 66, 66, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3708480 . Total input tokens: 827225876 . Total output tokens: 727380884
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 116.6632792041637,
    "estimated_duration": 3600.0706531374485,
    "input_throughput": 7390.279403770417,
    "output_throughput": 6396.63001611674,
    "total_throughput": 13786.909419887157,
    "itl": 87.10148876686509,
    "ttft": 2013010.6140249965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.214159681983316,
    "arrivals": 1235689,
    "finished_requests": 107423,
    "scheduler_time": 296.921250409377
}
#Debug simulation 
Total elapsed time: 116.66354291234165. Arrivals time: 0.8177247024141252 Scheduler time: 115.53906110487878 Scheduler overhead time: 0.12283623544499278 Adapter cache time: 0.025429617147892714 Engine time: 0.11871159542351961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [66, 34560, 34560, 33, 34560, 34560, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 34560, 66, 34560, 34560, 33, 34560, 66, 66, 34560, 34560, 66, 33, 33, 66, 34560, 34560, 66, 66, 33, 33, 66, 34560, 34560, 33, 34560, 66, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 66, 33, 34560, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 34560, 66, 33, 33, 66, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 33, 34560, 66, 34560, 34560, 66, 33, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 33, 66, 34560, 34560, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 66, 33, 34560, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 66, 33, 66, 34560, 66, 33, 33, 66, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 66, 34560, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 66, 33, 66, 33, 33, 33, 34560, 34560, 66, 66, 33, 33, 66, 33, 34560, 33, 33, 66, 33, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 66, 33, 66, 33, 66, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 34560, 33, 34560, 33, 33, 33, 34560, 66, 34560, 34560, 33, 66, 66, 66, 33, 66, 33, 66, 66, 34560, 33, 33, 66, 33, 66, 66, 33, 34560, 34560, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 66, 66, 66, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3708480 . Total input tokens: 827225876 . Total output tokens: 727380884
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 115.78866386599839,
    "estimated_duration": 3600.0247675132737,
    "input_throughput": 7397.983269541993,
    "output_throughput": 6412.660326208403,
    "total_throughput": 13810.643595750396,
    "itl": 88.96135260183283,
    "ttft": 2004888.858725414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.662095032217901,
    "arrivals": 1235689,
    "finished_requests": 107638,
    "scheduler_time": 296.1182155457634
}
#Debug simulation 
Total elapsed time: 115.78885292401537. Arrivals time: 0.8074296987615526 Scheduler time: 114.68970740307122 Scheduler overhead time: 0.11614725505933166 Adapter cache time: 0.024421101436018944 Engine time: 0.1131336409598589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_320_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [106 107 107]
Adapter prompts. [66, 34560, 34560, 33, 34560, 34560, 66, 66, 66, 66, 33, 66, 33, 66, 33, 66, 66, 34560, 66, 34560, 34560, 33, 34560, 66, 66, 34560, 34560, 66, 33, 33, 66, 34560, 34560, 66, 66, 33, 33, 66, 34560, 34560, 33, 34560, 66, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 33, 34560, 33, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 66, 33, 34560, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 34560, 66, 33, 33, 66, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 33, 34560, 66, 34560, 34560, 66, 33, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 33, 66, 34560, 34560, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 66, 33, 34560, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 66, 33, 66, 34560, 66, 33, 33, 66, 34560, 33, 33, 33, 34560, 34560, 34560, 33, 66, 34560, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 34560, 34560, 34560, 66, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 66, 33, 66, 33, 33, 33, 34560, 34560, 66, 66, 33, 33, 66, 33, 34560, 33, 33, 66, 33, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 33, 33, 34560, 66, 33, 66, 33, 66, 34560, 34560, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 34560, 33, 34560, 33, 33, 33, 34560, 66, 34560, 34560, 33, 66, 66, 66, 33, 66, 33, 66, 66, 34560, 33, 33, 66, 33, 66, 66, 33, 34560, 34560, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 66, 66, 66, 33, 33, 34560, 33, 33, 34560, 34560, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 3708480 . Total input tokens: 827225876 . Total output tokens: 727380884
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 110.8284025308676,
    "estimated_duration": 3600.075009319255,
    "input_throughput": 7277.8500259510865,
    "output_throughput": 6305.949998606486,
    "total_throughput": 13583.800024557573,
    "itl": 86.70690511461827,
    "ttft": 2012272.7896979593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1381548019871346,
    "arrivals": 1235689,
    "finished_requests": 105877,
    "scheduler_time": 301.7109284768672
}
#Debug simulation 
Total elapsed time: 110.82858571270481. Arrivals time: 0.8796223849058151 Scheduler time: 109.64987006830052 Scheduler overhead time: 0.11956862872466445 Adapter cache time: 0.024714719969779253 Engine time: 0.11580350622534752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 4320, 17280, 17280, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 17280, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 8640, 4320, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3231360 . Total input tokens: 720959501 . Total output tokens: 633549394
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 122.2358571710065,
    "estimated_duration": 3600.0369770068814,
    "input_throughput": 6809.60061148648,
    "output_throughput": 5927.441061380857,
    "total_throughput": 12737.041672867337,
    "itl": 78.8105055477788,
    "ttft": 2043547.588024506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.139368054000727,
    "arrivals": 1075699,
    "finished_requests": 98854,
    "scheduler_time": 320.50824618221424
}
#Debug simulation 
Total elapsed time: 122.23602780885994. Arrivals time: 0.8196001444011927 Scheduler time: 121.1046715946868 Scheduler overhead time: 0.12433561449870467 Adapter cache time: 0.028388035017997026 Engine time: 0.1192887076176703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 4320, 17280, 17280, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 17280, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 8640, 4320, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3231360 . Total input tokens: 720959501 . Total output tokens: 633549394
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.14558804873377,
    "estimated_duration": 3600.0725193625844,
    "input_throughput": 6800.146904911382,
    "output_throughput": 5917.911343567092,
    "total_throughput": 12718.058248478475,
    "itl": 78.18673711523203,
    "ttft": 2043655.157644314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6479483738401965,
    "arrivals": 1075699,
    "finished_requests": 98770,
    "scheduler_time": 320.8493524008599
}
#Debug simulation 
Total elapsed time: 123.14576880401. Arrivals time: 0.8437590315006673 Scheduler time: 121.99125882424414 Scheduler overhead time: 0.12450369680300355 Adapter cache time: 0.028736337553709745 Engine time: 0.11752454517409205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 4320, 17280, 17280, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 17280, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 8640, 4320, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3231360 . Total input tokens: 720959501 . Total output tokens: 633549394
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 124.40358496876433,
    "estimated_duration": 3600.0240399613576,
    "input_throughput": 6780.819441489626,
    "output_throughput": 5903.758076078885,
    "total_throughput": 12684.57751756851,
    "itl": 78.35861712715477,
    "ttft": 2046534.992943949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 632,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.750355448476066,
    "arrivals": 1075699,
    "finished_requests": 98532,
    "scheduler_time": 321.4587017995873
}
#Debug simulation 
Total elapsed time: 124.40375397074968. Arrivals time: 0.8250834159553051 Scheduler time: 123.26035452960059 Scheduler overhead time: 0.1286548119969666 Adapter cache time: 0.028906395193189383 Engine time: 0.12042621849104762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 4320, 17280, 17280, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 17280, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 8640, 4320, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3231360 . Total input tokens: 720959501 . Total output tokens: 633549394
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 125.04215259524062,
    "estimated_duration": 3600.022235407168,
    "input_throughput": 6800.591051691385,
    "output_throughput": 5918.099280181345,
    "total_throughput": 12718.690331872729,
    "itl": 78.1802420472446,
    "ttft": 2043544.331627725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.352257306524546,
    "arrivals": 1075699,
    "finished_requests": 98773,
    "scheduler_time": 320.87290492438143
}
#Debug simulation 
Total elapsed time: 125.0424384623766. Arrivals time: 0.8317984216846526 Scheduler time: 123.89623697055504 Scheduler overhead time: 0.1257378626614809 Adapter cache time: 0.02950988756492734 Engine time: 0.11924904584884644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 4320, 17280, 17280, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 17280, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 8640, 4320, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3231360 . Total input tokens: 720959501 . Total output tokens: 633549394
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 123.5602241246961,
    "estimated_duration": 3600.0405375421838,
    "input_throughput": 6781.023087219598,
    "output_throughput": 5903.967407686711,
    "total_throughput": 12684.99049490631,
    "itl": 78.35766609284697,
    "ttft": 2046507.0729542787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 632,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.706239489857135,
    "arrivals": 1075699,
    "finished_requests": 98535,
    "scheduler_time": 321.4647365901316
}
#Debug simulation 
Total elapsed time: 123.56041039898992. Arrivals time: 0.803427966311574 Scheduler time: 122.4467570092529 Scheduler overhead time: 0.1237595952115953 Adapter cache time: 0.028927887324243784 Engine time: 0.11722902860492468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 4320, 17280, 17280, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 17280, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 8640, 4320, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3231360 . Total input tokens: 720959501 . Total output tokens: 633549394
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 125.24553722701967,
    "estimated_duration": 3600.01054558189,
    "input_throughput": 6800.782300498148,
    "output_throughput": 5918.339052130798,
    "total_throughput": 12719.121352628947,
    "itl": 78.17484156076293,
    "ttft": 2043404.236770331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.05378979726227,
    "arrivals": 1075699,
    "finished_requests": 98776,
    "scheduler_time": 320.89676874395445
}
#Debug simulation 
Total elapsed time: 125.24580715689808. Arrivals time: 0.84782575070858 Scheduler time: 124.08452551206574 Scheduler overhead time: 0.1250603748485446 Adapter cache time: 0.028733992483466864 Engine time: 0.11940026842057705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 4320, 17280, 17280, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 17280, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 8640, 4320, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 17280, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3231360 . Total input tokens: 720959501 . Total output tokens: 633549394
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 124.26195181626827,
    "estimated_duration": 3600.00839265837,
    "input_throughput": 6784.241683938124,
    "output_throughput": 5902.705683502149,
    "total_throughput": 12686.947367440273,
    "itl": 77.31570561811101,
    "ttft": 2050638.1969810873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.588146154098241,
    "arrivals": 1075699,
    "finished_requests": 98498,
    "scheduler_time": 321.9490398081268
}
#Debug simulation 
Total elapsed time: 124.26211395813152. Arrivals time: 0.9165081069804728 Scheduler time: 123.03211723966524 Scheduler overhead time: 0.12577919010072947 Adapter cache time: 0.02805240172892809 Engine time: 0.11854652920737863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 1080, 17280, 17280, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 17280, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 8640, 1080, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2887920 . Total input tokens: 644274381 . Total output tokens: 566146664
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 98.60096292989329,
    "estimated_duration": 3600.0538416142263,
    "input_throughput": 6866.331196012387,
    "output_throughput": 5964.648570470187,
    "total_throughput": 12830.979766482575,
    "itl": 81.40953793103586,
    "ttft": 2023695.0128792836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8550344656268707,
    "arrivals": 961706,
    "finished_requests": 99587,
    "scheduler_time": 318.13102847999687
}
#Debug simulation 
Total elapsed time: 98.60117801697925. Arrivals time: 0.6529559772461653 Scheduler time: 97.69513894570991 Scheduler overhead time: 0.10084681212902069 Adapter cache time: 0.021645679138600826 Engine time: 0.09511875547468662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 1080, 17280, 17280, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 17280, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 8640, 1080, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2887920 . Total input tokens: 644274381 . Total output tokens: 566146664
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 122.84518946800381,
    "estimated_duration": 3600.0024712705044,
    "input_throughput": 6844.642523621338,
    "output_throughput": 5949.202582753092,
    "total_throughput": 12793.84510637443,
    "itl": 80.7961061493483,
    "ttft": 2021439.2659167077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.266008529891266,
    "arrivals": 961706,
    "finished_requests": 99241,
    "scheduler_time": 318.812936570971
}
#Debug simulation 
Total elapsed time: 122.8453596169129. Arrivals time: 0.8479309305548668 Scheduler time: 121.68071749573573 Scheduler overhead time: 0.1271464000456035 Adapter cache time: 0.02818406606093049 Engine time: 0.12136607244610786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 1080, 17280, 17280, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 17280, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 8640, 1080, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2887920 . Total input tokens: 644274381 . Total output tokens: 566146664
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 124.11334518622607,
    "estimated_duration": 3600.04040889682,
    "input_throughput": 6823.875070760098,
    "output_throughput": 5934.810883566489,
    "total_throughput": 12758.685954326587,
    "itl": 79.22055128122965,
    "ttft": 2029152.0198999494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.345006001400795,
    "arrivals": 961706,
    "finished_requests": 98927,
    "scheduler_time": 320.01032733001813
}
#Debug simulation 
Total elapsed time: 124.11357890907675. Arrivals time: 0.8524296605028212 Scheduler time: 122.94198715919629 Scheduler overhead time: 0.12866958416998386 Adapter cache time: 0.028271950781345367 Engine time: 0.12109652673825622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 1080, 17280, 17280, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 17280, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 8640, 1080, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2887920 . Total input tokens: 644274381 . Total output tokens: 566146664
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 124.66160296695307,
    "estimated_duration": 3600.0116387609023,
    "input_throughput": 6864.158641582993,
    "output_throughput": 5965.362380714876,
    "total_throughput": 12829.52102229787,
    "itl": 81.33701255555367,
    "ttft": 2028571.600661679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9253561628702975,
    "arrivals": 961706,
    "finished_requests": 99551,
    "scheduler_time": 318.0340724390131
}
#Debug simulation 
Total elapsed time: 124.66175879491493. Arrivals time: 0.8370904782786965 Scheduler time: 123.50556524284184 Scheduler overhead time: 0.12831972213461995 Adapter cache time: 0.028129178564995527 Engine time: 0.12212880188599229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 1080, 17280, 17280, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 17280, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 8640, 1080, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2887920 . Total input tokens: 644274381 . Total output tokens: 566146664
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 126.05696025723591,
    "estimated_duration": 3600.044644358489,
    "input_throughput": 6823.945652589993,
    "output_throughput": 5934.946677253591,
    "total_throughput": 12758.892329843584,
    "itl": 79.21911464351848,
    "ttft": 2029229.161874336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.303996800431085,
    "arrivals": 961706,
    "finished_requests": 98929,
    "scheduler_time": 320.01585953335535
}
#Debug simulation 
Total elapsed time: 126.05723106814548. Arrivals time: 0.9103561085648835 Scheduler time: 124.82774774497375 Scheduler overhead time: 0.12700989237055182 Adapter cache time: 0.028487506322562695 Engine time: 0.12188874324783683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 1080, 17280, 17280, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 17280, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 8640, 1080, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2887920 . Total input tokens: 644274381 . Total output tokens: 566146664
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 128.54915098380297,
    "estimated_duration": 3600.0466552267635,
    "input_throughput": 6845.402118392189,
    "output_throughput": 5950.389273121981,
    "total_throughput": 12795.791391514169,
    "itl": 80.78141727026006,
    "ttft": 2021387.965449394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7282098292931742,
    "arrivals": 961706,
    "finished_requests": 99258,
    "scheduler_time": 318.880344376796
}
#Debug simulation 
Total elapsed time: 128.54930965183303. Arrivals time: 0.855348102748394 Scheduler time: 127.37227453757077 Scheduler overhead time: 0.12789918156340718 Adapter cache time: 0.028330589644610882 Engine time: 0.12272530887275934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 1080, 17280, 17280, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 17280, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 8640, 1080, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 1080, 1080, 17280, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2887920 . Total input tokens: 644274381 . Total output tokens: 566146664
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.2010716847144,
    "estimated_duration": 3600.0045373838775,
    "input_throughput": 6824.02167688724,
    "output_throughput": 5935.012797380173,
    "total_throughput": 12759.034474267411,
    "itl": 79.21838802121214,
    "ttft": 2029212.3705479347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2644374196976775,
    "arrivals": 961706,
    "finished_requests": 98929,
    "scheduler_time": 320.01551420192106
}
#Debug simulation 
Total elapsed time: 123.20131256990135. Arrivals time: 0.8485275469720364 Scheduler time: 122.03129909047857 Scheduler overhead time: 0.13038367172703147 Adapter cache time: 0.027595485094934702 Engine time: 0.12283217487856746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 540, 17280, 17280, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 8640, 8640, 17280, 8640, 17280, 17280, 540, 17280, 8640, 8640, 17280, 17280, 8640, 540, 540, 8640, 17280, 17280, 8640, 8640, 540, 540, 8640, 17280, 17280, 540, 17280, 8640, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 17280, 8640, 540, 540, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 540, 17280, 8640, 17280, 17280, 8640, 540, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 17280, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 8640, 540, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 8640, 17280, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 8640, 540, 8640, 540, 540, 540, 17280, 17280, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 540, 8640, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 540, 540, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 17280, 540, 17280, 540, 540, 540, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2830680 . Total input tokens: 631481760 . Total output tokens: 554942354
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 126.76694362517446,
    "estimated_duration": 3600.0704743212427,
    "input_throughput": 6745.182399402429,
    "output_throughput": 5897.039280599822,
    "total_throughput": 12642.22168000225,
    "itl": 81.62785311426765,
    "ttft": 2041489.99434079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 542,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5839256953169145,
    "arrivals": 942321,
    "finished_requests": 98670,
    "scheduler_time": 321.27258242797603
}
#Debug simulation 
Total elapsed time: 126.76711387233809. Arrivals time: 0.841011602897197 Scheduler time: 125.60987479100004 Scheduler overhead time: 0.126913592685014 Adapter cache time: 0.0278650582768023 Engine time: 0.12054754886776209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 540, 17280, 17280, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 8640, 8640, 17280, 8640, 17280, 17280, 540, 17280, 8640, 8640, 17280, 17280, 8640, 540, 540, 8640, 17280, 17280, 8640, 8640, 540, 540, 8640, 17280, 17280, 540, 17280, 8640, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 17280, 8640, 540, 540, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 540, 17280, 8640, 17280, 17280, 8640, 540, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 17280, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 8640, 540, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 8640, 17280, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 8640, 540, 8640, 540, 540, 540, 17280, 17280, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 540, 8640, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 540, 540, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 17280, 540, 17280, 540, 540, 540, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2830680 . Total input tokens: 631481760 . Total output tokens: 554942354
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 126.11302331183106,
    "estimated_duration": 3600.0520238975364,
    "input_throughput": 6836.126766122659,
    "output_throughput": 5982.23438356983,
    "total_throughput": 12818.36114969249,
    "itl": 83.89734603175606,
    "ttft": 2015707.0922431226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.449206309951848,
    "arrivals": 942321,
    "finished_requests": 99975,
    "scheduler_time": 316.7642237530117
}
#Debug simulation 
Total elapsed time: 126.11327094677836. Arrivals time: 0.8398691471666098 Scheduler time: 124.95211620442569 Scheduler overhead time: 0.1284629236906767 Adapter cache time: 0.029271391220390797 Engine time: 0.12193218665197492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 540, 17280, 17280, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 8640, 8640, 17280, 8640, 17280, 17280, 540, 17280, 8640, 8640, 17280, 17280, 8640, 540, 540, 8640, 17280, 17280, 8640, 8640, 540, 540, 8640, 17280, 17280, 540, 17280, 8640, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 17280, 8640, 540, 540, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 540, 17280, 8640, 17280, 17280, 8640, 540, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 17280, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 8640, 540, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 8640, 17280, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 8640, 540, 8640, 540, 540, 540, 17280, 17280, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 540, 8640, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 540, 540, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 17280, 540, 17280, 540, 540, 540, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2830680 . Total input tokens: 631481760 . Total output tokens: 554942354
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 127.79277383675799,
    "estimated_duration": 3600.023170319186,
    "input_throughput": 6831.645752385375,
    "output_throughput": 5959.634698156058,
    "total_throughput": 12791.280450541433,
    "itl": 80.98183370523381,
    "ttft": 2025744.7307252514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.260054765678976,
    "arrivals": 942321,
    "finished_requests": 99813,
    "scheduler_time": 317.26655960576926
}
#Debug simulation 
Total elapsed time: 127.79294191580266. Arrivals time: 0.8477482036687434 Scheduler time: 126.62065465981141 Scheduler overhead time: 0.13209562236443162 Adapter cache time: 0.02876209607347846 Engine time: 0.12268679961562157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 540, 17280, 17280, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 8640, 8640, 17280, 8640, 17280, 17280, 540, 17280, 8640, 8640, 17280, 17280, 8640, 540, 540, 8640, 17280, 17280, 8640, 8640, 540, 540, 8640, 17280, 17280, 540, 17280, 8640, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 17280, 8640, 540, 540, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 540, 17280, 8640, 17280, 17280, 8640, 540, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 17280, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 8640, 540, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 8640, 17280, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 8640, 540, 8640, 540, 540, 540, 17280, 17280, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 540, 8640, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 540, 540, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 17280, 540, 17280, 540, 540, 540, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2830680 . Total input tokens: 631481760 . Total output tokens: 554942354
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 123.28672922682017,
    "estimated_duration": 3600.0239031807373,
    "input_throughput": 6821.83581567376,
    "output_throughput": 5952.8135302284145,
    "total_throughput": 12774.649345902175,
    "itl": 82.66678797311673,
    "ttft": 2036195.0286308676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.834593048728997,
    "arrivals": 942321,
    "finished_requests": 99747,
    "scheduler_time": 317.48412631812255
}
#Debug simulation 
Total elapsed time: 123.28693134104833. Arrivals time: 0.8405493167228997 Scheduler time: 122.12553301639855 Scheduler overhead time: 0.1291704406030476 Adapter cache time: 0.02831777697429061 Engine time: 0.12273360649123788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 540, 17280, 17280, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 8640, 8640, 17280, 8640, 17280, 17280, 540, 17280, 8640, 8640, 17280, 17280, 8640, 540, 540, 8640, 17280, 17280, 8640, 8640, 540, 540, 8640, 17280, 17280, 540, 17280, 8640, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 17280, 8640, 540, 540, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 540, 17280, 8640, 17280, 17280, 8640, 540, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 17280, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 8640, 540, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 8640, 17280, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 8640, 540, 8640, 540, 540, 540, 17280, 17280, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 540, 8640, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 540, 540, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 17280, 540, 17280, 540, 540, 540, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2830680 . Total input tokens: 631481760 . Total output tokens: 554942354
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 122.99393737362698,
    "estimated_duration": 3600.0582858687067,
    "input_throughput": 6831.96772022957,
    "output_throughput": 5959.92656125082,
    "total_throughput": 12791.894281480389,
    "itl": 80.98075089182919,
    "ttft": 2025871.3986375672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.220081150592339,
    "arrivals": 942321,
    "finished_requests": 99819,
    "scheduler_time": 317.27311391287276
}
#Debug simulation 
Total elapsed time: 122.99409918580204. Arrivals time: 0.8346558753401041 Scheduler time: 121.83842644561082 Scheduler overhead time: 0.12805210100486875 Adapter cache time: 0.027961065527051687 Engine time: 0.12255511665716767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 540, 17280, 17280, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 8640, 8640, 17280, 8640, 17280, 17280, 540, 17280, 8640, 8640, 17280, 17280, 8640, 540, 540, 8640, 17280, 17280, 8640, 8640, 540, 540, 8640, 17280, 17280, 540, 17280, 8640, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 17280, 8640, 540, 540, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 540, 17280, 8640, 17280, 17280, 8640, 540, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 17280, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 8640, 540, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 8640, 17280, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 8640, 540, 8640, 540, 540, 540, 17280, 17280, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 540, 8640, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 540, 540, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 17280, 540, 17280, 540, 540, 540, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2830680 . Total input tokens: 631481760 . Total output tokens: 554942354
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 115.38360844692215,
    "estimated_duration": 3600.0771505173057,
    "input_throughput": 6773.102069908762,
    "output_throughput": 5922.866402164761,
    "total_throughput": 12695.968472073524,
    "itl": 81.60739093056169,
    "ttft": 2041518.1793158895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.492004754492066,
    "arrivals": 942321,
    "finished_requests": 99033,
    "scheduler_time": 319.93132423363676
}
#Debug simulation 
Total elapsed time: 115.38383763795719. Arrivals time: 0.8121531731449068 Scheduler time: 114.26772508397698 Scheduler overhead time: 0.12329777423292398 Adapter cache time: 0.026787986047565937 Engine time: 0.1148791890591383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 540, 17280, 17280, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 8640, 8640, 17280, 8640, 17280, 17280, 540, 17280, 8640, 8640, 17280, 17280, 8640, 540, 540, 8640, 17280, 17280, 8640, 8640, 540, 540, 8640, 17280, 17280, 540, 17280, 8640, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 17280, 8640, 540, 540, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 540, 17280, 8640, 17280, 17280, 8640, 540, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 8640, 540, 17280, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 8640, 540, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 8640, 17280, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 8640, 540, 8640, 540, 540, 540, 17280, 17280, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 540, 8640, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 540, 540, 17280, 8640, 540, 8640, 540, 8640, 17280, 17280, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 17280, 540, 17280, 540, 540, 540, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 540, 17280, 17280, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2830680 . Total input tokens: 631481760 . Total output tokens: 554942354
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 124.16823269380257,
    "estimated_duration": 3600.018423922041,
    "input_throughput": 6832.043368601554,
    "output_throughput": 5959.992553767173,
    "total_throughput": 12792.035922368726,
    "itl": 80.97995700033019,
    "ttft": 2025854.833860527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.180521769858931,
    "arrivals": 942321,
    "finished_requests": 99819,
    "scheduler_time": 317.2728113469411
}
#Debug simulation 
Total elapsed time: 124.16840621689335. Arrivals time: 0.8494966751895845 Scheduler time: 123.00053439754993 Scheduler overhead time: 0.12893737712875009 Adapter cache time: 0.027867228724062443 Engine time: 0.12084240885451436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 270, 17280, 17280, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 8640, 8640, 17280, 8640, 17280, 17280, 270, 17280, 8640, 8640, 17280, 17280, 8640, 270, 270, 8640, 17280, 17280, 8640, 8640, 270, 270, 8640, 17280, 17280, 270, 17280, 8640, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 17280, 8640, 270, 270, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 270, 17280, 8640, 17280, 17280, 8640, 270, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 17280, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 8640, 270, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 8640, 17280, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 8640, 270, 8640, 270, 270, 270, 17280, 17280, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 270, 8640, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 270, 270, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 17280, 270, 17280, 270, 270, 270, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2802060 . Total input tokens: 625186108 . Total output tokens: 549346550
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 128.18431519530714,
    "estimated_duration": 3600.0398932138037,
    "input_throughput": 6908.080392908864,
    "output_throughput": 5995.18050916171,
    "total_throughput": 12903.260902070575,
    "itl": 83.76536418968601,
    "ttft": 2025721.7821021283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 552,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.650049785636416,
    "arrivals": 932854,
    "finished_requests": 100241,
    "scheduler_time": 316.2258027564206
}
#Debug simulation 
Total elapsed time: 128.18456731038168. Arrivals time: 0.8212016746401787 Scheduler time: 127.03906832030043 Scheduler overhead time: 0.13017135905101895 Adapter cache time: 0.02858623443171382 Engine time: 0.12300646444782615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 270, 17280, 17280, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 8640, 8640, 17280, 8640, 17280, 17280, 270, 17280, 8640, 8640, 17280, 17280, 8640, 270, 270, 8640, 17280, 17280, 8640, 8640, 270, 270, 8640, 17280, 17280, 270, 17280, 8640, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 17280, 8640, 270, 270, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 270, 17280, 8640, 17280, 17280, 8640, 270, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 17280, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 8640, 270, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 8640, 17280, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 8640, 270, 8640, 270, 270, 270, 17280, 17280, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 270, 8640, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 270, 270, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 17280, 270, 17280, 270, 270, 270, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2802060 . Total input tokens: 625186108 . Total output tokens: 549346550
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 125.30119113763794,
    "estimated_duration": 3600.010884396775,
    "input_throughput": 6747.213489069794,
    "output_throughput": 5874.555571946188,
    "total_throughput": 12621.769061015983,
    "itl": 81.12235531465001,
    "ttft": 2040974.4547169528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.824110808651899,
    "arrivals": 932854,
    "finished_requests": 97994,
    "scheduler_time": 323.22577825276494
}
#Debug simulation 
Total elapsed time: 125.30135499779135. Arrivals time: 1.2281911126337945 Scheduler time: 123.75524835800752 Scheduler overhead time: 0.12922154366970062 Adapter cache time: 0.02790443366393447 Engine time: 0.12035735603421926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 270, 17280, 17280, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 8640, 8640, 17280, 8640, 17280, 17280, 270, 17280, 8640, 8640, 17280, 17280, 8640, 270, 270, 8640, 17280, 17280, 8640, 8640, 270, 270, 8640, 17280, 17280, 270, 17280, 8640, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 17280, 8640, 270, 270, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 270, 17280, 8640, 17280, 17280, 8640, 270, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 17280, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 8640, 270, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 8640, 17280, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 8640, 270, 8640, 270, 270, 270, 17280, 17280, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 270, 8640, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 270, 270, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 17280, 270, 17280, 270, 270, 270, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2802060 . Total input tokens: 625186108 . Total output tokens: 549346550
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 124.35501666972414,
    "estimated_duration": 3600.013231488017,
    "input_throughput": 6715.930316180138,
    "output_throughput": 5840.143812835312,
    "total_throughput": 12556.074129015451,
    "itl": 80.03228845807051,
    "ttft": 2042140.687667264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9727337370161315,
    "arrivals": 932854,
    "finished_requests": 97551,
    "scheduler_time": 324.30990918763786
}
#Debug simulation 
Total elapsed time: 124.35523514961824. Arrivals time: 0.8365164813585579 Scheduler time: 123.19925015466288 Scheduler overhead time: 0.12765709217637777 Adapter cache time: 0.027881294954568148 Engine time: 0.12304291827604175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 270, 17280, 17280, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 8640, 8640, 17280, 8640, 17280, 17280, 270, 17280, 8640, 8640, 17280, 17280, 8640, 270, 270, 8640, 17280, 17280, 8640, 8640, 270, 270, 8640, 17280, 17280, 270, 17280, 8640, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 17280, 8640, 270, 270, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 270, 17280, 8640, 17280, 17280, 8640, 270, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 17280, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 8640, 270, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 8640, 17280, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 8640, 270, 8640, 270, 270, 270, 17280, 17280, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 270, 8640, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 270, 270, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 17280, 270, 17280, 270, 270, 270, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2802060 . Total input tokens: 625186108 . Total output tokens: 549346550
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 126.31173057900742,
    "estimated_duration": 3600.0125374665913,
    "input_throughput": 6747.561222965721,
    "output_throughput": 5874.65731852226,
    "total_throughput": 12622.21854148798,
    "itl": 81.1150581373826,
    "ttft": 2041308.7649501397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.588113243188704,
    "arrivals": 932854,
    "finished_requests": 98000,
    "scheduler_time": 323.2540455507951
}
#Debug simulation 
Total elapsed time: 126.31195408990607. Arrivals time: 1.1891920249909163 Scheduler time: 124.7970361309126 Scheduler overhead time: 0.13015544973313808 Adapter cache time: 0.028694151900708675 Engine time: 0.12327345320954919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 270, 17280, 17280, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 8640, 8640, 17280, 8640, 17280, 17280, 270, 17280, 8640, 8640, 17280, 17280, 8640, 270, 270, 8640, 17280, 17280, 8640, 8640, 270, 270, 8640, 17280, 17280, 270, 17280, 8640, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 17280, 8640, 270, 270, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 270, 17280, 8640, 17280, 17280, 8640, 270, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 17280, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 8640, 270, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 8640, 17280, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 8640, 270, 8640, 270, 270, 270, 17280, 17280, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 270, 8640, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 270, 270, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 17280, 270, 17280, 270, 270, 270, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2802060 . Total input tokens: 625186108 . Total output tokens: 549346550
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 125.68064408423379,
    "estimated_duration": 3600.04941174667,
    "input_throughput": 6715.909209776519,
    "output_throughput": 5840.144007856602,
    "total_throughput": 12556.05321763312,
    "itl": 80.0312808996727,
    "ttft": 2042208.882526084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.936902465461788,
    "arrivals": 932854,
    "finished_requests": 97553,
    "scheduler_time": 324.31652092821616
}
#Debug simulation 
Total elapsed time: 125.68085504882038. Arrivals time: 0.8382930490188301 Scheduler time: 124.52135542267933 Scheduler overhead time: 0.1303230174817145 Adapter cache time: 0.028422769159078598 Engine time: 0.12156121479347348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_320_slots_16_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 270, 17280, 17280, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 8640, 8640, 17280, 8640, 17280, 17280, 270, 17280, 8640, 8640, 17280, 17280, 8640, 270, 270, 8640, 17280, 17280, 8640, 8640, 270, 270, 8640, 17280, 17280, 270, 17280, 8640, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 17280, 8640, 270, 270, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 270, 17280, 8640, 17280, 17280, 8640, 270, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 17280, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 8640, 270, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 8640, 17280, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 8640, 270, 8640, 270, 270, 270, 17280, 17280, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 270, 8640, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 270, 270, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 17280, 270, 17280, 270, 270, 270, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2802060 . Total input tokens: 625186108 . Total output tokens: 549346550
Prompts distributed
Adapter sizes. Values: [16]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 127.48469784902409,
    "estimated_duration": 3600.0311919637584,
    "input_throughput": 6822.976160548576,
    "output_throughput": 5931.946103042261,
    "total_throughput": 12754.922263590837,
    "itl": 82.09004191780028,
    "ttft": 2027427.9119626377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.479236912610925,
    "arrivals": 932854,
    "finished_requests": 99123,
    "scheduler_time": 319.34667471905783
}
#Debug simulation 
Total elapsed time: 127.48486072290689. Arrivals time: 1.1474801455624402 Scheduler time: 126.01736748497933 Scheduler overhead time: 0.12976454850286245 Adapter cache time: 0.029079587664455175 Engine time: 0.12029448058456182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_320_slots_16_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 270, 17280, 17280, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 8640, 8640, 17280, 8640, 17280, 17280, 270, 17280, 8640, 8640, 17280, 17280, 8640, 270, 270, 8640, 17280, 17280, 8640, 8640, 270, 270, 8640, 17280, 17280, 270, 17280, 8640, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 17280, 8640, 270, 270, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 270, 17280, 8640, 17280, 17280, 8640, 270, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 8640, 270, 17280, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 8640, 270, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 8640, 17280, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 8640, 270, 8640, 270, 270, 270, 17280, 17280, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 270, 8640, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 270, 270, 17280, 8640, 270, 8640, 270, 8640, 17280, 17280, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 17280, 270, 17280, 270, 270, 270, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 270, 17280, 17280, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2802060 . Total input tokens: 625186108 . Total output tokens: 549346550
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 125.45883330516517,
    "estimated_duration": 3600.0125068971424,
    "input_throughput": 6715.9780566536765,
    "output_throughput": 5840.2038769918945,
    "total_throughput": 12556.18193364557,
    "itl": 80.03055251937465,
    "ttft": 2042193.1903551123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9002427252009855,
    "arrivals": 932854,
    "finished_requests": 97553,
    "scheduler_time": 324.31617468772765
}
#Debug simulation 
Total elapsed time: 125.45905773807317. Arrivals time: 0.8471369557082653 Scheduler time: 124.28561505675316 Scheduler overhead time: 0.13247057981789112 Adapter cache time: 0.028206840623170137 Engine time: 0.12266519386321306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 135, 17280, 17280, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 8640, 8640, 17280, 8640, 17280, 17280, 135, 17280, 8640, 8640, 17280, 17280, 8640, 135, 135, 8640, 17280, 17280, 8640, 8640, 135, 135, 8640, 17280, 17280, 135, 17280, 8640, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 17280, 8640, 135, 135, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 135, 17280, 8640, 17280, 17280, 8640, 135, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 17280, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 8640, 135, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 8640, 17280, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 8640, 135, 8640, 135, 135, 135, 17280, 17280, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 135, 8640, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 17280, 135, 17280, 135, 135, 135, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2787750 . Total input tokens: 621975333 . Total output tokens: 546547488
Prompts distributed
Adapter sizes. Values: [8]. Counts: [320]
---Simulation End---
#Simulation results
{
    "duration": 125.91282834066078,
    "estimated_duration": 3600.004011690327,
    "input_throughput": 6988.219712618494,
    "output_throughput": 6088.552104059561,
    "total_throughput": 13076.771816678054,
    "itl": 84.49575818610597,
    "ttft": 2011519.3390434987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6236001495086154,
    "arrivals": 928080,
    "finished_requests": 101628,
    "scheduler_time": 311.7947790788028
}
#Debug simulation 
Total elapsed time: 125.91299255285412. Arrivals time: 0.8667384847067297 Scheduler time: 124.7251448747702 Scheduler overhead time: 0.1302662924863398 Adapter cache time: 0.0288567035458982 Engine time: 0.12223793845623732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 135, 17280, 17280, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 8640, 8640, 17280, 8640, 17280, 17280, 135, 17280, 8640, 8640, 17280, 17280, 8640, 135, 135, 8640, 17280, 17280, 8640, 8640, 135, 135, 8640, 17280, 17280, 135, 17280, 8640, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 17280, 8640, 135, 135, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 135, 17280, 8640, 17280, 17280, 8640, 135, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 17280, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 8640, 135, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 8640, 17280, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 8640, 135, 8640, 135, 135, 135, 17280, 17280, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 135, 8640, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 17280, 135, 17280, 135, 135, 135, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2787750 . Total input tokens: 621975333 . Total output tokens: 546547488
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 124.28269576700404,
    "estimated_duration": 3600.0064453129626,
    "input_throughput": 6959.608650873374,
    "output_throughput": 6071.344407856996,
    "total_throughput": 13030.95305873037,
    "itl": 84.43285507549257,
    "ttft": 2011215.7312968154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.228536041295162,
    "arrivals": 928080,
    "finished_requests": 101326,
    "scheduler_time": 312.5632329040935
}
#Debug simulation 
Total elapsed time: 124.2829255182296. Arrivals time: 0.888344369828701 Scheduler time: 123.0677104354836 Scheduler overhead time: 0.12906951503828168 Adapter cache time: 0.03017901722341776 Engine time: 0.12717853393405676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 135, 17280, 17280, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 8640, 8640, 17280, 8640, 17280, 17280, 135, 17280, 8640, 8640, 17280, 17280, 8640, 135, 135, 8640, 17280, 17280, 8640, 8640, 135, 135, 8640, 17280, 17280, 135, 17280, 8640, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 17280, 8640, 135, 135, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 135, 17280, 8640, 17280, 17280, 8640, 135, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 17280, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 8640, 135, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 8640, 17280, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 8640, 135, 8640, 135, 135, 135, 17280, 17280, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 135, 8640, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 17280, 135, 17280, 135, 135, 135, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2787750 . Total input tokens: 621975333 . Total output tokens: 546547488
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [214 106]
---Simulation End---
#Simulation results
{
    "duration": 123.25402949284762,
    "estimated_duration": 3600.064025742546,
    "input_throughput": 6912.408174426069,
    "output_throughput": 6014.033596398137,
    "total_throughput": 12926.441770824207,
    "itl": 82.1328615432119,
    "ttft": 2014498.8034054572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.183952492526768,
    "arrivals": 928080,
    "finished_requests": 100547,
    "scheduler_time": 314.6128180611394
}
#Debug simulation 
Total elapsed time: 123.25420743180439. Arrivals time: 0.8624783759005368 Scheduler time: 122.06172024738044 Scheduler overhead time: 0.13101546606048942 Adapter cache time: 0.02956129051744938 Engine time: 0.12805346446111798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 135, 17280, 17280, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 8640, 8640, 17280, 8640, 17280, 17280, 135, 17280, 8640, 8640, 17280, 17280, 8640, 135, 135, 8640, 17280, 17280, 8640, 8640, 135, 135, 8640, 17280, 17280, 135, 17280, 8640, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 17280, 8640, 135, 135, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 135, 17280, 8640, 17280, 17280, 8640, 135, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 17280, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 8640, 135, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 8640, 17280, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 8640, 135, 8640, 135, 135, 135, 17280, 17280, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 135, 8640, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 17280, 135, 17280, 135, 135, 135, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2787750 . Total input tokens: 621975333 . Total output tokens: 546547488
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107 213]
---Simulation End---
#Simulation results
{
    "duration": 128.86399945802987,
    "estimated_duration": 3600.069350957073,
    "input_throughput": 6960.253694373562,
    "output_throughput": 6071.28609736069,
    "total_throughput": 13031.539791734253,
    "itl": 84.42306249616966,
    "ttft": 2011459.4671285118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9619976144190825,
    "arrivals": 928080,
    "finished_requests": 101337,
    "scheduler_time": 312.5945075484749
}
#Debug simulation 
Total elapsed time: 128.86423621838912. Arrivals time: 0.9345603524707258 Scheduler time: 127.58318163640797 Scheduler overhead time: 0.14081425918266177 Adapter cache time: 0.0312225092202425 Engine time: 0.13207828719168901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 320,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_320_slots_16_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [106 107 107]
Adapter prompts. [8640, 17280, 17280, 135, 17280, 17280, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 8640, 8640, 17280, 8640, 17280, 17280, 135, 17280, 8640, 8640, 17280, 17280, 8640, 135, 135, 8640, 17280, 17280, 8640, 8640, 135, 135, 8640, 17280, 17280, 135, 17280, 8640, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 17280, 8640, 135, 135, 8640, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 135, 17280, 8640, 17280, 17280, 8640, 135, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 8640, 135, 17280, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 8640, 135, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 8640, 17280, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 17280, 17280, 17280, 8640, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 8640, 135, 8640, 135, 135, 135, 17280, 17280, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 135, 8640, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 135, 135, 17280, 8640, 135, 8640, 135, 8640, 17280, 17280, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 17280, 135, 17280, 135, 135, 135, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 135, 17280, 17280, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2787750 . Total input tokens: 621975333 . Total output tokens: 546547488
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [107 107 106]
---Simulation End---
#Simulation results
{
    "duration": 124.75417586416006,
    "estimated_duration": 3600.0253448987023,
    "input_throughput": 6912.482445509066,
    "output_throughput": 6014.098214802767,
    "total_throughput": 12926.580660311833,
    "itl": 82.13213269885972,
    "ttft": 2014482.9277761925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.145635814853049,
    "arrivals": 928080,
    "finished_requests": 100547,
    "scheduler_time": 314.61245389496924
}
#Debug simulation 
Total elapsed time: 124.75433987705037. Arrivals time: 0.8801646563224494 Scheduler time: 123.55208616517484 Scheduler overhead time: 0.13034160900861025 Adapter cache time: 0.02860515657812357 Engine time: 0.12243251968175173 
