INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.340123055037111,
    "estimated_duration": 3600.066336869505,
    "input_throughput": 7238.596059499389,
    "output_throughput": 6349.506053790414,
    "total_throughput": 13588.102113289802,
    "itl": 95.01176197969373,
    "ttft": 1105827.8869603863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 145872,
    "finished_requests": 105588,
    "scheduler_time": 61.970811719799464
}
#Debug simulation 
Total elapsed time: 7.340238486067392. Arrivals time: 0.27059484948404133 Scheduler time: 6.914936094311997 Scheduler overhead time: 0.05661907827015966 Adapter cache time: 0.012851804844103754 Engine time: 0.05867305351421237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.378735611913726,
    "estimated_duration": 3600.1033557065175,
    "input_throughput": 7181.964917483825,
    "output_throughput": 6295.412870320824,
    "total_throughput": 13477.37778780465,
    "itl": 92.2614001580918,
    "ttft": 1119224.273158084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2348263315111399,
    "arrivals": 145872,
    "finished_requests": 104715,
    "scheduler_time": 59.97071291990832
}
#Debug simulation 
Total elapsed time: 7.378836291958578. Arrivals time: 0.2851562605937943 Scheduler time: 6.934477230650373 Scheduler overhead time: 0.05834111792501062 Adapter cache time: 0.01350092759821564 Engine time: 0.059915876598097384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.16348468803335,
    "estimated_duration": 3600.027340700287,
    "input_throughput": 7016.485878989588,
    "output_throughput": 6156.539909968755,
    "total_throughput": 13173.025788958343,
    "itl": 85.83856781391569,
    "ttft": 1153026.3898291881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2413462840113789,
    "arrivals": 145872,
    "finished_requests": 102345,
    "scheduler_time": 54.73614326250134
}
#Debug simulation 
Total elapsed time: 7.163575854036026. Arrivals time: 0.2792546146083623 Scheduler time: 6.716250182362273 Scheduler overhead time: 0.061386923654936254 Adapter cache time: 0.014458639663644135 Engine time: 0.06340124923735857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.378818667028099,
    "estimated_duration": 3600.0172613456407,
    "input_throughput": 7181.920563996328,
    "output_throughput": 6295.598147084546,
    "total_throughput": 13477.518711080875,
    "itl": 92.2640371248427,
    "ttft": 1119137.9135113512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 145872,
    "finished_requests": 104714,
    "scheduler_time": 59.970965065666014
}
#Debug simulation 
Total elapsed time: 7.378910981002264. Arrivals time: 0.28055248758755624 Scheduler time: 6.9397197550861165 Scheduler overhead time: 0.05800723715219647 Adapter cache time: 0.013420179369859397 Engine time: 0.059859115281142294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.25637217098847,
    "estimated_duration": 3600.0209390841305,
    "input_throughput": 7016.498355819618,
    "output_throughput": 6156.550857628788,
    "total_throughput": 13173.049213448407,
    "itl": 85.83856113691594,
    "ttft": 1153004.2934436537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861717,
    "arrivals": 145872,
    "finished_requests": 102345,
    "scheduler_time": 54.73707879566772
}
#Debug simulation 
Total elapsed time: 7.256461728946306. Arrivals time: 0.2834613242885098 Scheduler time: 6.803947942098603 Scheduler overhead time: 0.06163990206550807 Adapter cache time: 0.014393114717677236 Engine time: 0.06390635762363672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.4310616420116276,
    "estimated_duration": 3600.0346681907045,
    "input_throughput": 7181.644173719504,
    "output_throughput": 6295.231598808446,
    "total_throughput": 13476.87577252795,
    "itl": 92.26308674928194,
    "ttft": 1119287.357791406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 145872,
    "finished_requests": 104710,
    "scheduler_time": 59.96978052183544
}
#Debug simulation 
Total elapsed time: 7.43116191693116. Arrivals time: 0.2860698456643149 Scheduler time: 6.985303348628804 Scheduler overhead time: 0.05845774384215474 Adapter cache time: 0.013474501552991569 Engine time: 0.060428127413615584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 1080, 1080, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 438480 . Total input tokens: 97834932 . Total output tokens: 86185578
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.237056056037545,
    "estimated_duration": 3600.007955814143,
    "input_throughput": 7016.460327318248,
    "output_throughput": 6156.468061190091,
    "total_throughput": 13172.92838850834,
    "itl": 85.83858402378807,
    "ttft": 1153021.966066472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585545,
    "arrivals": 145872,
    "finished_requests": 102344,
    "scheduler_time": 54.735988921694975
}
#Debug simulation 
Total elapsed time: 7.2372161919483915. Arrivals time: 0.28222473594360054 Scheduler time: 6.785859794705175 Scheduler overhead time: 0.06172808853443712 Adapter cache time: 0.014508423395454884 Engine time: 0.06359791615977883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.543861769954674,
    "estimated_duration": 3600.057079193946,
    "input_throughput": 7401.468480595864,
    "output_throughput": 6451.326878739411,
    "total_throughput": 13852.795359335276,
    "itl": 93.03658318477378,
    "ttft": 1068568.7738748484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 144123,
    "finished_requests": 107234,
    "scheduler_time": 63.92152596893767
}
#Debug simulation 
Total elapsed time: 7.543958512949757. Arrivals time: 0.2846387989120558 Scheduler time: 7.101333354017697 Scheduler overhead time: 0.05761805654037744 Adapter cache time: 0.013698849594220519 Engine time: 0.059586278861388564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.3848419970599934,
    "estimated_duration": 3600.00984296238,
    "input_throughput": 7336.141052955445,
    "output_throughput": 6398.423894598421,
    "total_throughput": 13734.564947553865,
    "itl": 90.38093140062676,
    "ttft": 1081640.3661526674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 144123,
    "finished_requests": 106317,
    "scheduler_time": 61.879451410260586
}
#Debug simulation 
Total elapsed time: 7.384941534022801. Arrivals time: 0.28634735627565533 Scheduler time: 6.938775322167203 Scheduler overhead time: 0.058496891404502094 Adapter cache time: 0.013783977134153247 Engine time: 0.06010654103010893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.38360882003326,
    "estimated_duration": 3600.082812659792,
    "input_throughput": 7175.481605356381,
    "output_throughput": 6259.3379576597745,
    "total_throughput": 13434.819563016155,
    "itl": 84.04949523583528,
    "ttft": 1116994.5028939138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 144123,
    "finished_requests": 103982,
    "scheduler_time": 56.52624575467933
}
#Debug simulation 
Total elapsed time: 7.383704995037988. Arrivals time: 0.2825207784771919 Scheduler time: 6.927994157187641 Scheduler overhead time: 0.06300515367183834 Adapter cache time: 0.015090874047018588 Engine time: 0.06530422379728407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.45764814899303,
    "estimated_duration": 3600.0914490722153,
    "input_throughput": 7336.35336035882,
    "output_throughput": 6398.718011992899,
    "total_throughput": 13735.071372351718,
    "itl": 90.37853591846698,
    "ttft": 1081552.5295395816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 144123,
    "finished_requests": 106323,
    "scheduler_time": 61.87969943017582
}
#Debug simulation 
Total elapsed time: 7.457771176006645. Arrivals time: 0.2996282014064491 Scheduler time: 6.995692176395096 Scheduler overhead time: 0.05944243387784809 Adapter cache time: 0.01396755501627922 Engine time: 0.061153936898335814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.391856668982655,
    "estimated_duration": 3600.026229545893,
    "input_throughput": 7175.594385393822,
    "output_throughput": 6259.436338285362,
    "total_throughput": 13435.030723679183,
    "itl": 84.04839561462063,
    "ttft": 1116946.6482563023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 144123,
    "finished_requests": 103982,
    "scheduler_time": 56.52569608859644
}
#Debug simulation 
Total elapsed time: 7.391976995044388. Arrivals time: 0.2922857712255791 Scheduler time: 6.926276042708196 Scheduler overhead time: 0.0630349856801331 Adapter cache time: 0.014967717812396586 Engine time: 0.06537974462844431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.545356697984971,
    "estimated_duration": 3600.0406305239376,
    "input_throughput": 7336.309422751218,
    "output_throughput": 6398.584450600365,
    "total_throughput": 13734.893873351582,
    "itl": 90.37879774876237,
    "ttft": 1081577.6985658808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 144123,
    "finished_requests": 106320,
    "scheduler_time": 61.879465498929385
}
#Debug simulation 
Total elapsed time: 7.545478938962333. Arrivals time: 0.29166714986786246 Scheduler time: 7.09093470859807 Scheduler overhead time: 0.059430723544210196 Adapter cache time: 0.013998289476148784 Engine time: 0.06139703735243529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 540, 540, 4320, 540, 34560, 4320, 540, 540, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 34560, 34560, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 433080 . Total input tokens: 96622187 . Total output tokens: 85132142
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.350548182963394,
    "estimated_duration": 3600.029906059329,
    "input_throughput": 7175.482891550814,
    "output_throughput": 6259.34244659262,
    "total_throughput": 13434.825338143435,
    "itl": 84.05055852346483,
    "ttft": 1117045.2715334313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 144123,
    "finished_requests": 103981,
    "scheduler_time": 56.52728896782288
}
#Debug simulation 
Total elapsed time: 7.350719630951062. Arrivals time: 0.28071749303489923 Scheduler time: 6.897577460855246 Scheduler overhead time: 0.06285160186234862 Adapter cache time: 0.014883662457577884 Engine time: 0.06494750827550888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.660714029916562,
    "estimated_duration": 3600.0745081019195,
    "input_throughput": 7595.3978003684915,
    "output_throughput": 6562.387513600639,
    "total_throughput": 14157.78531396913,
    "itl": 91.35322844040446,
    "ttft": 1018395.0952034693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 143211,
    "finished_requests": 109843,
    "scheduler_time": 66.69456963181217
}
#Debug simulation 
Total elapsed time: 7.660803923965432. Arrivals time: 0.28767678525764495 Scheduler time: 7.214110591565259 Scheduler overhead time: 0.05846800480503589 Adapter cache time: 0.012679977808147669 Engine time: 0.06016989634372294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.604269323986955,
    "estimated_duration": 3600.032469420061,
    "input_throughput": 7530.135416908342,
    "output_throughput": 6508.568241823267,
    "total_throughput": 14038.70365873161,
    "itl": 88.73376006444032,
    "ttft": 1033349.5201372163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 143211,
    "finished_requests": 108894,
    "scheduler_time": 64.54384911106047
}
#Debug simulation 
Total elapsed time: 7.6043924089754. Arrivals time: 0.28851183701772243 Scheduler time: 7.152566748554818 Scheduler overhead time: 0.06005408789496869 Adapter cache time: 0.01289239979814738 Engine time: 0.06191920523997396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.510838555986993,
    "estimated_duration": 3600.0611011746664,
    "input_throughput": 7360.0817473221105,
    "output_throughput": 6361.5350840926885,
    "total_throughput": 13721.6168314148,
    "itl": 82.56026555484456,
    "ttft": 1069658.7818246435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 143211,
    "finished_requests": 106400,
    "scheduler_time": 58.92281407011115
}
#Debug simulation 
Total elapsed time: 7.510967932990752. Arrivals time: 0.2853522553341463 Scheduler time: 7.051495052641258 Scheduler overhead time: 0.06402426306158304 Adapter cache time: 0.013744858326390386 Engine time: 0.06611703324597329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.637906335992739,
    "estimated_duration": 3600.084587937641,
    "input_throughput": 7530.182510386496,
    "output_throughput": 6508.717622499896,
    "total_throughput": 14038.900132886392,
    "itl": 88.73384369487141,
    "ttft": 1033277.6925680991,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 143211,
    "finished_requests": 108897,
    "scheduler_time": 64.54577687211592
}
#Debug simulation 
Total elapsed time: 7.638027074048296. Arrivals time: 0.28854496218264103 Scheduler time: 7.185354377143085 Scheduler overhead time: 0.06031216017436236 Adapter cache time: 0.01284110767301172 Engine time: 0.0625249994918704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.491488078027032,
    "estimated_duration": 3600.018244715714,
    "input_throughput": 7359.949644391963,
    "output_throughput": 6361.2830389442715,
    "total_throughput": 13721.232683336235,
    "itl": 82.56019208298414,
    "ttft": 1069648.0842907594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 143211,
    "finished_requests": 106397,
    "scheduler_time": 58.92232645761103
}
#Debug simulation 
Total elapsed time: 7.49161055800505. Arrivals time: 0.28706392913591117 Scheduler time: 7.030703702010214 Scheduler overhead time: 0.06383017916232347 Adapter cache time: 0.013771792990155518 Engine time: 0.06595872121397406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.637523768935353,
    "estimated_duration": 3600.079175116457,
    "input_throughput": 7530.0999454055245,
    "output_throughput": 6508.61046666898,
    "total_throughput": 14038.710412074504,
    "itl": 88.73504741932204,
    "ttft": 1033343.0743626191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 143211,
    "finished_requests": 108895,
    "scheduler_time": 64.54454297681835
}
#Debug simulation 
Total elapsed time: 7.637656281935051. Arrivals time: 0.2868369831703603 Scheduler time: 7.186884013470262 Scheduler overhead time: 0.06053165753837675 Adapter cache time: 0.013025580672547221 Engine time: 0.06193749757949263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 270, 270, 4320, 270, 34560, 4320, 270, 270, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 34560, 34560, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 430380 . Total input tokens: 96006273 . Total output tokens: 84614994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.451661744038574,
    "estimated_duration": 3600.0027411739197,
    "input_throughput": 7360.016618031777,
    "output_throughput": 6361.4143228491575,
    "total_throughput": 13721.430940880935,
    "itl": 82.56192096100129,
    "ttft": 1069712.2332788594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 143211,
    "finished_requests": 106398,
    "scheduler_time": 58.92166029947933
}
#Debug simulation 
Total elapsed time: 7.451827861950733. Arrivals time: 0.28646433749236166 Scheduler time: 6.992163278744556 Scheduler overhead time: 0.06375538092106581 Adapter cache time: 0.013702108291909099 Engine time: 0.0655883977888152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.773485968937166,
    "estimated_duration": 3600.072761813297,
    "input_throughput": 7604.3176933488185,
    "output_throughput": 6631.6709632209795,
    "total_throughput": 14235.988656569798,
    "itl": 90.55428411377582,
    "ttft": 1011807.7752570863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 142732,
    "finished_requests": 110369,
    "scheduler_time": 67.60265249565329
}
#Debug simulation 
Total elapsed time: 7.77360398194287. Arrivals time: 0.29409408185165375 Scheduler time: 7.317711852025241 Scheduler overhead time: 0.05963997938670218 Adapter cache time: 0.012122324435040355 Engine time: 0.06186157476622611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.685589715954848,
    "estimated_duration": 3600.034628871207,
    "input_throughput": 7537.974157906589,
    "output_throughput": 6573.0878837184,
    "total_throughput": 14111.06204162499,
    "itl": 87.99618231632037,
    "ttft": 1026132.1666995161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 142732,
    "finished_requests": 109411,
    "scheduler_time": 65.41412470888444
}
#Debug simulation 
Total elapsed time: 7.685684738913551. Arrivals time: 0.30017648660577834 Scheduler time: 7.220797281363048 Scheduler overhead time: 0.06101374875288457 Adapter cache time: 0.012312604929320514 Engine time: 0.06278216640930623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.600392135092989,
    "estimated_duration": 3600.0846947780983,
    "input_throughput": 7366.566969512547,
    "output_throughput": 6420.3667301290225,
    "total_throughput": 13786.93369964157,
    "itl": 81.94556192955434,
    "ttft": 1064799.2910533317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 142732,
    "finished_requests": 106855,
    "scheduler_time": 59.66808670102089
}
#Debug simulation 
Total elapsed time: 7.600513625075109. Arrivals time: 0.2906721115577966 Scheduler time: 7.134468932519667 Scheduler overhead time: 0.06475084903649986 Adapter cache time: 0.013069064589217305 Engine time: 0.0669291663216427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.618401329033077,
    "estimated_duration": 3600.0570120347147,
    "input_throughput": 7537.79729301085,
    "output_throughput": 6572.956461771673,
    "total_throughput": 14110.753754782523,
    "itl": 87.99726045464853,
    "ttft": 1026225.6881079559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 142732,
    "finished_requests": 109412,
    "scheduler_time": 65.41549473123699
}
#Debug simulation 
Total elapsed time: 7.618519573006779. Arrivals time: 0.2895130456890911 Scheduler time: 7.164715783321299 Scheduler overhead time: 0.06076835573185235 Adapter cache time: 0.012257529189810157 Engine time: 0.06271045841276646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.460320323007181,
    "estimated_duration": 3600.0420135671206,
    "input_throughput": 7366.583750983091,
    "output_throughput": 6420.336182993011,
    "total_throughput": 13786.919933976102,
    "itl": 81.94478300269164,
    "ttft": 1064736.0631377548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 142732,
    "finished_requests": 106855,
    "scheduler_time": 59.669108434629806
}
#Debug simulation 
Total elapsed time: 7.460444060037844. Arrivals time: 0.28939965565223247 Scheduler time: 6.997330488520674 Scheduler overhead time: 0.06405412161257118 Adapter cache time: 0.012914268067106605 Engine time: 0.06646084587555379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.660853271954693,
    "estimated_duration": 3600.0431321783203,
    "input_throughput": 7537.957186524017,
    "output_throughput": 6573.06319151842,
    "total_throughput": 14111.020378042436,
    "itl": 87.99570241768322,
    "ttft": 1026122.0118216831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 142732,
    "finished_requests": 109411,
    "scheduler_time": 65.41643517864668
}
#Debug simulation 
Total elapsed time: 7.660946832969785. Arrivals time: 0.2908983692759648 Scheduler time: 7.205480444361456 Scheduler overhead time: 0.060902874916791916 Adapter cache time: 0.012236769311130047 Engine time: 0.06274113594554365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 135, 135, 4320, 135, 34560, 4320, 135, 135, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 34560, 34560, 135, 34560, 34560, 34560, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 429030 . Total input tokens: 95700211 . Total output tokens: 84347053
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.554049686994404,
    "estimated_duration": 3600.0180340212573,
    "input_throughput": 7366.665319278066,
    "output_throughput": 6420.3920040317,
    "total_throughput": 13787.057323309766,
    "itl": 81.94664710588627,
    "ttft": 1064773.9353015511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 142732,
    "finished_requests": 106854,
    "scheduler_time": 59.66822830235791
}
#Debug simulation 
Total elapsed time: 7.554211137932725. Arrivals time: 0.28751915064640343 Scheduler time: 7.08999349840451 Scheduler overhead time: 0.0659119636984542 Adapter cache time: 0.012971858261153102 Engine time: 0.06696932890918106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.778939507086761,
    "estimated_duration": 3600.016054298384,
    "input_throughput": 7634.609564361113,
    "output_throughput": 6683.707138269803,
    "total_throughput": 14318.316702630917,
    "itl": 89.97704087631676,
    "ttft": 999398.3806142694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 142534,
    "finished_requests": 110920,
    "scheduler_time": 68.69782304170158
}
#Debug simulation 
Total elapsed time: 7.77903429698199. Arrivals time: 0.29326548625249416 Scheduler time: 7.325084835989401 Scheduler overhead time: 0.059732197783887386 Adapter cache time: 0.011145050870254636 Engine time: 0.06169987469911575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.803061768994667,
    "estimated_duration": 3600.0017412551715,
    "input_throughput": 7569.6135609364555,
    "output_throughput": 6622.197908073235,
    "total_throughput": 14191.81146900969,
    "itl": 87.44433605174191,
    "ttft": 1013790.1548272617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 142534,
    "finished_requests": 109930,
    "scheduler_time": 66.4433148564715
}
#Debug simulation 
Total elapsed time: 7.8031580849783495. Arrivals time: 0.29113241110462695 Scheduler time: 7.34648518031463 Scheduler overhead time: 0.06157514394726604 Adapter cache time: 0.011578913661651313 Engine time: 0.06327365047764033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.557109253946692,
    "estimated_duration": 3600.0347172768465,
    "input_throughput": 7395.362848094617,
    "output_throughput": 6469.0378923946,
    "total_throughput": 13864.400740489216,
    "itl": 81.4824776918572,
    "ttft": 1052963.2630659516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 142534,
    "finished_requests": 107384,
    "scheduler_time": 60.55659420066387
}
#Debug simulation 
Total elapsed time: 7.557198993978091. Arrivals time: 0.2905537054175511 Scheduler time: 7.092596314032562 Scheduler overhead time: 0.06473880901467055 Adapter cache time: 0.01207265432458371 Engine time: 0.0667825173586607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.732666254043579,
    "estimated_duration": 3600.04596151733,
    "input_throughput": 7569.429471537824,
    "output_throughput": 6622.036289212315,
    "total_throughput": 14191.46576075014,
    "itl": 87.44345428035146,
    "ttft": 1013795.1723091316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 142534,
    "finished_requests": 109931,
    "scheduler_time": 66.44534321669177
}
#Debug simulation 
Total elapsed time: 7.7327579440316185. Arrivals time: 0.30358846962917596 Scheduler time: 7.264052196871489 Scheduler overhead time: 0.061477353097870946 Adapter cache time: 0.011645891470834613 Engine time: 0.06306857394520193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.601657558931038,
    "estimated_duration": 3600.065207055591,
    "input_throughput": 7395.333825571145,
    "output_throughput": 6469.167545731171,
    "total_throughput": 13864.501371302316,
    "itl": 81.48297521704066,
    "ttft": 1052996.7444487084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 142534,
    "finished_requests": 107387,
    "scheduler_time": 60.55708798877572
}
#Debug simulation 
Total elapsed time: 7.6017513750121. Arrivals time: 0.28707132826093584 Scheduler time: 7.139755431097001 Scheduler overhead time: 0.06514548929408193 Adapter cache time: 0.01214396650902927 Engine time: 0.06685574760194868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.735425458056852,
    "estimated_duration": 3600.0037156423045,
    "input_throughput": 7569.456076280217,
    "output_throughput": 6622.133998477436,
    "total_throughput": 14191.590074757652,
    "itl": 87.44390701693034,
    "ttft": 1013792.4651513178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 142534,
    "finished_requests": 109929,
    "scheduler_time": 66.44474202129842
}
#Debug simulation 
Total elapsed time: 7.7355236280709505. Arrivals time: 0.2924572395859286 Scheduler time: 7.2788390557980165 Scheduler overhead time: 0.06093707063701004 Adapter cache time: 0.011398658971302211 Engine time: 0.06309060007333755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 66, 66, 4320, 66, 34560, 4320, 66, 66, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 34560, 34560, 66, 34560, 34560, 34560, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428340 . Total input tokens: 95548107 . Total output tokens: 84209726
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.625140463002026,
    "estimated_duration": 3600.0387832865736,
    "input_throughput": 7395.304218277001,
    "output_throughput": 6468.971697782446,
    "total_throughput": 13864.275916059449,
    "itl": 81.48283709690472,
    "ttft": 1052907.3367098954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 142534,
    "finished_requests": 107383,
    "scheduler_time": 60.55658304353808
}
#Debug simulation 
Total elapsed time: 7.625298356986605. Arrivals time: 0.2907700438518077 Scheduler time: 7.158849339350127 Scheduler overhead time: 0.06524235475808382 Adapter cache time: 0.012133958633057773 Engine time: 0.06747025367803872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.864839524030685,
    "estimated_duration": 3600.047173589167,
    "input_throughput": 7721.726594011665,
    "output_throughput": 6705.4799106793125,
    "total_throughput": 14427.206504690977,
    "itl": 89.24599283638584,
    "ttft": 987161.8863776514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 142385,
    "finished_requests": 111657,
    "scheduler_time": 69.28043295494734
}
#Debug simulation 
Total elapsed time: 7.864934450946748. Arrivals time: 0.29340819432400167 Scheduler time: 7.409814389189705 Scheduler overhead time: 0.06045223108958453 Adapter cache time: 0.01068655599374324 Engine time: 0.06223798205610365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.323944009025581,
    "estimated_duration": 3600.0176332283695,
    "input_throughput": 7186.136468115156,
    "output_throughput": 6241.018041862114,
    "total_throughput": 13427.15450997727,
    "itl": 73.7791895342535,
    "ttft": 1094004.2388654992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 142385,
    "finished_requests": 103841,
    "scheduler_time": 52.10380068856623
}
#Debug simulation 
Total elapsed time: 7.324041209998541. Arrivals time: 0.2829536709468812 Scheduler time: 6.85274105751887 Scheduler overhead time: 0.0704064128221944 Adapter cache time: 0.012341218767687678 Engine time: 0.07275332731660455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.572044325992465,
    "estimated_duration": 3600.0407753927802,
    "input_throughput": 7460.221890700605,
    "output_throughput": 6480.677707728051,
    "total_throughput": 13940.899598428658,
    "itl": 80.76634032519719,
    "ttft": 1043139.692029415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 142385,
    "finished_requests": 107834,
    "scheduler_time": 60.8425234792267
}
#Debug simulation 
Total elapsed time: 7.572143279016018. Arrivals time: 0.29018829902634025 Scheduler time: 7.106644776649773 Scheduler overhead time: 0.0653790693031624 Adapter cache time: 0.011482818285003304 Engine time: 0.067596988636069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.733189153950661,
    "estimated_duration": 3600.0947492715327,
    "input_throughput": 7652.700531166504,
    "output_throughput": 6645.2951008694645,
    "total_throughput": 14297.995632035969,
    "itl": 86.7419903677666,
    "ttft": 1002995.5114330255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 142385,
    "finished_requests": 110632,
    "scheduler_time": 66.97503667290457
}
#Debug simulation 
Total elapsed time: 7.733304835972376. Arrivals time: 0.29495393915567547 Scheduler time: 7.272990218130872 Scheduler overhead time: 0.06178632646333426 Adapter cache time: 0.010953380377031863 Engine time: 0.06337483902461827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.541441604029387,
    "estimated_duration": 3600.0408182252345,
    "input_throughput": 7463.180657280824,
    "output_throughput": 6483.658707932921,
    "total_throughput": 13946.839365213746,
    "itl": 80.8562745554565,
    "ttft": 1042604.8098913829,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 142385,
    "finished_requests": 107879,
    "scheduler_time": 60.93849702546338
}
#Debug simulation 
Total elapsed time: 7.54156206396874. Arrivals time: 0.2901532877003774 Scheduler time: 7.077060955693014 Scheduler overhead time: 0.06515155336819589 Adapter cache time: 0.011455383151769638 Engine time: 0.06729765271302313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.761495972983539,
    "estimated_duration": 3600.0442866865897,
    "input_throughput": 7652.639191657371,
    "output_throughput": 6645.387138284039,
    "total_throughput": 14298.02632994141,
    "itl": 86.73984715571156,
    "ttft": 1002973.3528725173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 142385,
    "finished_requests": 110630,
    "scheduler_time": 66.97387698694634
}
#Debug simulation 
Total elapsed time: 7.761612211004831. Arrivals time: 0.2931705934461206 Scheduler time: 7.302967322990298 Scheduler overhead time: 0.06182589917443693 Adapter cache time: 0.010971026727929711 Engine time: 0.06357454461976886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 4320, 33, 33, 4320, 33, 34560, 4320, 33, 33, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 34560, 34560, 33, 34560, 34560, 34560, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560]
Prompts retrieved: 428010 . Total input tokens: 95474992 . Total output tokens: 84141861
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.559115677024238,
    "estimated_duration": 3600.021958322963,
    "input_throughput": 7463.290588515247,
    "output_throughput": 6483.6993413432965,
    "total_throughput": 13946.989929858542,
    "itl": 80.85560665767693,
    "ttft": 1042721.2490573047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 142385,
    "finished_requests": 107880,
    "scheduler_time": 60.938450694816005
}
#Debug simulation 
Total elapsed time: 7.559285140945576. Arrivals time: 0.2873816544888541 Scheduler time: 7.0983281643129885 Scheduler overhead time: 0.06474618031643331 Adapter cache time: 0.011383426026441157 Engine time: 0.06696783821098506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.624490821966901,
    "estimated_duration": 3600.0343042831423,
    "input_throughput": 7506.497081944081,
    "output_throughput": 6500.153060252487,
    "total_throughput": 14006.650142196568,
    "itl": 92.32165496943712,
    "ttft": 892116.8949380828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 132241,
    "finished_requests": 108773,
    "scheduler_time": 71.92070453375727
}
#Debug simulation 
Total elapsed time: 7.624610382015817. Arrivals time: 0.281144488370046 Scheduler time: 7.179992669611238 Scheduler overhead time: 0.05866651807446033 Adapter cache time: 0.016441638581454754 Engine time: 0.06069021893199533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.522178579005413,
    "estimated_duration": 3600.086004887233,
    "input_throughput": 7449.690913936952,
    "output_throughput": 6452.875005892442,
    "total_throughput": 13902.565919829394,
    "itl": 89.58939246242161,
    "ttft": 922155.9034924065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113982,
    "arrivals": 132241,
    "finished_requests": 107950,
    "scheduler_time": 69.86476973538701
}
#Debug simulation 
Total elapsed time: 7.522305877995677. Arrivals time: 0.28925768402405083 Scheduler time: 7.066510572913103 Scheduler overhead time: 0.05993919738102704 Adapter cache time: 0.016756497439928353 Engine time: 0.06163384974934161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.485321196960285,
    "estimated_duration": 3600.013132090467,
    "input_throughput": 7301.676976032608,
    "output_throughput": 6326.054146023515,
    "total_throughput": 13627.731122056124,
    "itl": 83.13143818479718,
    "ttft": 992979.8823953212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137895,
    "arrivals": 132241,
    "finished_requests": 105785,
    "scheduler_time": 64.23430043886984
}
#Debug simulation 
Total elapsed time: 7.485429629916325. Arrivals time: 0.2827810717280954 Scheduler time: 7.02398664271459 Scheduler overhead time: 0.06406784057617188 Adapter cache time: 0.018147127237170935 Engine time: 0.06623734813183546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.59329826105386,
    "estimated_duration": 3600.097673651044,
    "input_throughput": 7449.724543945701,
    "output_throughput": 6452.974365120463,
    "total_throughput": 13902.698909066163,
    "itl": 89.5879980298745,
    "ttft": 922097.2546113752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 132241,
    "finished_requests": 107952,
    "scheduler_time": 69.86576594950475
}
#Debug simulation 
Total elapsed time: 7.59342624200508. Arrivals time: 0.28079065796919167 Scheduler time: 7.14434188825544 Scheduler overhead time: 0.06052431161515415 Adapter cache time: 0.016955291386693716 Engine time: 0.06228129693772644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.435874670976773,
    "estimated_duration": 3600.0637023874615,
    "input_throughput": 7301.762738966903,
    "output_throughput": 6326.012504972312,
    "total_throughput": 13627.775243939215,
    "itl": 83.13267604836344,
    "ttft": 992896.7633454686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 132241,
    "finished_requests": 105786,
    "scheduler_time": 64.2335042609195
}
#Debug simulation 
Total elapsed time: 7.435998574015684. Arrivals time: 0.280434615095146 Scheduler time: 6.977024057297967 Scheduler overhead time: 0.06385968020185828 Adapter cache time: 0.018351010978221893 Engine time: 0.06605246372055262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.676866538007744,
    "estimated_duration": 3600.0856758565847,
    "input_throughput": 7449.6535401532465,
    "output_throughput": 6452.62754600216,
    "total_throughput": 13902.281086155406,
    "itl": 89.5889631959867,
    "ttft": 922266.2776084419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 132241,
    "finished_requests": 107946,
    "scheduler_time": 69.86243419484703
}
#Debug simulation 
Total elapsed time: 7.6769919759826735. Arrivals time: 0.28415810875594616 Scheduler time: 7.2246798179112375 Scheduler overhead time: 0.060370571562089026 Adapter cache time: 0.016874285298399627 Engine time: 0.06243966310285032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 540, 540, 1080, 540, 34560, 1080, 540, 540, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 34560, 34560, 540, 34560, 34560, 34560, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 397440 . Total input tokens: 88675653 . Total output tokens: 78110669
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.489781858050264,
    "estimated_duration": 3600.001371446779,
    "input_throughput": 7301.489718443178,
    "output_throughput": 6325.8828678856435,
    "total_throughput": 13627.372586328822,
    "itl": 83.13151682270785,
    "ttft": 992904.6529093881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 132241,
    "finished_requests": 105782,
    "scheduler_time": 64.2355860562694
}
#Debug simulation 
Total elapsed time: 7.489950736053288. Arrivals time: 0.2829175707884133 Scheduler time: 7.027207892038859 Scheduler overhead time: 0.0645247446373105 Adapter cache time: 0.01833740621805191 Engine time: 0.06658364983741194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.700170413008891,
    "estimated_duration": 3600.0354708916498,
    "input_throughput": 7623.964325329854,
    "output_throughput": 6606.025466218459,
    "total_throughput": 14229.989791548312,
    "itl": 90.45636395151841,
    "ttft": 815225.5779015555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 131339,
    "finished_requests": 110067,
    "scheduler_time": 74.7467801419098
}
#Debug simulation 
Total elapsed time: 7.700290257111192. Arrivals time: 0.27883695950731635 Scheduler time: 7.255913577624597 Scheduler overhead time: 0.05994577752426267 Adapter cache time: 0.016026751953177154 Engine time: 0.06148432020563632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.712441963958554,
    "estimated_duration": 3600.043494968643,
    "input_throughput": 7568.876053325945,
    "output_throughput": 6555.855792571632,
    "total_throughput": 14124.731845897577,
    "itl": 87.79957732746635,
    "ttft": 846080.824711982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 131339,
    "finished_requests": 109250,
    "scheduler_time": 72.66433182174016
}
#Debug simulation 
Total elapsed time: 7.712561173946597. Arrivals time: 0.27924223674926907 Scheduler time: 7.262440680991858 Scheduler overhead time: 0.06160039536189288 Adapter cache time: 0.016492830822244287 Engine time: 0.06391998520120978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.558860701974481,
    "estimated_duration": 3600.024566826609,
    "input_throughput": 7423.196009897428,
    "output_throughput": 6424.897544626679,
    "total_throughput": 13848.093554524106,
    "itl": 81.5072251324245,
    "ttft": 927153.1226016937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 131339,
    "finished_requests": 107078,
    "scheduler_time": 66.97236319868472
}
#Debug simulation 
Total elapsed time: 7.558979212073609. Arrivals time: 0.2796372427837923 Scheduler time: 7.0975247245514765 Scheduler overhead time: 0.06537766754627228 Adapter cache time: 0.01789197593461722 Engine time: 0.06776857282966375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.643066348973662,
    "estimated_duration": 3599.9930179414796,
    "input_throughput": 7569.006346457015,
    "output_throughput": 6555.842714799302,
    "total_throughput": 14124.849061256316,
    "itl": 87.80208933710189,
    "ttft": 846141.4807336635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 131339,
    "finished_requests": 109249,
    "scheduler_time": 72.66025538081348
}
#Debug simulation 
Total elapsed time: 7.643185458960943. Arrivals time: 0.2804047791287303 Scheduler time: 7.193252900848165 Scheduler overhead time: 0.06115224806126207 Adapter cache time: 0.01644879614468664 Engine time: 0.06315411510877311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.546649161027744,
    "estimated_duration": 3599.9732792039317,
    "input_throughput": 7423.30176570351,
    "output_throughput": 6424.98907800636,
    "total_throughput": 13848.290843709869,
    "itl": 81.50652118463226,
    "ttft": 927133.7975449854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 131339,
    "finished_requests": 107078,
    "scheduler_time": 66.97274287856693
}
#Debug simulation 
Total elapsed time: 7.546744601102546. Arrivals time: 0.280536757200025 Scheduler time: 7.084986316389404 Scheduler overhead time: 0.06536718201823533 Adapter cache time: 0.01781977352220565 Engine time: 0.06738034391310066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.736792235984467,
    "estimated_duration": 3599.9764154653117,
    "input_throughput": 7568.563750292864,
    "output_throughput": 6555.496280091505,
    "total_throughput": 14124.06003038437,
    "itl": 87.79993062189553,
    "ttft": 846312.217113104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 131339,
    "finished_requests": 109242,
    "scheduler_time": 72.65923260888218
}
#Debug simulation 
Total elapsed time: 7.736883211997338. Arrivals time: 0.2849876646650955 Scheduler time: 7.280558201833628 Scheduler overhead time: 0.06190703983884305 Adapter cache time: 0.016621440998278558 Engine time: 0.06385963794309646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 270, 270, 1080, 270, 34560, 1080, 270, 270, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 34560, 34560, 270, 34560, 34560, 34560, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 394740 . Total input tokens: 88092605 . Total output tokens: 77573433
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.673230046988465,
    "estimated_duration": 3599.968231352509,
    "input_throughput": 7423.312174607691,
    "output_throughput": 6424.998087083155,
    "total_throughput": 13848.310261690847,
    "itl": 81.50663071816814,
    "ttft": 927200.6916688874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 131339,
    "finished_requests": 107078,
    "scheduler_time": 66.96854447783593
}
#Debug simulation 
Total elapsed time: 7.673405140987597. Arrivals time: 0.2911457932787016 Scheduler time: 7.198947860626504 Scheduler overhead time: 0.06590064999181777 Adapter cache time: 0.01806263905018568 Engine time: 0.06810857495293021 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.773510615923442,
    "estimated_duration": 3600.094040393722,
    "input_throughput": 7730.421118931761,
    "output_throughput": 6681.899619869149,
    "total_throughput": 14412.32073880091,
    "itl": 89.4938089385097,
    "ttft": 744288.4083322003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 130909,
    "finished_requests": 111599,
    "scheduler_time": 77.1404643433975
}
#Debug simulation 
Total elapsed time: 7.7736304969294. Arrivals time: 0.27478548057843 Scheduler time: 7.333221242995933 Scheduler overhead time: 0.060444752452895045 Adapter cache time: 0.014933529077097774 Engine time: 0.06188929115887731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.745239051990211,
    "estimated_duration": 3600.0502064412235,
    "input_throughput": 7669.922755687216,
    "output_throughput": 6630.13697900485,
    "total_throughput": 14300.059734692066,
    "itl": 86.88581718419118,
    "ttft": 777528.7821818838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113982,
    "arrivals": 130909,
    "finished_requests": 110719,
    "scheduler_time": 75.04072094848586
}
#Debug simulation 
Total elapsed time: 7.745334685081616. Arrivals time: 0.27883733611088246 Scheduler time: 7.2955692518735304 Scheduler overhead time: 0.06208071147557348 Adapter cache time: 0.015569683397188783 Engine time: 0.06412613554857671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.621233489946462,
    "estimated_duration": 3600.0610962711967,
    "input_throughput": 7514.304417783177,
    "output_throughput": 6495.31226684451,
    "total_throughput": 14009.616684627686,
    "itl": 80.71598030132213,
    "ttft": 862242.0518634085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137895,
    "arrivals": 130909,
    "finished_requests": 108474,
    "scheduler_time": 69.35508191930434
}
#Debug simulation 
Total elapsed time: 7.621331415022723. Arrivals time: 0.27695406461134553 Scheduler time: 7.162200310616754 Scheduler overhead time: 0.06605786294676363 Adapter cache time: 0.016743517597205937 Engine time: 0.0684961051447317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.752289862954058,
    "estimated_duration": 3600.086202393223,
    "input_throughput": 7670.005785318131,
    "output_throughput": 6630.517342649099,
    "total_throughput": 14300.52312796723,
    "itl": 86.88560648832522,
    "ttft": 777438.7951326555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 130909,
    "finished_requests": 110723,
    "scheduler_time": 75.03906755516606
}
#Debug simulation 
Total elapsed time: 7.7524017370305955. Arrivals time: 0.2805401989025995 Scheduler time: 7.30149484006688 Scheduler overhead time: 0.06177001283504069 Adapter cache time: 0.015558652696199715 Engine time: 0.06412379234097898 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.640432839049026,
    "estimated_duration": 3600.007486084763,
    "input_throughput": 7514.254929902962,
    "output_throughput": 6495.408437450518,
    "total_throughput": 14009.66336735348,
    "itl": 80.7177649599678,
    "ttft": 862222.2631895025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 130909,
    "finished_requests": 108473,
    "scheduler_time": 69.35670058390726
}
#Debug simulation 
Total elapsed time: 7.640549427014776. Arrivals time: 0.2793984004529193 Scheduler time: 7.178866790374741 Scheduler overhead time: 0.06594434659928083 Adapter cache time: 0.016715386183932424 Engine time: 0.0686541594332084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.6888754919636995,
    "estimated_duration": 3600.0835618261317,
    "input_throughput": 7669.947523660719,
    "output_throughput": 6630.151936776842,
    "total_throughput": 14300.099460437561,
    "itl": 86.88621276740228,
    "ttft": 777474.5380896277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 130909,
    "finished_requests": 110721,
    "scheduler_time": 75.04184156530816
}
#Debug simulation 
Total elapsed time: 7.688995333039202. Arrivals time: 0.2772127074422315 Scheduler time: 7.242031946661882 Scheduler overhead time: 0.06156528159044683 Adapter cache time: 0.015442100702784956 Engine time: 0.06376260099932551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 135, 135, 1080, 135, 34560, 1080, 135, 135, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 34560, 34560, 135, 34560, 34560, 34560, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 393390 . Total input tokens: 87805640 . Total output tokens: 77299951
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.670632777037099,
    "estimated_duration": 3600.0143910542656,
    "input_throughput": 7514.240239489152,
    "output_throughput": 6495.339034784299,
    "total_throughput": 14009.579274273452,
    "itl": 80.7156230297605,
    "ttft": 862245.4228468605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 130909,
    "finished_requests": 108472,
    "scheduler_time": 69.35625099826133
}
#Debug simulation 
Total elapsed time: 7.670811934047379. Arrivals time: 0.2808710465906188 Scheduler time: 7.20753702451475 Scheduler overhead time: 0.06608523230534047 Adapter cache time: 0.0167030468583107 Engine time: 0.06854372285306454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.816008041030727,
    "estimated_duration": 3600.0384513435724,
    "input_throughput": 7807.302444092041,
    "output_throughput": 6744.356852893743,
    "total_throughput": 14551.659296985785,
    "itl": 88.67102415352383,
    "ttft": 694592.2624108071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 130707,
    "finished_requests": 112710,
    "scheduler_time": 78.95191849589523
}
#Debug simulation 
Total elapsed time: 7.816104506957345. Arrivals time: 0.27708607411477715 Scheduler time: 7.373139533796348 Scheduler overhead time: 0.060754657723009586 Adapter cache time: 0.014204176957719028 Engine time: 0.06251058646012098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.0019370529335,
    "estimated_duration": 3600.070267935324,
    "input_throughput": 7743.014698428878,
    "output_throughput": 6689.774145384173,
    "total_throughput": 14432.78884381305,
    "itl": 86.1054051873098,
    "ttft": 730626.1051637548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 130707,
    "finished_requests": 111758,
    "scheduler_time": 76.79416847694986
}
#Debug simulation 
Total elapsed time: 8.002064814907499. Arrivals time: 0.27801375568378717 Scheduler time: 7.550990030635148 Scheduler overhead time: 0.0629557374631986 Adapter cache time: 0.014867726131342351 Engine time: 0.06528103805612773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.656855751061812,
    "estimated_duration": 3600.0006222382244,
    "input_throughput": 7585.423133349938,
    "output_throughput": 6549.171923570803,
    "total_throughput": 14134.595056920742,
    "itl": 80.04237780322501,
    "ttft": 818267.17579556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 130707,
    "finished_requests": 109496,
    "scheduler_time": 70.99379139892989
}
#Debug simulation 
Total elapsed time: 7.6569825320038944. Arrivals time: 0.2812160273315385 Scheduler time: 7.193584167864174 Scheduler overhead time: 0.06652889505494386 Adapter cache time: 0.015829976997338235 Engine time: 0.0685210534138605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.801134865032509,
    "estimated_duration": 3600.022605977946,
    "input_throughput": 7743.104433208875,
    "output_throughput": 6689.79576961788,
    "total_throughput": 14432.900202826755,
    "itl": 86.10531652253589,
    "ttft": 730495.1650578186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 130707,
    "finished_requests": 111757,
    "scheduler_time": 76.79509481470642
}
#Debug simulation 
Total elapsed time: 7.801257692975923. Arrivals time: 0.2753414341714233 Scheduler time: 7.354419019422494 Scheduler overhead time: 0.06269887927919626 Adapter cache time: 0.014680104679428041 Engine time: 0.06482541013974696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.714922358980402,
    "estimated_duration": 3600.079688848332,
    "input_throughput": 7585.256261017849,
    "output_throughput": 6549.014199055769,
    "total_throughput": 14134.270460073618,
    "itl": 80.04228516542274,
    "ttft": 818424.832258216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 130707,
    "finished_requests": 109495,
    "scheduler_time": 70.99140212215158
}
#Debug simulation 
Total elapsed time: 7.7150471459608525. Arrivals time: 0.28002702014055103 Scheduler time: 7.25172973156441 Scheduler overhead time: 0.06678776815533638 Adapter cache time: 0.016042676055803895 Engine time: 0.06909753498621285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.981417754897848,
    "estimated_duration": 3600.038288034026,
    "input_throughput": 7743.180146904185,
    "output_throughput": 6689.916348959778,
    "total_throughput": 14433.096495863962,
    "itl": 86.10452121283413,
    "ttft": 730496.3659174741,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 130707,
    "finished_requests": 111759,
    "scheduler_time": 76.79419044273013
}
#Debug simulation 
Total elapsed time: 7.981541259912774. Arrivals time: 0.2784090347122401 Scheduler time: 7.529966221540235 Scheduler overhead time: 0.06330233218614012 Adapter cache time: 0.014886238612234592 Engine time: 0.06515595584642142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 66, 66, 1080, 66, 34560, 1080, 66, 66, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 34560, 34560, 66, 34560, 34560, 34560, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392700 . Total input tokens: 87660371 . Total output tokens: 77164780
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.641868458944373,
    "estimated_duration": 3600.0820269456067,
    "input_throughput": 7585.32883295678,
    "output_throughput": 6549.1963303969,
    "total_throughput": 14134.525163353681,
    "itl": 80.04027866084571,
    "ttft": 818284.1623028831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 130707,
    "finished_requests": 109499,
    "scheduler_time": 70.99431824522006
}
#Debug simulation 
Total elapsed time: 7.642043418018147. Arrivals time: 0.27935448905918747 Scheduler time: 7.18111723929178 Scheduler overhead time: 0.06621358741540462 Adapter cache time: 0.015718156122602522 Engine time: 0.06844742048997432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.920578695018776,
    "estimated_duration": 3600.072429489754,
    "input_throughput": 7815.973303622691,
    "output_throughput": 6769.000479096987,
    "total_throughput": 14584.973782719679,
    "itl": 88.23944161940855,
    "ttft": 686608.8314182386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 130593,
    "finished_requests": 112889,
    "scheduler_time": 79.27697340686514
}
#Debug simulation 
Total elapsed time: 7.920695065986365. Arrivals time: 0.28052326710894704 Scheduler time: 7.472476947819814 Scheduler overhead time: 0.06137913907878101 Adapter cache time: 0.013955069705843925 Engine time: 0.06343635800294578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.845142567064613,
    "estimated_duration": 3600.0132580331833,
    "input_throughput": 7751.817562818201,
    "output_throughput": 6713.826663295365,
    "total_throughput": 14465.644226113565,
    "itl": 85.72392475055508,
    "ttft": 721416.9569183907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 130593,
    "finished_requests": 111993,
    "scheduler_time": 77.10883341708333
}
#Debug simulation 
Total elapsed time: 7.8452662620693445. Arrivals time: 0.28027491725515574 Scheduler time: 7.393514123046771 Scheduler overhead time: 0.06275197304785252 Adapter cache time: 0.014224189100787044 Engine time: 0.0649399800458923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.792526236036792,
    "estimated_duration": 3600.057664385326,
    "input_throughput": 7588.417060720722,
    "output_throughput": 6566.428708590203,
    "total_throughput": 14154.845769310925,
    "itl": 79.72237497920514,
    "ttft": 813284.0015603037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 130593,
    "finished_requests": 109573,
    "scheduler_time": 71.19926417709488
}
#Debug simulation 
Total elapsed time: 7.79264807398431. Arrivals time: 0.28579075424931943 Scheduler time: 7.323356833308935 Scheduler overhead time: 0.06702714005950838 Adapter cache time: 0.015505360788665712 Engine time: 0.06942512071691453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.955369013012387,
    "estimated_duration": 3600.051286870393,
    "input_throughput": 7751.735677149168,
    "output_throughput": 6713.755742355386,
    "total_throughput": 14465.491419504555,
    "itl": 85.72334328086194,
    "ttft": 721392.89849955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 130593,
    "finished_requests": 111993,
    "scheduler_time": 77.109518909987
}
#Debug simulation 
Total elapsed time: 7.955490191001445. Arrivals time: 0.2853103724773973 Scheduler time: 7.4976548455888405 Scheduler overhead time: 0.0632672879146412 Adapter cache time: 0.014435580000281334 Engine time: 0.06508325145114213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.692879259004258,
    "estimated_duration": 3600.0216025037016,
    "input_throughput": 7588.436964100665,
    "output_throughput": 6566.2422646464365,
    "total_throughput": 14154.679228747102,
    "itl": 79.72065545356077,
    "ttft": 813334.8556822641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 130593,
    "finished_requests": 109571,
    "scheduler_time": 71.19779502201276
}
#Debug simulation 
Total elapsed time: 7.692975739017129. Arrivals time: 0.2870824502315372 Scheduler time: 7.22342336573638 Scheduler overhead time: 0.0667486273450777 Adapter cache time: 0.015430880011990666 Engine time: 0.06882716901600361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.875914684031159,
    "estimated_duration": 3600.075754316507,
    "input_throughput": 7751.679382451834,
    "output_throughput": 6713.729557223939,
    "total_throughput": 14465.408939675774,
    "itl": 85.72568957466207,
    "ttft": 721378.8786795232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 130593,
    "finished_requests": 111993,
    "scheduler_time": 77.11308738830147
}
#Debug simulation 
Total elapsed time: 7.87604491901584. Arrivals time: 0.2769563269102946 Scheduler time: 7.427230263245292 Scheduler overhead time: 0.06300362572073936 Adapter cache time: 0.01431431935634464 Engine time: 0.06486875587143004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 1080, 33, 33, 1080, 33, 34560, 1080, 33, 33, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 34560, 34560, 33, 34560, 34560, 34560, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560]
Prompts retrieved: 392370 . Total input tokens: 87591910 . Total output tokens: 77099552
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.736172812990844,
    "estimated_duration": 3600.0314275978003,
    "input_throughput": 7588.510697593859,
    "output_throughput": 6566.506841795562,
    "total_throughput": 14155.01753938942,
    "itl": 79.72117182810273,
    "ttft": 813282.3030854566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 130593,
    "finished_requests": 109573,
    "scheduler_time": 71.1992943698449
}
#Debug simulation 
Total elapsed time: 7.736347254016437. Arrivals time: 0.2801733970409259 Scheduler time: 7.272348666912876 Scheduler overhead time: 0.06708257470745593 Adapter cache time: 0.015558755840174854 Engine time: 0.06961524090729654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.928981528966688,
    "estimated_duration": 3600.0938584649543,
    "input_throughput": 7821.619409669098,
    "output_throughput": 6755.28665532339,
    "total_throughput": 14576.906064992489,
    "itl": 88.60906074795945,
    "ttft": 646481.1167562138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 129421,
    "finished_requests": 112995,
    "scheduler_time": 79.93962866164046
}
#Debug simulation 
Total elapsed time: 7.929113646037877. Arrivals time: 0.2760042365407571 Scheduler time: 7.482654936029576 Scheduler overhead time: 0.061368507449515164 Adapter cache time: 0.016998867504298687 Engine time: 0.06341833586338907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.788195380941033,
    "estimated_duration": 3600.054560462385,
    "input_throughput": 7763.202898905624,
    "output_throughput": 6703.276740589316,
    "total_throughput": 14466.47963949494,
    "itl": 85.9727951940216,
    "ttft": 679271.3148747588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 129421,
    "finished_requests": 112145,
    "scheduler_time": 77.9402637550447
}
#Debug simulation 
Total elapsed time: 7.788321806001477. Arrivals time: 0.27331016527023166 Scheduler time: 7.3414868883555755 Scheduler overhead time: 0.062484888010658324 Adapter cache time: 0.017166449455544353 Engine time: 0.06447705044411123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.669932949007489,
    "estimated_duration": 3600.0272800257667,
    "input_throughput": 7616.470061805684,
    "output_throughput": 6572.365751581375,
    "total_throughput": 14188.835813387059,
    "itl": 79.74889060074993,
    "ttft": 763843.6189853139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 129421,
    "finished_requests": 109952,
    "scheduler_time": 72.4618799995889
}
#Debug simulation 
Total elapsed time: 7.670064985053614. Arrivals time: 0.2765989237232134 Scheduler time: 7.208685837220401 Scheduler overhead time: 0.06630169635172933 Adapter cache time: 0.018292610300704837 Engine time: 0.0688080774853006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.873191094957292,
    "estimated_duration": 3600.095469891269,
    "input_throughput": 7763.109960204497,
    "output_throughput": 6703.323065132175,
    "total_throughput": 14466.433025336672,
    "itl": 85.97356690346231,
    "ttft": 679396.1302730107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 129421,
    "finished_requests": 112143,
    "scheduler_time": 77.93945564132494
}
#Debug simulation 
Total elapsed time: 7.873311814037152. Arrivals time: 0.27589200669899583 Scheduler time: 7.4229507526615635 Scheduler overhead time: 0.06271528243087232 Adapter cache time: 0.017368800938129425 Engine time: 0.06491321802604944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.7239526809426025,
    "estimated_duration": 3600.0829408096065,
    "input_throughput": 7616.658407829211,
    "output_throughput": 6572.441076782223,
    "total_throughput": 14189.099484611434,
    "itl": 79.74837097900104,
    "ttft": 763801.2178451656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 129421,
    "finished_requests": 109954,
    "scheduler_time": 72.46309880629869
}
#Debug simulation 
Total elapsed time: 7.724048862000927. Arrivals time: 0.27673778194002807 Scheduler time: 7.26209909748286 Scheduler overhead time: 0.06682596949394792 Adapter cache time: 0.018204390769824386 Engine time: 0.06893881352152675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.892291055060923,
    "estimated_duration": 3600.021878996263,
    "input_throughput": 7763.157819416411,
    "output_throughput": 6703.183983628521,
    "total_throughput": 14466.341803044932,
    "itl": 85.97238191267249,
    "ttft": 679349.8722346084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 129421,
    "finished_requests": 112141,
    "scheduler_time": 77.94014292586053
}
#Debug simulation 
Total elapsed time: 7.892415166017599. Arrivals time: 0.27389223931822926 Scheduler time: 7.444305365439504 Scheduler overhead time: 0.06264879018999636 Adapter cache time: 0.017185956705361605 Engine time: 0.06488285679370165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 270, 270, 540, 270, 34560, 540, 270, 270, 270, 34560, 34560, 270, 270, 540, 540, 540, 34560, 34560, 270, 34560, 34560, 34560, 34560, 540, 270, 540, 540, 540, 540, 34560]
Prompts retrieved: 388800 . Total input tokens: 86786767 . Total output tokens: 76392485
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.716614395962097,
    "estimated_duration": 3600.069326454621,
    "input_throughput": 7616.687489461222,
    "output_throughput": 6572.6195398860345,
    "total_throughput": 14189.307029347257,
    "itl": 79.74774157097863,
    "ttft": 763812.9620308971,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 129421,
    "finished_requests": 109955,
    "scheduler_time": 72.46411770012433
}
#Debug simulation 
Total elapsed time: 7.716784455929883. Arrivals time: 0.2775515120010823 Scheduler time: 7.253206624882296 Scheduler overhead time: 0.066811264725402 Adapter cache time: 0.01828169182408601 Engine time: 0.06938016065396369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.985159487929195,
    "estimated_duration": 3600.0516675586664,
    "input_throughput": 7855.263371588586,
    "output_throughput": 6845.4867528921095,
    "total_throughput": 14700.750124480695,
    "itl": 88.17438478549583,
    "ttft": 566815.2712517247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 128920,
    "finished_requests": 114636,
    "scheduler_time": 82.76367669241611
}
#Debug simulation 
Total elapsed time: 7.98527825193014. Arrivals time: 0.27311441604979336 Scheduler time: 7.542538270005025 Scheduler overhead time: 0.061439829296432436 Adapter cache time: 0.015999317867681384 Engine time: 0.0632329786894843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.900595283950679,
    "estimated_duration": 3600.0699863080467,
    "input_throughput": 7799.280599207067,
    "output_throughput": 6793.697648384335,
    "total_throughput": 14592.978247591402,
    "itl": 85.57213464793669,
    "ttft": 600447.1839538123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 128920,
    "finished_requests": 113793,
    "scheduler_time": 80.75721509194221
}
#Debug simulation 
Total elapsed time: 7.900705142994411. Arrivals time: 0.2748584649525583 Scheduler time: 7.4520234687952325 Scheduler overhead time: 0.06294620258267969 Adapter cache time: 0.016413498553447425 Engine time: 0.06505763507448137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.828281764057465,
    "estimated_duration": 3600.0101539715756,
    "input_throughput": 7639.6306742798015,
    "output_throughput": 6659.682327159708,
    "total_throughput": 14299.313001439508,
    "itl": 79.47400259689971,
    "ttft": 691214.9391105367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 128920,
    "finished_requests": 111457,
    "scheduler_time": 75.25573021293162
}
#Debug simulation 
Total elapsed time: 7.828393512056209. Arrivals time: 0.27428634057287127 Scheduler time: 7.367603971622884 Scheduler overhead time: 0.0674403024604544 Adapter cache time: 0.01754421112127602 Engine time: 0.06971512921154499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.898675400996581,
    "estimated_duration": 3600.0520707243936,
    "input_throughput": 7799.383577902021,
    "output_throughput": 6793.814511432498,
    "total_throughput": 14593.198089334519,
    "itl": 85.57055406017024,
    "ttft": 600430.7328087011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 128920,
    "finished_requests": 113794,
    "scheduler_time": 80.75600717782181
}
#Debug simulation 
Total elapsed time: 7.898803278920241. Arrivals time: 0.27161137119401246 Scheduler time: 7.453907700488344 Scheduler overhead time: 0.0629440313205123 Adapter cache time: 0.016184068517759442 Engine time: 0.06456680060364306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.919442347018048,
    "estimated_duration": 3600.0450911231505,
    "input_throughput": 7639.556534393192,
    "output_throughput": 6659.6176973217425,
    "total_throughput": 14299.174231714935,
    "itl": 79.47256499134377,
    "ttft": 691259.981595857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 128920,
    "finished_requests": 111457,
    "scheduler_time": 75.2568854969529
}
#Debug simulation 
Total elapsed time: 7.919562140945345. Arrivals time: 0.27846920443698764 Scheduler time: 7.453893008991145 Scheduler overhead time: 0.06762599421199411 Adapter cache time: 0.017575812409631908 Engine time: 0.07013412949163467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.914632825064473,
    "estimated_duration": 3600.024005577667,
    "input_throughput": 7799.356603316473,
    "output_throughput": 6793.794697509371,
    "total_throughput": 14593.151300825844,
    "itl": 85.57137281534514,
    "ttft": 600439.6393610758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 128920,
    "finished_requests": 113792,
    "scheduler_time": 80.75484440380323
}
#Debug simulation 
Total elapsed time: 7.914767712005414. Arrivals time: 0.2691249188501388 Scheduler time: 7.471957447938621 Scheduler overhead time: 0.06286490557249635 Adapter cache time: 0.016449103131890297 Engine time: 0.06467242224607617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 135, 135, 540, 135, 34560, 540, 135, 135, 135, 34560, 34560, 135, 135, 540, 540, 540, 34560, 34560, 135, 34560, 34560, 34560, 34560, 540, 135, 540, 540, 540, 540, 34560]
Prompts retrieved: 387450 . Total input tokens: 86468470 . Total output tokens: 76142591
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.832598460954614,
    "estimated_duration": 3600.0689206202637,
    "input_throughput": 7639.838738215402,
    "output_throughput": 6660.068884422635,
    "total_throughput": 14299.907622638037,
    "itl": 79.47304719435057,
    "ttft": 691052.774014517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 128920,
    "finished_requests": 111464,
    "scheduler_time": 75.2601877939902
}
#Debug simulation 
Total elapsed time: 7.832785272970796. Arrivals time: 0.27381318830884993 Scheduler time: 7.372897926834412 Scheduler overhead time: 0.06729569018352777 Adapter cache time: 0.017416646936908364 Engine time: 0.06984994537197053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.020349644939415,
    "estimated_duration": 3600.038340478162,
    "input_throughput": 7967.913751771334,
    "output_throughput": 6877.5664752257635,
    "total_throughput": 14845.480226997099,
    "itl": 87.0769715243886,
    "ttft": 537565.5195478029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 128685,
    "finished_requests": 115232,
    "scheduler_time": 83.50145748392792
}
#Debug simulation 
Total elapsed time: 8.020487150992267. Arrivals time: 0.2737959760706872 Scheduler time: 7.575781795196235 Scheduler overhead time: 0.062177475076168776 Adapter cache time: 0.015394858201034367 Engine time: 0.06412649410776794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 8.02641065302305,
    "estimated_duration": 3600.0856973336713,
    "input_throughput": 7901.9618952596675,
    "output_throughput": 6821.839274045411,
    "total_throughput": 14723.80116930508,
    "itl": 84.55348325308857,
    "ttft": 574918.663361291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113982,
    "arrivals": 128685,
    "finished_requests": 114263,
    "scheduler_time": 81.43730913629614
}
#Debug simulation 
Total elapsed time: 8.026539512095042. Arrivals time: 0.28309618891216815 Scheduler time: 7.5681526098633185 Scheduler overhead time: 0.06366645311936736 Adapter cache time: 0.01593936246354133 Engine time: 0.06577784998808056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.883050782023929,
    "estimated_duration": 3600.068032311756,
    "input_throughput": 7745.6486237827985,
    "output_throughput": 6677.04354035896,
    "total_throughput": 14422.69216414176,
    "itl": 78.52750212631561,
    "ttft": 668514.8957518863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137895,
    "arrivals": 128685,
    "finished_requests": 111859,
    "scheduler_time": 75.78379244203903
}
#Debug simulation 
Total elapsed time: 7.88317801093217. Arrivals time: 0.2876316806068644 Scheduler time: 7.408539498806931 Scheduler overhead time: 0.06770599342416972 Adapter cache time: 0.01687476970255375 Engine time: 0.0705671893665567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 7.98307269194629,
    "estimated_duration": 3600.037145532656,
    "input_throughput": 7901.644024784401,
    "output_throughput": 6821.717112134512,
    "total_throughput": 14723.361136918913,
    "itl": 84.55436242814541,
    "ttft": 575010.3088622058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 128685,
    "finished_requests": 114259,
    "scheduler_time": 81.4337616222559
}
#Debug simulation 
Total elapsed time: 7.983169381972402. Arrivals time: 0.28550420131068677 Scheduler time: 7.522208032431081 Scheduler overhead time: 0.06384958210401237 Adapter cache time: 0.015849147108383477 Engine time: 0.06585345871280879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.895833897986449,
    "estimated_duration": 3600.072596704547,
    "input_throughput": 7745.741023535394,
    "output_throughput": 6677.116462041384,
    "total_throughput": 14422.857485576777,
    "itl": 78.52525285503378,
    "ttft": 668511.952118556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 128685,
    "finished_requests": 111862,
    "scheduler_time": 75.7865046323997
}
#Debug simulation 
Total elapsed time: 7.8959420400206. Arrivals time: 0.2986977385589853 Scheduler time: 7.410385953146033 Scheduler overhead time: 0.06787782080937177 Adapter cache time: 0.01675139437429607 Engine time: 0.07014632830396295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.065782840945758,
    "estimated_duration": 3600.0785491410543,
    "input_throughput": 7901.627870532733,
    "output_throughput": 6821.4211620074875,
    "total_throughput": 14723.04903254022,
    "itl": 84.55358645647789,
    "ttft": 575089.6335293758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 128685,
    "finished_requests": 114257,
    "scheduler_time": 81.43295356467029
}
#Debug simulation 
Total elapsed time: 8.065909725031815. Arrivals time: 0.28193089307751507 Scheduler time: 7.607421851716936 Scheduler overhead time: 0.06421119498554617 Adapter cache time: 0.015954773989506066 Engine time: 0.06618751410860568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 66, 66, 540, 66, 34560, 540, 66, 66, 66, 34560, 34560, 66, 66, 540, 540, 540, 34560, 34560, 66, 34560, 34560, 34560, 34560, 540, 66, 540, 540, 540, 540, 34560]
Prompts retrieved: 386760 . Total input tokens: 86320137 . Total output tokens: 76017547
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.891855209018104,
    "estimated_duration": 3600.0663647083443,
    "input_throughput": 7745.46832618154,
    "output_throughput": 6676.933857564419,
    "total_throughput": 14422.402183745959,
    "itl": 78.52738761296024,
    "ttft": 668604.5124297547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 128685,
    "finished_requests": 111857,
    "scheduler_time": 75.78456797410682
}
#Debug simulation 
Total elapsed time: 7.89202989207115. Arrivals time: 0.2854551380733028 Scheduler time: 7.419658885453828 Scheduler overhead time: 0.06778375874273479 Adapter cache time: 0.01695045572705567 Engine time: 0.07013347663450986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 8.135383169981651,
    "estimated_duration": 3600.0663075518364,
    "input_throughput": 7919.34163551222,
    "output_throughput": 6922.801101668639,
    "total_throughput": 14842.142737180859,
    "itl": 86.80636159024534,
    "ttft": 530982.9602467299,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 128584,
    "finished_requests": 115220,
    "scheduler_time": 84.35557795725778
}
#Debug simulation 
Total elapsed time: 8.135501660988666. Arrivals time: 0.2815987035864964 Scheduler time: 7.682503722375259 Scheduler overhead time: 0.06269198562949896 Adapter cache time: 0.014839731738902628 Engine time: 0.06446372251957655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.989772794069722,
    "estimated_duration": 3600.0134631524847,
    "input_throughput": 7851.411748679558,
    "output_throughput": 6863.969608147365,
    "total_throughput": 14715.381356826923,
    "itl": 84.31953814274888,
    "ttft": 568924.8185956357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 128584,
    "finished_requests": 114234,
    "scheduler_time": 82.29113374885331
}
#Debug simulation 
Total elapsed time: 7.989869532990269. Arrivals time: 0.28245853062253445 Scheduler time: 7.5334922798210755 Scheduler overhead time: 0.06351706897839904 Adapter cache time: 0.015245348564349115 Engine time: 0.06525083654560149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.844884879072197,
    "estimated_duration": 3600.066653594739,
    "input_throughput": 7682.822753401596,
    "output_throughput": 6720.642512504885,
    "total_throughput": 14403.465265906481,
    "itl": 78.4106867509337,
    "ttft": 664649.8172357767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 128584,
    "finished_requests": 111785,
    "scheduler_time": 76.61013091429355
}
#Debug simulation 
Total elapsed time: 7.845007729018107. Arrivals time: 0.27368307346478105 Scheduler time: 7.386202222085558 Scheduler overhead time: 0.0674667403800413 Adapter cache time: 0.016072182916104794 Engine time: 0.07003359997179359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 8.011620937031694,
    "estimated_duration": 3600.0884790778964,
    "input_throughput": 7851.459252812547,
    "output_throughput": 6864.161018156273,
    "total_throughput": 14715.62027096882,
    "itl": 84.31805631575455,
    "ttft": 568958.9596750338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 128584,
    "finished_requests": 114239,
    "scheduler_time": 82.29205350919133
}
#Debug simulation 
Total elapsed time: 8.011757559957914. Arrivals time: 0.27298770588822663 Scheduler time: 7.5642265568021685 Scheduler overhead time: 0.06369767151772976 Adapter cache time: 0.015261569642461836 Engine time: 0.06561382603831589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 7.892095112008974,
    "estimated_duration": 3600.061007985519,
    "input_throughput": 7683.085352900013,
    "output_throughput": 6720.726939441167,
    "total_throughput": 14403.81229234118,
    "itl": 78.41076775419822,
    "ttft": 664513.9439920415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 128584,
    "finished_requests": 111787,
    "scheduler_time": 76.61156559831267
}
#Debug simulation 
Total elapsed time: 7.892222253954969. Arrivals time: 0.2801552333403379 Scheduler time: 7.425721852458082 Scheduler overhead time: 0.06793765886686742 Adapter cache time: 0.016289627994410694 Engine time: 0.07015920360572636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 7.984014778980054,
    "estimated_duration": 3600.0187410622625,
    "input_throughput": 7851.546626021062,
    "output_throughput": 6864.114266446432,
    "total_throughput": 14715.660892467495,
    "itl": 84.31981536361889,
    "ttft": 568881.541702211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 128584,
    "finished_requests": 114237,
    "scheduler_time": 82.29010710491937
}
#Debug simulation 
Total elapsed time: 7.9841449349187315. Arrivals time: 0.2738587512867525 Scheduler time: 7.535609901533462 Scheduler overhead time: 0.06380301422905177 Adapter cache time: 0.015126240905374289 Engine time: 0.06589588464703411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_32_slots_32_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 540, 33, 33, 540, 33, 34560, 540, 33, 33, 33, 34560, 34560, 33, 33, 540, 540, 540, 34560, 34560, 33, 34560, 34560, 34560, 34560, 540, 33, 540, 540, 540, 540, 34560]
Prompts retrieved: 386430 . Total input tokens: 86251425 . Total output tokens: 75953486
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 7.830289709963836,
    "estimated_duration": 3600.0092779255947,
    "input_throughput": 7683.152698966924,
    "output_throughput": 6720.697401630434,
    "total_throughput": 14403.850100597358,
    "itl": 78.411247112613,
    "ttft": 664446.0581016123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 128584,
    "finished_requests": 111786,
    "scheduler_time": 76.61193878406239
}
#Debug simulation 
Total elapsed time: 7.830467878957279. Arrivals time: 0.2762422248488292 Scheduler time: 7.367489248514175 Scheduler overhead time: 0.06815547193400562 Adapter cache time: 0.01619356300216168 Engine time: 0.07082851359155029 
