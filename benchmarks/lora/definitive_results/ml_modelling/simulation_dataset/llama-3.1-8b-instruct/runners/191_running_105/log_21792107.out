INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:05 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.71860839612782,
    "estimated_duration": 3600.017590587874,
    "input_throughput": 7070.13406449596,
    "output_throughput": 6176.837040501459,
    "total_throughput": 13246.97110499742,
    "itl": 83.76705563220513,
    "ttft": 1999807.506816119,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0320447796909327,
    "arrivals": 1113643,
    "finished_requests": 103394,
    "scheduler_time": 305.2024236039489
}
#Debug simulation 
Total elapsed time: 86.71882215002552. Arrivals time: 0.8963640984147787 Scheduler time: 85.59870806988329 Scheduler overhead time: 0.08737104805186391 Adapter cache time: 0.01781554101034999 Engine time: 0.08400062331929803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 87.56270257104188,
    "estimated_duration": 3600.0345604865815,
    "input_throughput": 7192.005677995631,
    "output_throughput": 6283.370512127147,
    "total_throughput": 13475.376190122777,
    "itl": 86.2182017260061,
    "ttft": 1998133.3568789477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.830353100793433,
    "arrivals": 1113643,
    "finished_requests": 105161,
    "scheduler_time": 299.8961202522117
}
#Debug simulation 
Total elapsed time: 87.56290374929085. Arrivals time: 0.5371745452284813 Scheduler time: 86.80265987338498 Scheduler overhead time: 0.08857866609469056 Adapter cache time: 0.017827771604061127 Engine time: 0.08311990601941943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 86.69961754698306,
    "estimated_duration": 3600.0832912140386,
    "input_throughput": 7070.33391758454,
    "output_throughput": 6176.975142289253,
    "total_throughput": 13247.309059873794,
    "itl": 83.76718769643593,
    "ttft": 1999786.6660783235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.003669726494718,
    "arrivals": 1113643,
    "finished_requests": 103397,
    "scheduler_time": 305.2096596746676
}
#Debug simulation 
Total elapsed time: 86.69979002187029. Arrivals time: 0.5908195115625858 Scheduler time: 85.88198948884383 Scheduler overhead time: 0.09013684699311852 Adapter cache time: 0.018102381378412247 Engine time: 0.08492569485679269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.54463426582515,
    "estimated_duration": 3600.055560784857,
    "input_throughput": 7192.235942701707,
    "output_throughput": 6283.508300929762,
    "total_throughput": 13475.74424363147,
    "itl": 86.2143317469278,
    "ttft": 1998078.5796644343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6429432693961896,
    "arrivals": 1113643,
    "finished_requests": 105164,
    "scheduler_time": 299.91530906728934
}
#Debug simulation 
Total elapsed time: 87.54481054795906. Arrivals time: 0.5275519550777972 Scheduler time: 86.79428586829454 Scheduler overhead time: 0.08785830112174153 Adapter cache time: 0.017733987886458635 Engine time: 0.08353930851444602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.47035003220662,
    "estimated_duration": 3600.0554493169693,
    "input_throughput": 7070.388597717097,
    "output_throughput": 6177.022913416263,
    "total_throughput": 13247.41151113336,
    "itl": 83.76646884369734,
    "ttft": 1999776.3200975023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.976330259181577,
    "arrivals": 1113643,
    "finished_requests": 103397,
    "scheduler_time": 305.20946063857673
}
#Debug simulation 
Total elapsed time: 86.47052122512832. Arrivals time: 0.537514426279813 Scheduler time: 85.70650490000844 Scheduler overhead time: 0.08865465130656958 Adapter cache time: 0.01811118796467781 Engine time: 0.08467453857883811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745472359 . Total output tokens: 656212625
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 88.05583325913176,
    "estimated_duration": 3600.0467920300675,
    "input_throughput": 7230.687961508749,
    "output_throughput": 6284.528315045034,
    "total_throughput": 13515.216276553783,
    "itl": 86.0503318521658,
    "ttft": 1996571.0551394639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5722271134285415,
    "arrivals": 1112647,
    "finished_requests": 105106,
    "scheduler_time": 300.1951045677886
}
#Debug simulation 
Total elapsed time: 88.05600378708914. Arrivals time: 0.7368901907466352 Scheduler time: 87.09561076294631 Scheduler overhead time: 0.0887315385043621 Adapter cache time: 0.0175597183406353 Engine time: 0.08343889331445098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745472359 . Total output tokens: 656212625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.99901742395014,
    "estimated_duration": 3600.062749084858,
    "input_throughput": 7089.965864202874,
    "output_throughput": 6165.398090808893,
    "total_throughput": 13255.363955011768,
    "itl": 84.19600534094178,
    "ttft": 1996008.5848220687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9361601809365716,
    "arrivals": 1112647,
    "finished_requests": 103050,
    "scheduler_time": 305.85743998036753
}
#Debug simulation 
Total elapsed time: 84.99918193928897. Arrivals time: 0.6436848510056734 Scheduler time: 84.13015823392197 Scheduler overhead time: 0.08902120171114802 Adapter cache time: 0.01793935103341937 Engine time: 0.08403661334887147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745472359 . Total output tokens: 656212625
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 87.2210911270231,
    "estimated_duration": 3600.0212213026853,
    "input_throughput": 7065.873903597543,
    "output_throughput": 6164.564772195846,
    "total_throughput": 13230.43867579339,
    "itl": 82.94951520812984,
    "ttft": 2000279.6243210847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.692544767400266,
    "arrivals": 1112647,
    "finished_requests": 102760,
    "scheduler_time": 307.09076451895845
}
#Debug simulation 
Total elapsed time: 87.22125998605043. Arrivals time: 0.6682400056160986 Scheduler time: 86.32697608182207 Scheduler overhead time: 0.08876730129122734 Adapter cache time: 0.017560155130922794 Engine time: 0.08561179181560874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745472359 . Total output tokens: 656212625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 88.06998603139073,
    "estimated_duration": 3600.0079826147958,
    "input_throughput": 7200.854866208168,
    "output_throughput": 6272.453591505321,
    "total_throughput": 13473.308457713489,
    "itl": 86.00907515163001,
    "ttft": 2002661.745215093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.556118353470226,
    "arrivals": 1112647,
    "finished_requests": 104773,
    "scheduler_time": 300.72731706006965
}
#Debug simulation 
Total elapsed time: 88.0701597770676. Arrivals time: 0.5483570545911789 Scheduler time: 87.2939961887896 Scheduler overhead time: 0.09129475057125092 Adapter cache time: 0.01835144590586424 Engine time: 0.08434813190251589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745472359 . Total output tokens: 656212625
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 85.79821490496397,
    "estimated_duration": 3600.0301831616393,
    "input_throughput": 7184.54865211298,
    "output_throughput": 6248.734831507372,
    "total_throughput": 13433.283483620351,
    "itl": 84.4580074229396,
    "ttft": 1999759.3805151142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8438607759447954,
    "arrivals": 1112647,
    "finished_requests": 104508,
    "scheduler_time": 301.55228727409644
}
#Debug simulation 
Total elapsed time: 85.79838481592014. Arrivals time: 0.663720223121345 Scheduler time: 84.90796397812665 Scheduler overhead time: 0.08958774851635098 Adapter cache time: 0.018081677611917257 Engine time: 0.08457616483792663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745472359 . Total output tokens: 656212625
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.43201549258083,
    "estimated_duration": 3600.0477671154713,
    "input_throughput": 7172.105669222313,
    "output_throughput": 6241.260520274456,
    "total_throughput": 13413.366189496768,
    "itl": 85.34502488191364,
    "ttft": 1997057.8149922767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4386577992979337,
    "arrivals": 1112647,
    "finished_requests": 104307,
    "scheduler_time": 302.7331799069007
}
#Debug simulation 
Total elapsed time: 86.4321865499951. Arrivals time: 0.6872270004823804 Scheduler time: 85.52072977321222 Scheduler overhead time: 0.08827083092182875 Adapter cache time: 0.018012722954154015 Engine time: 0.08355160942301154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745472359 . Total output tokens: 656212625
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.84794597700238,
    "estimated_duration": 3600.003031237342,
    "input_throughput": 7184.602839378774,
    "output_throughput": 6248.781960682994,
    "total_throughput": 13433.384800061769,
    "itl": 84.4574423017845,
    "ttft": 1999749.7510396815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.816728425808269,
    "arrivals": 1112647,
    "finished_requests": 104508,
    "scheduler_time": 301.5522676999354
}
#Debug simulation 
Total elapsed time: 85.84810899570584. Arrivals time: 0.6272945767268538 Scheduler time: 84.99314026115462 Scheduler overhead time: 0.0898209335282445 Adapter cache time: 0.017884651198983192 Engine time: 0.08524680323898792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693479501 . Total output tokens: 610448159
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.22404532087967,
    "estimated_duration": 3600.046046617229,
    "input_throughput": 7151.639080892412,
    "output_throughput": 6228.880605866382,
    "total_throughput": 13380.519686758795,
    "itl": 88.54780407662514,
    "ttft": 2004934.480295921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5259402502048904,
    "arrivals": 1035175,
    "finished_requests": 104129,
    "scheduler_time": 302.64501863568915
}
#Debug simulation 
Total elapsed time: 86.22421912290156. Arrivals time: 0.5372213260270655 Scheduler time: 85.4614113746211 Scheduler overhead time: 0.0889222682453692 Adapter cache time: 0.01787752751260996 Engine time: 0.08463714784011245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693479501 . Total output tokens: 610448159
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 88.37027870072052,
    "estimated_duration": 3600.008370649146,
    "input_throughput": 6990.817078423053,
    "output_throughput": 6108.97801774677,
    "total_throughput": 13099.795096169824,
    "itl": 86.61211041816338,
    "ttft": 1998412.8295693716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8626034247176757,
    "arrivals": 1035175,
    "finished_requests": 101787,
    "scheduler_time": 310.2606094636549
}
#Debug simulation 
Total elapsed time: 88.37052941974252. Arrivals time: 0.545382599812001 Scheduler time: 87.59324845625088 Scheduler overhead time: 0.09134624199941754 Adapter cache time: 0.018590778578072786 Engine time: 0.08673571003600955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693479501 . Total output tokens: 610448159
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.35679273493588,
    "estimated_duration": 3600.0485818470033,
    "input_throughput": 7055.096458439795,
    "output_throughput": 6137.400231600264,
    "total_throughput": 13192.49669004006,
    "itl": 85.51723559741936,
    "ttft": 2008798.5474634522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8922698367480373,
    "arrivals": 1035175,
    "finished_requests": 102646,
    "scheduler_time": 307.2925543082057
}
#Debug simulation 
Total elapsed time: 85.35695968102664. Arrivals time: 0.5439652246423066 Scheduler time: 84.58226725365967 Scheduler overhead time: 0.09120485233142972 Adapter cache time: 0.018277880270034075 Engine time: 0.08634227840229869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693479501 . Total output tokens: 610448159
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 85.3550303131342,
    "estimated_duration": 3600.018927996771,
    "input_throughput": 7110.9015013553735,
    "output_throughput": 6218.324805435601,
    "total_throughput": 13329.226306790975,
    "itl": 87.79075121435787,
    "ttft": 1988386.39181987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 431,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.95276196651626,
    "arrivals": 1035175,
    "finished_requests": 103573,
    "scheduler_time": 304.6577782482534
}
#Debug simulation 
Total elapsed time: 85.35520319035277. Arrivals time: 0.544243878684938 Scheduler time: 84.58236076310277 Scheduler overhead time: 0.0902770571410656 Adapter cache time: 0.01857478776946664 Engine time: 0.08534839330241084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693479501 . Total output tokens: 610448159
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 85.66112955287099,
    "estimated_duration": 3600.0803943347896,
    "input_throughput": 6947.465406427827,
    "output_throughput": 6045.264720823373,
    "total_throughput": 12992.7301272512,
    "itl": 84.61698764256815,
    "ttft": 2010736.7890635116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6856035283301183,
    "arrivals": 1035175,
    "finished_requests": 101022,
    "scheduler_time": 312.0944347821971
}
#Debug simulation 
Total elapsed time: 85.66130311973393. Arrivals time: 0.5383818256668746 Scheduler time: 84.89057055860758 Scheduler overhead time: 0.09185729781165719 Adapter cache time: 0.01824109023436904 Engine time: 0.0869380016811192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693479501 . Total output tokens: 610448159
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 88.4791068052873,
    "estimated_duration": 3600.007289962381,
    "input_throughput": 7084.15898798541,
    "output_throughput": 6169.761395186536,
    "total_throughput": 13253.920383171946,
    "itl": 87.42424016378006,
    "ttft": 2005792.0691652822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3428989851893762,
    "arrivals": 1035175,
    "finished_requests": 103055,
    "scheduler_time": 305.78782871564397
}
#Debug simulation 
Total elapsed time: 88.47927887318656. Arrivals time: 0.6130849565379322 Scheduler time: 87.6362220197916 Scheduler overhead time: 0.09058794286102057 Adapter cache time: 0.01818128628656268 Engine time: 0.08645677380263805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693479501 . Total output tokens: 610448159
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.74017247091979,
    "estimated_duration": 3600.0160095666365,
    "input_throughput": 7027.083194289817,
    "output_throughput": 6140.102138784904,
    "total_throughput": 13167.18533307472,
    "itl": 85.62991990170897,
    "ttft": 1994133.6420666885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0156750401668457,
    "arrivals": 1035175,
    "finished_requests": 102250,
    "scheduler_time": 308.69401346639415
}
#Debug simulation 
Total elapsed time: 82.74033544398844. Arrivals time: 0.5266591543331742 Scheduler time: 81.98230321425945 Scheduler overhead time: 0.09109135949984193 Adapter cache time: 0.01850678212940693 Engine time: 0.086639781948179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688402369 . Total output tokens: 605939539
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.85046453494579,
    "estimated_duration": 3600.0239856316657,
    "input_throughput": 7159.4259101797725,
    "output_throughput": 6264.379929136224,
    "total_throughput": 13423.805839315995,
    "itl": 88.84503288442168,
    "ttft": 1982680.5755568915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.074770199856753,
    "arrivals": 1027687,
    "finished_requests": 104310,
    "scheduler_time": 301.9855907491446
}
#Debug simulation 
Total elapsed time: 82.85063401795924. Arrivals time: 0.5427304948680103 Scheduler time: 82.08165013371035 Scheduler overhead time: 0.08862482942640781 Adapter cache time: 0.018440166488289833 Engine time: 0.08468182059004903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688402369 . Total output tokens: 605939539
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.20786658395082,
    "estimated_duration": 3600.003352707062,
    "input_throughput": 7217.653833700284,
    "output_throughput": 6293.761916349994,
    "total_throughput": 13511.415750050277,
    "itl": 87.95430979236816,
    "ttft": 2001560.008018251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8131941312877498,
    "arrivals": 1027687,
    "finished_requests": 105136,
    "scheduler_time": 298.99912889553826
}
#Debug simulation 
Total elapsed time: 85.20813369797543. Arrivals time: 0.535522541962564 Scheduler time: 84.44604745181277 Scheduler overhead time: 0.08981044543907046 Adapter cache time: 0.01768964482471347 Engine time: 0.08495829347521067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688402369 . Total output tokens: 605939539
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.59314843732864,
    "estimated_duration": 3600.028625453807,
    "input_throughput": 7030.89326041382,
    "output_throughput": 6157.823535974117,
    "total_throughput": 13188.716796387936,
    "itl": 85.25346214272611,
    "ttft": 1987395.5492472854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.419533273624283,
    "arrivals": 1027687,
    "finished_requests": 102508,
    "scheduler_time": 307.527472805789
}
#Debug simulation 
Total elapsed time: 81.59331603022292. Arrivals time: 0.5323169459588826 Scheduler time: 80.83066527545452 Scheduler overhead time: 0.09082460822537541 Adapter cache time: 0.018537484109401703 Engine time: 0.08592127868905663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688402369 . Total output tokens: 605939539
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.09589415835217,
    "estimated_duration": 3600.095359084332,
    "input_throughput": 7200.238719952415,
    "output_throughput": 6277.359554650235,
    "total_throughput": 13477.59827460265,
    "itl": 87.94658653544516,
    "ttft": 2001175.376298189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.509485501986925,
    "arrivals": 1027687,
    "finished_requests": 104933,
    "scheduler_time": 299.88433396095496
}
#Debug simulation 
Total elapsed time: 84.09607102442533. Arrivals time: 0.5209256131201982 Scheduler time: 83.34717143978924 Scheduler overhead time: 0.08899389300495386 Adapter cache time: 0.017769366968423128 Engine time: 0.08672253834083676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688402369 . Total output tokens: 605939539
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 80.60858274716884,
    "estimated_duration": 3600.031276600898,
    "input_throughput": 6986.727355254702,
    "output_throughput": 6114.435767009111,
    "total_throughput": 13101.163122263813,
    "itl": 85.04206795516895,
    "ttft": 1985696.4847646817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6269123406708546,
    "arrivals": 1027687,
    "finished_requests": 101841,
    "scheduler_time": 309.63001047649556
}
#Debug simulation 
Total elapsed time: 80.60875192889944. Arrivals time: 0.5277960677631199 Scheduler time: 79.85202324809507 Scheduler overhead time: 0.08953227521851659 Adapter cache time: 0.018678265623748302 Engine time: 0.0861074822023511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688402369 . Total output tokens: 605939539
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.79976654611528,
    "estimated_duration": 3600.0071404264836,
    "input_throughput": 7169.884112213778,
    "output_throughput": 6275.919496460616,
    "total_throughput": 13445.803608674394,
    "itl": 87.96884026845088,
    "ttft": 1990838.7734948054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.828076976672734,
    "arrivals": 1027687,
    "finished_requests": 104557,
    "scheduler_time": 301.545364969218
}
#Debug simulation 
Total elapsed time: 82.7999342191033. Arrivals time: 0.5243577575311065 Scheduler time: 82.04973150370643 Scheduler overhead time: 0.08887942181900144 Adapter cache time: 0.018161242362111807 Engine time: 0.0843741544522345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688402369 . Total output tokens: 605939539
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.7769331028685,
    "estimated_duration": 3600.0185738990867,
    "input_throughput": 7030.912891259297,
    "output_throughput": 6157.840729135473,
    "total_throughput": 13188.753620394771,
    "itl": 85.25287959743362,
    "ttft": 1987382.9355593538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.356983886286643,
    "arrivals": 1027687,
    "finished_requests": 102508,
    "scheduler_time": 307.53247788373545
}
#Debug simulation 
Total elapsed time: 81.7770974971354. Arrivals time: 0.5290835951454937 Scheduler time: 81.01853533089161 Scheduler overhead time: 0.08994585974141955 Adapter cache time: 0.018654923420399427 Engine time: 0.0858922409825027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685845451 . Total output tokens: 603659867
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.48541381116956,
    "estimated_duration": 3600.0215602349112,
    "input_throughput": 7270.067015457591,
    "output_throughput": 6293.168421614026,
    "total_throughput": 13563.235437071617,
    "itl": 89.1688005571464,
    "ttft": 1996759.588762267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6185139766521925,
    "arrivals": 1023854,
    "finished_requests": 105642,
    "scheduler_time": 299.1199600838221
}
#Debug simulation 
Total elapsed time: 86.48558576824144. Arrivals time: 0.5318934894166887 Scheduler time: 85.72741111228243 Scheduler overhead time: 0.0890377308242023 Adapter cache time: 0.017541805282235146 Engine time: 0.08512158738449216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685845451 . Total output tokens: 603659867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.9814572809264,
    "estimated_duration": 3600.013268326173,
    "input_throughput": 7266.915161166491,
    "output_throughput": 6309.6659114707345,
    "total_throughput": 13576.581072637226,
    "itl": 88.11884044036171,
    "ttft": 1995605.9763244453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7254813122144004,
    "arrivals": 1023854,
    "finished_requests": 105613,
    "scheduler_time": 299.02014180067266
}
#Debug simulation 
Total elapsed time: 85.98172605177388. Arrivals time: 0.6570798512548208 Scheduler time: 85.09917922038585 Scheduler overhead time: 0.08849497279152274 Adapter cache time: 0.01748119667172432 Engine time: 0.08528506802394986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685845451 . Total output tokens: 603659867
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.43820852506906,
    "estimated_duration": 3600.0269843591486,
    "input_throughput": 7200.373528481862,
    "output_throughput": 6249.331768274202,
    "total_throughput": 13449.705296756063,
    "itl": 86.16548306693136,
    "ttft": 1998887.2508055065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.739292401247673,
    "arrivals": 1023854,
    "finished_requests": 104716,
    "scheduler_time": 301.43810918072427
}
#Debug simulation 
Total elapsed time: 83.43837994383648. Arrivals time: 0.5212006107904017 Scheduler time: 82.68940706271678 Scheduler overhead time: 0.08903330704197288 Adapter cache time: 0.017907950095832348 Engine time: 0.08591243252158165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685845451 . Total output tokens: 603659867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 86.03428574930876,
    "estimated_duration": 3600.0204131657724,
    "input_throughput": 7277.773454890003,
    "output_throughput": 6324.617748480404,
    "total_throughput": 13602.391203370407,
    "itl": 88.17654547044557,
    "ttft": 1992065.1461576018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4400934086879693,
    "arrivals": 1023854,
    "finished_requests": 105766,
    "scheduler_time": 298.7011841407951
}
#Debug simulation 
Total elapsed time: 86.03445509728044. Arrivals time: 0.6627192264422774 Scheduler time: 85.14522565156221 Scheduler overhead time: 0.08925020229071379 Adapter cache time: 0.01747237704694271 Engine time: 0.08582784840837121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685845451 . Total output tokens: 603659867
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 85.28166142292321,
    "estimated_duration": 3600.0664995827788,
    "input_throughput": 7199.319513404464,
    "output_throughput": 6249.976771986746,
    "total_throughput": 13449.296285391209,
    "itl": 86.15485190444979,
    "ttft": 1997782.108784881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6845160734653617,
    "arrivals": 1023854,
    "finished_requests": 104725,
    "scheduler_time": 301.54703870308083
}
#Debug simulation 
Total elapsed time: 85.28183448826894. Arrivals time: 0.5309632732532918 Scheduler time: 84.52184656681493 Scheduler overhead time: 0.09042666899040341 Adapter cache time: 0.017795272637158632 Engine time: 0.08619733387604356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685845451 . Total output tokens: 603659867
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.57046910887584,
    "estimated_duration": 3600.0347338261877,
    "input_throughput": 7246.544527717198,
    "output_throughput": 6272.454203796087,
    "total_throughput": 13518.998731513286,
    "itl": 87.96633503267572,
    "ttft": 1998412.370271735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4003542736545107,
    "arrivals": 1023854,
    "finished_requests": 105252,
    "scheduler_time": 300.27534924910964
}
#Debug simulation 
Total elapsed time: 85.57064973888919. Arrivals time: 0.5385035965591669 Scheduler time: 84.80591265950352 Scheduler overhead time: 0.08888490730896592 Adapter cache time: 0.017722939606755972 Engine time: 0.08520427206531167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685845451 . Total output tokens: 603659867
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.11228117533028,
    "estimated_duration": 3600.059938743059,
    "input_throughput": 7200.404282449387,
    "output_throughput": 6249.442337855959,
    "total_throughput": 13449.846620305345,
    "itl": 86.1646617831453,
    "ttft": 1998935.3380430525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6889629273303,
    "arrivals": 1023854,
    "finished_requests": 104718,
    "scheduler_time": 301.4445654208642
}
#Debug simulation 
Total elapsed time: 83.11245149513707. Arrivals time: 0.6588598885573447 Scheduler time: 82.22543800482526 Scheduler overhead time: 0.09053702047094703 Adapter cache time: 0.017494858242571354 Engine time: 0.08535431744530797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 66, 1080, 66, 34560, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 1080, 34560, 66, 34560, 34560, 1080, 1080, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 66, 66, 34560, 1080, 66, 1080, 1080, 34560, 1080, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 34560, 66, 34560, 66, 1080, 34560, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 66, 34560, 1080, 66, 34560, 66, 34560, 66, 66, 1080, 1080, 34560, 1080, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 1080, 66, 1080, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 1080, 1080, 34560, 66, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 66, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 34560, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 66, 66, 66, 66]
Prompts retrieved: 3069570 . Total input tokens: 684530309 . Total output tokens: 602506475
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.19610378332436,
    "estimated_duration": 3600.028293971075,
    "input_throughput": 7296.951816737878,
    "output_throughput": 6335.921869891633,
    "total_throughput": 13632.873686629511,
    "itl": 89.06614905673753,
    "ttft": 1988545.658065604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 334,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.208544616671283,
    "arrivals": 1021838,
    "finished_requests": 106316,
    "scheduler_time": 297.0396501246261
}
#Debug simulation 
Total elapsed time: 87.1962767932564. Arrivals time: 0.5348902950063348 Scheduler time: 86.44015424745157 Scheduler overhead time: 0.08757911855354905 Adapter cache time: 0.01690959930419922 Engine time: 0.08324447087943554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 66, 1080, 66, 34560, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 1080, 34560, 66, 34560, 34560, 1080, 1080, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 66, 66, 34560, 1080, 66, 1080, 1080, 34560, 1080, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 34560, 66, 34560, 66, 1080, 34560, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 66, 34560, 1080, 66, 34560, 66, 34560, 66, 66, 1080, 1080, 34560, 1080, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 1080, 66, 1080, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 1080, 1080, 34560, 66, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 66, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 34560, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 66, 66, 66, 66]
Prompts retrieved: 3069570 . Total input tokens: 684530309 . Total output tokens: 602506475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.30824269959703,
    "estimated_duration": 3600.0196113696156,
    "input_throughput": 7280.402005929253,
    "output_throughput": 6327.744417851497,
    "total_throughput": 13608.14642378075,
    "itl": 88.17787946933169,
    "ttft": 1983634.7199607852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4176554084103583,
    "arrivals": 1021838,
    "finished_requests": 106085,
    "scheduler_time": 297.93685292172995
}
#Debug simulation 
Total elapsed time: 86.30841145059094. Arrivals time: 0.5244984365999699 Scheduler time: 85.5587640167214 Scheduler overhead time: 0.08845492266118526 Adapter cache time: 0.017248498275876045 Engine time: 0.08500619232654572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 66, 1080, 66, 34560, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 1080, 34560, 66, 34560, 34560, 1080, 1080, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 66, 66, 34560, 1080, 66, 1080, 1080, 34560, 1080, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 34560, 66, 34560, 66, 1080, 34560, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 66, 34560, 1080, 66, 34560, 66, 34560, 66, 66, 1080, 1080, 34560, 1080, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 1080, 66, 1080, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 1080, 1080, 34560, 66, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 66, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 34560, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 66, 66, 66, 66]
Prompts retrieved: 3069570 . Total input tokens: 684530309 . Total output tokens: 602506475
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.11007490986958,
    "estimated_duration": 3600.0126866254486,
    "input_throughput": 7191.707155973604,
    "output_throughput": 6245.222991443074,
    "total_throughput": 13436.930147416679,
    "itl": 85.83938499738055,
    "ttft": 1993029.0119432798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 331,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4903380024619515,
    "arrivals": 1021838,
    "finished_requests": 104747,
    "scheduler_time": 301.78703828484123
}
#Debug simulation 
Total elapsed time: 83.11024696705863. Arrivals time: 0.5357392621226609 Scheduler time: 82.34907427802682 Scheduler overhead time: 0.08940490847453475 Adapter cache time: 0.017300193198025227 Engine time: 0.08424737537279725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 66, 1080, 66, 34560, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 1080, 34560, 66, 34560, 34560, 1080, 1080, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 66, 66, 34560, 1080, 66, 1080, 1080, 34560, 1080, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 34560, 66, 34560, 66, 1080, 34560, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 66, 34560, 1080, 66, 34560, 66, 34560, 66, 66, 1080, 1080, 34560, 1080, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 1080, 66, 1080, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 1080, 1080, 34560, 66, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 66, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 34560, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 66, 66, 66, 66]
Prompts retrieved: 3069570 . Total input tokens: 684530309 . Total output tokens: 602506475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 86.1968427109532,
    "estimated_duration": 3600.008988563784,
    "input_throughput": 7280.729321305584,
    "output_throughput": 6328.436143457798,
    "total_throughput": 13609.165464763382,
    "itl": 88.17460325382352,
    "ttft": 1983563.9893716155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2635628803726258,
    "arrivals": 1021838,
    "finished_requests": 106091,
    "scheduler_time": 297.9521837419062
}
#Debug simulation 
Total elapsed time: 86.19701433787122. Arrivals time: 0.5376808643341064 Scheduler time: 85.43544988520443 Scheduler overhead time: 0.08816770883277059 Adapter cache time: 0.017219371162354946 Engine time: 0.08425322733819485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 66, 1080, 66, 34560, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 1080, 34560, 66, 34560, 34560, 1080, 1080, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 66, 66, 34560, 1080, 66, 1080, 1080, 34560, 1080, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 34560, 66, 34560, 66, 1080, 34560, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 66, 34560, 1080, 66, 34560, 66, 34560, 66, 66, 1080, 1080, 34560, 1080, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 1080, 66, 1080, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 1080, 1080, 34560, 66, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 66, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 34560, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 66, 66, 66, 66]
Prompts retrieved: 3069570 . Total input tokens: 684530309 . Total output tokens: 602506475
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 84.89773263502866,
    "estimated_duration": 3600.0734709158546,
    "input_throughput": 7207.015970537796,
    "output_throughput": 6256.3331504093185,
    "total_throughput": 13463.349120947114,
    "itl": 85.87337826817716,
    "ttft": 1993911.937857563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.528124735704635,
    "arrivals": 1021838,
    "finished_requests": 105011,
    "scheduler_time": 301.1861163896231
}
#Debug simulation 
Total elapsed time: 84.89790779724717. Arrivals time: 0.5441397232934833 Scheduler time: 84.12750187749043 Scheduler overhead time: 0.08939302898943424 Adapter cache time: 0.017250642646104097 Engine time: 0.08549557207152247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 66, 1080, 66, 34560, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 1080, 34560, 66, 34560, 34560, 1080, 1080, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 66, 66, 34560, 1080, 66, 1080, 1080, 34560, 1080, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 34560, 66, 34560, 66, 1080, 34560, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 66, 34560, 1080, 66, 34560, 66, 34560, 66, 66, 1080, 1080, 34560, 1080, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 1080, 66, 1080, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 1080, 1080, 34560, 66, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 66, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 34560, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 66, 66, 66, 66]
Prompts retrieved: 3069570 . Total input tokens: 684530309 . Total output tokens: 602506475
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.26751234382391,
    "estimated_duration": 3600.0270967889255,
    "input_throughput": 7250.114318106288,
    "output_throughput": 6318.343553660658,
    "total_throughput": 13568.457871766945,
    "itl": 87.90821814950048,
    "ttft": 1981134.2041869895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.144997436031691,
    "arrivals": 1021838,
    "finished_requests": 105660,
    "scheduler_time": 299.0626141821539
}
#Debug simulation 
Total elapsed time: 86.26768077816814. Arrivals time: 0.5345036708749831 Scheduler time: 85.51088844100013 Scheduler overhead time: 0.08806261606514454 Adapter cache time: 0.01721646124497056 Engine time: 0.08387155784294009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 66, 34560, 34560, 66, 34560, 1080, 34560, 66, 1080, 66, 34560, 1080, 1080, 1080, 1080, 34560, 66, 66, 66, 1080, 34560, 66, 34560, 34560, 1080, 1080, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 66, 34560, 34560, 1080, 66, 66, 34560, 1080, 66, 1080, 1080, 34560, 1080, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 34560, 66, 34560, 66, 1080, 34560, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 66, 34560, 1080, 66, 34560, 66, 34560, 66, 66, 1080, 1080, 34560, 1080, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 34560, 66, 34560, 1080, 66, 1080, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 1080, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 1080, 34560, 1080, 66, 1080, 34560, 1080, 34560, 66, 34560, 66, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 34560, 1080, 1080, 66, 1080, 34560, 1080, 1080, 34560, 66, 34560, 34560, 1080, 66, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 66, 34560, 1080, 34560, 34560, 66, 34560, 1080, 1080, 34560, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 66, 66, 66, 66]
Prompts retrieved: 3069570 . Total input tokens: 684530309 . Total output tokens: 602506475
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.35567127121612,
    "estimated_duration": 3600.051929120352,
    "input_throughput": 7206.97381894144,
    "output_throughput": 6256.055869033847,
    "total_throughput": 13463.029687975288,
    "itl": 85.87280683272037,
    "ttft": 1993993.9290563527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.503892026040715,
    "arrivals": 1021838,
    "finished_requests": 105007,
    "scheduler_time": 301.19762673573007
}
#Debug simulation 
Total elapsed time: 84.35584159800783. Arrivals time: 0.8991163857281208 Scheduler time: 83.22862471267581 Scheduler overhead time: 0.08914974052459002 Adapter cache time: 0.017674711998552084 Engine time: 0.08664278499782085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 33, 1080, 33, 34560, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 1080, 34560, 33, 34560, 34560, 1080, 1080, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 33, 33, 34560, 1080, 33, 1080, 1080, 34560, 1080, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 34560, 33, 34560, 33, 1080, 34560, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 33, 34560, 1080, 33, 34560, 33, 34560, 33, 33, 1080, 1080, 34560, 1080, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 1080, 1080, 34560, 33, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 33, 33, 33, 33]
Prompts retrieved: 3066765 . Total input tokens: 683904259 . Total output tokens: 601965153
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 90.14284435519949,
    "estimated_duration": 3600.06242473941,
    "input_throughput": 7300.1909131901675,
    "output_throughput": 6367.888468394936,
    "total_throughput": 13668.079381585103,
    "itl": 89.402778562582,
    "ttft": 1985660.7303179523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.248219070862984,
    "arrivals": 1020948,
    "finished_requests": 106693,
    "scheduler_time": 295.49559511392994
}
#Debug simulation 
Total elapsed time: 90.14301101397723. Arrivals time: 0.543397341389209 Scheduler time: 89.37564857443795 Scheduler overhead time: 0.08823856385424733 Adapter cache time: 0.01760939136147499 Engine time: 0.08439267985522747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 33, 1080, 33, 34560, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 1080, 34560, 33, 34560, 34560, 1080, 1080, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 33, 33, 34560, 1080, 33, 1080, 1080, 34560, 1080, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 34560, 33, 34560, 33, 1080, 34560, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 33, 34560, 1080, 33, 34560, 33, 34560, 33, 33, 1080, 1080, 34560, 1080, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 1080, 1080, 34560, 33, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 33, 33, 33, 33]
Prompts retrieved: 3066765 . Total input tokens: 683904259 . Total output tokens: 601965153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 89.74442994967103,
    "estimated_duration": 3600.0505507722573,
    "input_throughput": 7114.4412109729465,
    "output_throughput": 6214.601624191285,
    "total_throughput": 13329.042835164231,
    "itl": 87.44471126852667,
    "ttft": 2000806.6591342087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.323558668396438,
    "arrivals": 1020948,
    "finished_requests": 104091,
    "scheduler_time": 303.3627135094949
}
#Debug simulation 
Total elapsed time: 89.74459189875051. Arrivals time: 0.6092026084661484 Scheduler time: 88.90866024233401 Scheduler overhead time: 0.08968110848218203 Adapter cache time: 0.017582849599421024 Engine time: 0.08510290971025825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 33, 1080, 33, 34560, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 1080, 34560, 33, 34560, 34560, 1080, 1080, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 33, 33, 34560, 1080, 33, 1080, 1080, 34560, 1080, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 34560, 33, 34560, 33, 1080, 34560, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 33, 34560, 1080, 33, 34560, 33, 34560, 33, 33, 1080, 1080, 34560, 1080, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 1080, 1080, 34560, 33, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 33, 33, 33, 33]
Prompts retrieved: 3066765 . Total input tokens: 683904259 . Total output tokens: 601965153
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 88.38614209974185,
    "estimated_duration": 3600.0876121844585,
    "input_throughput": 7104.140997413477,
    "output_throughput": 6218.477829325935,
    "total_throughput": 13322.618826739412,
    "itl": 85.5254421698632,
    "ttft": 1997878.182107526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.41812130069827,
    "arrivals": 1020948,
    "finished_requests": 103941,
    "scheduler_time": 304.2644805229744
}
#Debug simulation 
Total elapsed time: 88.386317725759. Arrivals time: 0.5420250887982547 Scheduler time: 87.6146590122953 Scheduler overhead time: 0.0906463642604649 Adapter cache time: 0.017458727583289146 Engine time: 0.08633139776065946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 33, 1080, 33, 34560, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 1080, 34560, 33, 34560, 34560, 1080, 1080, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 33, 33, 34560, 1080, 33, 1080, 1080, 34560, 1080, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 34560, 33, 34560, 33, 1080, 34560, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 33, 34560, 1080, 33, 34560, 33, 34560, 33, 33, 1080, 1080, 34560, 1080, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 1080, 1080, 34560, 33, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 33, 33, 33, 33]
Prompts retrieved: 3066765 . Total input tokens: 683904259 . Total output tokens: 601965153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 89.64618747821078,
    "estimated_duration": 3600.0422776693003,
    "input_throughput": 7197.270754490883,
    "output_throughput": 6283.651761624673,
    "total_throughput": 13480.922516115555,
    "itl": 87.93105432337784,
    "ttft": 2001180.3758262089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2111032918747493,
    "arrivals": 1020948,
    "finished_requests": 105346,
    "scheduler_time": 299.84988150700866
}
#Debug simulation 
Total elapsed time: 89.64636772824451. Arrivals time: 0.5464646983891726 Scheduler time: 88.87366444338113 Scheduler overhead time: 0.08925873506814241 Adapter cache time: 0.01735579129308462 Engine time: 0.08519878098741174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 33, 1080, 33, 34560, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 1080, 34560, 33, 34560, 34560, 1080, 1080, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 33, 33, 34560, 1080, 33, 1080, 1080, 34560, 1080, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 34560, 33, 34560, 33, 1080, 34560, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 33, 34560, 1080, 33, 34560, 33, 34560, 33, 33, 1080, 1080, 34560, 1080, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 1080, 1080, 34560, 33, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 33, 33, 33, 33]
Prompts retrieved: 3066765 . Total input tokens: 683904259 . Total output tokens: 601965153
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 86.19365373393521,
    "estimated_duration": 3600.0339050669754,
    "input_throughput": 7123.509299150031,
    "output_throughput": 6217.3683888078795,
    "total_throughput": 13340.87768795791,
    "itl": 85.48618546518117,
    "ttft": 2004513.640507763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.400618223384966,
    "arrivals": 1020948,
    "finished_requests": 104190,
    "scheduler_time": 303.2268367982462
}
#Debug simulation 
Total elapsed time: 86.19382546236739. Arrivals time: 0.5241160909645259 Scheduler time: 85.44203530997038 Scheduler overhead time: 0.09009757824242115 Adapter cache time: 0.017438778188079596 Engine time: 0.085618254262954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 33, 1080, 33, 34560, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 1080, 34560, 33, 34560, 34560, 1080, 1080, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 33, 33, 34560, 1080, 33, 1080, 1080, 34560, 1080, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 34560, 33, 34560, 33, 1080, 34560, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 33, 34560, 1080, 33, 34560, 33, 34560, 33, 33, 1080, 1080, 34560, 1080, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 1080, 1080, 34560, 33, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 33, 33, 33, 33]
Prompts retrieved: 3066765 . Total input tokens: 683904259 . Total output tokens: 601965153
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 89.5870050387457,
    "estimated_duration": 3600.0689274505175,
    "input_throughput": 7197.564691726629,
    "output_throughput": 6283.773854302387,
    "total_throughput": 13481.338546029016,
    "itl": 87.92875960337273,
    "ttft": 2001131.9149711225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.055622542863704,
    "arrivals": 1020948,
    "finished_requests": 105351,
    "scheduler_time": 299.85953655325244
}
#Debug simulation 
Total elapsed time: 89.58717697393149. Arrivals time: 0.5432148091495037 Scheduler time: 88.81791391829029 Scheduler overhead time: 0.08953130710870028 Adapter cache time: 0.017615456599742174 Engine time: 0.08467743266373873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 33, 34560, 34560, 33, 34560, 1080, 34560, 33, 1080, 33, 34560, 1080, 1080, 1080, 1080, 34560, 33, 33, 33, 1080, 34560, 33, 34560, 34560, 1080, 1080, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 33, 34560, 34560, 1080, 33, 33, 34560, 1080, 33, 1080, 1080, 34560, 1080, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 34560, 33, 34560, 33, 1080, 34560, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 33, 34560, 1080, 33, 34560, 33, 34560, 33, 33, 1080, 1080, 34560, 1080, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 34560, 33, 34560, 1080, 33, 1080, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 1080, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 1080, 34560, 1080, 33, 1080, 34560, 1080, 34560, 33, 34560, 33, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 34560, 1080, 1080, 33, 1080, 34560, 1080, 1080, 34560, 33, 34560, 34560, 1080, 33, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 33, 34560, 1080, 34560, 34560, 33, 34560, 1080, 1080, 34560, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 33, 33, 33, 33]
Prompts retrieved: 3066765 . Total input tokens: 683904259 . Total output tokens: 601965153
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.47843848215416,
    "estimated_duration": 3600.002785171581,
    "input_throughput": 7150.976967583937,
    "output_throughput": 6246.464889589864,
    "total_throughput": 13397.4418571738,
    "itl": 86.04836275786245,
    "ttft": 2000200.5138658793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.424841953963054,
    "arrivals": 1020948,
    "finished_requests": 104659,
    "scheduler_time": 301.7219490996261
}
#Debug simulation 
Total elapsed time: 86.47860437072814. Arrivals time: 0.5419983095489442 Scheduler time: 85.71210956154391 Scheduler overhead time: 0.08869522577151656 Adapter cache time: 0.017444884404540062 Engine time: 0.08330322802066803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 270, 540, 270, 34560, 540, 540, 540, 540, 34560, 270, 270, 270, 540, 34560, 270, 34560, 34560, 540, 540, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 270, 270, 34560, 540, 270, 540, 540, 34560, 540, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 34560, 34560, 270, 34560, 270, 540, 34560, 34560, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 270, 34560, 540, 270, 34560, 270, 34560, 270, 270, 540, 540, 34560, 540, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 540, 540, 270, 540, 34560, 540, 540, 34560, 270, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 34560, 540, 270, 270, 270, 270]
Prompts retrieved: 3041010 . Total input tokens: 678150491 . Total output tokens: 596924125
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 90.29520433116704,
    "estimated_duration": 3600.002857757391,
    "input_throughput": 7114.8915742683375,
    "output_throughput": 6204.452296994608,
    "total_throughput": 13319.343871262947,
    "itl": 88.96936501496864,
    "ttft": 2000870.537092858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.426754114725638,
    "arrivals": 1012432,
    "finished_requests": 103857,
    "scheduler_time": 304.00446081249385
}
#Debug simulation 
Total elapsed time: 90.29537260578945. Arrivals time: 0.5389812923967838 Scheduler time: 89.53013886278495 Scheduler overhead time: 0.08962564636021852 Adapter cache time: 0.01767220627516508 Engine time: 0.08494782401248813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 270, 540, 270, 34560, 540, 540, 540, 540, 34560, 270, 270, 270, 540, 34560, 270, 34560, 34560, 540, 540, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 270, 270, 34560, 540, 270, 540, 540, 34560, 540, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 34560, 34560, 270, 34560, 270, 540, 34560, 34560, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 270, 34560, 540, 270, 34560, 270, 34560, 270, 270, 540, 540, 34560, 540, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 540, 540, 270, 540, 34560, 540, 540, 34560, 270, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 34560, 540, 270, 270, 270, 270]
Prompts retrieved: 3041010 . Total input tokens: 678150491 . Total output tokens: 596924125
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.1052066218108,
    "estimated_duration": 3600.0065694866307,
    "input_throughput": 7178.279400663735,
    "output_throughput": 6265.1160670566,
    "total_throughput": 13443.395467720335,
    "itl": 88.46582063475716,
    "ttft": 1999204.1505931204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8831434085127,
    "arrivals": 1012432,
    "finished_requests": 104914,
    "scheduler_time": 300.80826127182905
}
#Debug simulation 
Total elapsed time: 86.10537240095437. Arrivals time: 0.5475016520358622 Scheduler time: 85.33212455036119 Scheduler overhead time: 0.08891806844621897 Adapter cache time: 0.017696470022201538 Engine time: 0.08456517895683646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 270, 540, 270, 34560, 540, 540, 540, 540, 34560, 270, 270, 270, 540, 34560, 270, 34560, 34560, 540, 540, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 270, 270, 34560, 540, 270, 540, 540, 34560, 540, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 34560, 34560, 270, 34560, 270, 540, 34560, 34560, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 270, 34560, 540, 270, 34560, 270, 34560, 270, 270, 540, 540, 34560, 540, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 540, 540, 270, 540, 34560, 540, 540, 34560, 270, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 34560, 540, 270, 270, 270, 270]
Prompts retrieved: 3041010 . Total input tokens: 678150491 . Total output tokens: 596924125
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.91802462516353,
    "estimated_duration": 3600.0348749262007,
    "input_throughput": 7126.927346926316,
    "output_throughput": 6219.474748965868,
    "total_throughput": 13346.402095892183,
    "itl": 86.35104084962349,
    "ttft": 2000604.3009077983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.895687449537237,
    "arrivals": 1012432,
    "finished_requests": 104123,
    "scheduler_time": 303.0997530643376
}
#Debug simulation 
Total elapsed time: 83.91819387627766. Arrivals time: 0.474690776783973 Scheduler time: 83.22499975468963 Scheduler overhead time: 0.08544391114264727 Adapter cache time: 0.017048821784555912 Engine time: 0.08200203673914075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 270, 540, 270, 34560, 540, 540, 540, 540, 34560, 270, 270, 270, 540, 34560, 270, 34560, 34560, 540, 540, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 270, 270, 34560, 540, 270, 540, 540, 34560, 540, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 34560, 34560, 270, 34560, 270, 540, 34560, 34560, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 270, 34560, 540, 270, 34560, 270, 34560, 270, 270, 540, 540, 34560, 540, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 540, 540, 270, 540, 34560, 540, 540, 34560, 270, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 34560, 540, 270, 270, 270, 270]
Prompts retrieved: 3041010 . Total input tokens: 678150491 . Total output tokens: 596924125
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 81.45566590782255,
    "estimated_duration": 3600.018014306905,
    "input_throughput": 7122.593247616466,
    "output_throughput": 6246.507353749846,
    "total_throughput": 13369.100601366312,
    "itl": 88.52058374199014,
    "ttft": 1980510.1849260514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.016601175921965,
    "arrivals": 1012432,
    "finished_requests": 104114,
    "scheduler_time": 303.26351218313073
}
#Debug simulation 
Total elapsed time: 81.45582814188674. Arrivals time: 0.42408476350829005 Scheduler time: 80.81858308892697 Scheduler overhead time: 0.08367553073912859 Adapter cache time: 0.017280823085457087 Engine time: 0.07932552881538868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 270, 540, 270, 34560, 540, 540, 540, 540, 34560, 270, 270, 270, 540, 34560, 270, 34560, 34560, 540, 540, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 270, 270, 34560, 540, 270, 540, 540, 34560, 540, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 34560, 34560, 270, 34560, 270, 540, 34560, 34560, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 270, 34560, 540, 270, 34560, 270, 34560, 270, 270, 540, 540, 34560, 540, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 540, 540, 270, 540, 34560, 540, 540, 34560, 270, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 34560, 540, 270, 270, 270, 270]
Prompts retrieved: 3041010 . Total input tokens: 678150491 . Total output tokens: 596924125
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 80.83974909503013,
    "estimated_duration": 3600.06406999521,
    "input_throughput": 7106.9735156224715,
    "output_throughput": 6207.097308695878,
    "total_throughput": 13314.07082431835,
    "itl": 86.24219788832609,
    "ttft": 1998138.6676341884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.881343357604947,
    "arrivals": 1012432,
    "finished_requests": 103898,
    "scheduler_time": 303.6724204361868
}
#Debug simulation 
Total elapsed time: 80.83991168206558. Arrivals time: 0.4375253301113844 Scheduler time: 80.18849768023938 Scheduler overhead time: 0.08469719626009464 Adapter cache time: 0.016916233580559492 Engine time: 0.07923033647239208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 270, 540, 270, 34560, 540, 540, 540, 540, 34560, 270, 270, 270, 540, 34560, 270, 34560, 34560, 540, 540, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 270, 270, 34560, 540, 270, 540, 540, 34560, 540, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 34560, 34560, 270, 34560, 270, 540, 34560, 34560, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 270, 34560, 540, 270, 34560, 270, 34560, 270, 270, 540, 540, 34560, 540, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 540, 540, 270, 540, 34560, 540, 540, 34560, 270, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 34560, 540, 270, 270, 270, 270]
Prompts retrieved: 3041010 . Total input tokens: 678150491 . Total output tokens: 596924125
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.4484031740576,
    "estimated_duration": 3600.0677851421538,
    "input_throughput": 7226.506430620646,
    "output_throughput": 6300.950246997723,
    "total_throughput": 13527.45667761837,
    "itl": 88.53130574799076,
    "ttft": 1996828.026857437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 400,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5535683762282027,
    "arrivals": 1012432,
    "finished_requests": 105603,
    "scheduler_time": 298.82621687272166
}
#Debug simulation 
Total elapsed time: 85.44856240414083. Arrivals time: 0.4475305350497365 Scheduler time: 84.78884416073561 Scheduler overhead time: 0.08355540968477726 Adapter cache time: 0.01694366242736578 Engine time: 0.07901686010882258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 270, 34560, 34560, 270, 34560, 540, 34560, 270, 540, 270, 34560, 540, 540, 540, 540, 34560, 270, 270, 270, 540, 34560, 270, 34560, 34560, 540, 540, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 540, 540, 34560, 540, 34560, 270, 34560, 34560, 540, 270, 270, 34560, 540, 270, 540, 540, 34560, 540, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 34560, 34560, 270, 34560, 270, 540, 34560, 34560, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 270, 34560, 540, 270, 34560, 270, 34560, 270, 270, 540, 540, 34560, 540, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 270, 540, 34560, 34560, 270, 34560, 540, 270, 540, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 270, 540, 540, 270, 540, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 540, 34560, 540, 270, 540, 34560, 540, 34560, 270, 34560, 270, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 270, 34560, 270, 34560, 540, 540, 270, 540, 34560, 540, 540, 34560, 270, 34560, 34560, 540, 270, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 270, 34560, 540, 34560, 34560, 270, 34560, 540, 540, 34560, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 34560, 540, 270, 270, 270, 270]
Prompts retrieved: 3041010 . Total input tokens: 678150491 . Total output tokens: 596924125
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.57955569494516,
    "estimated_duration": 3600.0180749861674,
    "input_throughput": 7097.4535315626645,
    "output_throughput": 6195.852502792143,
    "total_throughput": 13293.306034354808,
    "itl": 86.3594331412049,
    "ttft": 2000824.3966246154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7485506883636286,
    "arrivals": 1012432,
    "finished_requests": 103640,
    "scheduler_time": 304.45654421103796
}
#Debug simulation 
Total elapsed time: 86.57971300184727. Arrivals time: 0.43990223854780197 Scheduler time: 85.92096725758165 Scheduler overhead time: 0.08672967972233891 Adapter cache time: 0.017443169839680195 Engine time: 0.08136523189023137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 135, 540, 135, 34560, 540, 540, 540, 540, 34560, 135, 135, 135, 540, 34560, 135, 34560, 34560, 540, 540, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 135, 135, 34560, 540, 135, 540, 540, 34560, 540, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 34560, 34560, 135, 34560, 135, 540, 34560, 34560, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 135, 34560, 540, 135, 34560, 135, 34560, 135, 135, 540, 540, 34560, 540, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 540, 540, 135, 540, 34560, 540, 540, 34560, 135, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 34560, 540, 135, 135, 135, 135]
Prompts retrieved: 3029535 . Total input tokens: 675594292 . Total output tokens: 594674410
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.645627184771,
    "estimated_duration": 3600.0496156488803,
    "input_throughput": 7223.332391576754,
    "output_throughput": 6283.215348387065,
    "total_throughput": 13506.547739963818,
    "itl": 89.1163418062975,
    "ttft": 1992741.0538164251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4796533869812394,
    "arrivals": 1008682,
    "finished_requests": 105193,
    "scheduler_time": 299.68751179811375
}
#Debug simulation 
Total elapsed time: 85.64577910490334. Arrivals time: 0.45088010421022773 Scheduler time: 84.9842314934358 Scheduler overhead time: 0.082976876758039 Adapter cache time: 0.016628318466246128 Engine time: 0.0784422648139298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 135, 540, 135, 34560, 540, 540, 540, 540, 34560, 135, 135, 135, 540, 34560, 135, 34560, 34560, 540, 540, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 135, 135, 34560, 540, 135, 540, 540, 34560, 540, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 34560, 34560, 135, 34560, 135, 540, 34560, 34560, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 135, 34560, 540, 135, 34560, 135, 34560, 135, 135, 540, 540, 34560, 540, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 540, 540, 135, 540, 34560, 540, 540, 34560, 135, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 34560, 540, 135, 135, 135, 135]
Prompts retrieved: 3029535 . Total input tokens: 675594292 . Total output tokens: 594674410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.81271554203704,
    "estimated_duration": 3600.0860699887107,
    "input_throughput": 7205.376620365452,
    "output_throughput": 6262.917208551823,
    "total_throughput": 13468.293828917276,
    "itl": 88.24622638923677,
    "ttft": 1997191.6774177502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6688570607965847,
    "arrivals": 1008682,
    "finished_requests": 104783,
    "scheduler_time": 300.6545562645075
}
#Debug simulation 
Total elapsed time: 84.81287418631837. Arrivals time: 0.44314946234226227 Scheduler time: 84.1562016531825 Scheduler overhead time: 0.08458474418148398 Adapter cache time: 0.0166788287460804 Engine time: 0.07932977098971605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 135, 540, 135, 34560, 540, 540, 540, 540, 34560, 135, 135, 135, 540, 34560, 135, 34560, 34560, 540, 540, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 135, 135, 34560, 540, 135, 540, 540, 34560, 540, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 34560, 34560, 135, 34560, 135, 540, 34560, 34560, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 135, 34560, 540, 135, 34560, 135, 34560, 135, 135, 540, 540, 34560, 540, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 540, 540, 135, 540, 34560, 540, 540, 34560, 135, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 34560, 540, 135, 135, 135, 135]
Prompts retrieved: 3029535 . Total input tokens: 675594292 . Total output tokens: 594674410
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.28655948117375,
    "estimated_duration": 3600.075072423415,
    "input_throughput": 7169.653821309053,
    "output_throughput": 6236.5171693173415,
    "total_throughput": 13406.170990626395,
    "itl": 86.23396505924494,
    "ttft": 1997138.0585072197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.715064181126669,
    "arrivals": 1008682,
    "finished_requests": 104437,
    "scheduler_time": 302.0388756239276
}
#Debug simulation 
Total elapsed time: 81.2867122553289. Arrivals time: 0.4379053092561662 Scheduler time: 80.63641391741112 Scheduler overhead time: 0.08377228630706668 Adapter cache time: 0.01645566336810589 Engine time: 0.07904880959540606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 135, 540, 135, 34560, 540, 540, 540, 540, 34560, 135, 135, 135, 540, 34560, 135, 34560, 34560, 540, 540, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 135, 135, 34560, 540, 135, 540, 540, 34560, 540, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 34560, 34560, 135, 34560, 135, 540, 34560, 34560, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 135, 34560, 540, 135, 34560, 135, 34560, 135, 135, 540, 540, 34560, 540, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 540, 540, 135, 540, 34560, 540, 540, 34560, 135, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 34560, 540, 135, 135, 135, 135]
Prompts retrieved: 3029535 . Total input tokens: 675594292 . Total output tokens: 594674410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 86.65533165913075,
    "estimated_duration": 3600.058556783745,
    "input_throughput": 7299.284604824972,
    "output_throughput": 6345.921500901946,
    "total_throughput": 13645.206105726918,
    "itl": 88.72182661504742,
    "ttft": 1995988.1350026773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.487557297218588,
    "arrivals": 1008682,
    "finished_requests": 106361,
    "scheduler_time": 296.4452458946023
}
#Debug simulation 
Total elapsed time: 86.65548852505162. Arrivals time: 0.451504394877702 Scheduler time: 85.99239743780345 Scheduler overhead time: 0.08423348050564528 Adapter cache time: 0.016871280036866665 Engine time: 0.07779805129393935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 135, 540, 135, 34560, 540, 540, 540, 540, 34560, 135, 135, 135, 540, 34560, 135, 34560, 34560, 540, 540, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 135, 135, 34560, 540, 135, 540, 540, 34560, 540, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 34560, 34560, 135, 34560, 135, 540, 34560, 34560, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 135, 34560, 540, 135, 34560, 135, 34560, 135, 135, 540, 540, 34560, 540, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 540, 540, 135, 540, 34560, 540, 540, 34560, 135, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 34560, 540, 135, 135, 135, 135]
Prompts retrieved: 3029535 . Total input tokens: 675594292 . Total output tokens: 594674410
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 82.46712427632883,
    "estimated_duration": 3600.057535195666,
    "input_throughput": 7112.048001924062,
    "output_throughput": 6195.334041734359,
    "total_throughput": 13307.382043658421,
    "itl": 85.85401340904568,
    "ttft": 2002352.60213773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.812797330245392,
    "arrivals": 1008682,
    "finished_requests": 103669,
    "scheduler_time": 304.2699828436953
}
#Debug simulation 
Total elapsed time: 82.4672681982629. Arrivals time: 0.45562468096613884 Scheduler time: 81.79617762286216 Scheduler overhead time: 0.08487289166077971 Adapter cache time: 0.017460378352552652 Engine time: 0.07965660374611616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 135, 540, 135, 34560, 540, 540, 540, 540, 34560, 135, 135, 135, 540, 34560, 135, 34560, 34560, 540, 540, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 135, 135, 34560, 540, 135, 540, 540, 34560, 540, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 34560, 34560, 135, 34560, 135, 540, 34560, 34560, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 135, 34560, 540, 135, 34560, 135, 34560, 135, 135, 540, 540, 34560, 540, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 540, 540, 135, 540, 34560, 540, 540, 34560, 135, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 34560, 540, 135, 135, 135, 135]
Prompts retrieved: 3029535 . Total input tokens: 675594292 . Total output tokens: 594674410
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.82166765071452,
    "estimated_duration": 3600.002723200949,
    "input_throughput": 7245.250352702046,
    "output_throughput": 6309.931060219375,
    "total_throughput": 13555.18141292142,
    "itl": 88.44095631887716,
    "ttft": 1989481.4819668976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3045954595459532,
    "arrivals": 1008682,
    "finished_requests": 105449,
    "scheduler_time": 299.0658452741469
}
#Debug simulation 
Total elapsed time: 85.82178476685658. Arrivals time: 0.46469914028421044 Scheduler time: 85.14218606427312 Scheduler overhead time: 0.08522267686203122 Adapter cache time: 0.017229971941560507 Engine time: 0.07861948059871793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 135, 34560, 34560, 135, 34560, 540, 34560, 135, 540, 135, 34560, 540, 540, 540, 540, 34560, 135, 135, 135, 540, 34560, 135, 34560, 34560, 540, 540, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 540, 540, 34560, 540, 34560, 135, 34560, 34560, 540, 135, 135, 34560, 540, 135, 540, 540, 34560, 540, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 34560, 34560, 135, 34560, 135, 540, 34560, 34560, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 135, 34560, 540, 135, 34560, 135, 34560, 135, 135, 540, 540, 34560, 540, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 135, 540, 34560, 34560, 135, 34560, 540, 135, 540, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 135, 540, 540, 135, 540, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 540, 34560, 540, 135, 540, 34560, 540, 34560, 135, 34560, 135, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 135, 34560, 135, 34560, 540, 540, 135, 540, 34560, 540, 540, 34560, 135, 34560, 34560, 540, 135, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 135, 34560, 540, 34560, 34560, 135, 34560, 540, 540, 34560, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 34560, 540, 135, 135, 135, 135]
Prompts retrieved: 3029535 . Total input tokens: 675594292 . Total output tokens: 594674410
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.09146657772362,
    "estimated_duration": 3600.0330137925753,
    "input_throughput": 7161.835433514038,
    "output_throughput": 6235.4697620818515,
    "total_throughput": 13397.30519559589,
    "itl": 86.04985328163494,
    "ttft": 1997781.319378065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6683594371751096,
    "arrivals": 1008682,
    "finished_requests": 104328,
    "scheduler_time": 302.21637188986347
}
#Debug simulation 
Total elapsed time: 82.09159275284037. Arrivals time: 0.46547021763399243 Scheduler time: 81.40782933030277 Scheduler overhead time: 0.08617920055985451 Adapter cache time: 0.01764705777168274 Engine time: 0.08015891723334789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 66, 540, 66, 34560, 540, 540, 540, 540, 34560, 66, 66, 66, 540, 34560, 66, 34560, 34560, 540, 540, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 66, 66, 34560, 540, 66, 540, 540, 34560, 540, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 34560, 34560, 66, 34560, 66, 540, 34560, 34560, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 66, 34560, 540, 66, 34560, 66, 34560, 66, 66, 540, 540, 34560, 540, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 540, 540, 66, 540, 34560, 540, 540, 34560, 66, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 34560, 540, 66, 66, 66, 66]
Prompts retrieved: 3023670 . Total input tokens: 674282115 . Total output tokens: 593526001
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 88.24868322722614,
    "estimated_duration": 3600.0680995172784,
    "input_throughput": 7271.965217410841,
    "output_throughput": 6330.552192347665,
    "total_throughput": 13602.517409758508,
    "itl": 89.4376580290044,
    "ttft": 1993688.5722966907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.294505934086635,
    "arrivals": 1006798,
    "finished_requests": 105949,
    "scheduler_time": 297.1525501287844
}
#Debug simulation 
Total elapsed time: 88.24880642723292. Arrivals time: 0.4727982650510967 Scheduler time: 87.5607668668963 Scheduler overhead time: 0.08584100101143122 Adapter cache time: 0.016668179538100958 Engine time: 0.07902082335203886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 66, 540, 66, 34560, 540, 540, 540, 540, 34560, 66, 66, 66, 540, 34560, 66, 34560, 34560, 540, 540, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 66, 66, 34560, 540, 66, 540, 540, 34560, 540, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 34560, 34560, 66, 34560, 66, 540, 34560, 34560, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 66, 34560, 540, 66, 34560, 66, 34560, 66, 66, 540, 540, 34560, 540, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 540, 540, 66, 540, 34560, 540, 540, 34560, 66, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 34560, 540, 66, 66, 66, 66]
Prompts retrieved: 3023670 . Total input tokens: 674282115 . Total output tokens: 593526001
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 87.57061221497133,
    "estimated_duration": 3600.0101639703857,
    "input_throughput": 7258.758395054066,
    "output_throughput": 6314.4652277673395,
    "total_throughput": 13573.223622821406,
    "itl": 88.11130173685683,
    "ttft": 1995277.9691150254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6150092513253944,
    "arrivals": 1006798,
    "finished_requests": 105690,
    "scheduler_time": 297.91404454595374
}
#Debug simulation 
Total elapsed time: 87.57074586581439. Arrivals time: 0.47758312663063407 Scheduler time: 86.87511591659859 Scheduler overhead time: 0.0860173194669187 Adapter cache time: 0.017105636652559042 Engine time: 0.08110361220315099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 66, 540, 66, 34560, 540, 540, 540, 540, 34560, 66, 66, 66, 540, 34560, 66, 34560, 34560, 540, 540, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 66, 66, 34560, 540, 66, 540, 540, 34560, 540, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 34560, 34560, 66, 34560, 66, 540, 34560, 34560, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 66, 34560, 540, 66, 34560, 66, 34560, 66, 66, 540, 540, 34560, 540, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 540, 540, 66, 540, 34560, 540, 540, 34560, 66, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 34560, 540, 66, 66, 66, 66]
Prompts retrieved: 3023670 . Total input tokens: 674282115 . Total output tokens: 593526001
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.68593774177134,
    "estimated_duration": 3600.052249702572,
    "input_throughput": 6752.635326892387,
    "output_throughput": 5873.197813100311,
    "total_throughput": 12625.833139992697,
    "itl": 80.37818399610454,
    "ttft": 2016222.907362976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.400260347556333,
    "arrivals": 1006798,
    "finished_requests": 98365,
    "scheduler_time": 321.0729876822047
}
#Debug simulation 
Total elapsed time: 84.68606167985126. Arrivals time: 0.45288070710375905 Scheduler time: 84.00281830038875 Scheduler overhead time: 0.09148394456133246 Adapter cache time: 0.017409771215170622 Engine time: 0.08468122221529484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 66, 540, 66, 34560, 540, 540, 540, 540, 34560, 66, 66, 66, 540, 34560, 66, 34560, 34560, 540, 540, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 66, 66, 34560, 540, 66, 540, 540, 34560, 540, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 34560, 34560, 66, 34560, 66, 540, 34560, 34560, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 66, 34560, 540, 66, 34560, 66, 34560, 66, 66, 540, 540, 34560, 540, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 540, 540, 66, 540, 34560, 540, 540, 34560, 66, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 34560, 540, 66, 66, 66, 66]
Prompts retrieved: 3023670 . Total input tokens: 674282115 . Total output tokens: 593526001
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 87.52897724602371,
    "estimated_duration": 3600.0576508605363,
    "input_throughput": 7258.895144013446,
    "output_throughput": 6314.626932312051,
    "total_throughput": 13573.522076325498,
    "itl": 88.10706415984005,
    "ttft": 1995352.2813192557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4456462925812197,
    "arrivals": 1006798,
    "finished_requests": 105692,
    "scheduler_time": 297.94163617073224
}
#Debug simulation 
Total elapsed time: 87.52911539934576. Arrivals time: 0.4832816915586591 Scheduler time: 86.8287361683324 Scheduler overhead time: 0.08693902613595128 Adapter cache time: 0.01672063721343875 Engine time: 0.07949662022292614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 66, 540, 66, 34560, 540, 540, 540, 540, 34560, 66, 66, 66, 540, 34560, 66, 34560, 34560, 540, 540, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 66, 66, 34560, 540, 66, 540, 540, 34560, 540, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 34560, 34560, 66, 34560, 66, 540, 34560, 34560, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 66, 34560, 540, 66, 34560, 66, 34560, 66, 66, 540, 540, 34560, 540, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 540, 540, 66, 540, 34560, 540, 540, 34560, 66, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 34560, 540, 66, 66, 66, 66]
Prompts retrieved: 3023670 . Total input tokens: 674282115 . Total output tokens: 593526001
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 84.86939189536497,
    "estimated_duration": 3600.0861554796284,
    "input_throughput": 6752.572008033312,
    "output_throughput": 5873.249163167047,
    "total_throughput": 12625.82117120036,
    "itl": 80.37760246008013,
    "ttft": 2016222.200062396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.37809880965856,
    "arrivals": 1006798,
    "finished_requests": 98366,
    "scheduler_time": 321.07484866229464
}
#Debug simulation 
Total elapsed time: 84.8695145631209. Arrivals time: 0.46040590619668365 Scheduler time: 84.17554720398039 Scheduler overhead time: 0.09304166864603758 Adapter cache time: 0.01821101363748312 Engine time: 0.0858209035359323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 66, 540, 66, 34560, 540, 540, 540, 540, 34560, 66, 66, 66, 540, 34560, 66, 34560, 34560, 540, 540, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 66, 66, 34560, 540, 66, 540, 540, 34560, 540, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 34560, 34560, 66, 34560, 66, 540, 34560, 34560, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 66, 34560, 540, 66, 34560, 66, 34560, 66, 66, 540, 540, 34560, 540, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 540, 540, 66, 540, 34560, 540, 540, 34560, 66, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 34560, 540, 66, 66, 66, 66]
Prompts retrieved: 3023670 . Total input tokens: 674282115 . Total output tokens: 593526001
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.64756318600848,
    "estimated_duration": 3600.038334979034,
    "input_throughput": 7258.934091364944,
    "output_throughput": 6314.660813225033,
    "total_throughput": 13573.594904589976,
    "itl": 88.1044449070703,
    "ttft": 1995301.720475884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2790597757836712,
    "arrivals": 1006798,
    "finished_requests": 105692,
    "scheduler_time": 297.94459255256834
}
#Debug simulation 
Total elapsed time: 87.64768981421366. Arrivals time: 0.4853204349055886 Scheduler time: 86.94635777268559 Scheduler overhead time: 0.08564499951899052 Adapter cache time: 0.01729705696925521 Engine time: 0.07930896570906043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 66, 34560, 34560, 66, 34560, 540, 34560, 66, 540, 66, 34560, 540, 540, 540, 540, 34560, 66, 66, 66, 540, 34560, 66, 34560, 34560, 540, 540, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 540, 540, 34560, 540, 34560, 66, 34560, 34560, 540, 66, 66, 34560, 540, 66, 540, 540, 34560, 540, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 34560, 34560, 66, 34560, 66, 540, 34560, 34560, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 66, 34560, 540, 66, 34560, 66, 34560, 66, 66, 540, 540, 34560, 540, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 66, 540, 34560, 34560, 66, 34560, 540, 66, 540, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 66, 540, 540, 66, 540, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 540, 34560, 540, 66, 540, 34560, 540, 34560, 66, 34560, 66, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 66, 34560, 66, 34560, 540, 540, 66, 540, 34560, 540, 540, 34560, 66, 34560, 34560, 540, 66, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 66, 34560, 540, 34560, 34560, 66, 34560, 540, 540, 34560, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 34560, 540, 66, 66, 66, 66]
Prompts retrieved: 3023670 . Total input tokens: 674282115 . Total output tokens: 593526001
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.54882371332496,
    "estimated_duration": 3600.091286292168,
    "input_throughput": 6752.562384338139,
    "output_throughput": 5873.240792673619,
    "total_throughput": 12625.803177011758,
    "itl": 80.37721099799431,
    "ttft": 2016234.335579648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3555230374075564,
    "arrivals": 1006798,
    "finished_requests": 98366,
    "scheduler_time": 321.07952943937426
}
#Debug simulation 
Total elapsed time: 84.54894825117663. Arrivals time: 0.5141525771468878 Scheduler time: 83.80563345504925 Scheduler overhead time: 0.09133019717410207 Adapter cache time: 0.017668443731963634 Engine time: 0.08406589971855283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 33, 540, 33, 34560, 540, 540, 540, 540, 34560, 33, 33, 33, 540, 34560, 33, 34560, 34560, 540, 540, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 33, 33, 34560, 540, 33, 540, 540, 34560, 540, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 34560, 34560, 33, 34560, 33, 540, 34560, 34560, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 33, 34560, 540, 33, 34560, 33, 34560, 33, 33, 540, 540, 34560, 540, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 540, 540, 33, 540, 34560, 540, 540, 34560, 33, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 34560, 540, 33, 33, 33, 33]
Prompts retrieved: 3020865 . Total input tokens: 673650492 . Total output tokens: 592979682
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.85123977158219,
    "estimated_duration": 3600.100085937239,
    "input_throughput": 7301.87644023691,
    "output_throughput": 6372.56477663381,
    "total_throughput": 13674.44121687072,
    "itl": 90.01991291140038,
    "ttft": 1987653.2685944391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.294505934086635,
    "arrivals": 1005871,
    "finished_requests": 106692,
    "scheduler_time": 295.1134816616398
}
#Debug simulation 
Total elapsed time: 86.85136851388961. Arrivals time: 0.4868762451224029 Scheduler time: 86.15237079048529 Scheduler overhead time: 0.08408232778310776 Adapter cache time: 0.016398908104747534 Engine time: 0.07868263125419617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 33, 540, 33, 34560, 540, 540, 540, 540, 34560, 33, 33, 33, 540, 34560, 33, 34560, 34560, 540, 540, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 33, 33, 34560, 540, 33, 540, 540, 34560, 540, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 34560, 34560, 33, 34560, 33, 540, 34560, 34560, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 33, 34560, 540, 33, 34560, 33, 34560, 33, 33, 540, 540, 34560, 540, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 540, 540, 33, 540, 34560, 540, 540, 34560, 33, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 34560, 540, 33, 33, 33, 33]
Prompts retrieved: 3020865 . Total input tokens: 673650492 . Total output tokens: 592979682
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.13093791808933,
    "estimated_duration": 3600.0222654140925,
    "input_throughput": 7311.876443900885,
    "output_throughput": 6409.75452337814,
    "total_throughput": 13721.630967279025,
    "itl": 89.31989247491089,
    "ttft": 1984228.9410879784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.328554368363696,
    "arrivals": 1005871,
    "finished_requests": 106981,
    "scheduler_time": 294.55293094935905
}
#Debug simulation 
Total elapsed time: 82.13105402281508. Arrivals time: 0.4714037589728832 Scheduler time: 81.44758867332712 Scheduler overhead time: 0.08465962577611208 Adapter cache time: 0.01612082589417696 Engine time: 0.07782203704118729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 33, 540, 33, 34560, 540, 540, 540, 540, 34560, 33, 33, 33, 540, 34560, 33, 34560, 34560, 540, 540, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 33, 33, 34560, 540, 33, 540, 540, 34560, 540, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 34560, 34560, 33, 34560, 33, 540, 34560, 34560, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 33, 34560, 540, 33, 34560, 33, 34560, 33, 33, 540, 540, 34560, 540, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 540, 540, 33, 540, 34560, 540, 540, 34560, 33, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 34560, 540, 33, 33, 33, 33]
Prompts retrieved: 3020865 . Total input tokens: 673650492 . Total output tokens: 592979682
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 79.79427967593074,
    "estimated_duration": 3600.0222106173514,
    "input_throughput": 7220.526285459333,
    "output_throughput": 6304.446381764945,
    "total_throughput": 13524.972667224278,
    "itl": 87.05020891548652,
    "ttft": 1995770.3552901568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4291470282618075,
    "arrivals": 1005871,
    "finished_requests": 105497,
    "scheduler_time": 298.57515542944736
}
#Debug simulation 
Total elapsed time: 79.79440589109436. Arrivals time: 0.47084010811522603 Scheduler time: 79.11083140829578 Scheduler overhead time: 0.08409663522616029 Adapter cache time: 0.016180966515094042 Engine time: 0.07877030689269304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 33, 540, 33, 34560, 540, 540, 540, 540, 34560, 33, 33, 33, 540, 34560, 33, 34560, 34560, 540, 540, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 33, 33, 34560, 540, 33, 540, 540, 34560, 540, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 34560, 34560, 33, 34560, 33, 540, 34560, 34560, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 33, 34560, 540, 33, 34560, 33, 34560, 33, 33, 540, 540, 34560, 540, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 540, 540, 33, 540, 34560, 540, 540, 34560, 33, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 34560, 540, 33, 33, 33, 33]
Prompts retrieved: 3020865 . Total input tokens: 673650492 . Total output tokens: 592979682
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 83.93112698663026,
    "estimated_duration": 3600.037482481919,
    "input_throughput": 7282.030292064234,
    "output_throughput": 6366.762321651777,
    "total_throughput": 13648.79261371601,
    "itl": 89.28451539491809,
    "ttft": 1988543.5391614768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.362929173475128,
    "arrivals": 1005871,
    "finished_requests": 106536,
    "scheduler_time": 295.3871257921435
}
#Debug simulation 
Total elapsed time: 83.93125046463683. Arrivals time: 0.4742956547997892 Scheduler time: 83.24460636544973 Scheduler overhead time: 0.08413019683212042 Adapter cache time: 0.016597378067672253 Engine time: 0.07842018594965339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 33, 540, 33, 34560, 540, 540, 540, 540, 34560, 33, 33, 33, 540, 34560, 33, 34560, 34560, 540, 540, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 33, 33, 34560, 540, 33, 540, 540, 34560, 540, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 34560, 34560, 33, 34560, 33, 540, 34560, 34560, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 33, 34560, 540, 33, 34560, 33, 34560, 33, 33, 540, 540, 34560, 540, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 540, 540, 33, 540, 34560, 540, 540, 34560, 33, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 34560, 540, 33, 33, 33, 33]
Prompts retrieved: 3020865 . Total input tokens: 673650492 . Total output tokens: 592979682
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 79.9184975172393,
    "estimated_duration": 3600.0408973209187,
    "input_throughput": 7220.599915780005,
    "output_throughput": 6307.696397810045,
    "total_throughput": 13528.29631359005,
    "itl": 87.09749935221933,
    "ttft": 1996107.8530412377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4076587108755603,
    "arrivals": 1005871,
    "finished_requests": 105562,
    "scheduler_time": 298.35476452855664
}
#Debug simulation 
Total elapsed time: 79.91862315917388. Arrivals time: 0.46482594031840563 Scheduler time: 79.23935535084456 Scheduler overhead time: 0.0854109413921833 Adapter cache time: 0.01636303076520562 Engine time: 0.07878561317920685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 33, 540, 33, 34560, 540, 540, 540, 540, 34560, 33, 33, 33, 540, 34560, 33, 34560, 34560, 540, 540, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 33, 33, 34560, 540, 33, 540, 540, 34560, 540, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 34560, 34560, 33, 34560, 33, 540, 34560, 34560, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 33, 34560, 540, 33, 34560, 33, 34560, 33, 33, 540, 540, 34560, 540, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 540, 540, 33, 540, 34560, 540, 540, 34560, 33, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 34560, 540, 33, 33, 33, 33]
Prompts retrieved: 3020865 . Total input tokens: 673650492 . Total output tokens: 592979682
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.04299775697291,
    "estimated_duration": 3600.00727670226,
    "input_throughput": 7274.3611296222025,
    "output_throughput": 6355.143820975166,
    "total_throughput": 13629.504950597367,
    "itl": 89.10363573463768,
    "ttft": 1990956.4730555464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2024527244968253,
    "arrivals": 1005871,
    "finished_requests": 106336,
    "scheduler_time": 296.01479163624657
}
#Debug simulation 
Total elapsed time: 83.04311404190958. Arrivals time: 0.4670425755903125 Scheduler time: 82.36193169699982 Scheduler overhead time: 0.08562059327960014 Adapter cache time: 0.016485911794006824 Engine time: 0.07868993375450373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [540, 540, 34560, 33, 34560, 34560, 33, 34560, 540, 34560, 33, 540, 33, 34560, 540, 540, 540, 540, 34560, 33, 33, 33, 540, 34560, 33, 34560, 34560, 540, 540, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 540, 540, 34560, 540, 34560, 33, 34560, 34560, 540, 33, 33, 34560, 540, 33, 540, 540, 34560, 540, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 34560, 34560, 33, 34560, 33, 540, 34560, 34560, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 33, 34560, 540, 33, 34560, 33, 34560, 33, 33, 540, 540, 34560, 540, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 33, 540, 34560, 34560, 33, 34560, 540, 33, 540, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 33, 540, 540, 33, 540, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 540, 34560, 540, 33, 540, 34560, 540, 34560, 33, 34560, 33, 540, 34560, 540, 34560, 540, 34560, 540, 34560, 540, 33, 34560, 33, 34560, 540, 540, 33, 540, 34560, 540, 540, 34560, 33, 34560, 34560, 540, 33, 34560, 34560, 540, 540, 34560, 34560, 540, 540, 540, 540, 34560, 34560, 540, 540, 34560, 33, 34560, 540, 34560, 34560, 33, 34560, 540, 540, 34560, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 34560, 540, 33, 33, 33, 33]
Prompts retrieved: 3020865 . Total input tokens: 673650492 . Total output tokens: 592979682
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 79.02406647987664,
    "estimated_duration": 3600.0879093719973,
    "input_throughput": 7223.796100172611,
    "output_throughput": 6305.428526038361,
    "total_throughput": 13529.224626210973,
    "itl": 87.0750864574848,
    "ttft": 1996589.1034915566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 327,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4154713085480157,
    "arrivals": 1005871,
    "finished_requests": 105552,
    "scheduler_time": 298.50636061873445
}
#Debug simulation 
Total elapsed time: 79.02418370405212. Arrivals time: 0.466997061856091 Scheduler time: 78.34396235411987 Scheduler overhead time: 0.08410495845600963 Adapter cache time: 0.0164930522441864 Engine time: 0.0789823867380619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 135, 270, 135, 34560, 270, 270, 270, 270, 34560, 135, 135, 135, 270, 34560, 135, 34560, 34560, 270, 270, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 135, 135, 34560, 270, 135, 270, 270, 34560, 270, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 34560, 34560, 135, 34560, 135, 270, 34560, 34560, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 135, 34560, 270, 135, 34560, 135, 34560, 135, 135, 270, 270, 34560, 270, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 270, 270, 135, 270, 34560, 270, 270, 34560, 135, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 34560, 270, 135, 135, 135, 135]
Prompts retrieved: 3006585 . Total input tokens: 670482214 . Total output tokens: 590155727
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.57274561421946,
    "estimated_duration": 3600.052135173277,
    "input_throughput": 7221.83319124311,
    "output_throughput": 6325.38145142833,
    "total_throughput": 13547.21464267144,
    "itl": 89.81834082030068,
    "ttft": 1988065.7994450652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4796533869812394,
    "arrivals": 1001073,
    "finished_requests": 105645,
    "scheduler_time": 297.5341386580941
}
#Debug simulation 
Total elapsed time: 85.57286753924564. Arrivals time: 0.5808791019953787 Scheduler time: 84.7779123717919 Scheduler overhead time: 0.08521742932498455 Adapter cache time: 0.01734999567270279 Engine time: 0.07810841780155897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 135, 270, 135, 34560, 270, 270, 270, 270, 34560, 135, 135, 135, 270, 34560, 135, 34560, 34560, 270, 270, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 135, 135, 34560, 270, 135, 270, 270, 34560, 270, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 34560, 34560, 135, 34560, 135, 270, 34560, 34560, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 135, 34560, 270, 135, 34560, 135, 34560, 135, 135, 270, 270, 34560, 270, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 270, 270, 135, 270, 34560, 270, 270, 34560, 135, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 34560, 270, 135, 135, 135, 135]
Prompts retrieved: 3006585 . Total input tokens: 670482214 . Total output tokens: 590155727
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.27109064906836,
    "estimated_duration": 3600.099322228834,
    "input_throughput": 7135.114256819174,
    "output_throughput": 6245.136310872843,
    "total_throughput": 13380.250567692017,
    "itl": 87.60791572470572,
    "ttft": 1999788.1081601428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6441524140816215,
    "arrivals": 1001073,
    "finished_requests": 104366,
    "scheduler_time": 301.544381243968
}
#Debug simulation 
Total elapsed time: 86.27121347282082. Arrivals time: 0.47107228403910995 Scheduler time: 85.58347095968202 Scheduler overhead time: 0.08629808342084289 Adapter cache time: 0.0174187277443707 Engine time: 0.0790296639315784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 135, 270, 135, 34560, 270, 270, 270, 270, 34560, 135, 135, 135, 270, 34560, 135, 34560, 34560, 270, 270, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 135, 135, 34560, 270, 135, 270, 270, 34560, 270, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 34560, 34560, 135, 34560, 135, 270, 34560, 34560, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 135, 34560, 270, 135, 34560, 135, 34560, 135, 135, 270, 270, 34560, 270, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 270, 270, 135, 270, 34560, 270, 270, 34560, 135, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 34560, 270, 135, 135, 135, 135]
Prompts retrieved: 3006585 . Total input tokens: 670482214 . Total output tokens: 590155727
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 78.96773215010762,
    "estimated_duration": 3600.0171576559314,
    "input_throughput": 7181.8282712948785,
    "output_throughput": 6300.471360744496,
    "total_throughput": 13482.299632039374,
    "itl": 87.0312984796611,
    "ttft": 1990525.054593148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7138233333313977,
    "arrivals": 1001073,
    "finished_requests": 105149,
    "scheduler_time": 298.65919405611044
}
#Debug simulation 
Total elapsed time: 78.96785024506971. Arrivals time: 0.4747159853577614 Scheduler time: 78.27652516635135 Scheduler overhead time: 0.08599385945126414 Adapter cache time: 0.016977332066744566 Engine time: 0.07985861459746957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 135, 270, 135, 34560, 270, 270, 270, 270, 34560, 135, 135, 135, 270, 34560, 135, 34560, 34560, 270, 270, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 135, 135, 34560, 270, 135, 270, 270, 34560, 270, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 34560, 34560, 135, 34560, 135, 270, 34560, 34560, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 135, 34560, 270, 135, 34560, 135, 34560, 135, 135, 270, 270, 34560, 270, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 270, 270, 135, 270, 34560, 270, 270, 34560, 135, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 34560, 270, 135, 135, 135, 135]
Prompts retrieved: 3006585 . Total input tokens: 670482214 . Total output tokens: 590155727
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 85.13130091689527,
    "estimated_duration": 3600.0439102947976,
    "input_throughput": 7255.119562655892,
    "output_throughput": 6350.390875684602,
    "total_throughput": 13605.510438340494,
    "itl": 88.86484187222253,
    "ttft": 1987899.5148659383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.474789455337447,
    "arrivals": 1001073,
    "finished_requests": 106035,
    "scheduler_time": 296.2074998859203
}
#Debug simulation 
Total elapsed time: 85.13142916699871. Arrivals time: 0.47753556771203876 Scheduler time: 84.43922383338213 Scheduler overhead time: 0.08497915789484978 Adapter cache time: 0.0170610174536705 Engine time: 0.07957558566704392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 135, 270, 135, 34560, 270, 270, 270, 270, 34560, 135, 135, 135, 270, 34560, 135, 34560, 34560, 270, 270, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 135, 135, 34560, 270, 135, 270, 270, 34560, 270, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 34560, 34560, 135, 34560, 135, 270, 34560, 34560, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 135, 34560, 270, 135, 34560, 135, 34560, 135, 135, 270, 270, 34560, 270, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 270, 270, 135, 270, 34560, 270, 270, 34560, 135, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 34560, 270, 135, 135, 135, 135]
Prompts retrieved: 3006585 . Total input tokens: 670482214 . Total output tokens: 590155727
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 81.4425094933249,
    "estimated_duration": 3600.001178871123,
    "input_throughput": 7179.702371126723,
    "output_throughput": 6308.35793423861,
    "total_throughput": 13488.060305365334,
    "itl": 87.07118454655169,
    "ttft": 1985086.8540546373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7013414407242227,
    "arrivals": 1001073,
    "finished_requests": 105098,
    "scheduler_time": 298.72306050916126
}
#Debug simulation 
Total elapsed time: 81.44263427518308. Arrivals time: 0.476582785602659 Scheduler time: 80.75134600419551 Scheduler overhead time: 0.08510547643527389 Adapter cache time: 0.017104311380535364 Engine time: 0.07855344889685512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 135, 270, 135, 34560, 270, 270, 270, 270, 34560, 135, 135, 135, 270, 34560, 135, 34560, 34560, 270, 270, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 135, 135, 34560, 270, 135, 270, 270, 34560, 270, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 34560, 34560, 135, 34560, 135, 270, 34560, 34560, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 135, 34560, 270, 135, 34560, 135, 34560, 135, 135, 270, 270, 34560, 270, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 270, 270, 135, 270, 34560, 270, 270, 34560, 135, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 34560, 270, 135, 135, 135, 135]
Prompts retrieved: 3006585 . Total input tokens: 670482214 . Total output tokens: 590155727
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.68979739584029,
    "estimated_duration": 3600.0758330017457,
    "input_throughput": 7176.724657618475,
    "output_throughput": 6278.1616411548075,
    "total_throughput": 13454.886298773283,
    "itl": 88.58826238341513,
    "ttft": 1986714.7042507662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.368434668951658,
    "arrivals": 1001073,
    "finished_requests": 104901,
    "scheduler_time": 299.87051404732773
}
#Debug simulation 
Total elapsed time: 84.68991183396429. Arrivals time: 0.5948096793144941 Scheduler time: 83.88087053364143 Scheduler overhead time: 0.08463437994942069 Adapter cache time: 0.01710488647222519 Engine time: 0.07870603539049625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 135, 34560, 34560, 135, 34560, 270, 34560, 135, 270, 135, 34560, 270, 270, 270, 270, 34560, 135, 135, 135, 270, 34560, 135, 34560, 34560, 270, 270, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 270, 270, 34560, 270, 34560, 135, 34560, 34560, 270, 135, 135, 34560, 270, 135, 270, 270, 34560, 270, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 34560, 34560, 135, 34560, 135, 270, 34560, 34560, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 135, 34560, 270, 135, 34560, 135, 34560, 135, 135, 270, 270, 34560, 270, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 135, 270, 34560, 34560, 135, 34560, 270, 135, 270, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 135, 270, 270, 135, 270, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 270, 34560, 270, 135, 270, 34560, 270, 34560, 135, 34560, 135, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 135, 34560, 135, 34560, 270, 270, 135, 270, 34560, 270, 270, 34560, 135, 34560, 34560, 270, 135, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 135, 34560, 270, 34560, 34560, 135, 34560, 270, 270, 34560, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 34560, 270, 135, 135, 135, 135]
Prompts retrieved: 3006585 . Total input tokens: 670482214 . Total output tokens: 590155727
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.36282127304003,
    "estimated_duration": 3600.0494262835764,
    "input_throughput": 7138.738932962534,
    "output_throughput": 6258.276299072484,
    "total_throughput": 13397.015232035017,
    "itl": 86.56613571630857,
    "ttft": 1988415.4210882587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.794623246435091,
    "arrivals": 1001073,
    "finished_requests": 104467,
    "scheduler_time": 300.93895845223454
}
#Debug simulation 
Total elapsed time: 83.36293844506145. Arrivals time: 0.5883351983502507 Scheduler time: 82.5589139717631 Scheduler overhead time: 0.08575681084766984 Adapter cache time: 0.01753793889656663 Engine time: 0.07892744289711118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 66, 270, 66, 34560, 270, 270, 270, 270, 34560, 66, 66, 66, 270, 34560, 66, 34560, 34560, 270, 270, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 66, 66, 34560, 270, 66, 270, 270, 34560, 270, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 34560, 34560, 66, 34560, 66, 270, 34560, 34560, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 66, 34560, 270, 66, 34560, 66, 34560, 66, 66, 270, 270, 34560, 270, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 270, 270, 66, 270, 34560, 270, 270, 34560, 66, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 34560, 270, 66, 66, 66, 66]
Prompts retrieved: 3000720 . Total input tokens: 669169615 . Total output tokens: 588984119
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.37678813142702,
    "estimated_duration": 3600.0043873290197,
    "input_throughput": 7356.68158994927,
    "output_throughput": 6415.937180881283,
    "total_throughput": 13772.618770830553,
    "itl": 90.26874069508102,
    "ttft": 1986449.5186986863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.327567979246386,
    "arrivals": 999127,
    "finished_requests": 107214,
    "scheduler_time": 292.75985619330385
}
#Debug simulation 
Total elapsed time: 82.3769189720042. Arrivals time: 0.4821391864679754 Scheduler time: 81.68326783087105 Scheduler overhead time: 0.08437420381233096 Adapter cache time: 0.016810859087854624 Engine time: 0.07722922135144472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 66, 270, 66, 34560, 270, 270, 270, 270, 34560, 66, 66, 66, 270, 34560, 66, 34560, 34560, 270, 270, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 66, 66, 34560, 270, 66, 270, 270, 34560, 270, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 34560, 34560, 66, 34560, 66, 270, 34560, 34560, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 66, 34560, 270, 66, 34560, 66, 34560, 66, 66, 270, 270, 34560, 270, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 270, 270, 66, 270, 34560, 270, 270, 34560, 66, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 34560, 270, 66, 66, 66, 66]
Prompts retrieved: 3000720 . Total input tokens: 669169615 . Total output tokens: 588984119
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.45688309520483,
    "estimated_duration": 3600.0333823048954,
    "input_throughput": 7203.99042060981,
    "output_throughput": 6275.60597383555,
    "total_throughput": 13479.59639444536,
    "itl": 88.13828122379748,
    "ttft": 1995657.0069950263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.582532462696548,
    "arrivals": 999127,
    "finished_requests": 104993,
    "scheduler_time": 300.06351601083463
}
#Debug simulation 
Total elapsed time: 86.4570023259148. Arrivals time: 0.5488513098098338 Scheduler time: 85.68960527563468 Scheduler overhead time: 0.08732256293296814 Adapter cache time: 0.017333547584712505 Engine time: 0.07971782609820366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 66, 270, 66, 34560, 270, 270, 270, 270, 34560, 66, 66, 66, 270, 34560, 66, 34560, 34560, 270, 270, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 66, 66, 34560, 270, 66, 270, 270, 34560, 270, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 34560, 34560, 66, 34560, 66, 270, 34560, 34560, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 66, 34560, 270, 66, 34560, 66, 34560, 66, 66, 270, 270, 34560, 270, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 270, 270, 66, 270, 34560, 270, 270, 34560, 66, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 34560, 270, 66, 66, 66, 66]
Prompts retrieved: 3000720 . Total input tokens: 669169615 . Total output tokens: 588984119
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.82593003986403,
    "estimated_duration": 3600.05242989281,
    "input_throughput": 7197.657952101414,
    "output_throughput": 6286.593165165059,
    "total_throughput": 13484.251117266473,
    "itl": 86.74425217862184,
    "ttft": 1987998.0518363076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 423,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.180363754606829,
    "arrivals": 999127,
    "finished_requests": 104920,
    "scheduler_time": 299.44947659499576
}
#Debug simulation 
Total elapsed time: 81.8260558010079. Arrivals time: 0.4722220627591014 Scheduler time: 81.13649293920025 Scheduler overhead time: 0.08582546934485435 Adapter cache time: 0.01762534212321043 Engine time: 0.07963884202763438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 66, 270, 66, 34560, 270, 270, 270, 270, 34560, 66, 66, 66, 270, 34560, 66, 34560, 34560, 270, 270, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 66, 66, 34560, 270, 66, 270, 270, 34560, 270, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 34560, 34560, 66, 34560, 66, 270, 34560, 34560, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 66, 34560, 270, 66, 34560, 66, 34560, 66, 66, 270, 270, 34560, 270, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 270, 270, 66, 270, 34560, 270, 270, 34560, 66, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 34560, 270, 66, 66, 66, 66]
Prompts retrieved: 3000720 . Total input tokens: 669169615 . Total output tokens: 588984119
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.77701744204387,
    "estimated_duration": 3600.041431006922,
    "input_throughput": 7303.039285480226,
    "output_throughput": 6375.812734349575,
    "total_throughput": 13678.852019829801,
    "itl": 88.862718848974,
    "ttft": 1988892.8027598944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.404009141065177,
    "arrivals": 999127,
    "finished_requests": 106487,
    "scheduler_time": 294.8888198581436
}
#Debug simulation 
Total elapsed time: 84.77714742906392. Arrivals time: 0.4720302908681333 Scheduler time: 84.09084545029327 Scheduler overhead time: 0.08483540872111917 Adapter cache time: 0.01696454267948866 Engine time: 0.0788670964539051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 66, 270, 66, 34560, 270, 270, 270, 270, 34560, 66, 66, 66, 270, 34560, 66, 34560, 34560, 270, 270, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 66, 66, 34560, 270, 66, 270, 270, 34560, 270, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 34560, 34560, 66, 34560, 66, 270, 34560, 34560, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 66, 34560, 270, 66, 34560, 66, 34560, 66, 66, 270, 270, 34560, 270, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 270, 270, 66, 270, 34560, 270, 270, 34560, 66, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 34560, 270, 66, 66, 66, 66]
Prompts retrieved: 3000720 . Total input tokens: 669169615 . Total output tokens: 588984119
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 85.99409189308062,
    "estimated_duration": 3600.0204714233714,
    "input_throughput": 7002.352403299821,
    "output_throughput": 6104.928339840917,
    "total_throughput": 13107.280743140738,
    "itl": 85.4205176048075,
    "ttft": 2000957.2828302484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.460359377348808,
    "arrivals": 999127,
    "finished_requests": 101998,
    "scheduler_time": 308.9327629218626
}
#Debug simulation 
Total elapsed time: 85.99421236524358. Arrivals time: 0.47154948441311717 Scheduler time: 85.29858016874641 Scheduler overhead time: 0.08973186975345016 Adapter cache time: 0.017225750721991062 Engine time: 0.08185647195205092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 66, 270, 66, 34560, 270, 270, 270, 270, 34560, 66, 66, 66, 270, 34560, 66, 34560, 34560, 270, 270, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 66, 66, 34560, 270, 66, 270, 270, 34560, 270, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 34560, 34560, 66, 34560, 66, 270, 34560, 34560, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 66, 34560, 270, 66, 34560, 66, 34560, 66, 66, 270, 270, 34560, 270, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 270, 270, 66, 270, 34560, 270, 270, 34560, 66, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 34560, 270, 66, 66, 66, 66]
Prompts retrieved: 3000720 . Total input tokens: 669169615 . Total output tokens: 588984119
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.97124255076051,
    "estimated_duration": 3600.071635692449,
    "input_throughput": 7295.863154386368,
    "output_throughput": 6362.567836957568,
    "total_throughput": 13658.430991343936,
    "itl": 88.82055220139588,
    "ttft": 1989380.1859151674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.368434668951658,
    "arrivals": 999127,
    "finished_requests": 106342,
    "scheduler_time": 295.7286630496026
}
#Debug simulation 
Total elapsed time: 83.97136980388314. Arrivals time: 0.555172162130475 Scheduler time: 83.20099927205592 Scheduler overhead time: 0.08503947453573346 Adapter cache time: 0.01704923016950488 Engine time: 0.07968309428542852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 66, 34560, 34560, 66, 34560, 270, 34560, 66, 270, 66, 34560, 270, 270, 270, 270, 34560, 66, 66, 66, 270, 34560, 66, 34560, 34560, 270, 270, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 270, 270, 34560, 270, 34560, 66, 34560, 34560, 270, 66, 66, 34560, 270, 66, 270, 270, 34560, 270, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 34560, 34560, 66, 34560, 66, 270, 34560, 34560, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 66, 34560, 270, 66, 34560, 66, 34560, 66, 66, 270, 270, 34560, 270, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 66, 270, 34560, 34560, 66, 34560, 270, 66, 270, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 66, 270, 270, 66, 270, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 270, 34560, 270, 66, 270, 34560, 270, 34560, 66, 34560, 66, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 66, 34560, 66, 34560, 270, 270, 66, 270, 34560, 270, 270, 34560, 66, 34560, 34560, 270, 66, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 66, 34560, 270, 34560, 34560, 66, 34560, 270, 270, 34560, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 34560, 270, 66, 66, 66, 66]
Prompts retrieved: 3000720 . Total input tokens: 669169615 . Total output tokens: 588984119
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.51133398897946,
    "estimated_duration": 3600.09954782294,
    "input_throughput": 7198.351783264227,
    "output_throughput": 6282.398222481697,
    "total_throughput": 13480.750005745924,
    "itl": 86.66434240819862,
    "ttft": 1990725.9232334706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8770372070558556,
    "arrivals": 999127,
    "finished_requests": 104906,
    "scheduler_time": 299.7121719871609
}
#Debug simulation 
Total elapsed time: 83.5114562031813. Arrivals time: 0.47831192798912525 Scheduler time: 82.81329118646681 Scheduler overhead time: 0.08786731958389282 Adapter cache time: 0.01732747070491314 Engine time: 0.0801421394571662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 33, 270, 33, 34560, 270, 270, 270, 270, 34560, 33, 33, 33, 270, 34560, 33, 34560, 34560, 270, 270, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 33, 33, 34560, 270, 33, 270, 270, 34560, 270, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 34560, 34560, 33, 34560, 33, 270, 34560, 34560, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 33, 34560, 270, 33, 34560, 33, 34560, 33, 33, 270, 270, 34560, 270, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 270, 270, 33, 270, 34560, 270, 270, 34560, 33, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 34560, 270, 33, 33, 33, 33]
Prompts retrieved: 2997915 . Total input tokens: 668555089 . Total output tokens: 588429336
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.95118562690914,
    "estimated_duration": 3600.0085042405644,
    "input_throughput": 7373.502859432745,
    "output_throughput": 6412.93818411971,
    "total_throughput": 13786.441043552455,
    "itl": 90.02644013136033,
    "ttft": 1975458.1196937035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3011183431185853,
    "arrivals": 998127,
    "finished_requests": 107639,
    "scheduler_time": 293.09903892297206
}
#Debug simulation 
Total elapsed time: 84.95130958408117. Arrivals time: 0.4736520694568753 Scheduler time: 84.26411687303334 Scheduler overhead time: 0.08530384721234441 Adapter cache time: 0.016596072353422642 Engine time: 0.07828346034511924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 33, 270, 33, 34560, 270, 270, 270, 270, 34560, 33, 33, 33, 270, 34560, 33, 34560, 34560, 270, 270, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 33, 33, 34560, 270, 33, 270, 270, 34560, 270, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 34560, 34560, 33, 34560, 33, 270, 34560, 34560, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 33, 34560, 270, 33, 34560, 33, 34560, 33, 33, 270, 270, 34560, 270, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 270, 270, 33, 270, 34560, 270, 270, 34560, 33, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 34560, 270, 33, 33, 33, 33]
Prompts retrieved: 2997915 . Total input tokens: 668555089 . Total output tokens: 588429336
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.87038276717067,
    "estimated_duration": 3600.016786247731,
    "input_throughput": 7331.195537982093,
    "output_throughput": 6380.766080799959,
    "total_throughput": 13711.961618782052,
    "itl": 88.93593255984133,
    "ttft": 1980716.8115242303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6299963512271676,
    "arrivals": 998127,
    "finished_requests": 107100,
    "scheduler_time": 294.6561989026969
}
#Debug simulation 
Total elapsed time: 85.87050838209689. Arrivals time: 0.48547625774517655 Scheduler time: 85.16981116496027 Scheduler overhead time: 0.08549611503258348 Adapter cache time: 0.017109648790210485 Engine time: 0.07908517168834805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 33, 270, 33, 34560, 270, 270, 270, 270, 34560, 33, 33, 33, 270, 34560, 33, 34560, 34560, 270, 270, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 33, 33, 34560, 270, 33, 270, 270, 34560, 270, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 34560, 34560, 33, 34560, 33, 270, 34560, 34560, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 33, 34560, 270, 33, 34560, 33, 34560, 33, 33, 270, 270, 34560, 270, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 270, 270, 33, 270, 34560, 270, 270, 34560, 33, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 34560, 270, 33, 33, 33, 33]
Prompts retrieved: 2997915 . Total input tokens: 668555089 . Total output tokens: 588429336
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.58423728076741,
    "estimated_duration": 3600.0963753869996,
    "input_throughput": 7205.565433567442,
    "output_throughput": 6268.995506425547,
    "total_throughput": 13474.560939992989,
    "itl": 86.42094900352838,
    "ttft": 1990515.4456555464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 334,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5043133842153593,
    "arrivals": 998127,
    "finished_requests": 105230,
    "scheduler_time": 300.5406078667263
}
#Debug simulation 
Total elapsed time: 84.58435829496011. Arrivals time: 0.4657653821632266 Scheduler time: 83.90364549169317 Scheduler overhead time: 0.08558835415169597 Adapter cache time: 0.016680835280567408 Engine time: 0.07874692231416702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 33, 270, 33, 34560, 270, 270, 270, 270, 34560, 33, 33, 33, 270, 34560, 33, 34560, 34560, 270, 270, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 33, 33, 34560, 270, 33, 270, 270, 34560, 270, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 34560, 34560, 33, 34560, 33, 270, 34560, 34560, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 33, 34560, 270, 33, 34560, 33, 34560, 33, 33, 270, 270, 34560, 270, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 270, 270, 33, 270, 34560, 270, 270, 34560, 33, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 34560, 270, 33, 33, 33, 33]
Prompts retrieved: 2997915 . Total input tokens: 668555089 . Total output tokens: 588429336
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.63953054510057,
    "estimated_duration": 3600.0707540693647,
    "input_throughput": 7319.799748439129,
    "output_throughput": 6382.7934420527945,
    "total_throughput": 13702.593190491923,
    "itl": 88.8992858590679,
    "ttft": 1975603.7294413056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3557142154872386,
    "arrivals": 998127,
    "finished_requests": 106962,
    "scheduler_time": 295.62959947714245
}
#Debug simulation 
Total elapsed time: 84.63965572789311. Arrivals time: 0.4767723665572703 Scheduler time: 83.94674898823723 Scheduler overhead time: 0.0861119907349348 Adapter cache time: 0.017133325338363647 Engine time: 0.07898890366777778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 33, 270, 33, 34560, 270, 270, 270, 270, 34560, 33, 33, 33, 270, 34560, 33, 34560, 34560, 270, 270, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 33, 33, 34560, 270, 33, 270, 270, 34560, 270, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 34560, 34560, 33, 34560, 33, 270, 34560, 34560, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 33, 34560, 270, 33, 34560, 33, 34560, 33, 33, 270, 270, 34560, 270, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 270, 270, 33, 270, 34560, 270, 270, 34560, 33, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 34560, 270, 33, 33, 33, 33]
Prompts retrieved: 2997915 . Total input tokens: 668555089 . Total output tokens: 588429336
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 84.82967462344095,
    "estimated_duration": 3600.0840552354616,
    "input_throughput": 7233.584994252799,
    "output_throughput": 6293.277226972462,
    "total_throughput": 13526.862221225261,
    "itl": 86.52164140350713,
    "ttft": 1984374.0802857322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.564619531184448,
    "arrivals": 998127,
    "finished_requests": 105556,
    "scheduler_time": 299.404424802711
}
#Debug simulation 
Total elapsed time: 84.82979230722412. Arrivals time: 0.47192879719659686 Scheduler time: 84.14144737459719 Scheduler overhead time: 0.08573146723210812 Adapter cache time: 0.01700973091647029 Engine time: 0.07961250748485327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 33, 270, 33, 34560, 270, 270, 270, 270, 34560, 33, 33, 33, 270, 34560, 33, 34560, 34560, 270, 270, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 33, 33, 34560, 270, 33, 270, 270, 34560, 270, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 34560, 34560, 33, 34560, 33, 270, 34560, 34560, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 33, 34560, 270, 33, 34560, 33, 34560, 33, 33, 270, 270, 34560, 270, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 270, 270, 33, 270, 34560, 270, 270, 34560, 33, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 34560, 270, 33, 33, 33, 33]
Prompts retrieved: 2997915 . Total input tokens: 668555089 . Total output tokens: 588429336
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.3993051731959,
    "estimated_duration": 3600.0303586231485,
    "input_throughput": 7337.297291599055,
    "output_throughput": 6388.576958777681,
    "total_throughput": 13725.874250376735,
    "itl": 89.04917718340673,
    "ttft": 1982747.19722695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3237472223676647,
    "arrivals": 998127,
    "finished_requests": 107244,
    "scheduler_time": 294.4099898119503
}
#Debug simulation 
Total elapsed time: 85.39942946983501. Arrivals time: 0.47592724626883864 Scheduler time: 84.70790151739493 Scheduler overhead time: 0.08627333119511604 Adapter cache time: 0.0172934764996171 Engine time: 0.07860332727432251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [270, 270, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 33, 270, 33, 34560, 270, 270, 270, 270, 34560, 33, 33, 33, 270, 34560, 33, 34560, 34560, 270, 270, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 270, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 33, 33, 34560, 270, 33, 270, 270, 34560, 270, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 34560, 34560, 33, 34560, 33, 270, 34560, 34560, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 33, 34560, 270, 33, 34560, 33, 34560, 33, 33, 270, 270, 34560, 270, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 33, 270, 34560, 34560, 33, 34560, 270, 33, 270, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 33, 270, 270, 33, 270, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 270, 270, 33, 270, 34560, 270, 270, 34560, 33, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 34560, 33, 34560, 270, 34560, 34560, 33, 34560, 270, 270, 34560, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 34560, 270, 33, 33, 33, 33]
Prompts retrieved: 2997915 . Total input tokens: 668555089 . Total output tokens: 588429336
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.6601429330185,
    "estimated_duration": 3600.0023348297354,
    "input_throughput": 7203.09893944178,
    "output_throughput": 6287.335644483828,
    "total_throughput": 13490.434583925608,
    "itl": 86.30514408856615,
    "ttft": 1983100.7253741336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.403097284697007,
    "arrivals": 998127,
    "finished_requests": 105277,
    "scheduler_time": 300.2131063853656
}
#Debug simulation 
Total elapsed time: 86.66026391508058. Arrivals time: 0.4732496221549809 Scheduler time: 85.96870573796332 Scheduler overhead time: 0.08706195652484894 Adapter cache time: 0.01722908904775977 Engine time: 0.07973224204033613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 66, 135, 66, 34560, 135, 135, 135, 135, 34560, 66, 66, 66, 135, 34560, 66, 34560, 34560, 135, 135, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 66, 66, 34560, 135, 66, 135, 135, 34560, 135, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 34560, 34560, 66, 34560, 66, 135, 34560, 34560, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 66, 34560, 135, 66, 34560, 66, 34560, 66, 66, 135, 135, 34560, 135, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 135, 135, 66, 135, 34560, 135, 135, 34560, 66, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 34560, 135, 66, 66, 66, 66]
Prompts retrieved: 2989245 . Total input tokens: 666609101 . Total output tokens: 586712027
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.06618553306907,
    "estimated_duration": 3600.060010984818,
    "input_throughput": 7398.706665646282,
    "output_throughput": 6448.58861495737,
    "total_throughput": 13847.295280603652,
    "itl": 90.54539875918508,
    "ttft": 1973627.5794893226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.453203750853439,
    "arrivals": 995247,
    "finished_requests": 107983,
    "scheduler_time": 292.0140040676401
}
#Debug simulation 
Total elapsed time: 84.06630448810756. Arrivals time: 0.4811297468841076 Scheduler time: 83.37346284138039 Scheduler overhead time: 0.0841873143799603 Adapter cache time: 0.016697008162736893 Engine time: 0.0775770852342248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 66, 135, 66, 34560, 135, 135, 135, 135, 34560, 66, 66, 66, 135, 34560, 66, 34560, 34560, 135, 135, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 66, 66, 34560, 135, 66, 135, 135, 34560, 135, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 34560, 34560, 66, 34560, 66, 135, 34560, 34560, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 66, 34560, 135, 66, 34560, 66, 34560, 66, 66, 135, 135, 34560, 135, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 135, 135, 66, 135, 34560, 135, 135, 34560, 66, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 34560, 135, 66, 66, 66, 66]
Prompts retrieved: 2989245 . Total input tokens: 666609101 . Total output tokens: 586712027
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.77136586606503,
    "estimated_duration": 3600.0502160488345,
    "input_throughput": 7380.318441546868,
    "output_throughput": 6429.903087686819,
    "total_throughput": 13810.221529233688,
    "itl": 89.4320380088983,
    "ttft": 1977123.0503851378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 366,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.675240981737155,
    "arrivals": 995247,
    "finished_requests": 107753,
    "scheduler_time": 292.01664297083266
}
#Debug simulation 
Total elapsed time: 84.77148889610544. Arrivals time: 0.4937986661680043 Scheduler time: 84.06358241476119 Scheduler overhead time: 0.08482802659273148 Adapter cache time: 0.017361332662403584 Engine time: 0.0791078470647335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 66, 135, 66, 34560, 135, 135, 135, 135, 34560, 66, 66, 66, 135, 34560, 66, 34560, 34560, 135, 135, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 66, 66, 34560, 135, 66, 135, 135, 34560, 135, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 34560, 34560, 66, 34560, 66, 135, 34560, 34560, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 66, 34560, 135, 66, 34560, 66, 34560, 66, 66, 135, 135, 34560, 135, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 135, 135, 66, 135, 34560, 135, 135, 34560, 66, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 34560, 135, 66, 66, 66, 66]
Prompts retrieved: 2989245 . Total input tokens: 666609101 . Total output tokens: 586712027
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.52035229094326,
    "estimated_duration": 3600.0741765222497,
    "input_throughput": 7256.694923224324,
    "output_throughput": 6326.983523990517,
    "total_throughput": 13583.678447214841,
    "itl": 86.47924983029027,
    "ttft": 1983156.2673126296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7057472599577315,
    "arrivals": 995247,
    "finished_requests": 106020,
    "scheduler_time": 297.4446433364373
}
#Debug simulation 
Total elapsed time: 85.52047122083604. Arrivals time: 0.4781824368983507 Scheduler time: 84.82539590355009 Scheduler overhead time: 0.08645842550322413 Adapter cache time: 0.017114168498665094 Engine time: 0.07975694024935365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 66, 135, 66, 34560, 135, 135, 135, 135, 34560, 66, 66, 66, 135, 34560, 66, 34560, 34560, 135, 135, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 66, 66, 34560, 135, 66, 135, 135, 34560, 135, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 34560, 34560, 66, 34560, 66, 135, 34560, 34560, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 66, 34560, 135, 66, 34560, 66, 34560, 66, 66, 135, 135, 34560, 135, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 135, 135, 66, 135, 34560, 135, 135, 34560, 66, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 34560, 135, 66, 66, 66, 66]
Prompts retrieved: 2989245 . Total input tokens: 666609101 . Total output tokens: 586712027
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.03142686840147,
    "estimated_duration": 3600.0930217971904,
    "input_throughput": 7366.247993992513,
    "output_throughput": 6410.735183858857,
    "total_throughput": 13776.98317785137,
    "itl": 89.62754568338303,
    "ttft": 1980397.3174785653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4295448248274596,
    "arrivals": 995247,
    "finished_requests": 107468,
    "scheduler_time": 293.0637593280892
}
#Debug simulation 
Total elapsed time: 84.03156114416197. Arrivals time: 0.49340308643877506 Scheduler time: 83.32079322310165 Scheduler overhead time: 0.08686580555513501 Adapter cache time: 0.017091065645217896 Engine time: 0.0798003226518631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 66, 135, 66, 34560, 135, 135, 135, 135, 34560, 66, 66, 66, 135, 34560, 66, 34560, 34560, 135, 135, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 66, 66, 34560, 135, 66, 135, 135, 34560, 135, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 34560, 34560, 66, 34560, 66, 135, 34560, 34560, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 66, 34560, 135, 66, 34560, 66, 34560, 66, 66, 135, 135, 34560, 135, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 135, 135, 66, 135, 34560, 135, 135, 34560, 66, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 34560, 135, 66, 66, 66, 66]
Prompts retrieved: 2989245 . Total input tokens: 666609101 . Total output tokens: 586712027
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 83.59988098638132,
    "estimated_duration": 3600.0910534206323,
    "input_throughput": 7323.917814508489,
    "output_throughput": 6381.6158144585925,
    "total_throughput": 13705.533628967081,
    "itl": 86.90808812943474,
    "ttft": 1986176.6890328296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 370,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.756993616651761,
    "arrivals": 995247,
    "finished_requests": 106968,
    "scheduler_time": 294.71368572598004
}
#Debug simulation 
Total elapsed time: 83.60000430606306. Arrivals time: 0.4917755643837154 Scheduler time: 82.89079392747954 Scheduler overhead time: 0.08633636170998216 Adapter cache time: 0.017604324035346508 Engine time: 0.07938721450045705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 66, 135, 66, 34560, 135, 135, 135, 135, 34560, 66, 66, 66, 135, 34560, 66, 34560, 34560, 135, 135, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 66, 66, 34560, 135, 66, 135, 135, 34560, 135, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 34560, 34560, 66, 34560, 66, 135, 34560, 34560, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 66, 34560, 135, 66, 34560, 66, 34560, 66, 66, 135, 135, 34560, 135, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 135, 135, 66, 135, 34560, 135, 135, 34560, 66, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 34560, 135, 66, 66, 66, 66]
Prompts retrieved: 2989245 . Total input tokens: 666609101 . Total output tokens: 586712027
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 81.44542286591604,
    "estimated_duration": 3600.0905030447398,
    "input_throughput": 7395.3690823836305,
    "output_throughput": 6443.496067774194,
    "total_throughput": 13838.865150157824,
    "itl": 89.71044869727112,
    "ttft": 1983292.8258232528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 334,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.13222959415055,
    "arrivals": 995247,
    "finished_requests": 107960,
    "scheduler_time": 291.4599230938919
}
#Debug simulation 
Total elapsed time: 81.44554670993239. Arrivals time: 0.4831734048202634 Scheduler time: 80.74836177704856 Scheduler overhead time: 0.0857863761484623 Adapter cache time: 0.016709007788449526 Engine time: 0.0781026491895318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 66, 135, 66, 34560, 135, 135, 135, 135, 34560, 66, 66, 66, 135, 34560, 66, 34560, 34560, 135, 135, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 135, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 66, 66, 34560, 135, 66, 135, 135, 34560, 135, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 34560, 34560, 66, 34560, 66, 135, 34560, 34560, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 66, 34560, 135, 66, 34560, 66, 34560, 66, 66, 135, 135, 34560, 135, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 66, 135, 34560, 34560, 66, 34560, 135, 66, 135, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 66, 135, 135, 66, 135, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 135, 135, 66, 135, 34560, 135, 135, 34560, 66, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 66, 34560, 135, 34560, 34560, 66, 34560, 135, 135, 34560, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 34560, 135, 66, 66, 66, 66]
Prompts retrieved: 2989245 . Total input tokens: 666609101 . Total output tokens: 586712027
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.50882105715573,
    "estimated_duration": 3600.065232038726,
    "input_throughput": 7323.970345134116,
    "output_throughput": 6381.661586445626,
    "total_throughput": 13705.631931579743,
    "itl": 86.90750629870425,
    "ttft": 1986167.5870577954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 370,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7313110867515378,
    "arrivals": 995247,
    "finished_requests": 106968,
    "scheduler_time": 294.71364800519603
}
#Debug simulation 
Total elapsed time: 83.50894452724606. Arrivals time: 0.4898080648854375 Scheduler time: 82.80082575511187 Scheduler overhead time: 0.08663515048101544 Adapter cache time: 0.01749517535790801 Engine time: 0.08005209919065237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 33, 135, 33, 34560, 135, 135, 135, 135, 34560, 33, 33, 33, 135, 34560, 33, 34560, 34560, 135, 135, 135, 33, 34560, 33, 135, 135, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 33, 33, 34560, 135, 33, 135, 135, 34560, 135, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 34560, 34560, 33, 34560, 33, 135, 34560, 34560, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 33, 34560, 135, 33, 34560, 33, 34560, 33, 33, 135, 135, 34560, 135, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 33, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 135, 135, 33, 135, 34560, 135, 135, 34560, 33, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 34560, 135, 33, 33, 33, 33]
Prompts retrieved: 2986440 . Total input tokens: 665969540 . Total output tokens: 586163675
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.36639803787693,
    "estimated_duration": 3600.0660306447335,
    "input_throughput": 7425.35908298677,
    "output_throughput": 6433.789492425482,
    "total_throughput": 13859.148575412253,
    "itl": 90.06841814361862,
    "ttft": 1968419.173239229,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5656147043965913,
    "arrivals": 994382,
    "finished_requests": 107896,
    "scheduler_time": 291.70274419377205
}
#Debug simulation 
Total elapsed time: 83.36651617195457. Arrivals time: 0.49302989337593317 Scheduler time: 82.66011514794081 Scheduler overhead time: 0.08468082826584578 Adapter cache time: 0.01697189686819911 Engine time: 0.07827668264508247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [135, 135, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 33, 135, 33, 34560, 135, 135, 135, 135, 34560, 33, 33, 33, 135, 34560, 33, 34560, 34560, 135, 135, 135, 33, 34560, 33, 135, 135, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 135, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 33, 33, 34560, 135, 33, 135, 135, 34560, 135, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 34560, 34560, 33, 34560, 33, 135, 34560, 34560, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 33, 34560, 135, 33, 34560, 33, 34560, 33, 33, 135, 135, 34560, 135, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 33, 135, 34560, 34560, 33, 34560, 135, 33, 135, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 33, 135, 135, 33, 135, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 135, 135, 33, 135, 34560, 135, 135, 34560, 33, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 34560, 33, 34560, 135, 34560, 34560, 33, 34560, 135, 135, 34560, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 34560, 135, 33, 33, 33, 33]
Prompts retrieved: 2986440 . Total input tokens: 665969540 . Total output tokens: 586163675
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 91.78319795290008,
    "estimated_duration": 3600.0841460949105,
    "input_throughput": 7332.680273219384,
    "output_throughput": 6360.667437409477,
    "total_throughput": 13693.347710628861,
    "itl": 88.0406608475377,
    "ttft": 1979065.033895522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3363265102775794,
    "arrivals": 994382,
    "finished_requests": 106450,
    "scheduler_time": 297.16330357129425
}
#Debug simulation 
Total elapsed time: 91.78331242594868. Arrivals time: 0.49562754621729255 Scheduler time: 91.06893379846588 Scheduler overhead time: 0.08726671850308776 Adapter cache time: 0.017149241641163826 Engine time: 0.08027177816256881 
