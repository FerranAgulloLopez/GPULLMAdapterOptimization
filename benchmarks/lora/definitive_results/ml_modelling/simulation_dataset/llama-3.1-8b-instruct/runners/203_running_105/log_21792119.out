INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:05 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 135, 270, 135, 8640, 270, 270, 270, 270, 8640, 135, 135, 135, 270, 8640, 135, 8640, 8640, 270, 270, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 270, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 135, 135, 8640, 270, 135, 270, 270, 8640, 270, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 8640, 8640, 135, 8640, 135, 270, 8640, 8640, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 135, 8640, 270, 135, 8640, 135, 8640, 135, 135, 270, 270, 8640, 270, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 135, 270, 8640, 8640, 135, 8640, 270, 135, 270, 8640, 135, 135, 135, 135, 135, 135, 8640, 135, 135, 135, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 8640, 135, 135, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 270, 270, 135, 270, 8640, 270, 270, 8640, 135, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 135, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 8640, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 8640, 270, 135, 135, 135, 135]
Prompts retrieved: 777465 . Total input tokens: 173094731 . Total output tokens: 152764019
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.45301756402478,
    "estimated_duration": 3600.0235534657704,
    "input_throughput": 6355.460918574667,
    "output_throughput": 5535.830170003772,
    "total_throughput": 11891.291088578439,
    "itl": 88.52206283917796,
    "ttft": 1676623.906759828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0210190521273956,
    "arrivals": 258949,
    "finished_requests": 92501,
    "scheduler_time": 244.8649968234024
}
#Debug simulation 
Total elapsed time: 82.45326367625967. Arrivals time: 0.4289000341668725 Scheduler time: 81.81292667705566 Scheduler overhead time: 0.08087867638096213 Adapter cache time: 0.017039055936038494 Engine time: 0.08096673060208559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 135, 270, 135, 8640, 270, 270, 270, 270, 8640, 135, 135, 135, 270, 8640, 135, 8640, 8640, 270, 270, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 270, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 135, 135, 8640, 270, 135, 270, 270, 8640, 270, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 8640, 8640, 135, 8640, 135, 270, 8640, 8640, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 135, 8640, 270, 135, 8640, 135, 8640, 135, 135, 270, 270, 8640, 270, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 135, 270, 8640, 8640, 135, 8640, 270, 135, 270, 8640, 135, 135, 135, 135, 135, 135, 8640, 135, 135, 135, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 8640, 135, 135, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 270, 270, 135, 270, 8640, 270, 270, 8640, 135, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 135, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 8640, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 8640, 270, 135, 135, 135, 135]
Prompts retrieved: 777465 . Total input tokens: 173094731 . Total output tokens: 152764019
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 82.6311258468777,
    "estimated_duration": 3600.0146300855527,
    "input_throughput": 6535.4192739602495,
    "output_throughput": 5693.139641355551,
    "total_throughput": 12228.558915315802,
    "itl": 94.57778005120527,
    "ttft": 1656730.4546182563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2605878703203017,
    "arrivals": 258949,
    "finished_requests": 95119,
    "scheduler_time": 236.92986457537347
}
#Debug simulation 
Total elapsed time: 82.6313439249061. Arrivals time: 0.43523982679471374 Scheduler time: 81.9902887265198 Scheduler overhead time: 0.07870275806635618 Adapter cache time: 0.01759700058028102 Engine time: 0.07854025298729539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 135, 270, 135, 8640, 270, 270, 270, 270, 8640, 135, 135, 135, 270, 8640, 135, 8640, 8640, 270, 270, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 270, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 135, 135, 8640, 270, 135, 270, 270, 8640, 270, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 8640, 8640, 135, 8640, 135, 270, 8640, 8640, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 135, 8640, 270, 135, 8640, 135, 8640, 135, 135, 270, 270, 8640, 270, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 135, 270, 8640, 8640, 135, 8640, 270, 135, 270, 8640, 135, 135, 135, 135, 135, 135, 8640, 135, 135, 135, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 8640, 135, 135, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 270, 270, 135, 270, 8640, 270, 270, 8640, 135, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 135, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 8640, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 8640, 270, 135, 135, 135, 135]
Prompts retrieved: 777465 . Total input tokens: 173094731 . Total output tokens: 152764019
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 81.9998563490808,
    "estimated_duration": 3600.0067608966046,
    "input_throughput": 6408.549075683976,
    "output_throughput": 5583.802569024491,
    "total_throughput": 11992.351644708468,
    "itl": 89.53487799209974,
    "ttft": 1675267.6021858961,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6428055012598928,
    "arrivals": 258949,
    "finished_requests": 93253,
    "scheduler_time": 242.2310411051871
}
#Debug simulation 
Total elapsed time: 82.00004929676652. Arrivals time: 0.422107195481658 Scheduler time: 81.36334636760876 Scheduler overhead time: 0.08218185603618622 Adapter cache time: 0.018737989012151957 Engine time: 0.08120988868176937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 135, 270, 135, 8640, 270, 270, 270, 270, 8640, 135, 135, 135, 270, 8640, 135, 8640, 8640, 270, 270, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 270, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 135, 135, 8640, 270, 135, 270, 270, 8640, 270, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 8640, 8640, 135, 8640, 135, 270, 8640, 8640, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 135, 8640, 270, 135, 8640, 135, 8640, 135, 135, 270, 270, 8640, 270, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 135, 270, 8640, 8640, 135, 8640, 270, 135, 270, 8640, 135, 135, 135, 135, 135, 135, 8640, 135, 135, 135, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 8640, 135, 135, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 270, 270, 135, 270, 8640, 270, 270, 8640, 135, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 135, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 8640, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 8640, 270, 135, 135, 135, 135]
Prompts retrieved: 777465 . Total input tokens: 173094731 . Total output tokens: 152764019
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.85990044428036,
    "estimated_duration": 3600.08941401057,
    "input_throughput": 6555.5563448376915,
    "output_throughput": 5706.177996594527,
    "total_throughput": 12261.734341432219,
    "itl": 94.55312365864619,
    "ttft": 1655314.8578669473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8599965813755865,
    "arrivals": 258949,
    "finished_requests": 95339,
    "scheduler_time": 236.32734838015293
}
#Debug simulation 
Total elapsed time: 85.86008520517498. Arrivals time: 0.4469296969473362 Scheduler time: 85.20556780416518 Scheduler overhead time: 0.07929580798372626 Adapter cache time: 0.01774514326825738 Engine time: 0.07900029374286532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 135, 8640, 8640, 135, 8640, 270, 8640, 135, 270, 135, 8640, 270, 270, 270, 270, 8640, 135, 135, 135, 270, 8640, 135, 8640, 8640, 270, 270, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 270, 270, 8640, 270, 8640, 135, 8640, 8640, 270, 135, 135, 8640, 270, 135, 270, 270, 8640, 270, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 8640, 8640, 135, 8640, 135, 270, 8640, 8640, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 135, 8640, 270, 135, 8640, 135, 8640, 135, 135, 270, 270, 8640, 270, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 135, 270, 8640, 8640, 135, 8640, 270, 135, 270, 8640, 135, 135, 135, 135, 135, 135, 8640, 135, 135, 135, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 135, 270, 270, 135, 270, 135, 8640, 135, 135, 8640, 135, 8640, 8640, 270, 8640, 270, 135, 270, 8640, 270, 8640, 135, 8640, 135, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 135, 8640, 135, 8640, 270, 270, 135, 270, 8640, 270, 270, 8640, 135, 8640, 8640, 270, 135, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 135, 8640, 270, 8640, 8640, 135, 8640, 270, 270, 8640, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 8640, 270, 135, 135, 135, 135]
Prompts retrieved: 777465 . Total input tokens: 173094731 . Total output tokens: 152764019
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.23855816805735,
    "estimated_duration": 3600.093506076654,
    "input_throughput": 6410.638212881074,
    "output_throughput": 5586.658781515482,
    "total_throughput": 11997.296994396555,
    "itl": 89.47448031410089,
    "ttft": 1673615.8363889116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2603337024897576,
    "arrivals": 258949,
    "finished_requests": 93339,
    "scheduler_time": 242.13318274225384
}
#Debug simulation 
Total elapsed time: 82.23875041212887. Arrivals time: 0.4373193928040564 Scheduler time: 81.58827567799017 Scheduler overhead time: 0.08140239166095853 Adapter cache time: 0.01791626401245594 Engine time: 0.08154002204537392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 66, 270, 66, 8640, 270, 270, 270, 270, 8640, 66, 66, 66, 270, 8640, 66, 8640, 8640, 270, 270, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 270, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 66, 66, 8640, 270, 66, 270, 270, 8640, 270, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 8640, 8640, 66, 8640, 66, 270, 8640, 8640, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 66, 8640, 270, 66, 8640, 66, 8640, 66, 66, 270, 270, 8640, 270, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 66, 270, 8640, 8640, 66, 8640, 270, 66, 270, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 270, 270, 66, 270, 8640, 270, 270, 8640, 66, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 66, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 8640, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 8640, 270, 66, 66, 66, 66]
Prompts retrieved: 771600 . Total input tokens: 171813918 . Total output tokens: 151618324
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.56272915471345,
    "estimated_duration": 3600.066933067795,
    "input_throughput": 6573.206676419971,
    "output_throughput": 5753.993574321666,
    "total_throughput": 12327.200250741636,
    "itl": 96.64343718584719,
    "ttft": 1655391.6756839438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.691250476003644,
    "arrivals": 257044,
    "finished_requests": 95791,
    "scheduler_time": 233.58092677615127
}
#Debug simulation 
Total elapsed time: 84.56292621372268. Arrivals time: 0.4512546081095934 Scheduler time: 83.90771553525701 Scheduler overhead time: 0.07822894072160125 Adapter cache time: 0.017080543097108603 Engine time: 0.07781835505738854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 66, 270, 66, 8640, 270, 270, 270, 270, 8640, 66, 66, 66, 270, 8640, 66, 8640, 8640, 270, 270, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 270, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 66, 66, 8640, 270, 66, 270, 270, 8640, 270, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 8640, 8640, 66, 8640, 66, 270, 8640, 8640, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 66, 8640, 270, 66, 8640, 66, 8640, 66, 66, 270, 270, 8640, 270, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 66, 270, 8640, 8640, 66, 8640, 270, 66, 270, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 270, 270, 66, 270, 8640, 270, 270, 8640, 66, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 66, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 8640, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 8640, 270, 66, 66, 66, 66]
Prompts retrieved: 771600 . Total input tokens: 171813918 . Total output tokens: 151618324
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.99940407183021,
    "estimated_duration": 3600.0178554725635,
    "input_throughput": 6535.563695672707,
    "output_throughput": 5717.841640343186,
    "total_throughput": 12253.405336015892,
    "itl": 94.75856936714482,
    "ttft": 1655939.1817035323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4249646067805632,
    "arrivals": 257044,
    "finished_requests": 95147,
    "scheduler_time": 235.223094614634
}
#Debug simulation 
Total elapsed time: 81.9995956257917. Arrivals time: 0.4497293853200972 Scheduler time: 81.34435993712395 Scheduler overhead time: 0.07853817567229271 Adapter cache time: 0.018014233093708754 Engine time: 0.07805751264095306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 66, 270, 66, 8640, 270, 270, 270, 270, 8640, 66, 66, 66, 270, 8640, 66, 8640, 8640, 270, 270, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 270, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 66, 66, 8640, 270, 66, 270, 270, 8640, 270, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 8640, 8640, 66, 8640, 66, 270, 8640, 8640, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 66, 8640, 270, 66, 8640, 66, 8640, 66, 66, 270, 270, 8640, 270, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 66, 270, 8640, 8640, 66, 8640, 270, 66, 270, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 270, 270, 66, 270, 8640, 270, 270, 8640, 66, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 66, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 8640, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 8640, 270, 66, 66, 66, 66]
Prompts retrieved: 771600 . Total input tokens: 171813918 . Total output tokens: 151618324
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.22748526418582,
    "estimated_duration": 3600.052585720579,
    "input_throughput": 6389.540555946027,
    "output_throughput": 5593.97912127139,
    "total_throughput": 11983.519677217417,
    "itl": 89.5346002564976,
    "ttft": 1669089.9762495768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.773944584867018,
    "arrivals": 257044,
    "finished_requests": 93002,
    "scheduler_time": 241.37315839493758
}
#Debug simulation 
Total elapsed time: 80.22767969034612. Arrivals time: 0.43709379248321056 Scheduler time: 79.57983396248892 Scheduler overhead time: 0.0803191140294075 Adapter cache time: 0.018280471675097942 Engine time: 0.08003844320774078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 66, 270, 66, 8640, 270, 270, 270, 270, 8640, 66, 66, 66, 270, 8640, 66, 8640, 8640, 270, 270, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 270, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 66, 66, 8640, 270, 66, 270, 270, 8640, 270, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 8640, 8640, 66, 8640, 66, 270, 8640, 8640, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 66, 8640, 270, 66, 8640, 66, 8640, 66, 66, 270, 270, 8640, 270, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 66, 270, 8640, 8640, 66, 8640, 270, 66, 270, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 270, 270, 66, 270, 8640, 270, 270, 8640, 66, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 66, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 8640, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 8640, 270, 66, 66, 66, 66]
Prompts retrieved: 771600 . Total input tokens: 171813918 . Total output tokens: 151618324
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 85.33661575894803,
    "estimated_duration": 3600.1056802838007,
    "input_throughput": 6482.048604239971,
    "output_throughput": 5676.120596101255,
    "total_throughput": 12158.169200341224,
    "itl": 94.04047249893515,
    "ttft": 1659000.7080871933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.82174992183223,
    "arrivals": 257044,
    "finished_requests": 94390,
    "scheduler_time": 237.39585629163096
}
#Debug simulation 
Total elapsed time: 85.33680921886116. Arrivals time: 0.4583688760176301 Scheduler time: 84.66820738418028 Scheduler overhead time: 0.0811788528226316 Adapter cache time: 0.01768775237724185 Engine time: 0.07972182938829064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 66, 270, 66, 8640, 270, 270, 270, 270, 8640, 66, 66, 66, 270, 8640, 66, 8640, 8640, 270, 270, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 270, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 66, 66, 8640, 270, 66, 270, 270, 8640, 270, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 8640, 8640, 66, 8640, 66, 270, 8640, 8640, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 66, 8640, 270, 66, 8640, 66, 8640, 66, 66, 270, 270, 8640, 270, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 66, 270, 8640, 8640, 66, 8640, 270, 66, 270, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 270, 270, 66, 270, 8640, 270, 270, 8640, 66, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 66, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 8640, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 8640, 270, 66, 66, 66, 66]
Prompts retrieved: 771600 . Total input tokens: 171813918 . Total output tokens: 151618324
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 80.73916266579181,
    "estimated_duration": 3600.005925391913,
    "input_throughput": 6385.532267560761,
    "output_throughput": 5590.563853810598,
    "total_throughput": 11976.09612137136,
    "itl": 89.48156889139624,
    "ttft": 1670324.0868774625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6288282642420686,
    "arrivals": 257044,
    "finished_requests": 92946,
    "scheduler_time": 241.598402383015
}
#Debug simulation 
Total elapsed time: 80.73934668302536. Arrivals time: 0.44032512279227376 Scheduler time: 80.08877977496013 Scheduler overhead time: 0.07970150094479322 Adapter cache time: 0.0185999171808362 Engine time: 0.07967158639803529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 66, 270, 66, 8640, 270, 270, 270, 270, 8640, 66, 66, 66, 270, 8640, 66, 8640, 8640, 270, 270, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 270, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 66, 66, 8640, 270, 66, 270, 270, 8640, 270, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 8640, 8640, 66, 8640, 66, 270, 8640, 8640, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 66, 8640, 270, 66, 8640, 66, 8640, 66, 66, 270, 270, 8640, 270, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 66, 270, 8640, 8640, 66, 8640, 270, 66, 270, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 270, 270, 66, 270, 8640, 270, 270, 8640, 66, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 66, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 8640, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 8640, 270, 66, 66, 66, 66]
Prompts retrieved: 771600 . Total input tokens: 171813918 . Total output tokens: 151618324
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.94848304335028,
    "estimated_duration": 3600.059521180915,
    "input_throughput": 6502.906094263858,
    "output_throughput": 5692.2522751173665,
    "total_throughput": 12195.158369381224,
    "itl": 94.24731245348259,
    "ttft": 1662183.823261983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5280326924659207,
    "arrivals": 257044,
    "finished_requests": 94728,
    "scheduler_time": 236.67353270080514
}
#Debug simulation 
Total elapsed time: 84.94866739306599. Arrivals time: 0.45902601396664977 Scheduler time: 84.27990246843547 Scheduler overhead time: 0.08022809401154518 Adapter cache time: 0.01762655656784773 Engine time: 0.07990201748907566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 66, 8640, 8640, 66, 8640, 270, 8640, 66, 270, 66, 8640, 270, 270, 270, 270, 8640, 66, 66, 66, 270, 8640, 66, 8640, 8640, 270, 270, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 270, 270, 8640, 270, 8640, 66, 8640, 8640, 270, 66, 66, 8640, 270, 66, 270, 270, 8640, 270, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 8640, 8640, 66, 8640, 66, 270, 8640, 8640, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 66, 8640, 270, 66, 8640, 66, 8640, 66, 66, 270, 270, 8640, 270, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 66, 270, 8640, 8640, 66, 8640, 270, 66, 270, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 66, 270, 270, 66, 270, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 270, 8640, 270, 66, 270, 8640, 270, 8640, 66, 8640, 66, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 66, 8640, 66, 8640, 270, 270, 66, 270, 8640, 270, 270, 8640, 66, 8640, 8640, 270, 66, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 66, 8640, 270, 8640, 8640, 66, 8640, 270, 270, 8640, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 8640, 270, 66, 66, 66, 66]
Prompts retrieved: 771600 . Total input tokens: 171813918 . Total output tokens: 151618324
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 79.91884737275541,
    "estimated_duration": 3600.066999922083,
    "input_throughput": 6421.32188109286,
    "output_throughput": 5619.376528391789,
    "total_throughput": 12040.69840948465,
    "itl": 89.78208550729029,
    "ttft": 1671856.2216247825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.640987329594822,
    "arrivals": 257044,
    "finished_requests": 93503,
    "scheduler_time": 240.123649023843
}
#Debug simulation 
Total elapsed time: 79.9190196339041. Arrivals time: 0.439951058011502 Scheduler time: 79.26855308748782 Scheduler overhead time: 0.08024622220546007 Adapter cache time: 0.018564295955002308 Engine time: 0.07980705332010984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 33, 270, 33, 8640, 270, 270, 270, 270, 8640, 33, 33, 33, 270, 8640, 33, 8640, 8640, 270, 270, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 270, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 33, 33, 8640, 270, 33, 270, 270, 8640, 270, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 8640, 8640, 33, 8640, 33, 270, 8640, 8640, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 33, 8640, 270, 33, 8640, 33, 8640, 33, 33, 270, 270, 8640, 270, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 33, 270, 8640, 8640, 33, 8640, 270, 33, 270, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 270, 270, 33, 270, 8640, 270, 270, 8640, 33, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 33, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 8640, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 8640, 270, 33, 33, 33, 33]
Prompts retrieved: 768795 . Total input tokens: 171181302 . Total output tokens: 151089824
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.01207421021536,
    "estimated_duration": 3600.057615858686,
    "input_throughput": 6570.829282230174,
    "output_throughput": 5732.401311882245,
    "total_throughput": 12303.23059411242,
    "itl": 94.22899162160385,
    "ttft": 1655499.673650909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8896227469621487,
    "arrivals": 255937,
    "finished_requests": 95377,
    "scheduler_time": 234.90184025498493
}
#Debug simulation 
Total elapsed time: 87.01225175801665. Arrivals time: 0.4647691482678056 Scheduler time: 86.3370932135731 Scheduler overhead time: 0.0806549908593297 Adapter cache time: 0.018139210995286703 Engine time: 0.07997296238318086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 33, 270, 33, 8640, 270, 270, 270, 270, 8640, 33, 33, 33, 270, 8640, 33, 8640, 8640, 270, 270, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 270, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 33, 33, 8640, 270, 33, 270, 270, 8640, 270, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 8640, 8640, 33, 8640, 33, 270, 8640, 8640, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 33, 8640, 270, 33, 8640, 33, 8640, 33, 33, 270, 270, 8640, 270, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 33, 270, 8640, 8640, 33, 8640, 270, 33, 270, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 270, 270, 33, 270, 8640, 270, 270, 8640, 33, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 33, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 8640, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 8640, 270, 33, 33, 33, 33]
Prompts retrieved: 768795 . Total input tokens: 171181302 . Total output tokens: 151089824
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.98219581693411,
    "estimated_duration": 3600.0351358688176,
    "input_throughput": 6558.055715837409,
    "output_throughput": 5732.294608569058,
    "total_throughput": 12290.350324406467,
    "itl": 94.54762033896554,
    "ttft": 1653992.7722033348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.178475323556926,
    "arrivals": 255937,
    "finished_requests": 95248,
    "scheduler_time": 234.82148186680593
}
#Debug simulation 
Total elapsed time: 84.98239118792117. Arrivals time: 0.45289736427366734 Scheduler time: 84.31999542703852 Scheduler overhead time: 0.08042106544598937 Adapter cache time: 0.017734705470502377 Engine time: 0.07982131326571107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 33, 270, 33, 8640, 270, 270, 270, 270, 8640, 33, 33, 33, 270, 8640, 33, 8640, 8640, 270, 270, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 270, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 33, 33, 8640, 270, 33, 270, 270, 8640, 270, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 8640, 8640, 33, 8640, 33, 270, 8640, 8640, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 33, 8640, 270, 33, 8640, 33, 8640, 33, 33, 270, 270, 8640, 270, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 33, 270, 8640, 8640, 33, 8640, 270, 33, 270, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 270, 270, 33, 270, 8640, 270, 270, 8640, 33, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 33, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 8640, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 8640, 270, 33, 33, 33, 33]
Prompts retrieved: 768795 . Total input tokens: 171181302 . Total output tokens: 151089824
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.38695834390819,
    "estimated_duration": 3600.0355634126063,
    "input_throughput": 6410.901668460774,
    "output_throughput": 5592.641974046088,
    "total_throughput": 12003.543642506862,
    "itl": 89.07528762361981,
    "ttft": 1671554.5654231333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2572389169549814,
    "arrivals": 255937,
    "finished_requests": 93021,
    "scheduler_time": 241.65464978426064
}
#Debug simulation 
Total elapsed time: 85.38712934404612. Arrivals time: 0.439981522038579 Scheduler time: 84.73311973968521 Scheduler overhead time: 0.0817983066663146 Adapter cache time: 0.018188599031418562 Engine time: 0.08152796933427453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 33, 270, 33, 8640, 270, 270, 270, 270, 8640, 33, 33, 33, 270, 8640, 33, 8640, 8640, 270, 270, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 270, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 33, 33, 8640, 270, 33, 270, 270, 8640, 270, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 8640, 8640, 33, 8640, 33, 270, 8640, 8640, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 33, 8640, 270, 33, 8640, 33, 8640, 33, 33, 270, 270, 8640, 270, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 33, 270, 8640, 8640, 33, 8640, 270, 33, 270, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 270, 270, 33, 270, 8640, 270, 270, 8640, 33, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 33, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 8640, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 8640, 270, 33, 33, 33, 33]
Prompts retrieved: 768795 . Total input tokens: 171181302 . Total output tokens: 151089824
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.90910533908755,
    "estimated_duration": 3600.0140382508544,
    "input_throughput": 6244.583982489881,
    "output_throughput": 5454.715118150231,
    "total_throughput": 11699.299100640112,
    "itl": 85.70628182060432,
    "ttft": 1690945.685509279,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7540199026279106,
    "arrivals": 255937,
    "finished_requests": 90573,
    "scheduler_time": 248.52247469023084
}
#Debug simulation 
Total elapsed time: 84.90927615715191. Arrivals time: 0.4284546272829175 Scheduler time: 84.26454391283914 Scheduler overhead time: 0.08276077220216393 Adapter cache time: 0.017898636404424906 Engine time: 0.08267506584525108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 33, 270, 33, 8640, 270, 270, 270, 270, 8640, 33, 33, 33, 270, 8640, 33, 8640, 8640, 270, 270, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 270, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 33, 33, 8640, 270, 33, 270, 270, 8640, 270, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 8640, 8640, 33, 8640, 33, 270, 8640, 8640, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 33, 8640, 270, 33, 8640, 33, 8640, 33, 33, 270, 270, 8640, 270, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 33, 270, 8640, 8640, 33, 8640, 270, 33, 270, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 270, 270, 33, 270, 8640, 270, 270, 8640, 33, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 33, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 8640, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 8640, 270, 33, 33, 33, 33]
Prompts retrieved: 768795 . Total input tokens: 171181302 . Total output tokens: 151089824
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 86.06711265491322,
    "estimated_duration": 3600.0055354899355,
    "input_throughput": 6383.436295709064,
    "output_throughput": 5574.324762050384,
    "total_throughput": 11957.761057759448,
    "itl": 88.932543067827,
    "ttft": 1667850.2962911923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0089495386090346,
    "arrivals": 255937,
    "finished_requests": 92719,
    "scheduler_time": 242.61467957895712
}
#Debug simulation 
Total elapsed time: 86.06729068793356. Arrivals time: 0.44145253812894225 Scheduler time: 85.40910616517067 Scheduler overhead time: 0.08346880692988634 Adapter cache time: 0.01799620036035776 Engine time: 0.08227017056196928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 33, 270, 33, 8640, 270, 270, 270, 270, 8640, 33, 33, 33, 270, 8640, 33, 8640, 8640, 270, 270, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 270, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 33, 33, 8640, 270, 33, 270, 270, 8640, 270, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 8640, 8640, 33, 8640, 33, 270, 8640, 8640, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 33, 8640, 270, 33, 8640, 33, 8640, 33, 33, 270, 270, 8640, 270, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 33, 270, 8640, 8640, 33, 8640, 270, 33, 270, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 270, 270, 33, 270, 8640, 270, 270, 8640, 33, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 33, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 8640, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 8640, 270, 33, 33, 33, 33]
Prompts retrieved: 768795 . Total input tokens: 171181302 . Total output tokens: 151089824
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.41355287376791,
    "estimated_duration": 3600.039661168187,
    "input_throughput": 6527.086702254592,
    "output_throughput": 5692.048679638885,
    "total_throughput": 12219.135381893477,
    "itl": 93.19490927647585,
    "ttft": 1658216.900350385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.917451869840721,
    "arrivals": 255937,
    "finished_requests": 94688,
    "scheduler_time": 236.849989933995
}
#Debug simulation 
Total elapsed time: 86.41375621175393. Arrivals time: 0.45204605255275965 Scheduler time: 85.7497933153063 Scheduler overhead time: 0.08017421280965209 Adapter cache time: 0.018314602319151163 Engine time: 0.0813114414922893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 8640, 33, 8640, 8640, 33, 8640, 270, 8640, 33, 270, 33, 8640, 270, 270, 270, 270, 8640, 33, 33, 33, 270, 8640, 33, 8640, 8640, 270, 270, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 270, 270, 8640, 270, 8640, 33, 8640, 8640, 270, 33, 33, 8640, 270, 33, 270, 270, 8640, 270, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 8640, 8640, 33, 8640, 33, 270, 8640, 8640, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 33, 8640, 270, 33, 8640, 33, 8640, 33, 33, 270, 270, 8640, 270, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 33, 270, 8640, 8640, 33, 8640, 270, 33, 270, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 33, 270, 270, 33, 270, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 270, 8640, 270, 33, 270, 8640, 270, 8640, 33, 8640, 33, 270, 8640, 270, 8640, 270, 8640, 270, 8640, 270, 33, 8640, 33, 8640, 270, 270, 33, 270, 8640, 270, 270, 8640, 33, 8640, 8640, 270, 33, 8640, 8640, 270, 270, 8640, 8640, 270, 270, 270, 270, 8640, 8640, 270, 270, 8640, 33, 8640, 270, 8640, 8640, 33, 8640, 270, 270, 8640, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 8640, 270, 33, 33, 33, 33]
Prompts retrieved: 768795 . Total input tokens: 171181302 . Total output tokens: 151089824
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.49586928123608,
    "estimated_duration": 3600.015696150285,
    "input_throughput": 6400.932091668868,
    "output_throughput": 5586.00953365413,
    "total_throughput": 11986.941625322997,
    "itl": 89.0732467201112,
    "ttft": 1672512.1002945683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5360480307042903,
    "arrivals": 255937,
    "finished_requests": 92905,
    "scheduler_time": 241.92399709227178
}
#Debug simulation 
Total elapsed time: 82.49606472300366. Arrivals time: 0.43476073257625103 Scheduler time: 81.84743875125423 Scheduler overhead time: 0.0816835411824286 Adapter cache time: 0.018592008855193853 Engine time: 0.08120360318571329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 66, 135, 66, 8640, 135, 135, 135, 135, 8640, 66, 66, 66, 135, 8640, 66, 8640, 8640, 135, 135, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 135, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 66, 66, 8640, 135, 66, 135, 135, 8640, 135, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 8640, 8640, 66, 8640, 66, 135, 8640, 8640, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 66, 8640, 135, 66, 8640, 66, 8640, 66, 66, 135, 135, 8640, 135, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 66, 135, 8640, 8640, 66, 8640, 135, 66, 135, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 135, 135, 66, 135, 8640, 135, 135, 8640, 66, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 66, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 8640, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 8640, 135, 66, 66, 66, 66]
Prompts retrieved: 760125 . Total input tokens: 169231577 . Total output tokens: 149371354
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.16287148278207,
    "estimated_duration": 3600.0008579917244,
    "input_throughput": 6622.827032685893,
    "output_throughput": 5792.382508384373,
    "total_throughput": 12415.209541070266,
    "itl": 97.26583271019244,
    "ttft": 1629714.192651544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 483,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1937935624318556,
    "arrivals": 253041,
    "finished_requests": 96755,
    "scheduler_time": 231.52356595760796
}
#Debug simulation 
Total elapsed time: 85.16306822281331. Arrivals time: 0.45588157465681434 Scheduler time: 84.50087341153994 Scheduler overhead time: 0.07801187224686146 Adapter cache time: 0.017937848810106516 Engine time: 0.07923195650801063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 66, 135, 66, 8640, 135, 135, 135, 135, 8640, 66, 66, 66, 135, 8640, 66, 8640, 8640, 135, 135, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 135, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 66, 66, 8640, 135, 66, 135, 135, 8640, 135, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 8640, 8640, 66, 8640, 66, 135, 8640, 8640, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 66, 8640, 135, 66, 8640, 66, 8640, 66, 66, 135, 135, 8640, 135, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 66, 135, 8640, 8640, 66, 8640, 135, 66, 135, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 135, 135, 66, 135, 8640, 135, 135, 8640, 66, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 66, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 8640, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 8640, 135, 66, 66, 66, 66]
Prompts retrieved: 760125 . Total input tokens: 169231577 . Total output tokens: 149371354
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.64469290198758,
    "estimated_duration": 3600.0919107563323,
    "input_throughput": 6537.880582903004,
    "output_throughput": 5717.920128232324,
    "total_throughput": 12255.800711135329,
    "itl": 94.41655995065527,
    "ttft": 1641838.741322441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9825097016152022,
    "arrivals": 253041,
    "finished_requests": 95510,
    "scheduler_time": 235.26229594479523
}
#Debug simulation 
Total elapsed time: 85.64488670509309. Arrivals time: 0.4545870004221797 Scheduler time: 84.97918244730681 Scheduler overhead time: 0.08070546621456742 Adapter cache time: 0.01773823006078601 Engine time: 0.0807965500280261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 66, 135, 66, 8640, 135, 135, 135, 135, 8640, 66, 66, 66, 135, 8640, 66, 8640, 8640, 135, 135, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 135, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 66, 66, 8640, 135, 66, 135, 135, 8640, 135, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 8640, 8640, 66, 8640, 66, 135, 8640, 8640, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 66, 8640, 135, 66, 8640, 66, 8640, 66, 66, 135, 135, 8640, 135, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 66, 135, 8640, 8640, 66, 8640, 135, 66, 135, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 135, 135, 66, 135, 8640, 135, 135, 8640, 66, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 66, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 8640, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 8640, 135, 66, 66, 66, 66]
Prompts retrieved: 760125 . Total input tokens: 169231577 . Total output tokens: 149371354
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.43990843789652,
    "estimated_duration": 3600.011296895591,
    "input_throughput": 6432.992868653118,
    "output_throughput": 5629.019280432475,
    "total_throughput": 12062.012149085593,
    "itl": 89.72905567448537,
    "ttft": 1653693.601127684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 489,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.655285179852539,
    "arrivals": 253041,
    "finished_requests": 94051,
    "scheduler_time": 239.47851096285885
}
#Debug simulation 
Total elapsed time: 80.4400924090296. Arrivals time: 0.45416660560294986 Scheduler time: 79.77222195221111 Scheduler overhead time: 0.08127186400815845 Adapter cache time: 0.01844449480995536 Engine time: 0.08167855627834797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 66, 135, 66, 8640, 135, 135, 135, 135, 8640, 66, 66, 66, 135, 8640, 66, 8640, 8640, 135, 135, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 135, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 66, 66, 8640, 135, 66, 135, 135, 8640, 135, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 8640, 8640, 66, 8640, 66, 135, 8640, 8640, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 66, 8640, 135, 66, 8640, 66, 8640, 66, 66, 135, 135, 8640, 135, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 66, 135, 8640, 8640, 66, 8640, 135, 66, 135, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 135, 135, 66, 135, 8640, 135, 135, 8640, 66, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 66, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 8640, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 8640, 135, 66, 66, 66, 66]
Prompts retrieved: 760125 . Total input tokens: 169231577 . Total output tokens: 149371354
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 82.14647718798369,
    "estimated_duration": 3600.0834576843818,
    "input_throughput": 6597.126227533743,
    "output_throughput": 5775.431665513025,
    "total_throughput": 12372.55789304677,
    "itl": 95.20789224270254,
    "ttft": 1641603.155320781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.017432212969284,
    "arrivals": 253041,
    "finished_requests": 96533,
    "scheduler_time": 232.52895283222963
}
#Debug simulation 
Total elapsed time: 82.14667150611058. Arrivals time: 0.4452939750626683 Scheduler time: 81.49252549279481 Scheduler overhead time: 0.08103518048301339 Adapter cache time: 0.017913782503455877 Engine time: 0.07857907051220536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 66, 135, 66, 8640, 135, 135, 135, 135, 8640, 66, 66, 66, 135, 8640, 66, 8640, 8640, 135, 135, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 135, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 66, 66, 8640, 135, 66, 135, 135, 8640, 135, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 8640, 8640, 66, 8640, 66, 135, 8640, 8640, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 66, 8640, 135, 66, 8640, 66, 8640, 66, 66, 135, 135, 8640, 135, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 66, 135, 8640, 8640, 66, 8640, 135, 66, 135, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 135, 135, 66, 135, 8640, 135, 135, 8640, 66, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 66, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 8640, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 8640, 135, 66, 66, 66, 66]
Prompts retrieved: 760125 . Total input tokens: 169231577 . Total output tokens: 149371354
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 85.48332421295345,
    "estimated_duration": 3600.015474629308,
    "input_throughput": 6397.270279059403,
    "output_throughput": 5595.4803922264255,
    "total_throughput": 11992.750671285829,
    "itl": 89.23936077335762,
    "ttft": 1647272.9864585043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.089654692835198,
    "arrivals": 253041,
    "finished_requests": 93521,
    "scheduler_time": 241.25601673536684
}
#Debug simulation 
Total elapsed time: 85.48351171286777. Arrivals time: 0.46003388287499547 Scheduler time: 84.80470869131386 Scheduler overhead time: 0.08367966627702117 Adapter cache time: 0.018393646460026503 Engine time: 0.08349927887320518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 66, 135, 66, 8640, 135, 135, 135, 135, 8640, 66, 66, 66, 135, 8640, 66, 8640, 8640, 135, 135, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 135, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 66, 66, 8640, 135, 66, 135, 135, 8640, 135, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 8640, 8640, 66, 8640, 66, 135, 8640, 8640, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 66, 8640, 135, 66, 8640, 66, 8640, 66, 66, 135, 135, 8640, 135, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 66, 135, 8640, 8640, 66, 8640, 135, 66, 135, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 135, 135, 66, 135, 8640, 135, 135, 8640, 66, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 66, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 8640, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 8640, 135, 66, 66, 66, 66]
Prompts retrieved: 760125 . Total input tokens: 169231577 . Total output tokens: 149371354
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.96906458493322,
    "estimated_duration": 3600.061141461846,
    "input_throughput": 6568.848714163048,
    "output_throughput": 5745.338811551739,
    "total_throughput": 12314.187525714786,
    "itl": 95.15270699178272,
    "ttft": 1634472.8309159381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.930219711721862,
    "arrivals": 253041,
    "finished_requests": 95952,
    "scheduler_time": 233.93721599991932
}
#Debug simulation 
Total elapsed time: 85.96925886115059. Arrivals time: 0.4487581145949662 Scheduler time: 85.31206413311884 Scheduler overhead time: 0.07964234426617622 Adapter cache time: 0.017914886586368084 Engine time: 0.0795488809235394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 66, 8640, 8640, 66, 8640, 135, 8640, 66, 135, 66, 8640, 135, 135, 135, 135, 8640, 66, 66, 66, 135, 8640, 66, 8640, 8640, 135, 135, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 135, 135, 8640, 135, 8640, 66, 8640, 8640, 135, 66, 66, 8640, 135, 66, 135, 135, 8640, 135, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 8640, 8640, 66, 8640, 66, 135, 8640, 8640, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 66, 8640, 135, 66, 8640, 66, 8640, 66, 66, 135, 135, 8640, 135, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 66, 135, 8640, 8640, 66, 8640, 135, 66, 135, 8640, 66, 66, 66, 66, 66, 66, 8640, 66, 66, 66, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 66, 135, 135, 66, 135, 66, 8640, 66, 66, 8640, 66, 8640, 8640, 135, 8640, 135, 66, 135, 8640, 135, 8640, 66, 8640, 66, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 66, 8640, 66, 8640, 135, 135, 66, 135, 8640, 135, 135, 8640, 66, 8640, 8640, 135, 66, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 66, 8640, 135, 8640, 8640, 66, 8640, 135, 135, 8640, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 8640, 135, 66, 66, 66, 66]
Prompts retrieved: 760125 . Total input tokens: 169231577 . Total output tokens: 149371354
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.48403530893847,
    "estimated_duration": 3600.064421129754,
    "input_throughput": 6397.5182957343795,
    "output_throughput": 5598.128156183242,
    "total_throughput": 11995.646451917622,
    "itl": 89.28842145140008,
    "ttft": 1645074.7411060838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0621081083454427,
    "arrivals": 253041,
    "finished_requests": 93498,
    "scheduler_time": 241.0947980210986
}
#Debug simulation 
Total elapsed time: 86.48424276290461. Arrivals time: 0.4562682951800525 Scheduler time: 85.80953825591132 Scheduler overhead time: 0.08314329199492931 Adapter cache time: 0.0186607763171196 Engine time: 0.08353003300726414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 33, 135, 33, 8640, 135, 135, 135, 135, 8640, 33, 33, 33, 135, 8640, 33, 8640, 8640, 135, 135, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 135, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 33, 33, 8640, 135, 33, 135, 135, 8640, 135, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 8640, 8640, 33, 8640, 33, 135, 8640, 8640, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 33, 8640, 135, 33, 8640, 33, 8640, 33, 33, 135, 135, 8640, 135, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 33, 135, 8640, 8640, 33, 8640, 135, 33, 135, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 135, 135, 33, 135, 8640, 135, 135, 8640, 33, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 33, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 8640, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 8640, 135, 33, 33, 33, 33]
Prompts retrieved: 757320 . Total input tokens: 168597892 . Total output tokens: 148798696
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 92.09324130229652,
    "estimated_duration": 3600.0084567501453,
    "input_throughput": 6648.438548837871,
    "output_throughput": 5764.333959018876,
    "total_throughput": 12412.772507856747,
    "itl": 96.41909764098817,
    "ttft": 1623111.8090339326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8036614295467968,
    "arrivals": 252137,
    "finished_requests": 96422,
    "scheduler_time": 232.4748962740802
}
#Debug simulation 
Total elapsed time: 92.0934352981858. Arrivals time: 0.4581759385764599 Scheduler time: 91.42516662925482 Scheduler overhead time: 0.08062945725396276 Adapter cache time: 0.01758003979921341 Engine time: 0.08061157446354628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 33, 135, 33, 8640, 135, 135, 135, 135, 8640, 33, 33, 33, 135, 8640, 33, 8640, 8640, 135, 135, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 135, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 33, 33, 8640, 135, 33, 135, 135, 8640, 135, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 8640, 8640, 33, 8640, 33, 135, 8640, 8640, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 33, 8640, 135, 33, 8640, 33, 8640, 33, 33, 135, 135, 8640, 135, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 33, 135, 8640, 8640, 33, 8640, 135, 33, 135, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 135, 135, 33, 135, 8640, 135, 135, 8640, 33, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 33, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 8640, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 8640, 135, 33, 33, 33, 33]
Prompts retrieved: 757320 . Total input tokens: 168597892 . Total output tokens: 148798696
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 88.59732357598841,
    "estimated_duration": 3600.0321844984433,
    "input_throughput": 6580.54866898378,
    "output_throughput": 5701.386806590333,
    "total_throughput": 12281.935475574113,
    "itl": 94.1043953383513,
    "ttft": 1635456.695462311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1335140238516073,
    "arrivals": 252137,
    "finished_requests": 95391,
    "scheduler_time": 235.59802462206378
}
#Debug simulation 
Total elapsed time: 88.59751592110842. Arrivals time: 0.4536762209609151 Scheduler time: 87.9349345699884 Scheduler overhead time: 0.07998447818681598 Adapter cache time: 0.0177105818875134 Engine time: 0.07972860056906939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 33, 135, 33, 8640, 135, 135, 135, 135, 8640, 33, 33, 33, 135, 8640, 33, 8640, 8640, 135, 135, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 135, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 33, 33, 8640, 135, 33, 135, 135, 8640, 135, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 8640, 8640, 33, 8640, 33, 135, 8640, 8640, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 33, 8640, 135, 33, 8640, 33, 8640, 33, 33, 135, 135, 8640, 135, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 33, 135, 8640, 8640, 33, 8640, 135, 33, 135, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 135, 135, 33, 135, 8640, 135, 135, 8640, 33, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 33, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 8640, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 8640, 135, 33, 33, 33, 33]
Prompts retrieved: 757320 . Total input tokens: 168597892 . Total output tokens: 148798696
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.57705761911348,
    "estimated_duration": 3600.073145533549,
    "input_throughput": 6420.050111669214,
    "output_throughput": 5568.286584640781,
    "total_throughput": 11988.336696309996,
    "itl": 88.85039641735905,
    "ttft": 1654689.5444954468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3061633157963173,
    "arrivals": 252137,
    "finished_requests": 93137,
    "scheduler_time": 242.18922515125644
}
#Debug simulation 
Total elapsed time: 85.57725679175928. Arrivals time: 0.4391641151160002 Scheduler time: 84.9217404644005 Scheduler overhead time: 0.08300713216885924 Adapter cache time: 0.017842269968241453 Engine time: 0.08276444533839822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 33, 135, 33, 8640, 135, 135, 135, 135, 8640, 33, 33, 33, 135, 8640, 33, 8640, 8640, 135, 135, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 135, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 33, 33, 8640, 135, 33, 135, 135, 8640, 135, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 8640, 8640, 33, 8640, 33, 135, 8640, 8640, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 33, 8640, 135, 33, 8640, 33, 8640, 33, 33, 135, 135, 8640, 135, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 33, 135, 8640, 8640, 33, 8640, 135, 33, 135, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 135, 135, 33, 135, 8640, 135, 135, 8640, 33, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 33, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 8640, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 8640, 135, 33, 33, 33, 33]
Prompts retrieved: 757320 . Total input tokens: 168597892 . Total output tokens: 148798696
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 88.21282856306061,
    "estimated_duration": 3600.0834409715494,
    "input_throughput": 6581.297736145233,
    "output_throughput": 5702.226166863512,
    "total_throughput": 12283.523903008745,
    "itl": 94.10085294322334,
    "ttft": 1635346.8561369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9363866456411745,
    "arrivals": 252137,
    "finished_requests": 95402,
    "scheduler_time": 235.61335538516147
}
#Debug simulation 
Total elapsed time: 88.21302733710036. Arrivals time: 0.4424727624282241 Scheduler time: 87.5595030002296 Scheduler overhead time: 0.08201519073918462 Adapter cache time: 0.017477552872151136 Engine time: 0.07975308410823345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 33, 135, 33, 8640, 135, 135, 135, 135, 8640, 33, 33, 33, 135, 8640, 33, 8640, 8640, 135, 135, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 135, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 33, 33, 8640, 135, 33, 135, 135, 8640, 135, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 8640, 8640, 33, 8640, 33, 135, 8640, 8640, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 33, 8640, 135, 33, 8640, 33, 8640, 33, 33, 135, 135, 8640, 135, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 33, 135, 8640, 8640, 33, 8640, 135, 33, 135, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 135, 135, 33, 135, 8640, 135, 135, 8640, 33, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 33, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 8640, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 8640, 135, 33, 33, 33, 33]
Prompts retrieved: 757320 . Total input tokens: 168597892 . Total output tokens: 148798696
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 85.44712357688695,
    "estimated_duration": 3600.0839580679544,
    "input_throughput": 6433.282187238043,
    "output_throughput": 5579.669872693796,
    "total_throughput": 12012.952059931838,
    "itl": 88.93110564362189,
    "ttft": 1654752.4608466479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2752509874990445,
    "arrivals": 252137,
    "finished_requests": 93312,
    "scheduler_time": 241.5598118365523
}
#Debug simulation 
Total elapsed time: 85.4473136770539. Arrivals time: 0.4301697537302971 Scheduler time: 84.80194574734196 Scheduler overhead time: 0.08202818362042308 Adapter cache time: 0.018198165111243725 Engine time: 0.08237887965515256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 33, 135, 33, 8640, 135, 135, 135, 135, 8640, 33, 33, 33, 135, 8640, 33, 8640, 8640, 135, 135, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 135, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 33, 33, 8640, 135, 33, 135, 135, 8640, 135, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 8640, 8640, 33, 8640, 33, 135, 8640, 8640, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 33, 8640, 135, 33, 8640, 33, 8640, 33, 33, 135, 135, 8640, 135, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 33, 135, 8640, 8640, 33, 8640, 135, 33, 135, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 135, 135, 33, 135, 8640, 135, 135, 8640, 33, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 33, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 8640, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 8640, 135, 33, 33, 33, 33]
Prompts retrieved: 757320 . Total input tokens: 168597892 . Total output tokens: 148798696
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 89.48067675577477,
    "estimated_duration": 3600.0202762024496,
    "input_throughput": 6586.665957618771,
    "output_throughput": 5713.248099173582,
    "total_throughput": 12299.914056792353,
    "itl": 94.20179999654921,
    "ttft": 1634724.7532868271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7195503206830356,
    "arrivals": 252137,
    "finished_requests": 95546,
    "scheduler_time": 235.06908974081225
}
#Debug simulation 
Total elapsed time: 89.48088075593114. Arrivals time: 0.4490047884173691 Scheduler time: 88.82108600344509 Scheduler overhead time: 0.08021525340154767 Adapter cache time: 0.017951370682567358 Engine time: 0.08136410219594836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 8640, 33, 8640, 8640, 33, 8640, 135, 8640, 33, 135, 33, 8640, 135, 135, 135, 135, 8640, 33, 33, 33, 135, 8640, 33, 8640, 8640, 135, 135, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 135, 135, 8640, 135, 8640, 33, 8640, 8640, 135, 33, 33, 8640, 135, 33, 135, 135, 8640, 135, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 8640, 8640, 33, 8640, 33, 135, 8640, 8640, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 33, 8640, 135, 33, 8640, 33, 8640, 33, 33, 135, 135, 8640, 135, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 33, 135, 8640, 8640, 33, 8640, 135, 33, 135, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 33, 135, 135, 33, 135, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 135, 8640, 135, 33, 135, 8640, 135, 8640, 33, 8640, 33, 135, 8640, 135, 8640, 135, 8640, 135, 8640, 135, 33, 8640, 33, 8640, 135, 135, 33, 135, 8640, 135, 135, 8640, 33, 8640, 8640, 135, 33, 8640, 8640, 135, 135, 8640, 8640, 135, 135, 135, 135, 8640, 8640, 135, 135, 8640, 33, 8640, 135, 8640, 8640, 33, 8640, 135, 135, 8640, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 8640, 135, 33, 33, 33, 33]
Prompts retrieved: 757320 . Total input tokens: 168597892 . Total output tokens: 148798696
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.82250031502917,
    "estimated_duration": 3600.0485424225203,
    "input_throughput": 6433.345474951593,
    "output_throughput": 5579.724762956392,
    "total_throughput": 12013.070237907985,
    "itl": 88.93030366959611,
    "ttft": 1654739.074816308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.242733590770536,
    "arrivals": 252137,
    "finished_requests": 93312,
    "scheduler_time": 241.55920673385157
}
#Debug simulation 
Total elapsed time: 85.82269604178146. Arrivals time: 0.43704513367265463 Scheduler time: 85.16894824476913 Scheduler overhead time: 0.08299174811691046 Adapter cache time: 0.018493353854864836 Engine time: 0.08240930223837495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 33, 66, 33, 8640, 66, 66, 66, 66, 8640, 33, 33, 33, 66, 8640, 33, 8640, 8640, 66, 66, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 66, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 33, 33, 8640, 66, 33, 66, 66, 8640, 66, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 8640, 8640, 33, 8640, 33, 66, 8640, 8640, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 33, 8640, 66, 33, 8640, 33, 8640, 33, 33, 66, 66, 8640, 66, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 33, 66, 8640, 8640, 33, 8640, 66, 33, 66, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 66, 66, 33, 66, 8640, 66, 66, 8640, 33, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 8640, 33, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 8640, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 8640, 66, 33, 33, 33, 33]
Prompts retrieved: 751455 . Total input tokens: 167298773 . Total output tokens: 147616649
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.25355825014412,
    "estimated_duration": 3600.0492319570744,
    "input_throughput": 6660.530302516126,
    "output_throughput": 5799.551521313459,
    "total_throughput": 12460.081823829585,
    "itl": 97.37400343430944,
    "ttft": 1622639.5741027573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.603762922412765,
    "arrivals": 250206,
    "finished_requests": 97030,
    "scheduler_time": 230.76741491027678
}
#Debug simulation 
Total elapsed time: 84.25374658592045. Arrivals time: 0.45137814059853554 Scheduler time: 83.59838693821803 Scheduler overhead time: 0.07733579771593213 Adapter cache time: 0.018335231114178896 Engine time: 0.07736929506063461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 33, 66, 33, 8640, 66, 66, 66, 66, 8640, 33, 33, 33, 66, 8640, 33, 8640, 8640, 66, 66, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 66, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 33, 33, 8640, 66, 33, 66, 66, 8640, 66, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 8640, 8640, 33, 8640, 33, 66, 8640, 8640, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 33, 8640, 66, 33, 8640, 33, 8640, 33, 33, 66, 66, 8640, 66, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 33, 66, 8640, 8640, 33, 8640, 66, 33, 66, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 66, 66, 33, 66, 8640, 66, 66, 8640, 33, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 8640, 33, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 8640, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 8640, 66, 33, 33, 33, 33]
Prompts retrieved: 751455 . Total input tokens: 167298773 . Total output tokens: 147616649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.49048749683425,
    "estimated_duration": 3600.0895101617143,
    "input_throughput": 6601.622524361185,
    "output_throughput": 5751.5806041916685,
    "total_throughput": 12353.203128552854,
    "itl": 94.98990263806041,
    "ttft": 1630240.0579294516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.708085863869642,
    "arrivals": 250206,
    "finished_requests": 96213,
    "scheduler_time": 233.1063761734695
}
#Debug simulation 
Total elapsed time: 81.49067553691566. Arrivals time: 0.44554048031568527 Scheduler time: 80.8378815986216 Scheduler overhead time: 0.07908883318305016 Adapter cache time: 0.01822893274948001 Engine time: 0.07879883889108896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 33, 66, 33, 8640, 66, 66, 66, 66, 8640, 33, 33, 33, 66, 8640, 33, 8640, 8640, 66, 66, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 66, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 33, 33, 8640, 66, 33, 66, 66, 8640, 66, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 8640, 8640, 33, 8640, 33, 66, 8640, 8640, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 33, 8640, 66, 33, 8640, 33, 8640, 33, 33, 66, 66, 8640, 66, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 33, 66, 8640, 8640, 33, 8640, 66, 33, 66, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 66, 66, 33, 66, 8640, 66, 66, 8640, 33, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 8640, 33, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 8640, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 8640, 66, 33, 33, 33, 33]
Prompts retrieved: 751455 . Total input tokens: 167298773 . Total output tokens: 147616649
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 88.4636329789646,
    "estimated_duration": 3600.0161042800646,
    "input_throughput": 6399.251095741153,
    "output_throughput": 5583.7266883615575,
    "total_throughput": 11982.97778410271,
    "itl": 89.28123654393005,
    "ttft": 1636280.1364487137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 478,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5784100175043854,
    "arrivals": 250206,
    "finished_requests": 93257,
    "scheduler_time": 241.28574809147509
}
#Debug simulation 
Total elapsed time: 88.46382708102465. Arrivals time: 0.4433989580720663 Scheduler time: 87.8029408250004 Scheduler overhead time: 0.08296001981943846 Adapter cache time: 0.01904905214905739 Engine time: 0.08252306561917067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 33, 66, 33, 8640, 66, 66, 66, 66, 8640, 33, 33, 33, 66, 8640, 33, 8640, 8640, 66, 66, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 66, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 33, 33, 8640, 66, 33, 66, 66, 8640, 66, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 8640, 8640, 33, 8640, 33, 66, 8640, 8640, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 33, 8640, 66, 33, 8640, 33, 8640, 33, 33, 66, 66, 8640, 66, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 33, 66, 8640, 8640, 33, 8640, 66, 33, 66, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 66, 66, 33, 66, 8640, 66, 66, 8640, 33, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 8640, 33, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 8640, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 8640, 66, 33, 33, 33, 33]
Prompts retrieved: 751455 . Total input tokens: 167298773 . Total output tokens: 147616649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 88.29755634488538,
    "estimated_duration": 3600.0615591758233,
    "input_throughput": 6584.789068281108,
    "output_throughput": 5731.266996646462,
    "total_throughput": 12316.056064927569,
    "itl": 94.6755220531742,
    "ttft": 1628133.0653073336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.394093026146288,
    "arrivals": 250206,
    "finished_requests": 95858,
    "scheduler_time": 234.10514405623974
}
#Debug simulation 
Total elapsed time: 88.29773622611538. Arrivals time: 0.449995887465775 Scheduler time: 87.63680951856077 Scheduler overhead time: 0.08088181493803859 Adapter cache time: 0.018628021702170372 Engine time: 0.08015521336346865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 33, 66, 33, 8640, 66, 66, 66, 66, 8640, 33, 33, 33, 66, 8640, 33, 8640, 8640, 66, 66, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 66, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 33, 33, 8640, 66, 33, 66, 66, 8640, 66, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 8640, 8640, 33, 8640, 33, 66, 8640, 8640, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 33, 8640, 66, 33, 8640, 33, 8640, 33, 33, 66, 66, 8640, 66, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 33, 66, 8640, 8640, 33, 8640, 66, 33, 66, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 66, 66, 33, 66, 8640, 66, 66, 8640, 33, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 8640, 33, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 8640, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 8640, 66, 33, 33, 33, 33]
Prompts retrieved: 751455 . Total input tokens: 167298773 . Total output tokens: 147616649
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 79.33564048213884,
    "estimated_duration": 3600.0282305486726,
    "input_throughput": 6400.114533681985,
    "output_throughput": 5582.387335039619,
    "total_throughput": 11982.501868721605,
    "itl": 89.70936954126472,
    "ttft": 1644326.5821418688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.219505743500811,
    "arrivals": 250206,
    "finished_requests": 93296,
    "scheduler_time": 241.30868991725993
}
#Debug simulation 
Total elapsed time: 79.33582257013768. Arrivals time: 0.43828568700701 Scheduler time: 78.6855975529179 Scheduler overhead time: 0.08046223036944866 Adapter cache time: 0.01891129231080413 Engine time: 0.08020931528881192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 33, 66, 33, 8640, 66, 66, 66, 66, 8640, 33, 33, 33, 66, 8640, 33, 8640, 8640, 66, 66, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 66, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 33, 33, 8640, 66, 33, 66, 66, 8640, 66, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 8640, 8640, 33, 8640, 33, 66, 8640, 8640, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 33, 8640, 66, 33, 8640, 33, 8640, 33, 33, 66, 66, 8640, 66, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 33, 66, 8640, 8640, 33, 8640, 66, 33, 66, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 66, 66, 33, 66, 8640, 66, 66, 8640, 33, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 8640, 33, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 8640, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 8640, 66, 33, 33, 33, 33]
Prompts retrieved: 751455 . Total input tokens: 167298773 . Total output tokens: 147616649
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.9178826850839,
    "estimated_duration": 3600.060902031926,
    "input_throughput": 6552.651925050905,
    "output_throughput": 5707.953437232648,
    "total_throughput": 12260.605362283553,
    "itl": 94.56604121967116,
    "ttft": 1624450.2716005852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1153534189984065,
    "arrivals": 250206,
    "finished_requests": 95448,
    "scheduler_time": 235.13570370766183
}
#Debug simulation 
Total elapsed time: 86.91806475585327. Arrivals time: 0.4337344318628311 Scheduler time: 86.273899119813 Scheduler overhead time: 0.08069129241630435 Adapter cache time: 0.01821833522990346 Engine time: 0.07986271241679788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 8640, 33, 8640, 8640, 33, 8640, 66, 8640, 33, 66, 33, 8640, 66, 66, 66, 66, 8640, 33, 33, 33, 66, 8640, 33, 8640, 8640, 66, 66, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 66, 66, 8640, 66, 8640, 33, 8640, 8640, 66, 33, 33, 8640, 66, 33, 66, 66, 8640, 66, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 8640, 8640, 33, 8640, 33, 66, 8640, 8640, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 33, 8640, 66, 33, 8640, 33, 8640, 33, 33, 66, 66, 8640, 66, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 33, 66, 8640, 8640, 33, 8640, 66, 33, 66, 8640, 33, 33, 33, 33, 33, 33, 8640, 33, 33, 33, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 33, 66, 66, 33, 66, 33, 8640, 33, 33, 8640, 33, 8640, 8640, 66, 8640, 66, 33, 66, 8640, 66, 8640, 33, 8640, 33, 66, 8640, 66, 8640, 66, 8640, 66, 8640, 66, 33, 8640, 33, 8640, 66, 66, 33, 66, 8640, 66, 66, 8640, 33, 8640, 8640, 66, 33, 8640, 8640, 66, 66, 8640, 8640, 66, 66, 66, 66, 8640, 8640, 66, 66, 8640, 33, 8640, 66, 8640, 8640, 33, 8640, 66, 66, 8640, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 8640, 66, 33, 33, 33, 33]
Prompts retrieved: 751455 . Total input tokens: 167298773 . Total output tokens: 147616649
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 79.34580723987892,
    "estimated_duration": 3600.0351421889586,
    "input_throughput": 6403.724155309916,
    "output_throughput": 5586.801018773034,
    "total_throughput": 11990.52517408295,
    "itl": 89.74473169355366,
    "ttft": 1644751.1247575067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.232180925905739,
    "arrivals": 250206,
    "finished_requests": 93354,
    "scheduler_time": 241.09946326074325
}
#Debug simulation 
Total elapsed time: 79.34598445799202. Arrivals time: 0.4385177753865719 Scheduler time: 78.69583002431318 Scheduler overhead time: 0.08072845544666052 Adapter cache time: 0.018770726397633553 Engine time: 0.08026495855301619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 540, 1080, 540, 4320, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 1080, 4320, 540, 4320, 4320, 1080, 1080, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 540, 540, 4320, 1080, 540, 1080, 1080, 4320, 1080, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 4320, 540, 4320, 540, 1080, 4320, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 540, 4320, 1080, 540, 4320, 540, 4320, 540, 540, 1080, 1080, 4320, 1080, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 1080, 540, 1080, 4320, 540, 540, 540, 540, 540, 540, 4320, 540, 540, 540, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 4320, 540, 540, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 1080, 1080, 4320, 540, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 540, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 4320, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 540, 540, 540, 540]
Prompts retrieved: 509220 . Total input tokens: 113454088 . Total output tokens: 100219872
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 94.7449945169501,
    "estimated_duration": 3600.025299062951,
    "input_throughput": 6185.741529593234,
    "output_throughput": 5441.3275943085155,
    "total_throughput": 11627.069123901749,
    "itl": 91.62199187196181,
    "ttft": 1500316.2452197226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7690731482115187,
    "arrivals": 169450,
    "finished_requests": 90232,
    "scheduler_time": 229.13404760647154
}
#Debug simulation 
Total elapsed time: 94.74518911074847. Arrivals time: 0.43421541014686227 Scheduler time: 94.08487967541441 Scheduler overhead time: 0.08703186456114054 Adapter cache time: 0.019925810396671295 Engine time: 0.0852733333595097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 540, 1080, 540, 4320, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 1080, 4320, 540, 4320, 4320, 1080, 1080, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 540, 540, 4320, 1080, 540, 1080, 1080, 4320, 1080, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 4320, 540, 4320, 540, 1080, 4320, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 540, 4320, 1080, 540, 4320, 540, 4320, 540, 540, 1080, 1080, 4320, 1080, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 1080, 540, 1080, 4320, 540, 540, 540, 540, 540, 540, 4320, 540, 540, 540, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 4320, 540, 540, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 1080, 1080, 4320, 540, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 540, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 4320, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 540, 540, 540, 540]
Prompts retrieved: 509220 . Total input tokens: 113454088 . Total output tokens: 100219872
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.79285991610959,
    "estimated_duration": 3600.0158126192746,
    "input_throughput": 6190.019199884192,
    "output_throughput": 5447.529127859976,
    "total_throughput": 11637.548327744167,
    "itl": 90.90325566340321,
    "ttft": 1494127.1955431972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.420337000316946,
    "arrivals": 169450,
    "finished_requests": 90300,
    "scheduler_time": 228.79335223254577
}
#Debug simulation 
Total elapsed time: 82.79304253309965. Arrivals time: 0.4198513375595212 Scheduler time: 82.15096185542643 Scheduler overhead time: 0.08538942085579038 Adapter cache time: 0.02017894759774208 Engine time: 0.08351683849468827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 540, 1080, 540, 4320, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 1080, 4320, 540, 4320, 4320, 1080, 1080, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 540, 540, 4320, 1080, 540, 1080, 1080, 4320, 1080, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 4320, 540, 4320, 540, 1080, 4320, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 540, 4320, 1080, 540, 4320, 540, 4320, 540, 540, 1080, 1080, 4320, 1080, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 1080, 540, 1080, 4320, 540, 540, 540, 540, 540, 540, 4320, 540, 540, 540, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 4320, 540, 540, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 1080, 1080, 4320, 540, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 540, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 4320, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 540, 540, 540, 540]
Prompts retrieved: 509220 . Total input tokens: 113454088 . Total output tokens: 100219872
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 88.41538053005934,
    "estimated_duration": 3600.0234586991137,
    "input_throughput": 6038.102042772489,
    "output_throughput": 5310.971780975275,
    "total_throughput": 11349.073823747764,
    "itl": 85.49727209023227,
    "ttft": 1525214.352686433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.28971433558039,
    "arrivals": 169450,
    "finished_requests": 88072,
    "scheduler_time": 235.53479482842042
}
#Debug simulation 
Total elapsed time: 88.41556957596913. Arrivals time: 0.4161712736822665 Scheduler time: 87.76923229079694 Scheduler overhead time: 0.08860676735639572 Adapter cache time: 0.020250296220183372 Engine time: 0.08677087118849158 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 540, 1080, 540, 4320, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 1080, 4320, 540, 4320, 4320, 1080, 1080, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 540, 540, 4320, 1080, 540, 1080, 1080, 4320, 1080, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 4320, 540, 4320, 540, 1080, 4320, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 540, 4320, 1080, 540, 4320, 540, 4320, 540, 540, 1080, 1080, 4320, 1080, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 1080, 540, 1080, 4320, 540, 540, 540, 540, 540, 540, 4320, 540, 540, 540, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 4320, 540, 540, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 1080, 1080, 4320, 540, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 540, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 4320, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 540, 540, 540, 540]
Prompts retrieved: 509220 . Total input tokens: 113454088 . Total output tokens: 100219872
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 81.4202645807527,
    "estimated_duration": 3600.0174822196177,
    "input_throughput": 6206.630692866428,
    "output_throughput": 5470.807043930492,
    "total_throughput": 11677.437736796921,
    "itl": 91.34444860667581,
    "ttft": 1492428.0545364807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.176000631330529,
    "arrivals": 169450,
    "finished_requests": 90678,
    "scheduler_time": 227.8392533784525
}
#Debug simulation 
Total elapsed time: 81.42045173700899. Arrivals time: 0.4255760540254414 Scheduler time: 80.7735525756143 Scheduler overhead time: 0.08460010448470712 Adapter cache time: 0.01982358144596219 Engine time: 0.08298987476155162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 540, 1080, 540, 4320, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 1080, 4320, 540, 4320, 4320, 1080, 1080, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 540, 540, 4320, 1080, 540, 1080, 1080, 4320, 1080, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 4320, 540, 4320, 540, 1080, 4320, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 540, 4320, 1080, 540, 4320, 540, 4320, 540, 540, 1080, 1080, 4320, 1080, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 1080, 540, 1080, 4320, 540, 540, 540, 540, 540, 540, 4320, 540, 540, 540, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 4320, 540, 540, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 1080, 1080, 4320, 540, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 540, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 4320, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 540, 540, 540, 540]
Prompts retrieved: 509220 . Total input tokens: 113454088 . Total output tokens: 100219872
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 90.44099030969664,
    "estimated_duration": 3600.0070157803866,
    "input_throughput": 5961.126160568889,
    "output_throughput": 5242.717282846365,
    "total_throughput": 11203.843443415255,
    "itl": 84.19114396132491,
    "ttft": 1536005.6461988245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.392519041872617,
    "arrivals": 169450,
    "finished_requests": 86889,
    "scheduler_time": 238.76781143176206
}
#Debug simulation 
Total elapsed time: 90.44118443503976. Arrivals time: 0.4197466103360057 Scheduler time: 89.7897910173051 Scheduler overhead time: 0.08906106557697058 Adapter cache time: 0.020552878733724356 Engine time: 0.08681672113016248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 540, 1080, 540, 4320, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 1080, 4320, 540, 4320, 4320, 1080, 1080, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 540, 540, 4320, 1080, 540, 1080, 1080, 4320, 1080, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 4320, 540, 4320, 540, 1080, 4320, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 540, 4320, 1080, 540, 4320, 540, 4320, 540, 540, 1080, 1080, 4320, 1080, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 1080, 540, 1080, 4320, 540, 540, 540, 540, 540, 540, 4320, 540, 540, 540, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 4320, 540, 540, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 1080, 1080, 4320, 540, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 540, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 4320, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 540, 540, 540, 540]
Prompts retrieved: 509220 . Total input tokens: 113454088 . Total output tokens: 100219872
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 92.17897756397724,
    "estimated_duration": 3600.052247639342,
    "input_throughput": 6151.7904398532755,
    "output_throughput": 5410.77758323452,
    "total_throughput": 11562.568023087795,
    "itl": 89.8994192621327,
    "ttft": 1506612.0702110468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.530308280135489,
    "arrivals": 169450,
    "finished_requests": 89687,
    "scheduler_time": 230.65354512080492
}
#Debug simulation 
Total elapsed time: 92.17916268808767. Arrivals time: 0.438392105512321 Scheduler time: 91.51546076918021 Scheduler overhead time: 0.08658193703740835 Adapter cache time: 0.020165173336863518 Engine time: 0.08478202996775508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 540, 4320, 4320, 540, 4320, 1080, 4320, 540, 1080, 540, 4320, 1080, 1080, 1080, 1080, 4320, 540, 540, 540, 1080, 4320, 540, 4320, 4320, 1080, 1080, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 540, 4320, 4320, 1080, 540, 540, 4320, 1080, 540, 1080, 1080, 4320, 1080, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 4320, 540, 4320, 540, 1080, 4320, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 540, 4320, 1080, 540, 4320, 540, 4320, 540, 540, 1080, 1080, 4320, 1080, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 4320, 540, 4320, 1080, 540, 1080, 4320, 540, 540, 540, 540, 540, 540, 4320, 540, 540, 540, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 1080, 540, 4320, 540, 540, 4320, 540, 4320, 4320, 1080, 4320, 1080, 540, 1080, 4320, 1080, 4320, 540, 4320, 540, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 4320, 1080, 1080, 540, 1080, 4320, 1080, 1080, 4320, 540, 4320, 4320, 1080, 540, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 540, 4320, 1080, 4320, 4320, 540, 4320, 1080, 1080, 4320, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 540, 540, 540, 540]
Prompts retrieved: 509220 . Total input tokens: 113454088 . Total output tokens: 100219872
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 93.42428859975189,
    "estimated_duration": 3600.0497538390837,
    "input_throughput": 5979.236808337221,
    "output_throughput": 5262.947263380209,
    "total_throughput": 11242.18407171743,
    "itl": 84.53119170572856,
    "ttft": 1533301.248470247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.890872079785948,
    "arrivals": 169450,
    "finished_requests": 87175,
    "scheduler_time": 237.97253601328364
}
#Debug simulation 
Total elapsed time: 93.42448708368465. Arrivals time: 0.433230385184288 Scheduler time: 92.75912828324363 Scheduler overhead time: 0.0897340620867908 Adapter cache time: 0.019934871699661016 Engine time: 0.08696198789402843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 270, 1080, 270, 4320, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 1080, 4320, 270, 4320, 4320, 1080, 1080, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 270, 270, 4320, 1080, 270, 1080, 1080, 4320, 1080, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 4320, 270, 4320, 270, 1080, 4320, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 270, 4320, 1080, 270, 4320, 270, 4320, 270, 270, 1080, 1080, 4320, 1080, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 1080, 270, 1080, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 1080, 1080, 4320, 270, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 270, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 4320, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 270, 270, 270, 270]
Prompts retrieved: 486270 . Total input tokens: 108373359 . Total output tokens: 95684083
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.60654690582305,
    "estimated_duration": 3600.078458142818,
    "input_throughput": 6255.481724033751,
    "output_throughput": 5485.368230054444,
    "total_throughput": 11740.849954088195,
    "itl": 92.74063741731972,
    "ttft": 1452913.9733264556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.119530826904876,
    "arrivals": 161934,
    "finished_requests": 91498,
    "scheduler_time": 224.04771818030295
}
#Debug simulation 
Total elapsed time: 87.60674787312746. Arrivals time: 0.4353356780484319 Scheduler time: 86.94993934966624 Scheduler overhead time: 0.08449392765760422 Adapter cache time: 0.020520842168480158 Engine time: 0.08315694937482476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 270, 1080, 270, 4320, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 1080, 4320, 270, 4320, 4320, 1080, 1080, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 270, 270, 4320, 1080, 270, 1080, 1080, 4320, 1080, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 4320, 270, 4320, 270, 1080, 4320, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 270, 4320, 1080, 270, 4320, 270, 4320, 270, 270, 1080, 1080, 4320, 1080, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 1080, 270, 1080, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 1080, 1080, 4320, 270, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 270, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 4320, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 270, 270, 270, 270]
Prompts retrieved: 486270 . Total input tokens: 108373359 . Total output tokens: 95684083
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 89.18225795868784,
    "estimated_duration": 3600.081461243074,
    "input_throughput": 6210.730296163799,
    "output_throughput": 5438.381384081151,
    "total_throughput": 11649.11168024495,
    "itl": 90.46433231171436,
    "ttft": 1449356.0021359273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.407569158435805,
    "arrivals": 161934,
    "finished_requests": 90592,
    "scheduler_time": 227.0693406502258
}
#Debug simulation 
Total elapsed time: 89.18244848586619. Arrivals time: 0.4367343964986503 Scheduler time: 88.52135881315917 Scheduler overhead time: 0.08582370961084962 Adapter cache time: 0.02070642402395606 Engine time: 0.08422342920675874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 270, 1080, 270, 4320, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 1080, 4320, 270, 4320, 4320, 1080, 1080, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 270, 270, 4320, 1080, 270, 1080, 1080, 4320, 1080, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 4320, 270, 4320, 270, 1080, 4320, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 270, 4320, 1080, 270, 4320, 270, 4320, 270, 270, 1080, 1080, 4320, 1080, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 1080, 270, 1080, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 1080, 1080, 4320, 270, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 270, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 4320, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 270, 270, 270, 270]
Prompts retrieved: 486270 . Total input tokens: 108373359 . Total output tokens: 95684083
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.67054606694728,
    "estimated_duration": 3600.052183981809,
    "input_throughput": 6070.391728552351,
    "output_throughput": 5333.9301817456735,
    "total_throughput": 11404.321910298026,
    "itl": 85.97845997405732,
    "ttft": 1468852.9261901975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.643657688223789,
    "arrivals": 161934,
    "finished_requests": 88724,
    "scheduler_time": 231.73281008056847
}
#Debug simulation 
Total elapsed time: 81.67074199486524. Arrivals time: 0.4225440979935229 Scheduler time: 81.02082102186978 Scheduler overhead time: 0.08664432819932699 Adapter cache time: 0.020559548866003752 Engine time: 0.08593582781031728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 270, 1080, 270, 4320, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 1080, 4320, 270, 4320, 4320, 1080, 1080, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 270, 270, 4320, 1080, 270, 1080, 1080, 4320, 1080, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 4320, 270, 4320, 270, 1080, 4320, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 270, 4320, 1080, 270, 4320, 270, 4320, 270, 270, 1080, 1080, 4320, 1080, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 1080, 270, 1080, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 1080, 1080, 4320, 270, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 270, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 4320, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 270, 270, 270, 270]
Prompts retrieved: 486270 . Total input tokens: 108373359 . Total output tokens: 95684083
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 88.2220477629453,
    "estimated_duration": 3600.0265000764484,
    "input_throughput": 6210.6145606220425,
    "output_throughput": 5440.630228578615,
    "total_throughput": 11651.244789200657,
    "itl": 90.65274785980785,
    "ttft": 1451300.7742158882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.124372079879971,
    "arrivals": 161934,
    "finished_requests": 90733,
    "scheduler_time": 226.3649491932011
}
#Debug simulation 
Total elapsed time: 88.22224176116288. Arrivals time: 0.43797473702579737 Scheduler time: 87.55980913853273 Scheduler overhead time: 0.08602426387369633 Adapter cache time: 0.020305422600358725 Engine time: 0.08435163460671902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 270, 1080, 270, 4320, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 1080, 4320, 270, 4320, 4320, 1080, 1080, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 270, 270, 4320, 1080, 270, 1080, 1080, 4320, 1080, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 4320, 270, 4320, 270, 1080, 4320, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 270, 4320, 1080, 270, 4320, 270, 4320, 270, 270, 1080, 1080, 4320, 1080, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 1080, 270, 1080, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 1080, 1080, 4320, 270, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 270, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 4320, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 270, 270, 270, 270]
Prompts retrieved: 486270 . Total input tokens: 108373359 . Total output tokens: 95684083
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 82.36214898619801,
    "estimated_duration": 3600.079210384549,
    "input_throughput": 6081.296749484977,
    "output_throughput": 5332.607111705565,
    "total_throughput": 11413.903861190542,
    "itl": 85.95212265537856,
    "ttft": 1471610.7176549097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.493933279481749,
    "arrivals": 161934,
    "finished_requests": 88963,
    "scheduler_time": 231.4801202656593
}
#Debug simulation 
Total elapsed time: 82.36235791211948. Arrivals time: 0.42125899717211723 Scheduler time: 81.71447737561539 Scheduler overhead time: 0.08682709839195013 Adapter cache time: 0.02008214360103011 Engine time: 0.08544215187430382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 270, 1080, 270, 4320, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 1080, 4320, 270, 4320, 4320, 1080, 1080, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 270, 270, 4320, 1080, 270, 1080, 1080, 4320, 1080, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 4320, 270, 4320, 270, 1080, 4320, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 270, 4320, 1080, 270, 4320, 270, 4320, 270, 270, 1080, 1080, 4320, 1080, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 1080, 270, 1080, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 1080, 1080, 4320, 270, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 270, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 4320, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 270, 270, 270, 270]
Prompts retrieved: 486270 . Total input tokens: 108373359 . Total output tokens: 95684083
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.72617541300133,
    "estimated_duration": 3600.0649555063465,
    "input_throughput": 6214.364261895207,
    "output_throughput": 5442.034308307208,
    "total_throughput": 11656.398570202417,
    "itl": 90.67415627051497,
    "ttft": 1454256.8475449402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.830352564342302,
    "arrivals": 161934,
    "finished_requests": 90768,
    "scheduler_time": 226.0928552526257
}
#Debug simulation 
Total elapsed time: 87.72635963186622. Arrivals time: 0.43819641415029764 Scheduler time: 87.06428866181523 Scheduler overhead time: 0.08570340648293495 Adapter cache time: 0.020213140174746513 Engine time: 0.08421222679316998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 270, 4320, 4320, 270, 4320, 1080, 4320, 270, 1080, 270, 4320, 1080, 1080, 1080, 1080, 4320, 270, 270, 270, 1080, 4320, 270, 4320, 4320, 1080, 1080, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 270, 4320, 4320, 1080, 270, 270, 4320, 1080, 270, 1080, 1080, 4320, 1080, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 4320, 270, 4320, 270, 1080, 4320, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 270, 4320, 1080, 270, 4320, 270, 4320, 270, 270, 1080, 1080, 4320, 1080, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 4320, 270, 4320, 1080, 270, 1080, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 1080, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 1080, 4320, 1080, 270, 1080, 4320, 1080, 4320, 270, 4320, 270, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 4320, 1080, 1080, 270, 1080, 4320, 1080, 1080, 4320, 270, 4320, 4320, 1080, 270, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 270, 4320, 1080, 4320, 4320, 270, 4320, 1080, 1080, 4320, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 270, 270, 270, 270]
Prompts retrieved: 486270 . Total input tokens: 108373359 . Total output tokens: 95684083
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.67776039801538,
    "estimated_duration": 3600.0246291137096,
    "input_throughput": 6073.612336753092,
    "output_throughput": 5325.63745396377,
    "total_throughput": 11399.249790716862,
    "itl": 85.7386906764372,
    "ttft": 1466507.886818328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.615116911232504,
    "arrivals": 161934,
    "finished_requests": 88843,
    "scheduler_time": 231.74509751455824
}
#Debug simulation 
Total elapsed time: 82.67794957989827. Arrivals time: 0.41513223852962255 Scheduler time: 82.03478559898213 Scheduler overhead time: 0.0871570105664432 Adapter cache time: 0.020714391954243183 Engine time: 0.08530244790017605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 135, 1080, 135, 4320, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 1080, 4320, 135, 4320, 4320, 1080, 1080, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 135, 135, 4320, 1080, 135, 1080, 1080, 4320, 1080, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 4320, 135, 4320, 135, 1080, 4320, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 135, 4320, 1080, 135, 4320, 135, 4320, 135, 135, 1080, 1080, 4320, 1080, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 1080, 135, 1080, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 1080, 1080, 4320, 135, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 135, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 4320, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 135, 135, 135, 135]
Prompts retrieved: 474795 . Total input tokens: 105797295 . Total output tokens: 93426675
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 98.81164706125855,
    "estimated_duration": 3600.0062566155357,
    "input_throughput": 6094.450519268389,
    "output_throughput": 5353.585695742379,
    "total_throughput": 11448.036215010769,
    "itl": 90.38463758764749,
    "ttft": 1465849.827985052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7955227843393193,
    "arrivals": 158124,
    "finished_requests": 89073,
    "scheduler_time": 229.1859969374391
}
#Debug simulation 
Total elapsed time: 98.81186060234904. Arrivals time: 0.42590081691741943 Scheduler time: 98.1564693422988 Scheduler overhead time: 0.08875278010964394 Adapter cache time: 0.020472516771405935 Engine time: 0.08574628131464124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 135, 1080, 135, 4320, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 1080, 4320, 135, 4320, 4320, 1080, 1080, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 135, 135, 4320, 1080, 135, 1080, 1080, 4320, 1080, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 4320, 135, 4320, 135, 1080, 4320, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 135, 4320, 1080, 135, 4320, 135, 4320, 135, 135, 1080, 1080, 4320, 1080, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 1080, 135, 1080, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 1080, 1080, 4320, 135, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 135, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 4320, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 135, 135, 135, 135]
Prompts retrieved: 474795 . Total input tokens: 105797295 . Total output tokens: 93426675
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.3250440149568,
    "estimated_duration": 3600.0509388917135,
    "input_throughput": 6241.54112855898,
    "output_throughput": 5472.170348251998,
    "total_throughput": 11713.711476810977,
    "itl": 91.33428821184292,
    "ttft": 1422145.7552613204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.401185237495234,
    "arrivals": 158124,
    "finished_requests": 91079,
    "scheduler_time": 223.96446538525973
}
#Debug simulation 
Total elapsed time: 81.32523681875318. Arrivals time: 0.4145414545200765 Scheduler time: 80.69045565044507 Scheduler overhead time: 0.08414511382579803 Adapter cache time: 0.019837733823806047 Engine time: 0.08304689731448889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 135, 1080, 135, 4320, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 1080, 4320, 135, 4320, 4320, 1080, 1080, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 135, 135, 4320, 1080, 135, 1080, 1080, 4320, 1080, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 4320, 135, 4320, 135, 1080, 4320, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 135, 4320, 1080, 135, 4320, 135, 4320, 135, 135, 1080, 1080, 4320, 1080, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 1080, 135, 1080, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 1080, 1080, 4320, 135, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 135, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 4320, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 135, 135, 135, 135]
Prompts retrieved: 474795 . Total input tokens: 105797295 . Total output tokens: 93426675
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 73.48806503182277,
    "estimated_duration": 3600.0444445000976,
    "input_throughput": 6106.006839327342,
    "output_throughput": 5359.811329406902,
    "total_throughput": 11465.818168734244,
    "itl": 86.6851934159281,
    "ttft": 1450728.142418252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 671,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.032854988551731,
    "arrivals": 158124,
    "finished_requests": 89171,
    "scheduler_time": 229.05196723021928
}
#Debug simulation 
Total elapsed time: 73.48826293507591. Arrivals time: 0.4012067294679582 Scheduler time: 72.86494261212647 Scheduler overhead time: 0.08448368636891246 Adapter cache time: 0.02049947250634432 Engine time: 0.08292120276018977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 135, 1080, 135, 4320, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 1080, 4320, 135, 4320, 4320, 1080, 1080, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 135, 135, 4320, 1080, 135, 1080, 1080, 4320, 1080, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 4320, 135, 4320, 135, 1080, 4320, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 135, 4320, 1080, 135, 4320, 135, 4320, 135, 135, 1080, 1080, 4320, 1080, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 1080, 135, 1080, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 1080, 1080, 4320, 135, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 135, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 4320, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 135, 135, 135, 135]
Prompts retrieved: 474795 . Total input tokens: 105797295 . Total output tokens: 93426675
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 99.22734882403165,
    "estimated_duration": 3600.0466585877352,
    "input_throughput": 6078.152056113617,
    "output_throughput": 5344.423232433254,
    "total_throughput": 11422.575288546872,
    "itl": 88.8497450748867,
    "ttft": 1468334.3714584017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6910870152851514,
    "arrivals": 158124,
    "finished_requests": 88865,
    "scheduler_time": 229.89609490981013
}
#Debug simulation 
Total elapsed time: 99.22753608971834. Arrivals time: 0.429354514926672 Scheduler time: 98.56756427278742 Scheduler overhead time: 0.08943226607516408 Adapter cache time: 0.02017260529100895 Engine time: 0.08634643396362662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 135, 1080, 135, 4320, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 1080, 4320, 135, 4320, 4320, 1080, 1080, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 135, 135, 4320, 1080, 135, 1080, 1080, 4320, 1080, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 4320, 135, 4320, 135, 1080, 4320, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 135, 4320, 1080, 135, 4320, 135, 4320, 135, 135, 1080, 1080, 4320, 1080, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 1080, 135, 1080, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 1080, 1080, 4320, 135, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 135, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 4320, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 135, 135, 135, 135]
Prompts retrieved: 474795 . Total input tokens: 105797295 . Total output tokens: 93426675
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 74.42748211929575,
    "estimated_duration": 3600.0675807741945,
    "input_throughput": 6095.882787644948,
    "output_throughput": 5339.796147900634,
    "total_throughput": 11435.678935545582,
    "itl": 86.24740504770722,
    "ttft": 1449529.8549733956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 658,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.900527643631246,
    "arrivals": 158124,
    "finished_requests": 88898,
    "scheduler_time": 229.93192792891634
}
#Debug simulation 
Total elapsed time: 74.42767092306167. Arrivals time: 0.4110273062251508 Scheduler time: 73.7950480086729 Scheduler overhead time: 0.0843741619028151 Adapter cache time: 0.02035769447684288 Engine time: 0.08258274476975203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 135, 1080, 135, 4320, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 1080, 4320, 135, 4320, 4320, 1080, 1080, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 135, 135, 4320, 1080, 135, 1080, 1080, 4320, 1080, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 4320, 135, 4320, 135, 1080, 4320, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 135, 4320, 1080, 135, 4320, 135, 4320, 135, 135, 1080, 1080, 4320, 1080, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 1080, 135, 1080, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 1080, 1080, 4320, 135, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 135, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 4320, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 135, 135, 135, 135]
Prompts retrieved: 474795 . Total input tokens: 105797295 . Total output tokens: 93426675
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 96.98779023624957,
    "estimated_duration": 3600.000806925359,
    "input_throughput": 6074.784193917384,
    "output_throughput": 5334.211859913661,
    "total_throughput": 11408.996053831046,
    "itl": 88.78059801417322,
    "ttft": 1476162.2315239646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7345937502337447,
    "arrivals": 158124,
    "finished_requests": 88756,
    "scheduler_time": 230.16276881722158
}
#Debug simulation 
Total elapsed time: 96.98797693802044. Arrivals time: 0.43606078531593084 Scheduler time: 96.32250691903755 Scheduler overhead time: 0.08830681163817644 Adapter cache time: 0.020580145064741373 Engine time: 0.08607287099584937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 135, 4320, 4320, 135, 4320, 1080, 4320, 135, 1080, 135, 4320, 1080, 1080, 1080, 1080, 4320, 135, 135, 135, 1080, 4320, 135, 4320, 4320, 1080, 1080, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 135, 4320, 4320, 1080, 135, 135, 4320, 1080, 135, 1080, 1080, 4320, 1080, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 4320, 135, 4320, 135, 1080, 4320, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 135, 4320, 1080, 135, 4320, 135, 4320, 135, 135, 1080, 1080, 4320, 1080, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 4320, 135, 4320, 1080, 135, 1080, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 1080, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 1080, 4320, 1080, 135, 1080, 4320, 1080, 4320, 135, 4320, 135, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 4320, 1080, 1080, 135, 1080, 4320, 1080, 1080, 4320, 135, 4320, 4320, 1080, 135, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 135, 4320, 1080, 4320, 4320, 135, 4320, 1080, 1080, 4320, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 135, 135, 135, 135]
Prompts retrieved: 474795 . Total input tokens: 105797295 . Total output tokens: 93426675
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 75.16190907917917,
    "estimated_duration": 3600.008531622882,
    "input_throughput": 6091.8780628871455,
    "output_throughput": 5336.671519314209,
    "total_throughput": 11428.549582201354,
    "itl": 86.22702050774467,
    "ttft": 1448138.2395508839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.613615222014523,
    "arrivals": 158124,
    "finished_requests": 88848,
    "scheduler_time": 230.14957115364516
}
#Debug simulation 
Total elapsed time: 75.1621032920666. Arrivals time: 0.40908462414518 Scheduler time: 74.53188054729253 Scheduler overhead time: 0.08503902796655893 Adapter cache time: 0.019777829758822918 Engine time: 0.08256312320008874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 66, 1080, 66, 4320, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 1080, 4320, 66, 4320, 4320, 1080, 1080, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 66, 66, 4320, 1080, 66, 1080, 1080, 4320, 1080, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 4320, 66, 4320, 66, 1080, 4320, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 66, 4320, 1080, 66, 4320, 66, 4320, 66, 66, 1080, 1080, 4320, 1080, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 1080, 66, 1080, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 1080, 1080, 4320, 66, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 66, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 4320, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 66, 66, 66, 66]
Prompts retrieved: 468930 . Total input tokens: 104504919 . Total output tokens: 92266567
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.59835772495717,
    "estimated_duration": 3600.025151510203,
    "input_throughput": 6289.723278877994,
    "output_throughput": 5526.086114052422,
    "total_throughput": 11815.809392930416,
    "itl": 93.23692032079953,
    "ttft": 1403099.3337445483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.119530826904876,
    "arrivals": 156118,
    "finished_requests": 92013,
    "scheduler_time": 220.32870982835536
}
#Debug simulation 
Total elapsed time: 82.5985453678295. Arrivals time: 0.4304626928642392 Scheduler time: 81.9500434328802 Scheduler overhead time: 0.0827287151478231 Adapter cache time: 0.020522035658359528 Engine time: 0.08201399864628911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 66, 1080, 66, 4320, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 1080, 4320, 66, 4320, 4320, 1080, 1080, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 66, 66, 4320, 1080, 66, 1080, 1080, 4320, 1080, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 4320, 66, 4320, 66, 1080, 4320, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 66, 4320, 1080, 66, 4320, 66, 4320, 66, 66, 1080, 1080, 4320, 1080, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 1080, 66, 1080, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 1080, 1080, 4320, 66, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 66, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 4320, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 66, 66, 66, 66]
Prompts retrieved: 468930 . Total input tokens: 104504919 . Total output tokens: 92266567
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.52529271598905,
    "estimated_duration": 3600.0274999818125,
    "input_throughput": 6237.637073637201,
    "output_throughput": 5480.2311927060755,
    "total_throughput": 11717.868266343276,
    "itl": 91.25523645820064,
    "ttft": 1405916.0389988797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.943280288884423,
    "arrivals": 156118,
    "finished_requests": 91152,
    "scheduler_time": 222.32829528127755
}
#Debug simulation 
Total elapsed time: 81.5254759718664. Arrivals time: 0.4400714379735291 Scheduler time: 80.86478859884664 Scheduler overhead time: 0.08393707778304815 Adapter cache time: 0.020985365845263004 Engine time: 0.08261286281049252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 66, 1080, 66, 4320, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 1080, 4320, 66, 4320, 4320, 1080, 1080, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 66, 66, 4320, 1080, 66, 1080, 1080, 4320, 1080, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 4320, 66, 4320, 66, 1080, 4320, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 66, 4320, 1080, 66, 4320, 66, 4320, 66, 66, 1080, 1080, 4320, 1080, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 1080, 66, 1080, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 1080, 1080, 4320, 66, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 66, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 4320, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 66, 66, 66, 66]
Prompts retrieved: 468930 . Total input tokens: 104504919 . Total output tokens: 92266567
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.93570072809234,
    "estimated_duration": 3600.076871350682,
    "input_throughput": 6065.408262187347,
    "output_throughput": 5334.874416943848,
    "total_throughput": 11400.282679131195,
    "itl": 85.66937536909943,
    "ttft": 1448148.71134808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.498288367497769,
    "arrivals": 156118,
    "finished_requests": 88775,
    "scheduler_time": 229.56397613663353
}
#Debug simulation 
Total elapsed time: 80.93590244743973. Arrivals time: 0.42650312930345535 Scheduler time: 80.28576416149735 Scheduler overhead time: 0.08514557546004653 Adapter cache time: 0.020244496874511242 Engine time: 0.08430638909339905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 66, 1080, 66, 4320, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 1080, 4320, 66, 4320, 4320, 1080, 1080, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 66, 66, 4320, 1080, 66, 1080, 1080, 4320, 1080, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 4320, 66, 4320, 66, 1080, 4320, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 66, 4320, 1080, 66, 4320, 66, 4320, 66, 66, 1080, 1080, 4320, 1080, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 1080, 66, 1080, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 1080, 1080, 4320, 66, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 66, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 4320, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 66, 66, 66, 66]
Prompts retrieved: 468930 . Total input tokens: 104504919 . Total output tokens: 92266567
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 82.81312795588747,
    "estimated_duration": 3600.078423988089,
    "input_throughput": 6220.149775291142,
    "output_throughput": 5472.764389997406,
    "total_throughput": 11692.914165288548,
    "itl": 91.07062979378298,
    "ttft": 1410804.2832118028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.384450764348724,
    "arrivals": 156118,
    "finished_requests": 90987,
    "scheduler_time": 222.78488094862047
}
#Debug simulation 
Total elapsed time: 82.8133208649233. Arrivals time: 0.44093729835003614 Scheduler time: 82.1516950041987 Scheduler overhead time: 0.08407852845266461 Adapter cache time: 0.020346546079963446 Engine time: 0.08299989812076092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 66, 1080, 66, 4320, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 1080, 4320, 66, 4320, 4320, 1080, 1080, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 66, 66, 4320, 1080, 66, 1080, 1080, 4320, 1080, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 4320, 66, 4320, 66, 1080, 4320, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 66, 4320, 1080, 66, 4320, 66, 4320, 66, 66, 1080, 1080, 4320, 1080, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 1080, 66, 1080, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 1080, 1080, 4320, 66, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 66, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 4320, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 66, 66, 66, 66]
Prompts retrieved: 468930 . Total input tokens: 104504919 . Total output tokens: 92266567
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 80.52315480215475,
    "estimated_duration": 3600.084021002578,
    "input_throughput": 6066.925069686967,
    "output_throughput": 5341.150064226857,
    "total_throughput": 11408.075133913822,
    "itl": 85.90882630123043,
    "ttft": 1447664.8956074035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.444283791147207,
    "arrivals": 156118,
    "finished_requests": 88803,
    "scheduler_time": 229.22420668932415
}
#Debug simulation 
Total elapsed time: 80.523355148267. Arrivals time: 0.4325423720292747 Scheduler time: 79.8650561934337 Scheduler overhead time: 0.08647629711776972 Adapter cache time: 0.020976825151592493 Engine time: 0.08416282990947366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 66, 1080, 66, 4320, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 1080, 4320, 66, 4320, 4320, 1080, 1080, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 66, 66, 4320, 1080, 66, 1080, 1080, 4320, 1080, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 4320, 66, 4320, 66, 1080, 4320, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 66, 4320, 1080, 66, 4320, 66, 4320, 66, 66, 1080, 1080, 4320, 1080, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 1080, 66, 1080, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 1080, 1080, 4320, 66, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 66, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 4320, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 66, 66, 66, 66]
Prompts retrieved: 468930 . Total input tokens: 104504919 . Total output tokens: 92266567
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 78.88635929813609,
    "estimated_duration": 3600.0706965545237,
    "input_throughput": 6246.813158842477,
    "output_throughput": 5493.666282422814,
    "total_throughput": 11740.47944126529,
    "itl": 91.53260757269891,
    "ttft": 1417988.1481648777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 634,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.047405876321699,
    "arrivals": 156118,
    "finished_requests": 91295,
    "scheduler_time": 221.84740834879506
}
#Debug simulation 
Total elapsed time: 78.88654979411513. Arrivals time: 0.4367683231830597 Scheduler time: 78.23077420331538 Scheduler overhead time: 0.0842669801786542 Adapter cache time: 0.020472289994359016 Engine time: 0.08156574843451381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 66, 4320, 4320, 66, 4320, 1080, 4320, 66, 1080, 66, 4320, 1080, 1080, 1080, 1080, 4320, 66, 66, 66, 1080, 4320, 66, 4320, 4320, 1080, 1080, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 66, 4320, 4320, 1080, 66, 66, 4320, 1080, 66, 1080, 1080, 4320, 1080, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 4320, 66, 4320, 66, 1080, 4320, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 66, 4320, 1080, 66, 4320, 66, 4320, 66, 66, 1080, 1080, 4320, 1080, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 4320, 66, 4320, 1080, 66, 1080, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 1080, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 1080, 4320, 1080, 66, 1080, 4320, 1080, 4320, 66, 4320, 66, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 4320, 1080, 1080, 66, 1080, 4320, 1080, 1080, 4320, 66, 4320, 4320, 1080, 66, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 66, 4320, 1080, 4320, 4320, 66, 4320, 1080, 1080, 4320, 66, 1080, 66, 66, 66, 66, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 66, 66, 66, 66]
Prompts retrieved: 468930 . Total input tokens: 104504919 . Total output tokens: 92266567
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.44480306794867,
    "estimated_duration": 3600.0225582732937,
    "input_throughput": 6062.240623977565,
    "output_throughput": 5334.40324029829,
    "total_throughput": 11396.643864275855,
    "itl": 85.82651659531304,
    "ttft": 1442112.9702512722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 596,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.376044846884942,
    "arrivals": 156118,
    "finished_requests": 88721,
    "scheduler_time": 229.52031196416732
}
#Debug simulation 
Total elapsed time: 82.44501067930833. Arrivals time: 0.43906120024621487 Scheduler time: 81.77851461339742 Scheduler overhead time: 0.08630282571539283 Adapter cache time: 0.020517957396805286 Engine time: 0.08535191509872675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 33, 1080, 33, 4320, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 1080, 4320, 33, 4320, 4320, 1080, 1080, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 33, 33, 4320, 1080, 33, 1080, 1080, 4320, 1080, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 4320, 33, 4320, 33, 1080, 4320, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 33, 4320, 1080, 33, 4320, 33, 4320, 33, 33, 1080, 1080, 4320, 1080, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 1080, 33, 1080, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 1080, 1080, 4320, 33, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 33, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 4320, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 33, 33, 33, 33]
Prompts retrieved: 466125 . Total input tokens: 103890922 . Total output tokens: 91717117
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 96.97393335402012,
    "estimated_duration": 3600.0103167040465,
    "input_throughput": 6162.1409519487515,
    "output_throughput": 5410.58088351064,
    "total_throughput": 11572.721835459392,
    "itl": 90.33343915780607,
    "ttft": 1437711.3275265354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7095614669239674,
    "arrivals": 155196,
    "finished_requests": 90003,
    "scheduler_time": 225.30899990167998
}
#Debug simulation 
Total elapsed time: 96.97411264898255. Arrivals time: 0.44108851812779903 Scheduler time: 96.30662470730022 Scheduler overhead time: 0.08751026168465614 Adapter cache time: 0.019941067323088646 Engine time: 0.08493579644709826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 33, 1080, 33, 4320, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 1080, 4320, 33, 4320, 4320, 1080, 1080, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 33, 33, 4320, 1080, 33, 1080, 1080, 4320, 1080, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 4320, 33, 4320, 33, 1080, 4320, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 33, 4320, 1080, 33, 4320, 33, 4320, 33, 33, 1080, 1080, 4320, 1080, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 1080, 33, 1080, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 1080, 1080, 4320, 33, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 33, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 4320, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 33, 33, 33, 33]
Prompts retrieved: 466125 . Total input tokens: 103890922 . Total output tokens: 91717117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 95.10076893493533,
    "estimated_duration": 3600.053932288234,
    "input_throughput": 6076.458134085686,
    "output_throughput": 5346.306017078583,
    "total_throughput": 11422.764151164269,
    "itl": 88.24696280112039,
    "ttft": 1444777.2507277757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8768537279544453,
    "arrivals": 155196,
    "finished_requests": 88923,
    "scheduler_time": 228.4366139091035
}
#Debug simulation 
Total elapsed time: 95.1009513111785. Arrivals time: 0.43743014615029097 Scheduler time: 94.43387486599386 Scheduler overhead time: 0.08835314447060227 Adapter cache time: 0.020131238736212254 Engine time: 0.08666880149394274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 33, 1080, 33, 4320, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 1080, 4320, 33, 4320, 4320, 1080, 1080, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 33, 33, 4320, 1080, 33, 1080, 1080, 4320, 1080, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 4320, 33, 4320, 33, 1080, 4320, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 33, 4320, 1080, 33, 4320, 33, 4320, 33, 33, 1080, 1080, 4320, 1080, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 1080, 33, 1080, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 1080, 1080, 4320, 33, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 33, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 4320, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 33, 33, 33, 33]
Prompts retrieved: 466125 . Total input tokens: 103890922 . Total output tokens: 91717117
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 89.81251831399277,
    "estimated_duration": 3600.0344929390526,
    "input_throughput": 6010.93074037009,
    "output_throughput": 5282.872716164828,
    "total_throughput": 11293.803456534917,
    "itl": 84.40262204212398,
    "ttft": 1462666.0220059648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5422493751580255,
    "arrivals": 155196,
    "finished_requests": 87888,
    "scheduler_time": 231.47758608319847
}
#Debug simulation 
Total elapsed time: 89.81269597401842. Arrivals time: 0.4286659932695329 Scheduler time: 89.15021678712219 Scheduler overhead time: 0.08960856217890978 Adapter cache time: 0.020806956104934216 Engine time: 0.08823811495676637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 33, 1080, 33, 4320, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 1080, 4320, 33, 4320, 4320, 1080, 1080, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 33, 33, 4320, 1080, 33, 1080, 1080, 4320, 1080, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 4320, 33, 4320, 33, 1080, 4320, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 33, 4320, 1080, 33, 4320, 33, 4320, 33, 33, 1080, 1080, 4320, 1080, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 1080, 33, 1080, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 1080, 1080, 4320, 33, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 33, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 4320, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 33, 33, 33, 33]
Prompts retrieved: 466125 . Total input tokens: 103890922 . Total output tokens: 91717117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 91.98926172591746,
    "estimated_duration": 3600.0856744524594,
    "input_throughput": 6160.921712890445,
    "output_throughput": 5411.240387484077,
    "total_throughput": 11572.162100374522,
    "itl": 89.43444512986957,
    "ttft": 1447383.8866496799,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.993917219121935,
    "arrivals": 155196,
    "finished_requests": 90010,
    "scheduler_time": 225.1959657836437
}
#Debug simulation 
Total elapsed time: 91.98946131998673. Arrivals time: 0.4316606973297894 Scheduler time: 91.32963536772877 Scheduler overhead time: 0.08732508262619376 Adapter cache time: 0.02012349246069789 Engine time: 0.08641162374988198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 33, 1080, 33, 4320, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 1080, 4320, 33, 4320, 4320, 1080, 1080, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 33, 33, 4320, 1080, 33, 1080, 1080, 4320, 1080, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 4320, 33, 4320, 33, 1080, 4320, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 33, 4320, 1080, 33, 4320, 33, 4320, 33, 33, 1080, 1080, 4320, 1080, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 1080, 33, 1080, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 1080, 1080, 4320, 33, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 33, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 4320, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 33, 33, 33, 33]
Prompts retrieved: 466125 . Total input tokens: 103890922 . Total output tokens: 91717117
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 92.83869616594166,
    "estimated_duration": 3600.0770480378415,
    "input_throughput": 5992.659799255838,
    "output_throughput": 5265.785078220008,
    "total_throughput": 11258.444877475846,
    "itl": 84.03445706755186,
    "ttft": 1466198.4229915733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 513,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.833630097806488,
    "arrivals": 155196,
    "finished_requests": 87629,
    "scheduler_time": 232.41458784597634
}
#Debug simulation 
Total elapsed time: 92.8388991188258. Arrivals time: 0.4256612388417125 Scheduler time: 92.17990195844322 Scheduler overhead time: 0.089912551920861 Adapter cache time: 0.020028716884553432 Engine time: 0.08814017893746495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 33, 1080, 33, 4320, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 1080, 4320, 33, 4320, 4320, 1080, 1080, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 33, 33, 4320, 1080, 33, 1080, 1080, 4320, 1080, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 4320, 33, 4320, 33, 1080, 4320, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 33, 4320, 1080, 33, 4320, 33, 4320, 33, 33, 1080, 1080, 4320, 1080, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 1080, 33, 1080, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 1080, 1080, 4320, 33, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 33, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 4320, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 33, 33, 33, 33]
Prompts retrieved: 466125 . Total input tokens: 103890922 . Total output tokens: 91717117
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 93.96643366804346,
    "estimated_duration": 3600.0416190963333,
    "input_throughput": 6117.605664105705,
    "output_throughput": 5397.498989158225,
    "total_throughput": 11515.10465326393,
    "itl": 89.22911220736461,
    "ttft": 1432881.0213012344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.555843963897771,
    "arrivals": 155196,
    "finished_requests": 89542,
    "scheduler_time": 227.15457578693247
}
#Debug simulation 
Total elapsed time: 93.96662036981434. Arrivals time: 0.43480733409523964 Scheduler time: 93.30335844121873 Scheduler overhead time: 0.08759240666404366 Adapter cache time: 0.02036032546311617 Engine time: 0.08611775888130069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 4320, 33, 4320, 4320, 33, 4320, 1080, 4320, 33, 1080, 33, 4320, 1080, 1080, 1080, 1080, 4320, 33, 33, 33, 1080, 4320, 33, 4320, 4320, 1080, 1080, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 1080, 1080, 4320, 1080, 4320, 33, 4320, 4320, 1080, 33, 33, 4320, 1080, 33, 1080, 1080, 4320, 1080, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 4320, 33, 4320, 33, 1080, 4320, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 33, 4320, 1080, 33, 4320, 33, 4320, 33, 33, 1080, 1080, 4320, 1080, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 4320, 33, 4320, 1080, 33, 1080, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 1080, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 1080, 4320, 1080, 33, 1080, 4320, 1080, 4320, 33, 4320, 33, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 4320, 1080, 1080, 33, 1080, 4320, 1080, 1080, 4320, 33, 4320, 4320, 1080, 33, 4320, 4320, 1080, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 1080, 1080, 4320, 33, 4320, 1080, 4320, 4320, 33, 4320, 1080, 1080, 4320, 33, 1080, 33, 33, 33, 33, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 33, 33, 33, 33]
Prompts retrieved: 466125 . Total input tokens: 103890922 . Total output tokens: 91717117
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 95.05913783283904,
    "estimated_duration": 3600.012420214424,
    "input_throughput": 5988.387950815999,
    "output_throughput": 5280.732058937642,
    "total_throughput": 11269.120009753642,
    "itl": 84.33247897003032,
    "ttft": 1455059.852226255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8474423900433234,
    "arrivals": 155196,
    "finished_requests": 87627,
    "scheduler_time": 232.25311273458163
}
#Debug simulation 
Total elapsed time: 95.05932835303247. Arrivals time: 0.42407246259972453 Scheduler time: 94.40275280270725 Scheduler overhead time: 0.08956953696906567 Adapter cache time: 0.020320050418376923 Engine time: 0.08723873691633344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 270, 540, 270, 4320, 540, 540, 540, 540, 4320, 270, 270, 270, 540, 4320, 270, 4320, 4320, 540, 540, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 540, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 270, 270, 4320, 540, 270, 540, 540, 4320, 540, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 4320, 4320, 270, 4320, 270, 540, 4320, 4320, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 270, 4320, 540, 270, 4320, 270, 4320, 270, 270, 540, 540, 4320, 540, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 270, 540, 4320, 4320, 270, 4320, 540, 270, 540, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 540, 540, 270, 540, 4320, 540, 540, 4320, 270, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 270, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 4320, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 4320, 540, 270, 270, 270, 270]
Prompts retrieved: 440370 . Total input tokens: 98200304 . Total output tokens: 86622275
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 99.24039835995063,
    "estimated_duration": 3600.0390695685883,
    "input_throughput": 6016.811646045753,
    "output_throughput": 5299.439986990541,
    "total_throughput": 11316.251633036294,
    "itl": 88.3824696481656,
    "ttft": 1423308.123405203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.557476059189114,
    "arrivals": 146502,
    "finished_requests": 88050,
    "scheduler_time": 227.10708930647877
}
#Debug simulation 
Total elapsed time: 99.24060252914205. Arrivals time: 0.433675286360085 Scheduler time: 98.57396030379459 Scheduler overhead time: 0.08970965584740043 Adapter cache time: 0.020900243427604437 Engine time: 0.08722757594659925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 270, 540, 270, 4320, 540, 540, 540, 540, 4320, 270, 270, 270, 540, 4320, 270, 4320, 4320, 540, 540, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 540, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 270, 270, 4320, 540, 270, 540, 540, 4320, 540, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 4320, 4320, 270, 4320, 270, 540, 4320, 4320, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 270, 4320, 540, 270, 4320, 270, 4320, 270, 270, 540, 540, 4320, 540, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 270, 540, 4320, 4320, 270, 4320, 540, 270, 540, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 540, 540, 270, 540, 4320, 540, 540, 4320, 270, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 270, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 4320, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 4320, 540, 270, 270, 270, 270]
Prompts retrieved: 440370 . Total input tokens: 98200304 . Total output tokens: 86622275
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 98.68129412410781,
    "estimated_duration": 3600.0720633204755,
    "input_throughput": 6007.848904016406,
    "output_throughput": 5289.276065889939,
    "total_throughput": 11297.124969906345,
    "itl": 87.53532403637145,
    "ttft": 1430619.9341182762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8840686859423346,
    "arrivals": 146502,
    "finished_requests": 87932,
    "scheduler_time": 227.28845517227603
}
#Debug simulation 
Total elapsed time: 98.68148887017742. Arrivals time: 0.43537898547947407 Scheduler time: 98.01242179935798 Scheduler overhead time: 0.0897176880389452 Adapter cache time: 0.019989808090031147 Engine time: 0.08853311324492097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 270, 540, 270, 4320, 540, 540, 540, 540, 4320, 270, 270, 270, 540, 4320, 270, 4320, 4320, 540, 540, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 540, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 270, 270, 4320, 540, 270, 540, 540, 4320, 540, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 4320, 4320, 270, 4320, 270, 540, 4320, 4320, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 270, 4320, 540, 270, 4320, 270, 4320, 270, 270, 540, 540, 4320, 540, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 270, 540, 4320, 4320, 270, 4320, 540, 270, 540, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 540, 540, 270, 540, 4320, 540, 540, 4320, 270, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 270, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 4320, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 4320, 540, 270, 270, 270, 270]
Prompts retrieved: 440370 . Total input tokens: 98200304 . Total output tokens: 86622275
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 94.48761170031503,
    "estimated_duration": 3600.0368420336845,
    "input_throughput": 5845.331568360406,
    "output_throughput": 5148.96841709226,
    "total_throughput": 10994.299985452666,
    "itl": 82.09284649462444,
    "ttft": 1452806.4541739721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.436324504101681,
    "arrivals": 146502,
    "finished_requests": 85540,
    "scheduler_time": 233.92657163580583
}
#Debug simulation 
Total elapsed time: 94.48779024602845. Arrivals time: 0.405316102784127 Scheduler time: 93.84382586646825 Scheduler overhead time: 0.09198553627356887 Adapter cache time: 0.020966333337128162 Engine time: 0.08942926721647382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 270, 540, 270, 4320, 540, 540, 540, 540, 4320, 270, 270, 270, 540, 4320, 270, 4320, 4320, 540, 540, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 540, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 270, 270, 4320, 540, 270, 540, 540, 4320, 540, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 4320, 4320, 270, 4320, 270, 540, 4320, 4320, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 270, 4320, 540, 270, 4320, 270, 4320, 270, 270, 540, 540, 4320, 540, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 270, 540, 4320, 4320, 270, 4320, 540, 270, 540, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 540, 540, 270, 540, 4320, 540, 540, 4320, 270, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 270, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 4320, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 4320, 540, 270, 270, 270, 270]
Prompts retrieved: 440370 . Total input tokens: 98200304 . Total output tokens: 86622275
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 96.93939132802188,
    "estimated_duration": 3600.0490976480187,
    "input_throughput": 5936.085986705449,
    "output_throughput": 5230.4703322801815,
    "total_throughput": 11166.556318985631,
    "itl": 85.87587895352077,
    "ttft": 1434613.1393420596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9236940887756595,
    "arrivals": 146502,
    "finished_requests": 86912,
    "scheduler_time": 230.0110809616405
}
#Debug simulation 
Total elapsed time: 96.93957024998963. Arrivals time: 0.41409389209002256 Scheduler time: 96.29247258417308 Scheduler overhead time: 0.08964880649000406 Adapter cache time: 0.020663523580878973 Engine time: 0.08749070018529892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 270, 540, 270, 4320, 540, 540, 540, 540, 4320, 270, 270, 270, 540, 4320, 270, 4320, 4320, 540, 540, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 540, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 270, 270, 4320, 540, 270, 540, 540, 4320, 540, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 4320, 4320, 270, 4320, 270, 540, 4320, 4320, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 270, 4320, 540, 270, 4320, 270, 4320, 270, 270, 540, 540, 4320, 540, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 270, 540, 4320, 4320, 270, 4320, 540, 270, 540, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 540, 540, 270, 540, 4320, 540, 540, 4320, 270, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 270, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 4320, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 4320, 540, 270, 270, 270, 270]
Prompts retrieved: 440370 . Total input tokens: 98200304 . Total output tokens: 86622275
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 93.09407451283187,
    "estimated_duration": 3600.0339541586222,
    "input_throughput": 5872.447390552722,
    "output_throughput": 5190.165492304888,
    "total_throughput": 11062.612882857611,
    "itl": 82.80565514691428,
    "ttft": 1441903.2147921089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1003898702236246,
    "arrivals": 146502,
    "finished_requests": 85931,
    "scheduler_time": 232.9454464997299
}
#Debug simulation 
Total elapsed time: 93.09427404403687. Arrivals time: 0.4164941315539181 Scheduler time: 92.44136330625042 Scheduler overhead time: 0.09073019726201892 Adapter cache time: 0.021037192083895206 Engine time: 0.08866199152544141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 270, 540, 270, 4320, 540, 540, 540, 540, 4320, 270, 270, 270, 540, 4320, 270, 4320, 4320, 540, 540, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 540, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 270, 270, 4320, 540, 270, 540, 540, 4320, 540, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 4320, 4320, 270, 4320, 270, 540, 4320, 4320, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 270, 4320, 540, 270, 4320, 270, 4320, 270, 270, 540, 540, 4320, 540, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 270, 540, 4320, 4320, 270, 4320, 540, 270, 540, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 540, 540, 270, 540, 4320, 540, 540, 4320, 270, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 270, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 4320, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 4320, 540, 270, 270, 270, 270]
Prompts retrieved: 440370 . Total input tokens: 98200304 . Total output tokens: 86622275
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 75.83181669004261,
    "estimated_duration": 3600.0957359909166,
    "input_throughput": 6178.531803370943,
    "output_throughput": 5446.954036238295,
    "total_throughput": 11625.485839609239,
    "itl": 90.48145134273445,
    "ttft": 1397418.7287274324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 625,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9899505878565646,
    "arrivals": 146502,
    "finished_requests": 90501,
    "scheduler_time": 219.84393069925133
}
#Debug simulation 
Total elapsed time: 75.83200452616438. Arrivals time: 0.41508987825363874 Scheduler time: 75.19827234139666 Scheduler overhead time: 0.08313788613304496 Adapter cache time: 0.020316416397690773 Engine time: 0.08178236894309521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 270, 4320, 4320, 270, 4320, 540, 4320, 270, 540, 270, 4320, 540, 540, 540, 540, 4320, 270, 270, 270, 540, 4320, 270, 4320, 4320, 540, 540, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 540, 540, 4320, 540, 4320, 270, 4320, 4320, 540, 270, 270, 4320, 540, 270, 540, 540, 4320, 540, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 4320, 4320, 270, 4320, 270, 540, 4320, 4320, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 270, 4320, 540, 270, 4320, 270, 4320, 270, 270, 540, 540, 4320, 540, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 270, 540, 4320, 4320, 270, 4320, 540, 270, 540, 4320, 270, 270, 270, 270, 270, 270, 4320, 270, 270, 270, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 270, 540, 540, 270, 540, 270, 4320, 270, 270, 4320, 270, 4320, 4320, 540, 4320, 540, 270, 540, 4320, 540, 4320, 270, 4320, 270, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 270, 4320, 270, 4320, 540, 540, 270, 540, 4320, 540, 540, 4320, 270, 4320, 4320, 540, 270, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 270, 4320, 540, 4320, 4320, 270, 4320, 540, 540, 4320, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 4320, 540, 270, 270, 270, 270]
Prompts retrieved: 440370 . Total input tokens: 98200304 . Total output tokens: 86622275
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 97.83310319110751,
    "estimated_duration": 3600.0404816040145,
    "input_throughput": 5865.168213495251,
    "output_throughput": 5186.073627616949,
    "total_throughput": 11051.2418411122,
    "itl": 82.68043567357027,
    "ttft": 1444244.209339189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.871409768741612,
    "arrivals": 146502,
    "finished_requests": 85891,
    "scheduler_time": 233.20092446693712
}
#Debug simulation 
Total elapsed time: 97.83329266821966. Arrivals time: 0.41279460536316037 Scheduler time: 97.18384826881811 Scheduler overhead time: 0.09133423678576946 Adapter cache time: 0.02048057271167636 Engine time: 0.08911733189597726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 135, 540, 135, 4320, 540, 540, 540, 540, 4320, 135, 135, 135, 540, 4320, 135, 4320, 4320, 540, 540, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 540, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 135, 135, 4320, 540, 135, 540, 540, 4320, 540, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 4320, 4320, 135, 4320, 135, 540, 4320, 4320, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 135, 4320, 540, 135, 4320, 135, 4320, 135, 135, 540, 540, 4320, 540, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 135, 540, 4320, 4320, 135, 4320, 540, 135, 540, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 540, 540, 135, 540, 4320, 540, 540, 4320, 135, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 135, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 4320, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 4320, 540, 135, 135, 135, 135]
Prompts retrieved: 428895 . Total input tokens: 95613894 . Total output tokens: 84364510
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 92.57294410886243,
    "estimated_duration": 3600.1114178544726,
    "input_throughput": 6131.421069505435,
    "output_throughput": 5345.037629826447,
    "total_throughput": 11476.458699331883,
    "itl": 89.22978889632041,
    "ttft": 1374786.8303549567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.238554189479979,
    "arrivals": 142715,
    "finished_requests": 89129,
    "scheduler_time": 221.35498152168455
}
#Debug simulation 
Total elapsed time: 92.57314418582246. Arrivals time: 0.4153965865261853 Scheduler time: 91.92651822185144 Scheduler overhead time: 0.08864201046526432 Adapter cache time: 0.02129415236413479 Engine time: 0.0869207656942308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 135, 540, 135, 4320, 540, 540, 540, 540, 4320, 135, 135, 135, 540, 4320, 135, 4320, 4320, 540, 540, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 540, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 135, 135, 4320, 540, 135, 540, 540, 4320, 540, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 4320, 4320, 135, 4320, 135, 540, 4320, 4320, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 135, 4320, 540, 135, 4320, 135, 4320, 135, 135, 540, 540, 4320, 540, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 135, 540, 4320, 4320, 135, 4320, 540, 135, 540, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 540, 540, 135, 540, 4320, 540, 540, 4320, 135, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 135, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 4320, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 4320, 540, 135, 135, 135, 135]
Prompts retrieved: 428895 . Total input tokens: 95613894 . Total output tokens: 84364510
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 96.76546720415354,
    "estimated_duration": 3600.0303552166015,
    "input_throughput": 6055.406440784966,
    "output_throughput": 5267.754471158895,
    "total_throughput": 11323.160911943862,
    "itl": 86.74823929325437,
    "ttft": 1428684.9568344129,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.340953507083474,
    "arrivals": 142715,
    "finished_requests": 87874,
    "scheduler_time": 225.22561357564481
}
#Debug simulation 
Total elapsed time: 96.76566611602902. Arrivals time: 0.41656707786023617 Scheduler time: 96.11212170450017 Scheduler overhead time: 0.09135513752698898 Adapter cache time: 0.021132991183549166 Engine time: 0.08855954790487885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 135, 540, 135, 4320, 540, 540, 540, 540, 4320, 135, 135, 135, 540, 4320, 135, 4320, 4320, 540, 540, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 540, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 135, 135, 4320, 540, 135, 540, 540, 4320, 540, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 4320, 4320, 135, 4320, 135, 540, 4320, 4320, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 135, 4320, 540, 135, 4320, 135, 4320, 135, 135, 540, 540, 4320, 540, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 135, 540, 4320, 4320, 135, 4320, 540, 135, 540, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 540, 540, 135, 540, 4320, 540, 540, 4320, 135, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 135, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 4320, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 4320, 540, 135, 135, 135, 135]
Prompts retrieved: 428895 . Total input tokens: 95613894 . Total output tokens: 84364510
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 88.62066793581471,
    "estimated_duration": 3600.0111340009635,
    "input_throughput": 5926.6486146357265,
    "output_throughput": 5162.992087566979,
    "total_throughput": 11089.640702202705,
    "itl": 82.08851690379605,
    "ttft": 1440601.264678511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.188009189046019,
    "arrivals": 142715,
    "finished_requests": 86153,
    "scheduler_time": 230.24764155876207
}
#Debug simulation 
Total elapsed time: 88.62086372682825. Arrivals time: 0.4184339065104723 Scheduler time: 87.96367332478985 Scheduler overhead time: 0.09265617607161403 Adapter cache time: 0.022244663909077644 Engine time: 0.08783798944205046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 135, 540, 135, 4320, 540, 540, 540, 540, 4320, 135, 135, 135, 540, 4320, 135, 4320, 4320, 540, 540, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 540, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 135, 135, 4320, 540, 135, 540, 540, 4320, 540, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 4320, 4320, 135, 4320, 135, 540, 4320, 4320, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 135, 4320, 540, 135, 4320, 135, 4320, 135, 135, 540, 540, 4320, 540, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 135, 540, 4320, 4320, 135, 4320, 540, 135, 540, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 540, 540, 135, 540, 4320, 540, 540, 4320, 135, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 135, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 4320, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 4320, 540, 135, 135, 135, 135]
Prompts retrieved: 428895 . Total input tokens: 95613894 . Total output tokens: 84364510
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 100.05905003705993,
    "estimated_duration": 3600.063409499148,
    "input_throughput": 6058.978556445663,
    "output_throughput": 5269.107746810228,
    "total_throughput": 11328.08630325589,
    "itl": 86.73238595744331,
    "ttft": 1419294.064180327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 587,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.026394007750782,
    "arrivals": 142715,
    "finished_requests": 87942,
    "scheduler_time": 225.29146979152716
}
#Debug simulation 
Total elapsed time: 100.05924937268719. Arrivals time: 0.42779928306117654 Scheduler time: 99.39529332052916 Scheduler overhead time: 0.09088164800778031 Adapter cache time: 0.020948892924934626 Engine time: 0.08885032357648015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 135, 540, 135, 4320, 540, 540, 540, 540, 4320, 135, 135, 135, 540, 4320, 135, 4320, 4320, 540, 540, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 540, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 135, 135, 4320, 540, 135, 540, 540, 4320, 540, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 4320, 4320, 135, 4320, 135, 540, 4320, 4320, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 135, 4320, 540, 135, 4320, 135, 4320, 135, 135, 540, 540, 4320, 540, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 135, 540, 4320, 4320, 135, 4320, 540, 135, 540, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 540, 540, 135, 540, 4320, 540, 540, 4320, 135, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 135, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 4320, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 4320, 540, 135, 135, 135, 135]
Prompts retrieved: 428895 . Total input tokens: 95613894 . Total output tokens: 84364510
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 94.16255223471671,
    "estimated_duration": 3600.074900667351,
    "input_throughput": 5951.80672380679,
    "output_throughput": 5189.461473853449,
    "total_throughput": 11141.268197660238,
    "itl": 82.49887867137461,
    "ttft": 1433227.4068060387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.398473929762874,
    "arrivals": 142715,
    "finished_requests": 86401,
    "scheduler_time": 229.1085731582629
}
#Debug simulation 
Total elapsed time: 94.16274401266128. Arrivals time: 0.4090187903493643 Scheduler time: 93.51428064284846 Scheduler overhead time: 0.09251030208542943 Adapter cache time: 0.020927172619849443 Engine time: 0.08977293688803911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 135, 540, 135, 4320, 540, 540, 540, 540, 4320, 135, 135, 135, 540, 4320, 135, 4320, 4320, 540, 540, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 540, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 135, 135, 4320, 540, 135, 540, 540, 4320, 540, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 4320, 4320, 135, 4320, 135, 540, 4320, 4320, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 135, 4320, 540, 135, 4320, 135, 4320, 135, 135, 540, 540, 4320, 540, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 135, 540, 4320, 4320, 135, 4320, 540, 135, 540, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 540, 540, 135, 540, 4320, 540, 540, 4320, 135, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 135, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 4320, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 4320, 540, 135, 135, 135, 135]
Prompts retrieved: 428895 . Total input tokens: 95613894 . Total output tokens: 84364510
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 93.08613520488143,
    "estimated_duration": 3600.0293592323374,
    "input_throughput": 6129.501678482253,
    "output_throughput": 5337.209528784836,
    "total_throughput": 11466.71120726709,
    "itl": 87.78478866950708,
    "ttft": 1389726.9979897237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.568611805778912,
    "arrivals": 142715,
    "finished_requests": 89052,
    "scheduler_time": 221.99515621204077
}
#Debug simulation 
Total elapsed time: 93.08633859828115. Arrivals time: 0.43202701350674033 Scheduler time: 92.4199082008563 Scheduler overhead time: 0.09039203077554703 Adapter cache time: 0.020516689866781235 Engine time: 0.088177181314677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 135, 4320, 4320, 135, 4320, 540, 4320, 135, 540, 135, 4320, 540, 540, 540, 540, 4320, 135, 135, 135, 540, 4320, 135, 4320, 4320, 540, 540, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 540, 540, 4320, 540, 4320, 135, 4320, 4320, 540, 135, 135, 4320, 540, 135, 540, 540, 4320, 540, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 4320, 4320, 135, 4320, 135, 540, 4320, 4320, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 135, 4320, 540, 135, 4320, 135, 4320, 135, 135, 540, 540, 4320, 540, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 135, 540, 4320, 4320, 135, 4320, 540, 135, 540, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 135, 540, 540, 135, 540, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 540, 4320, 540, 135, 540, 4320, 540, 4320, 135, 4320, 135, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 135, 4320, 135, 4320, 540, 540, 135, 540, 4320, 540, 540, 4320, 135, 4320, 4320, 540, 135, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 135, 4320, 540, 4320, 4320, 135, 4320, 540, 540, 4320, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 4320, 540, 135, 135, 135, 135]
Prompts retrieved: 428895 . Total input tokens: 95613894 . Total output tokens: 84364510
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 97.96595059800893,
    "estimated_duration": 3600.0503915725026,
    "input_throughput": 5923.2448662157985,
    "output_throughput": 5161.772469491211,
    "total_throughput": 11085.01733570701,
    "itl": 81.93488201288713,
    "ttft": 1430990.7543921128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.16628554668281,
    "arrivals": 142715,
    "finished_requests": 86034,
    "scheduler_time": 230.49428904276743
}
#Debug simulation 
Total elapsed time: 97.96614510007203. Arrivals time: 0.4288022662512958 Scheduler time: 97.29888208582997 Scheduler overhead time: 0.09253984922543168 Adapter cache time: 0.02129194187000394 Engine time: 0.08881505578756332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 66, 540, 66, 4320, 540, 540, 540, 540, 4320, 66, 66, 66, 540, 4320, 66, 4320, 4320, 540, 540, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 540, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 66, 66, 4320, 540, 66, 540, 540, 4320, 540, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 4320, 4320, 66, 4320, 66, 540, 4320, 4320, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 66, 4320, 540, 66, 4320, 66, 4320, 66, 66, 540, 540, 4320, 540, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 66, 540, 4320, 4320, 66, 4320, 540, 66, 540, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 540, 540, 66, 540, 4320, 540, 540, 4320, 66, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 66, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 4320, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 4320, 540, 66, 66, 66, 66]
Prompts retrieved: 423030 . Total input tokens: 94340544 . Total output tokens: 83215656
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 94.11653775908053,
    "estimated_duration": 3600.0020805748186,
    "input_throughput": 6199.026417350482,
    "output_throughput": 5367.389953539895,
    "total_throughput": 11566.416370890378,
    "itl": 88.78351323781226,
    "ttft": 1380584.9059521074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.112918417872926,
    "arrivals": 140795,
    "finished_requests": 89611,
    "scheduler_time": 219.5940883609439
}
#Debug simulation 
Total elapsed time: 94.11674008797854. Arrivals time: 0.4382280777208507 Scheduler time: 93.44373026676476 Scheduler overhead time: 0.090035161934793 Adapter cache time: 0.02202562987804413 Engine time: 0.08762530377134681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 66, 540, 66, 4320, 540, 540, 540, 540, 4320, 66, 66, 66, 540, 4320, 66, 4320, 4320, 540, 540, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 540, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 66, 66, 4320, 540, 66, 540, 540, 4320, 540, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 4320, 4320, 66, 4320, 66, 540, 4320, 4320, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 66, 4320, 540, 66, 4320, 66, 4320, 66, 66, 540, 540, 4320, 540, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 66, 540, 4320, 4320, 66, 4320, 540, 66, 540, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 540, 540, 66, 540, 4320, 540, 540, 4320, 66, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 66, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 4320, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 4320, 540, 66, 66, 66, 66]
Prompts retrieved: 423030 . Total input tokens: 94340544 . Total output tokens: 83215656
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 96.93192822718993,
    "estimated_duration": 3600.0973473760732,
    "input_throughput": 6092.97801793819,
    "output_throughput": 5284.9620896688675,
    "total_throughput": 11377.940107607057,
    "itl": 86.03213361744598,
    "ttft": 1412799.5656000182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4331048421980865,
    "arrivals": 140795,
    "finished_requests": 88071,
    "scheduler_time": 223.50780029142888
}
#Debug simulation 
Total elapsed time: 96.93211708217859. Arrivals time: 0.432475030887872 Scheduler time: 96.2641509193927 Scheduler overhead time: 0.08997499756515026 Adapter cache time: 0.021732733584940434 Engine time: 0.08862600289285183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 66, 540, 66, 4320, 540, 540, 540, 540, 4320, 66, 66, 66, 540, 4320, 66, 4320, 4320, 540, 540, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 540, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 66, 66, 4320, 540, 66, 540, 540, 4320, 540, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 4320, 4320, 66, 4320, 66, 540, 4320, 4320, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 66, 4320, 540, 66, 4320, 66, 4320, 66, 66, 540, 540, 4320, 540, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 66, 540, 4320, 4320, 66, 4320, 540, 66, 540, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 540, 540, 66, 540, 4320, 540, 540, 4320, 66, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 66, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 4320, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 4320, 540, 66, 66, 66, 66]
Prompts retrieved: 423030 . Total input tokens: 94340544 . Total output tokens: 83215656
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 90.93863487290218,
    "estimated_duration": 3600.0398888248237,
    "input_throughput": 5999.50574632396,
    "output_throughput": 5202.31819045584,
    "total_throughput": 11201.8239367798,
    "itl": 82.18604739168192,
    "ttft": 1422554.7462231247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.606694934144649,
    "arrivals": 140795,
    "finished_requests": 86718,
    "scheduler_time": 227.5746170682897
}
#Debug simulation 
Total elapsed time: 90.93883629282936. Arrivals time: 0.4064293736591935 Scheduler time: 90.29592983331531 Scheduler overhead time: 0.09060154901817441 Adapter cache time: 0.021155607886612415 Engine time: 0.08875099755823612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 66, 540, 66, 4320, 540, 540, 540, 540, 4320, 66, 66, 66, 540, 4320, 66, 4320, 4320, 540, 540, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 540, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 66, 66, 4320, 540, 66, 540, 540, 4320, 540, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 4320, 4320, 66, 4320, 66, 540, 4320, 4320, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 66, 4320, 540, 66, 4320, 66, 4320, 66, 66, 540, 540, 4320, 540, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 66, 540, 4320, 4320, 66, 4320, 540, 66, 540, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 540, 540, 66, 540, 4320, 540, 540, 4320, 66, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 66, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 4320, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 4320, 540, 66, 66, 66, 66]
Prompts retrieved: 423030 . Total input tokens: 94340544 . Total output tokens: 83215656
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 96.05955084925517,
    "estimated_duration": 3600.003755871119,
    "input_throughput": 6140.513871394804,
    "output_throughput": 5332.498881061517,
    "total_throughput": 11473.01275245632,
    "itl": 87.08105634577609,
    "ttft": 1396524.1658103385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.952280067605892,
    "arrivals": 140795,
    "finished_requests": 88869,
    "scheduler_time": 221.4359562736047
}
#Debug simulation 
Total elapsed time: 96.05973603622988. Arrivals time: 0.4173410083167255 Scheduler time: 95.40923567162827 Scheduler overhead time: 0.08907725475728512 Adapter cache time: 0.02076641470193863 Engine time: 0.08795544598251581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 66, 540, 66, 4320, 540, 540, 540, 540, 4320, 66, 66, 66, 540, 4320, 66, 4320, 4320, 540, 540, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 540, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 66, 66, 4320, 540, 66, 540, 540, 4320, 540, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 4320, 4320, 66, 4320, 66, 540, 4320, 4320, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 66, 4320, 540, 66, 4320, 66, 4320, 66, 66, 540, 540, 4320, 540, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 66, 540, 4320, 4320, 66, 4320, 540, 66, 540, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 540, 540, 66, 540, 4320, 540, 540, 4320, 66, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 66, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 4320, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 4320, 540, 66, 66, 66, 66]
Prompts retrieved: 423030 . Total input tokens: 94340544 . Total output tokens: 83215656
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 89.92680783616379,
    "estimated_duration": 3600.004480749181,
    "input_throughput": 6014.20751440128,
    "output_throughput": 5226.8290499694485,
    "total_throughput": 11241.036564370728,
    "itl": 82.6107268453276,
    "ttft": 1419952.0661844239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6536922163144085,
    "arrivals": 140795,
    "finished_requests": 87086,
    "scheduler_time": 226.35875445367054
}
#Debug simulation 
Total elapsed time: 89.92699921224266. Arrivals time: 0.41506241355091333 Scheduler time: 89.27580012660474 Scheduler overhead time: 0.0906656333245337 Adapter cache time: 0.021162540651857853 Engine time: 0.08861899329349399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 66, 540, 66, 4320, 540, 540, 540, 540, 4320, 66, 66, 66, 540, 4320, 66, 4320, 4320, 540, 540, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 540, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 66, 66, 4320, 540, 66, 540, 540, 4320, 540, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 4320, 4320, 66, 4320, 66, 540, 4320, 4320, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 66, 4320, 540, 66, 4320, 66, 4320, 66, 66, 540, 540, 4320, 540, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 66, 540, 4320, 4320, 66, 4320, 540, 66, 540, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 540, 540, 66, 540, 4320, 540, 540, 4320, 66, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 66, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 4320, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 4320, 540, 66, 66, 66, 66]
Prompts retrieved: 423030 . Total input tokens: 94340544 . Total output tokens: 83215656
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 93.96123312413692,
    "estimated_duration": 3600.0169600684776,
    "input_throughput": 6146.688819926862,
    "output_throughput": 5325.747687487368,
    "total_throughput": 11472.43650741423,
    "itl": 86.86197723744517,
    "ttft": 1391517.126386916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.002718429737706,
    "arrivals": 140795,
    "finished_requests": 88892,
    "scheduler_time": 221.70156929779878
}
#Debug simulation 
Total elapsed time: 93.96143845515326. Arrivals time: 0.43082897597923875 Scheduler time: 93.29812368145213 Scheduler overhead time: 0.08862081961706281 Adapter cache time: 0.021300431806594133 Engine time: 0.08722254214808345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 66, 4320, 4320, 66, 4320, 540, 4320, 66, 540, 66, 4320, 540, 540, 540, 540, 4320, 66, 66, 66, 540, 4320, 66, 4320, 4320, 540, 540, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 540, 540, 4320, 540, 4320, 66, 4320, 4320, 540, 66, 66, 4320, 540, 66, 540, 540, 4320, 540, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 4320, 4320, 66, 4320, 66, 540, 4320, 4320, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 66, 4320, 540, 66, 4320, 66, 4320, 66, 66, 540, 540, 4320, 540, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 66, 540, 4320, 4320, 66, 4320, 540, 66, 540, 4320, 66, 66, 66, 66, 66, 66, 4320, 66, 66, 66, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 66, 540, 540, 66, 540, 66, 4320, 66, 66, 4320, 66, 4320, 4320, 540, 4320, 540, 66, 540, 4320, 540, 4320, 66, 4320, 66, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 66, 4320, 66, 4320, 540, 540, 66, 540, 4320, 540, 540, 4320, 66, 4320, 4320, 540, 66, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 66, 4320, 540, 4320, 4320, 66, 4320, 540, 540, 4320, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 4320, 540, 66, 66, 66, 66]
Prompts retrieved: 423030 . Total input tokens: 94340544 . Total output tokens: 83215656
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 96.70417144615203,
    "estimated_duration": 3600.0379920269233,
    "input_throughput": 5967.670910023921,
    "output_throughput": 5186.907205244947,
    "total_throughput": 11154.578115268869,
    "itl": 81.90129070083078,
    "ttft": 1413462.0760202615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.274168575219845,
    "arrivals": 140795,
    "finished_requests": 86402,
    "scheduler_time": 228.39412880125911
}
#Debug simulation 
Total elapsed time: 96.70436819177121. Arrivals time: 0.40113826794549823 Scheduler time: 96.06377634685487 Scheduler overhead time: 0.09230014914646745 Adapter cache time: 0.021029050927609205 Engine time: 0.08975760964676738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 33, 4320, 4320, 33, 4320, 540, 4320, 33, 540, 33, 4320, 540, 540, 540, 540, 4320, 33, 33, 33, 540, 4320, 33, 4320, 4320, 540, 540, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 540, 540, 4320, 540, 4320, 33, 4320, 4320, 540, 33, 33, 4320, 540, 33, 540, 540, 4320, 540, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 4320, 4320, 33, 4320, 33, 540, 4320, 4320, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 33, 4320, 540, 33, 4320, 33, 4320, 33, 33, 540, 540, 4320, 540, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 33, 540, 4320, 4320, 33, 4320, 540, 33, 540, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 540, 4320, 540, 33, 540, 4320, 540, 4320, 33, 4320, 33, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 33, 4320, 33, 4320, 540, 540, 33, 540, 4320, 540, 540, 4320, 33, 4320, 4320, 540, 33, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 33, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 4320, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 4320, 540, 33, 33, 33, 33]
Prompts retrieved: 420225 . Total input tokens: 93705357 . Total output tokens: 82676561
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 80.52882758667693,
    "estimated_duration": 3600.049606019024,
    "input_throughput": 6269.328056553655,
    "output_throughput": 5426.1930078240075,
    "total_throughput": 11695.521064377663,
    "itl": 90.73214971125563,
    "ttft": 1345663.7941360793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.231941780448029,
    "arrivals": 139799,
    "finished_requests": 90829,
    "scheduler_time": 215.7246432339313
}
#Debug simulation 
Total elapsed time: 80.52901433967054. Arrivals time: 0.4082534625194967 Scheduler time: 79.89792116731405 Scheduler overhead time: 0.0847523151896894 Adapter cache time: 0.02055787807330489 Engine time: 0.08368780510500073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 33, 4320, 4320, 33, 4320, 540, 4320, 33, 540, 33, 4320, 540, 540, 540, 540, 4320, 33, 33, 33, 540, 4320, 33, 4320, 4320, 540, 540, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 540, 540, 4320, 540, 4320, 33, 4320, 4320, 540, 33, 33, 4320, 540, 33, 540, 540, 4320, 540, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 4320, 4320, 33, 4320, 33, 540, 4320, 4320, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 33, 4320, 540, 33, 4320, 33, 4320, 33, 33, 540, 540, 4320, 540, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 33, 540, 4320, 4320, 33, 4320, 540, 33, 540, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 540, 4320, 540, 33, 540, 4320, 540, 4320, 33, 4320, 33, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 33, 4320, 33, 4320, 540, 540, 33, 540, 4320, 540, 540, 4320, 33, 4320, 4320, 540, 33, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 33, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 4320, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 4320, 540, 33, 33, 33, 33]
Prompts retrieved: 420225 . Total input tokens: 93705357 . Total output tokens: 82676561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.13540324615315,
    "estimated_duration": 3600.019884746446,
    "input_throughput": 6203.846843910699,
    "output_throughput": 5372.080882648266,
    "total_throughput": 11575.927726558966,
    "itl": 88.27890255441734,
    "ttft": 1373857.1243587052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 709,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.18976957210806,
    "arrivals": 139799,
    "finished_requests": 89929,
    "scheduler_time": 218.32375388326355
}
#Debug simulation 
Total elapsed time: 84.13559312280267. Arrivals time: 0.40723178861662745 Scheduler time: 83.50180147960782 Scheduler overhead time: 0.08589007379487157 Adapter cache time: 0.02150542661547661 Engine time: 0.08476427337154746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 33, 4320, 4320, 33, 4320, 540, 4320, 33, 540, 33, 4320, 540, 540, 540, 540, 4320, 33, 33, 33, 540, 4320, 33, 4320, 4320, 540, 540, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 540, 540, 4320, 540, 4320, 33, 4320, 4320, 540, 33, 33, 4320, 540, 33, 540, 540, 4320, 540, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 4320, 4320, 33, 4320, 33, 540, 4320, 4320, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 33, 4320, 540, 33, 4320, 33, 4320, 33, 33, 540, 540, 4320, 540, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 33, 540, 4320, 4320, 33, 4320, 540, 33, 540, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 540, 4320, 540, 33, 540, 4320, 540, 4320, 33, 4320, 33, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 33, 4320, 33, 4320, 540, 540, 33, 540, 4320, 540, 540, 4320, 33, 4320, 4320, 540, 33, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 33, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 4320, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 4320, 540, 33, 33, 33, 33]
Prompts retrieved: 420225 . Total input tokens: 93705357 . Total output tokens: 82676561
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 67.43458264367655,
    "estimated_duration": 3600.056919595578,
    "input_throughput": 6171.996859009688,
    "output_throughput": 5352.706757248925,
    "total_throughput": 11524.703616258612,
    "itl": 85.46998093829131,
    "ttft": 1340185.4833895515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.641175992633247,
    "arrivals": 139799,
    "finished_requests": 89396,
    "scheduler_time": 219.3471370586349
}
#Debug simulation 
Total elapsed time: 67.43478331388906. Arrivals time: 0.3960618479177356 Scheduler time: 66.81677201855928 Scheduler overhead time: 0.08429843187332153 Adapter cache time: 0.02004461456090212 Engine time: 0.08336196141317487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 33, 4320, 4320, 33, 4320, 540, 4320, 33, 540, 33, 4320, 540, 540, 540, 540, 4320, 33, 33, 33, 540, 4320, 33, 4320, 4320, 540, 540, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 540, 540, 4320, 540, 4320, 33, 4320, 4320, 540, 33, 33, 4320, 540, 33, 540, 540, 4320, 540, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 4320, 4320, 33, 4320, 33, 540, 4320, 4320, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 33, 4320, 540, 33, 4320, 33, 4320, 33, 33, 540, 540, 4320, 540, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 33, 540, 4320, 4320, 33, 4320, 540, 33, 540, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 540, 4320, 540, 33, 540, 4320, 540, 4320, 33, 4320, 33, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 33, 4320, 33, 4320, 540, 540, 33, 540, 4320, 540, 540, 4320, 33, 4320, 4320, 540, 33, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 33, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 4320, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 4320, 540, 33, 33, 33, 33]
Prompts retrieved: 420225 . Total input tokens: 93705357 . Total output tokens: 82676561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 87.37589269597083,
    "estimated_duration": 3600.078514997272,
    "input_throughput": 6167.565209342893,
    "output_throughput": 5352.391599163387,
    "total_throughput": 11519.956808506278,
    "itl": 88.23572867350023,
    "ttft": 1377810.3423954635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.187937436164351,
    "arrivals": 139799,
    "finished_requests": 89442,
    "scheduler_time": 219.49582516244192
}
#Debug simulation 
Total elapsed time: 87.37609091214836. Arrivals time: 0.40113808447495103 Scheduler time: 86.7454330790788 Scheduler overhead time: 0.08865811210125685 Adapter cache time: 0.02061556652188301 Engine time: 0.0858204304240644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 33, 4320, 4320, 33, 4320, 540, 4320, 33, 540, 33, 4320, 540, 540, 540, 540, 4320, 33, 33, 33, 540, 4320, 33, 4320, 4320, 540, 540, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 540, 540, 4320, 540, 4320, 33, 4320, 4320, 540, 33, 33, 4320, 540, 33, 540, 540, 4320, 540, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 4320, 4320, 33, 4320, 33, 540, 4320, 4320, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 33, 4320, 540, 33, 4320, 33, 4320, 33, 33, 540, 540, 4320, 540, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 33, 540, 4320, 4320, 33, 4320, 540, 33, 540, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 540, 4320, 540, 33, 540, 4320, 540, 4320, 33, 4320, 33, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 33, 4320, 33, 4320, 540, 540, 33, 540, 4320, 540, 540, 4320, 33, 4320, 4320, 540, 33, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 33, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 4320, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 4320, 540, 33, 33, 33, 33]
Prompts retrieved: 420225 . Total input tokens: 93705357 . Total output tokens: 82676561
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 67.5664359210059,
    "estimated_duration": 3600.032607940538,
    "input_throughput": 6200.333005530566,
    "output_throughput": 5374.699928362313,
    "total_throughput": 11575.032933892879,
    "itl": 85.65709585588942,
    "ttft": 1347088.5828539405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 700,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.21522995325274,
    "arrivals": 139799,
    "finished_requests": 89832,
    "scheduler_time": 218.2112842761538
}
#Debug simulation 
Total elapsed time: 67.56663514301181. Arrivals time: 0.3985385657288134 Scheduler time: 66.9483166700229 Scheduler overhead time: 0.08332699909806252 Adapter cache time: 0.020370890386402607 Engine time: 0.08261028490960598 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 33, 4320, 4320, 33, 4320, 540, 4320, 33, 540, 33, 4320, 540, 540, 540, 540, 4320, 33, 33, 33, 540, 4320, 33, 4320, 4320, 540, 540, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 540, 540, 4320, 540, 4320, 33, 4320, 4320, 540, 33, 33, 4320, 540, 33, 540, 540, 4320, 540, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 4320, 4320, 33, 4320, 33, 540, 4320, 4320, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 33, 4320, 540, 33, 4320, 33, 4320, 33, 33, 540, 540, 4320, 540, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 33, 540, 4320, 4320, 33, 4320, 540, 33, 540, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 540, 4320, 540, 33, 540, 4320, 540, 4320, 33, 4320, 33, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 33, 4320, 33, 4320, 540, 540, 33, 540, 4320, 540, 540, 4320, 33, 4320, 4320, 540, 33, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 33, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 4320, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 4320, 540, 33, 33, 33, 33]
Prompts retrieved: 420225 . Total input tokens: 93705357 . Total output tokens: 82676561
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 89.61411066632718,
    "estimated_duration": 3600.011591391562,
    "input_throughput": 6184.547309025141,
    "output_throughput": 5359.591631909661,
    "total_throughput": 11544.138940934803,
    "itl": 88.32897230899744,
    "ttft": 1361588.5694762382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 638,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.072941560083981,
    "arrivals": 139799,
    "finished_requests": 89579,
    "scheduler_time": 219.0794953584116
}
#Debug simulation 
Total elapsed time: 89.6143075521104. Arrivals time: 0.4077559970319271 Scheduler time: 88.9778549731709 Scheduler overhead time: 0.08795854449272156 Adapter cache time: 0.020499917212873697 Engine time: 0.08581289416179061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_256_slots_32_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 4320, 33, 4320, 4320, 33, 4320, 540, 4320, 33, 540, 33, 4320, 540, 540, 540, 540, 4320, 33, 33, 33, 540, 4320, 33, 4320, 4320, 540, 540, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 540, 540, 4320, 540, 4320, 33, 4320, 4320, 540, 33, 33, 4320, 540, 33, 540, 540, 4320, 540, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 4320, 4320, 33, 4320, 33, 540, 4320, 4320, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 33, 4320, 540, 33, 4320, 33, 4320, 33, 33, 540, 540, 4320, 540, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 33, 540, 4320, 4320, 33, 4320, 540, 33, 540, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 33, 540, 540, 33, 540, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 540, 4320, 540, 33, 540, 4320, 540, 4320, 33, 4320, 33, 540, 4320, 540, 4320, 540, 4320, 540, 4320, 540, 33, 4320, 33, 4320, 540, 540, 33, 540, 4320, 540, 540, 4320, 33, 4320, 4320, 540, 33, 4320, 4320, 540, 540, 4320, 4320, 540, 540, 540, 540, 4320, 4320, 540, 540, 4320, 33, 4320, 540, 4320, 4320, 33, 4320, 540, 540, 4320, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 4320, 540, 33, 33, 33, 33]
Prompts retrieved: 420225 . Total input tokens: 93705357 . Total output tokens: 82676561
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 65.03334358287975,
    "estimated_duration": 3600.0040336160014,
    "input_throughput": 6180.080575535583,
    "output_throughput": 5358.8276068186715,
    "total_throughput": 11538.908182354255,
    "itl": 85.55601744030085,
    "ttft": 1350918.2395273615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.502728815041514,
    "arrivals": 139799,
    "finished_requests": 89539,
    "scheduler_time": 219.0674634233217
}
#Debug simulation 
Total elapsed time: 65.03352553863078. Arrivals time: 0.3856035084463656 Scheduler time: 64.43004628177732 Scheduler overhead time: 0.08325574873015285 Adapter cache time: 0.019503621850162745 Engine time: 0.08137829136103392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_256_slots_32_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 4320, 135, 4320, 4320, 135, 4320, 270, 4320, 135, 270, 135, 4320, 270, 270, 270, 270, 4320, 135, 135, 135, 270, 4320, 135, 4320, 4320, 270, 270, 270, 135, 4320, 135, 270, 270, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 270, 270, 4320, 270, 4320, 135, 4320, 4320, 270, 135, 135, 4320, 270, 135, 270, 270, 4320, 270, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 4320, 4320, 135, 4320, 135, 270, 4320, 4320, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 135, 4320, 270, 135, 4320, 135, 4320, 135, 135, 270, 270, 4320, 270, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 4320, 270, 4320, 4320, 135, 4320, 270, 270, 135, 270, 4320, 4320, 135, 4320, 270, 135, 270, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 135, 270, 270, 135, 270, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 270, 4320, 270, 135, 270, 4320, 270, 4320, 135, 4320, 135, 270, 4320, 270, 4320, 270, 4320, 270, 4320, 270, 135, 4320, 135, 4320, 270, 270, 135, 270, 4320, 270, 270, 4320, 135, 4320, 4320, 270, 135, 4320, 4320, 270, 270, 4320, 4320, 270, 270, 270, 270, 4320, 4320, 270, 270, 4320, 135, 4320, 270, 4320, 4320, 135, 4320, 270, 270, 4320, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 4320, 270, 135, 135, 135, 135]
Prompts retrieved: 405945 . Total input tokens: 90517161 . Total output tokens: 79860254
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.72842468880117,
    "estimated_duration": 3600.0627074057584,
    "input_throughput": 6299.0761114662155,
    "output_throughput": 5484.697241351285,
    "total_throughput": 11783.7733528175,
    "itl": 91.79064806591566,
    "ttft": 1266272.6506803778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.29806587076753,
    "arrivals": 135085,
    "finished_requests": 91524,
    "scheduler_time": 209.22676698844015
}
#Debug simulation 
Total elapsed time: 83.72860297374427. Arrivals time: 0.3764495635405183 Scheduler time: 83.12078999029472 Scheduler overhead time: 0.09640585072338581 Adapter cache time: 0.01974130654707551 Engine time: 0.08223492465913296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_256_slots_32_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 4320, 135, 4320, 4320, 135, 4320, 270, 4320, 135, 270, 135, 4320, 270, 270, 270, 270, 4320, 135, 135, 135, 270, 4320, 135, 4320, 4320, 270, 270, 270, 135, 4320, 135, 270, 270, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 270, 270, 4320, 270, 4320, 135, 4320, 4320, 270, 135, 135, 4320, 270, 135, 270, 270, 4320, 270, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 4320, 4320, 135, 4320, 135, 270, 4320, 4320, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 135, 4320, 270, 135, 4320, 135, 4320, 135, 135, 270, 270, 4320, 270, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 4320, 270, 4320, 4320, 135, 4320, 270, 270, 135, 270, 4320, 4320, 135, 4320, 270, 135, 270, 4320, 135, 135, 135, 135, 135, 135, 4320, 135, 135, 135, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 135, 270, 270, 135, 270, 135, 4320, 135, 135, 4320, 135, 4320, 4320, 270, 4320, 270, 135, 270, 4320, 270, 4320, 135, 4320, 135, 270, 4320, 270, 4320, 270, 4320, 270, 4320, 270, 135, 4320, 135, 4320, 270, 270, 135, 270, 4320, 270, 270, 4320, 135, 4320, 4320, 270, 135, 4320, 4320, 270, 270, 4320, 4320, 270, 270, 270, 270, 4320, 4320, 270, 270, 4320, 135, 4320, 270, 4320, 4320, 135, 4320, 270, 270, 4320, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 4320, 270, 135, 135, 135, 135]
Prompts retrieved: 405945 . Total input tokens: 90517161 . Total output tokens: 79860254
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 72.92906300118193,
    "estimated_duration": 3600.0348413381957,
    "input_throughput": 6253.378645533263,
    "output_throughput": 5435.8443355303125,
    "total_throughput": 11689.222981063574,
    "itl": 89.95736950006906,
    "ttft": 1307684.945176852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.69263582042419,
    "arrivals": 135085,
    "finished_requests": 90800,
    "scheduler_time": 211.35894052237504
}
#Debug simulation 
Total elapsed time: 72.92923402413726. Arrivals time: 0.3683538530021906 Scheduler time: 72.351240364369 Scheduler overhead time: 0.08025480154901743 Adapter cache time: 0.01880167704075575 Engine time: 0.07824390148743987 
