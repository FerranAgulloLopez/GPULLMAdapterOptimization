INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 33, 33, 270, 33, 17280, 270, 33, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 17280, 17280, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 270, 270, 17280]
Prompts retrieved: 193380 . Total input tokens: 43064457 . Total output tokens: 37937465
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.507117917295545,
    "estimated_duration": 3599.9957643317302,
    "input_throughput": 4457.488022344605,
    "output_throughput": 3876.2392829065934,
    "total_throughput": 8333.7273052512,
    "itl": 35.87437173067284,
    "ttft": 7280.969814308642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.979144001058396,
    "arrivals": 64833,
    "finished_requests": 64703,
    "scheduler_time": 38.56069635884364
}
#Debug simulation 
Total elapsed time: 4.50722690904513. Arrivals time: 0.16503449017181993 Scheduler time: 4.068955722730607 Scheduler overhead time: 0.10243281861767173 Adapter cache time: 0.0199589803814888 Engine time: 0.10180992586538196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 33, 33, 270, 33, 17280, 270, 33, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 17280, 17280, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 270, 270, 17280]
Prompts retrieved: 193380 . Total input tokens: 43064457 . Total output tokens: 37937465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.500605367124081,
    "estimated_duration": 3600.0184190419104,
    "input_throughput": 4457.459971627213,
    "output_throughput": 3876.214889954302,
    "total_throughput": 8333.674861581516,
    "itl": 35.88513754594419,
    "ttft": 7281.107230255612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 750,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.584779544109491,
    "arrivals": 64833,
    "finished_requests": 64703,
    "scheduler_time": 38.56602376987156
}
#Debug simulation 
Total elapsed time: 4.500717442017049. Arrivals time: 0.16391874244436622 Scheduler time: 4.067684192210436 Scheduler overhead time: 0.10119558311998844 Adapter cache time: 0.020080750342458487 Engine time: 0.09982583951205015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 33, 33, 270, 33, 17280, 270, 33, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 17280, 17280, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 270, 270, 17280]
Prompts retrieved: 193380 . Total input tokens: 43064457 . Total output tokens: 37937465
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.508231608197093,
    "estimated_duration": 3600.002380713088,
    "input_throughput": 4457.479830005397,
    "output_throughput": 3876.232158834269,
    "total_throughput": 8333.711988839666,
    "itl": 35.888299813899174,
    "ttft": 7281.043127481643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.780533324922473,
    "arrivals": 64833,
    "finished_requests": 64703,
    "scheduler_time": 38.567525070528376
}
#Debug simulation 
Total elapsed time: 4.508349428884685. Arrivals time: 0.1651239232160151 Scheduler time: 4.07380615407601 Scheduler overhead time: 0.10147555824369192 Adapter cache time: 0.019985125865787268 Engine time: 0.09985109884291887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 33, 33, 270, 33, 17280, 270, 33, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 17280, 17280, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 270, 270, 17280]
Prompts retrieved: 193380 . Total input tokens: 43064457 . Total output tokens: 37937465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.50999138597399,
    "estimated_duration": 3600.0123168567598,
    "input_throughput": 4457.467527225265,
    "output_throughput": 3876.221460315418,
    "total_throughput": 8333.688987540683,
    "itl": 35.876673247694214,
    "ttft": 7280.922773090029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 752,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.076964520998288,
    "arrivals": 64833,
    "finished_requests": 64703,
    "scheduler_time": 38.561639414849225
}
#Debug simulation 
Total elapsed time: 4.510091471020132. Arrivals time: 0.1646709805354476 Scheduler time: 4.075258219614625 Scheduler overhead time: 0.10216815490275621 Adapter cache time: 0.019906661938875914 Engine time: 0.09955814247950912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 33, 33, 270, 33, 17280, 270, 33, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 17280, 17280, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 270, 270, 17280]
Prompts retrieved: 193380 . Total input tokens: 43064457 . Total output tokens: 37937465
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.500669423025101,
    "estimated_duration": 3600.0186922360235,
    "input_throughput": 4457.459633364574,
    "output_throughput": 3876.214595800527,
    "total_throughput": 8333.6742291651,
    "itl": 35.887100042400064,
    "ttft": 7280.973057939556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.702450149338731,
    "arrivals": 64833,
    "finished_requests": 64703,
    "scheduler_time": 38.56695395059905
}
#Debug simulation 
Total elapsed time: 4.500767684075981. Arrivals time: 0.16317463526502252 Scheduler time: 4.067220929078758 Scheduler overhead time: 0.10161784524098039 Adapter cache time: 0.01982350740581751 Engine time: 0.10093982284888625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 33, 33, 270, 33, 17280, 270, 33, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 17280, 17280, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 270, 270, 17280]
Prompts retrieved: 193380 . Total input tokens: 43064457 . Total output tokens: 37937465
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.50523692695424,
    "estimated_duration": 3599.987124354282,
    "input_throughput": 4457.498720326197,
    "output_throughput": 3876.248585889863,
    "total_throughput": 8333.74730621606,
    "itl": 35.871283377941786,
    "ttft": 7281.059581145992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 750,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.787940705427877,
    "arrivals": 64833,
    "finished_requests": 64703,
    "scheduler_time": 38.559047218335266
}
#Debug simulation 
Total elapsed time: 4.505341529846191. Arrivals time: 0.16460406174883246 Scheduler time: 4.070181104820222 Scheduler overhead time: 0.10123403696343303 Adapter cache time: 0.020115711726248264 Engine time: 0.10064848512411118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 33, 33, 270, 33, 17280, 270, 33, 33, 33, 17280, 17280, 33, 33, 270, 270, 270, 17280, 17280, 33, 17280, 17280, 17280, 17280, 270, 33, 270, 270, 270, 270, 17280]
Prompts retrieved: 193380 . Total input tokens: 43064457 . Total output tokens: 37937465
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.50946050696075,
    "estimated_duration": 3600.0133777805368,
    "input_throughput": 4457.46621360979,
    "output_throughput": 3876.2203179931316,
    "total_throughput": 8333.686531602922,
    "itl": 35.88597734399445,
    "ttft": 7281.0907164949385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 752,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.653157757818738,
    "arrivals": 64833,
    "finished_requests": 64703,
    "scheduler_time": 38.56650185501743
}
#Debug simulation 
Total elapsed time: 4.509621798992157. Arrivals time: 0.16412960272282362 Scheduler time: 4.074942620005459 Scheduler overhead time: 0.10153415519744158 Adapter cache time: 0.02000029431656003 Engine time: 0.10079412953928113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 66, 66, 135, 66, 17280, 135, 66, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 17280, 17280, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 135, 135, 17280]
Prompts retrieved: 192225 . Total input tokens: 42813064 . Total output tokens: 37694590
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.501376653090119,
    "estimated_duration": 3600.004625423099,
    "input_throughput": 4431.494584017386,
    "output_throughput": 3833.0884084289823,
    "total_throughput": 8264.582992446369,
    "itl": 35.19028842638765,
    "ttft": 9557.839546996782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.782297966275419,
    "arrivals": 64433,
    "finished_requests": 64262,
    "scheduler_time": 37.611600041062104
}
#Debug simulation 
Total elapsed time: 4.501495722215623. Arrivals time: 0.1657884414307773 Scheduler time: 4.0619280049577355 Scheduler overhead time: 0.10327764879912138 Adapter cache time: 0.019114908296614885 Engine time: 0.10236559761688113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 66, 66, 135, 66, 17280, 135, 66, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 17280, 17280, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 135, 135, 17280]
Prompts retrieved: 192225 . Total input tokens: 42813064 . Total output tokens: 37694590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.576430008746684,
    "estimated_duration": 3600.011570877223,
    "input_throughput": 4431.4874232786415,
    "output_throughput": 3833.102124346093,
    "total_throughput": 8264.589547624735,
    "itl": 35.19699793289952,
    "ttft": 9501.916189239873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.177738526891922,
    "arrivals": 64433,
    "finished_requests": 64263,
    "scheduler_time": 37.61495435441266
}
#Debug simulation 
Total elapsed time: 4.576528067700565. Arrivals time: 0.1647633370012045 Scheduler time: 4.138396643102169 Scheduler overhead time: 0.102998998016119 Adapter cache time: 0.019151750952005386 Engine time: 0.10205555381253362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 66, 66, 135, 66, 17280, 135, 66, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 17280, 17280, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 135, 135, 17280]
Prompts retrieved: 192225 . Total input tokens: 42813064 . Total output tokens: 37694590
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.5035758637823164,
    "estimated_duration": 3600.0086057140657,
    "input_throughput": 4431.48968440747,
    "output_throughput": 3833.0841704371223,
    "total_throughput": 8264.573854844592,
    "itl": 35.19785504229488,
    "ttft": 9557.839205937018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.289714335580389,
    "arrivals": 64433,
    "finished_requests": 64262,
    "scheduler_time": 37.61590652028374
}
#Debug simulation 
Total elapsed time: 4.503672139719129. Arrivals time: 0.1636182381771505 Scheduler time: 4.0658026854507625 Scheduler overhead time: 0.1032802895642817 Adapter cache time: 0.019125280901789665 Engine time: 0.10249967221170664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 66, 66, 135, 66, 17280, 135, 66, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 17280, 17280, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 135, 135, 17280]
Prompts retrieved: 192225 . Total input tokens: 42813064 . Total output tokens: 37694590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.534745204262435,
    "estimated_duration": 3600.017985583119,
    "input_throughput": 4431.47952701573,
    "output_throughput": 3833.0952943183283,
    "total_throughput": 8264.574821334058,
    "itl": 35.19186063279644,
    "ttft": 9501.952682666424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8598359240032654,
    "arrivals": 64433,
    "finished_requests": 64263,
    "scheduler_time": 37.61235712903001
}
#Debug simulation 
Total elapsed time: 4.534843775909394. Arrivals time: 0.16589471884071827 Scheduler time: 4.094620730727911 Scheduler overhead time: 0.1031476422213018 Adapter cache time: 0.019046572502702475 Engine time: 0.10311509249731898 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 66, 66, 135, 66, 17280, 135, 66, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 17280, 17280, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 135, 135, 17280]
Prompts retrieved: 192225 . Total input tokens: 42813064 . Total output tokens: 37694590
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.5476506487466395,
    "estimated_duration": 3600.0047935458015,
    "input_throughput": 4431.494377063537,
    "output_throughput": 3833.088229421114,
    "total_throughput": 8264.582606484651,
    "itl": 35.197718602213214,
    "ttft": 9557.87986198912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2422845021356235,
    "arrivals": 64433,
    "finished_requests": 64262,
    "scheduler_time": 37.61545010469751
}
#Debug simulation 
Total elapsed time: 4.547745442017913. Arrivals time: 0.16308371908962727 Scheduler time: 4.11039112880826 Scheduler overhead time: 0.10371582536026835 Adapter cache time: 0.019183880649507046 Engine time: 0.1022829320281744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 66, 66, 135, 66, 17280, 135, 66, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 17280, 17280, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 135, 135, 17280]
Prompts retrieved: 192225 . Total input tokens: 42813064 . Total output tokens: 37694590
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.600207640789449,
    "estimated_duration": 3600.0123923022484,
    "input_throughput": 4431.486412133603,
    "output_throughput": 3833.1012497363236,
    "total_throughput": 8264.587661869926,
    "itl": 35.18877081928688,
    "ttft": 9501.853612897949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6516027780063283,
    "arrivals": 64433,
    "finished_requests": 64263,
    "scheduler_time": 37.61057333953307
}
#Debug simulation 
Total elapsed time: 4.600310896988958. Arrivals time: 0.16711191739887 Scheduler time: 4.154601717367768 Scheduler overhead time: 0.10415876749902964 Adapter cache time: 0.01912521617487073 Engine time: 0.10598877258598804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 66, 66, 135, 66, 17280, 135, 66, 66, 66, 17280, 17280, 66, 66, 135, 135, 135, 17280, 17280, 66, 17280, 17280, 17280, 17280, 135, 66, 135, 135, 135, 135, 17280]
Prompts retrieved: 192225 . Total input tokens: 42813064 . Total output tokens: 37694590
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.529922011774033,
    "estimated_duration": 3600.031363988522,
    "input_throughput": 4431.463058789857,
    "output_throughput": 3833.0810498027636,
    "total_throughput": 8264.544108592621,
    "itl": 35.197482003716736,
    "ttft": 9557.721801587562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.211216925643418,
    "arrivals": 64433,
    "finished_requests": 64263,
    "scheduler_time": 37.61551272418542
}
#Debug simulation 
Total elapsed time: 4.530079782940447. Arrivals time: 0.1637227050960064 Scheduler time: 4.093370304442942 Scheduler overhead time: 0.10292693646624684 Adapter cache time: 0.018764388747513294 Engine time: 0.10233569564297795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 33, 33, 135, 33, 17280, 135, 33, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 17280, 17280, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 135, 135, 17280]
Prompts retrieved: 191895 . Total input tokens: 42735553 . Total output tokens: 37628459
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.5715190982446074,
    "estimated_duration": 3600.0330029355778,
    "input_throughput": 4407.5548715973655,
    "output_throughput": 3856.6357554718375,
    "total_throughput": 8264.190627069203,
    "itl": 35.240399577162016,
    "ttft": 8898.873323296031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8896227469621487,
    "arrivals": 64340,
    "finished_requests": 64182,
    "scheduler_time": 37.945583946551835
}
#Debug simulation 
Total elapsed time: 4.571628449019045. Arrivals time: 0.16363438637927175 Scheduler time: 4.133488484658301 Scheduler overhead time: 0.10346610564738512 Adapter cache time: 0.018568784929811954 Engine time: 0.10329234693199396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 33, 33, 135, 33, 17280, 135, 33, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 17280, 17280, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 135, 135, 17280]
Prompts retrieved: 191895 . Total input tokens: 42735553 . Total output tokens: 37628459
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.626871739048511,
    "estimated_duration": 3600.030318447529,
    "input_throughput": 4407.5581582442355,
    "output_throughput": 3856.6386313066714,
    "total_throughput": 8264.196789550906,
    "itl": 35.24593321265801,
    "ttft": 8898.89418368887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.234004162489442,
    "arrivals": 64340,
    "finished_requests": 64182,
    "scheduler_time": 37.94838248521881
}
#Debug simulation 
Total elapsed time: 4.626991051249206. Arrivals time: 0.16417477233335376 Scheduler time: 4.189121828880161 Scheduler overhead time: 0.10326183307915926 Adapter cache time: 0.018487595487385988 Engine time: 0.10267149051651359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 33, 33, 135, 33, 17280, 135, 33, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 17280, 17280, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 135, 135, 17280]
Prompts retrieved: 191895 . Total input tokens: 42735553 . Total output tokens: 37628459
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.5667641325853765,
    "estimated_duration": 3600.037573455948,
    "input_throughput": 4407.672607918731,
    "output_throughput": 3856.6314147303974,
    "total_throughput": 8264.304022649128,
    "itl": 35.247037134931496,
    "ttft": 8842.87544220579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3293137161247697,
    "arrivals": 64340,
    "finished_requests": 64183,
    "scheduler_time": 37.949281513994855
}
#Debug simulation 
Total elapsed time: 4.566858356818557. Arrivals time: 0.16314942855387926 Scheduler time: 4.130664666648954 Scheduler overhead time: 0.10339177586138248 Adapter cache time: 0.01844692323356867 Engine time: 0.10217925626784563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 33, 33, 135, 33, 17280, 135, 33, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 17280, 17280, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 135, 135, 17280]
Prompts retrieved: 191895 . Total input tokens: 42735553 . Total output tokens: 37628459
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.555496772285551,
    "estimated_duration": 3600.0152708024298,
    "input_throughput": 4407.531025962361,
    "output_throughput": 3856.600307394071,
    "total_throughput": 8264.131333356432,
    "itl": 35.2412991772261,
    "ttft": 8899.081644071453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9716303985333017,
    "arrivals": 64340,
    "finished_requests": 64181,
    "scheduler_time": 37.946020660698096
}
#Debug simulation 
Total elapsed time: 4.555621060077101. Arrivals time: 0.16294704005122185 Scheduler time: 4.118745841551572 Scheduler overhead time: 0.10309921903535724 Adapter cache time: 0.018556772265583277 Engine time: 0.10303536569699645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 33, 33, 135, 33, 17280, 135, 33, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 17280, 17280, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 135, 135, 17280]
Prompts retrieved: 191895 . Total input tokens: 42735553 . Total output tokens: 37628459
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.586122960783541,
    "estimated_duration": 3600.007382935388,
    "input_throughput": 4407.540683169977,
    "output_throughput": 3856.608757474091,
    "total_throughput": 8264.149440644069,
    "itl": 35.246632405679854,
    "ttft": 8899.088332999489,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2901685697445906,
    "arrivals": 64340,
    "finished_requests": 64181,
    "scheduler_time": 37.948590406410446
}
#Debug simulation 
Total elapsed time: 4.586253495886922. Arrivals time: 0.16377484193071723 Scheduler time: 4.147925977129489 Scheduler overhead time: 0.10351982666179538 Adapter cache time: 0.018437090329825878 Engine time: 0.10308392997831106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 33, 33, 135, 33, 17280, 135, 33, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 17280, 17280, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 135, 135, 17280]
Prompts retrieved: 191895 . Total input tokens: 42735553 . Total output tokens: 37628459
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.597162486985326,
    "estimated_duration": 3600.019638349433,
    "input_throughput": 4407.525678741829,
    "output_throughput": 3856.5956285631732,
    "total_throughput": 8264.121307305002,
    "itl": 35.23816630811565,
    "ttft": 8899.006241861578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.789773451029311,
    "arrivals": 64340,
    "finished_requests": 64181,
    "scheduler_time": 37.94457175628274
}
#Debug simulation 
Total elapsed time: 4.597266879864037. Arrivals time: 0.16568841924890876 Scheduler time: 4.157709691207856 Scheduler overhead time: 0.10317492112517357 Adapter cache time: 0.018414115998893976 Engine time: 0.10303144017234445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 135, 33, 33, 135, 33, 17280, 135, 33, 33, 33, 17280, 17280, 33, 33, 135, 135, 135, 17280, 17280, 33, 17280, 17280, 17280, 17280, 135, 33, 135, 135, 135, 135, 17280]
Prompts retrieved: 191895 . Total input tokens: 42735553 . Total output tokens: 37628459
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.575763697735965,
    "estimated_duration": 3600.001000728531,
    "input_throughput": 4407.548497011241,
    "output_throughput": 3856.615594604095,
    "total_throughput": 8264.164091615336,
    "itl": 35.24682832628387,
    "ttft": 8899.019255080157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2630362196080647,
    "arrivals": 64340,
    "finished_requests": 64181,
    "scheduler_time": 37.94829992108585
}
#Debug simulation 
Total elapsed time: 4.575944622978568. Arrivals time: 0.16645881114527583 Scheduler time: 4.136444143485278 Scheduler overhead time: 0.10316214570775628 Adapter cache time: 0.018328305333852768 Engine time: 0.10220038145780563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 66, 33, 33, 66, 33, 17280, 66, 33, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 17280, 17280, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 66, 66, 17280]
Prompts retrieved: 191136 . Total input tokens: 42569535 . Total output tokens: 37480677
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.519601913169026,
    "estimated_duration": 3600.007618697439,
    "input_throughput": 4423.401194290181,
    "output_throughput": 3829.837172674817,
    "total_throughput": 8253.238366964997,
    "itl": 34.684969128628204,
    "ttft": 8878.843967683379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9308234373293793,
    "arrivals": 64071,
    "finished_requests": 63914,
    "scheduler_time": 37.30351968630607
}
#Debug simulation 
Total elapsed time: 4.51970390509814. Arrivals time: 0.16500896215438843 Scheduler time: 4.078998972661793 Scheduler overhead time: 0.10468154773116112 Adapter cache time: 0.01820537867024541 Engine time: 0.10307896882295609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 66, 33, 33, 66, 33, 17280, 66, 33, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 17280, 17280, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 66, 66, 17280]
Prompts retrieved: 191136 . Total input tokens: 42569535 . Total output tokens: 37480677
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.596409887075424,
    "estimated_duration": 3600.023026456542,
    "input_throughput": 4423.423095622312,
    "output_throughput": 3829.922169573223,
    "total_throughput": 8253.345265195534,
    "itl": 34.68778052202429,
    "ttft": 8822.76433645608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.145913772229108,
    "arrivals": 64071,
    "finished_requests": 63915,
    "scheduler_time": 37.30543824991379
}
#Debug simulation 
Total elapsed time: 4.596515461336821. Arrivals time: 0.1662138532847166 Scheduler time: 4.151616318151355 Scheduler overhead time: 0.10495155770331621 Adapter cache time: 0.017692857421934605 Engine time: 0.10618136962875724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 66, 33, 33, 66, 33, 17280, 66, 33, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 17280, 17280, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 66, 66, 17280]
Prompts retrieved: 191136 . Total input tokens: 42569535 . Total output tokens: 37480677
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.571444688364863,
    "estimated_duration": 3600.007004977132,
    "input_throughput": 4423.401948380696,
    "output_throughput": 3829.8378255760035,
    "total_throughput": 8253.239773956699,
    "itl": 34.68884591824499,
    "ttft": 8878.906654865965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.206129655991689,
    "arrivals": 64071,
    "finished_requests": 63914,
    "scheduler_time": 37.30576347053888
}
#Debug simulation 
Total elapsed time: 4.571540544275194. Arrivals time: 0.16425572708249092 Scheduler time: 4.132737090811133 Scheduler overhead time: 0.10403909394517541 Adapter cache time: 0.017861859407275915 Engine time: 0.10285119619220495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 66, 33, 33, 66, 33, 17280, 66, 33, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 17280, 17280, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 66, 66, 17280]
Prompts retrieved: 191136 . Total input tokens: 42569535 . Total output tokens: 37480677
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.553752196021378,
    "estimated_duration": 3600.019218030563,
    "input_throughput": 4423.386942004044,
    "output_throughput": 3829.8248328636973,
    "total_throughput": 8253.211774867741,
    "itl": 34.68504028100053,
    "ttft": 8878.860553800534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9834919183514983,
    "arrivals": 64071,
    "finished_requests": 63914,
    "scheduler_time": 37.30405286011313
}
#Debug simulation 
Total elapsed time: 4.553879991173744. Arrivals time: 0.16814343677833676 Scheduler time: 4.110998918302357 Scheduler overhead time: 0.104075628798455 Adapter cache time: 0.01793282199651003 Engine time: 0.10315343923866749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 66, 33, 33, 66, 33, 17280, 66, 33, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 17280, 17280, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 66, 66, 17280]
Prompts retrieved: 191136 . Total input tokens: 42569535 . Total output tokens: 37480677
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.524422326125205,
    "estimated_duration": 3600.004237805086,
    "input_throughput": 4423.405348463977,
    "output_throughput": 3829.8407694114744,
    "total_throughput": 8253.246117875451,
    "itl": 34.688329705127536,
    "ttft": 8878.906183770378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.181896946327768,
    "arrivals": 64071,
    "finished_requests": 63914,
    "scheduler_time": 37.30551361955816
}
#Debug simulation 
Total elapsed time: 4.524519986007363. Arrivals time: 0.16652082977816463 Scheduler time: 4.081986820790917 Scheduler overhead time: 0.10412505874410272 Adapter cache time: 0.01766363438218832 Engine time: 0.1043637222610414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 66, 33, 33, 66, 33, 17280, 66, 33, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 17280, 17280, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 66, 66, 17280]
Prompts retrieved: 191136 . Total input tokens: 42569535 . Total output tokens: 37480677
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.593866001814604,
    "estimated_duration": 3600.012627045752,
    "input_throughput": 4423.395040441234,
    "output_throughput": 3829.83184459391,
    "total_throughput": 8253.226885035145,
    "itl": 34.68348273397204,
    "ttft": 8878.876884827589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.864104914646589,
    "arrivals": 64071,
    "finished_requests": 63914,
    "scheduler_time": 37.303036718167505
}
#Debug simulation 
Total elapsed time: 4.5939716580323875. Arrivals time: 0.16822607396170497 Scheduler time: 4.1488032117486 Scheduler overhead time: 0.10538037028163671 Adapter cache time: 0.017865740228444338 Engine time: 0.10359763819724321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 66, 33, 33, 66, 33, 17280, 66, 33, 33, 33, 17280, 17280, 33, 33, 66, 66, 66, 17280, 17280, 33, 17280, 17280, 17280, 17280, 66, 33, 66, 66, 66, 66, 17280]
Prompts retrieved: 191136 . Total input tokens: 42569535 . Total output tokens: 37480677
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.583021669648588,
    "estimated_duration": 3600.012499035953,
    "input_throughput": 4423.395197728997,
    "output_throughput": 3829.8319807756607,
    "total_throughput": 8253.227178504658,
    "itl": 34.688457724348154,
    "ttft": 8878.896661146304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.164084869138904,
    "arrivals": 64071,
    "finished_requests": 63914,
    "scheduler_time": 37.30546582271575
}
#Debug simulation 
Total elapsed time: 4.5832160408608615. Arrivals time: 0.17041626805439591 Scheduler time: 4.138948063831776 Scheduler overhead time: 0.10381152480840683 Adapter cache time: 0.01786983758211136 Engine time: 0.1020672912709415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 153360 . Total input tokens: 34137255 . Total output tokens: 30078100
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 6.623250708915293,
    "estimated_duration": 3600.023053933295,
    "input_throughput": 3513.9974968155857,
    "output_throughput": 3083.6388638875906,
    "total_throughput": 6597.636360703176,
    "itl": 33.36585605492531,
    "ttft": 42798.70396862672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3194293340389085,
    "arrivals": 51326,
    "finished_requests": 50978,
    "scheduler_time": 27.652122629583683
}
#Debug simulation 
Total elapsed time: 6.623349052853882. Arrivals time: 0.15659030713140965 Scheduler time: 6.156258035916835 Scheduler overhead time: 0.12061950471252203 Adapter cache time: 0.020514511037617922 Engine time: 0.11510433815419674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 153360 . Total input tokens: 34137255 . Total output tokens: 30078100
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 6.739642819855362,
    "estimated_duration": 3600.019526427558,
    "input_throughput": 3516.9181464312737,
    "output_throughput": 3084.4674364916796,
    "total_throughput": 6601.385582922953,
    "itl": 33.31919814077195,
    "ttft": 41362.7170132539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 497,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6031572091905435,
    "arrivals": 51326,
    "finished_requests": 51003,
    "scheduler_time": 27.707728558069487
}
#Debug simulation 
Total elapsed time: 6.739783287979662. Arrivals time: 0.15770491119474173 Scheduler time: 6.2724901041947305 Scheduler overhead time: 0.11982485605403781 Adapter cache time: 0.020479471888393164 Engine time: 0.11496487120166421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 153360 . Total input tokens: 34137255 . Total output tokens: 30078100
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 6.648253342136741,
    "estimated_duration": 3600.0197361795526,
    "input_throughput": 3516.1201681169537,
    "output_throughput": 3082.8778210471623,
    "total_throughput": 6598.997989164116,
    "itl": 33.33184091400028,
    "ttft": 42211.15002349576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7439589589601683,
    "arrivals": 51326,
    "finished_requests": 50989,
    "scheduler_time": 27.683690908491368
}
#Debug simulation 
Total elapsed time: 6.648354390170425. Arrivals time: 0.1606164714321494 Scheduler time: 6.179184859152883 Scheduler overhead time: 0.1194911333732307 Adapter cache time: 0.02054873574525118 Engine time: 0.11418162425979972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 153360 . Total input tokens: 34137255 . Total output tokens: 30078100
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 6.5932550500147045,
    "estimated_duration": 3600.019303854447,
    "input_throughput": 3515.0769848515315,
    "output_throughput": 3083.3979107043137,
    "total_throughput": 6598.474895555845,
    "itl": 33.35095268939217,
    "ttft": 42322.019188863494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.484846662604243,
    "arrivals": 51326,
    "finished_requests": 50982,
    "scheduler_time": 27.61815990829261
}
#Debug simulation 
Total elapsed time: 6.593353088013828. Arrivals time: 0.15443571098148823 Scheduler time: 6.129190262872726 Scheduler overhead time: 0.12000028509646654 Adapter cache time: 0.02059885673224926 Engine time: 0.11452666483819485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 153360 . Total input tokens: 34137255 . Total output tokens: 30078100
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 6.666020809207112,
    "estimated_duration": 3600.0043058487668,
    "input_throughput": 3516.135238903727,
    "output_throughput": 3082.891034871511,
    "total_throughput": 6599.026273775238,
    "itl": 33.331866459755,
    "ttft": 42211.2205872649,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7072992186993656,
    "arrivals": 51326,
    "finished_requests": 50989,
    "scheduler_time": 27.683411478517016
}
#Debug simulation 
Total elapsed time: 6.666129906196147. Arrivals time: 0.15689849946647882 Scheduler time: 6.199651982169598 Scheduler overhead time: 0.12028993992134929 Adapter cache time: 0.0204037600196898 Engine time: 0.11436670878902078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 153360 . Total input tokens: 34137255 . Total output tokens: 30078100
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 6.6561399810016155,
    "estimated_duration": 3600.0289415548464,
    "input_throughput": 3516.1111772987547,
    "output_throughput": 3082.8699380418343,
    "total_throughput": 6598.981115340589,
    "itl": 33.32568572480517,
    "ttft": 42209.86793609696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2174961540475344,
    "arrivals": 51326,
    "finished_requests": 50989,
    "scheduler_time": 27.680086862170068
}
#Debug simulation 
Total elapsed time: 6.656239605043083. Arrivals time: 0.15779256727546453 Scheduler time: 6.188482304569334 Scheduler overhead time: 0.12064217450097203 Adapter cache time: 0.020372258964926004 Engine time: 0.11472048936411738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 1080, 1080, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 8640, 8640, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 153360 . Total input tokens: 34137255 . Total output tokens: 30078100
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 6.677064611110836,
    "estimated_duration": 3600.0334574048798,
    "input_throughput": 3516.106766720085,
    "output_throughput": 3082.866070917132,
    "total_throughput": 6598.972837637217,
    "itl": 33.33175333491148,
    "ttft": 42211.11411603194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.679131282679766,
    "arrivals": 51326,
    "finished_requests": 50989,
    "scheduler_time": 27.683440938219086
}
#Debug simulation 
Total elapsed time: 6.677251465152949. Arrivals time: 0.1576132862828672 Scheduler time: 6.20945562934503 Scheduler overhead time: 0.12020869413390756 Adapter cache time: 0.020723541267216206 Engine time: 0.11476093018427491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 540, 540, 4320, 540, 8640, 4320, 540, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 8640, 8640, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 147960 . Total input tokens: 32972365 . Total output tokens: 29026215
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.894160377793014,
    "estimated_duration": 3599.9672794276585,
    "input_throughput": 3387.439121929678,
    "output_throughput": 2964.189441659739,
    "total_throughput": 6351.628563589417,
    "itl": 32.5437886127254,
    "ttft": 37926.67455597202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 542,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5839256953169145,
    "arrivals": 49599,
    "finished_requests": 49276,
    "scheduler_time": 24.981502462676044
}
#Debug simulation 
Total elapsed time: 5.8942552311345935. Arrivals time: 0.1481966134160757 Scheduler time: 5.439172419719398 Scheduler overhead time: 0.11787713319063187 Adapter cache time: 0.020530538633465767 Engine time: 0.11411694157868624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 540, 540, 4320, 540, 8640, 4320, 540, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 8640, 8640, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 147960 . Total input tokens: 32972365 . Total output tokens: 29026215
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.87740817386657,
    "estimated_duration": 3599.9635406173143,
    "input_throughput": 3387.051802727181,
    "output_throughput": 2962.620004249024,
    "total_throughput": 6349.6718069762055,
    "itl": 32.50553267341646,
    "ttft": 38383.792570678605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9598352224985174,
    "arrivals": 49599,
    "finished_requests": 49268,
    "scheduler_time": 24.953764781806548
}
#Debug simulation 
Total elapsed time: 5.877552160993218. Arrivals time: 0.1485094027593732 Scheduler time: 5.422744107898325 Scheduler overhead time: 0.1175134046934545 Adapter cache time: 0.020765580236911774 Engine time: 0.11338565731421113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 540, 540, 4320, 540, 8640, 4320, 540, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 8640, 8640, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 147960 . Total input tokens: 32972365 . Total output tokens: 29026215
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.836081121116877,
    "estimated_duration": 3599.965712162394,
    "input_throughput": 3387.049759614478,
    "output_throughput": 2962.618217158977,
    "total_throughput": 6349.667976773455,
    "itl": 32.50783020990866,
    "ttft": 38384.26861019839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.058762792530503,
    "arrivals": 49599,
    "finished_requests": 49268,
    "scheduler_time": 24.954474478612475
}
#Debug simulation 
Total elapsed time: 5.836174771655351. Arrivals time: 0.14707958465442061 Scheduler time: 5.382025886327028 Scheduler overhead time: 0.11714998772367835 Adapter cache time: 0.020635114051401615 Engine time: 0.11459059501066804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 540, 540, 4320, 540, 8640, 4320, 540, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 8640, 8640, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 147960 . Total input tokens: 32972365 . Total output tokens: 29026215
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.89978140918538,
    "estimated_duration": 3599.978781351403,
    "input_throughput": 3387.0374634327004,
    "output_throughput": 2962.607461812962,
    "total_throughput": 6349.644925245662,
    "itl": 32.50297550349289,
    "ttft": 38383.107532284885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.706621821429573,
    "arrivals": 49599,
    "finished_requests": 49268,
    "scheduler_time": 24.951844215956633
}
#Debug simulation 
Total elapsed time: 5.899899565149099. Arrivals time: 0.1488601271994412 Scheduler time: 5.4424955626018345 Scheduler overhead time: 0.11750380042940378 Adapter cache time: 0.020578126423060894 Engine time: 0.11552215041592717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 540, 540, 4320, 540, 8640, 4320, 540, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 8640, 8640, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 147960 . Total input tokens: 32972365 . Total output tokens: 29026215
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.869108289014548,
    "estimated_duration": 3599.977053212322,
    "input_throughput": 3387.0390893519,
    "output_throughput": 2962.608883988065,
    "total_throughput": 6349.647973339965,
    "itl": 32.506309475225244,
    "ttft": 38384.43043002101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.019824763326941,
    "arrivals": 49599,
    "finished_requests": 49268,
    "scheduler_time": 24.95424963240735
}
#Debug simulation 
Total elapsed time: 5.869210300035775. Arrivals time: 0.14866417041048408 Scheduler time: 5.413412753492594 Scheduler overhead time: 0.11771347839385271 Adapter cache time: 0.0205713021568954 Engine time: 0.11413297755643725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 540, 540, 4320, 540, 8640, 4320, 540, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 8640, 8640, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 147960 . Total input tokens: 32972365 . Total output tokens: 29026215
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.861052535008639,
    "estimated_duration": 3599.9807777128867,
    "input_throughput": 3387.035585158467,
    "output_throughput": 2962.6058189054597,
    "total_throughput": 6349.6414040639265,
    "itl": 32.50072447205156,
    "ttft": 38382.82019548135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.492004754492066,
    "arrivals": 49599,
    "finished_requests": 49268,
    "scheduler_time": 24.95038168464013
}
#Debug simulation 
Total elapsed time: 5.861151630058885. Arrivals time: 0.14868550701066852 Scheduler time: 5.405291982460767 Scheduler overhead time: 0.1174413519911468 Adapter cache time: 0.020774923264980316 Engine time: 0.11441689729690552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 540, 540, 4320, 540, 8640, 4320, 540, 540, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 8640, 8640, 540, 8640, 8640, 8640, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 147960 . Total input tokens: 32972365 . Total output tokens: 29026215
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.873891392722726,
    "estimated_duration": 3599.9689378703215,
    "input_throughput": 3387.046724690164,
    "output_throughput": 2962.6155625413307,
    "total_throughput": 6349.662287231495,
    "itl": 32.50660455288441,
    "ttft": 38383.99124330092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9889643040113505,
    "arrivals": 49599,
    "finished_requests": 49268,
    "scheduler_time": 24.95396894202319
}
#Debug simulation 
Total elapsed time: 5.874121861997992. Arrivals time: 0.14879256207495928 Scheduler time: 5.415887297131121 Scheduler overhead time: 0.117363715544343 Adapter cache time: 0.020580726210027933 Engine time: 0.11662718886509538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 270, 270, 4320, 270, 8640, 4320, 270, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 8640, 8640, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 145260 . Total input tokens: 32371057 . Total output tokens: 28510179
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.469014185946435,
    "estimated_duration": 3600.0203797070426,
    "input_throughput": 3342.4433005513133,
    "output_throughput": 2922.1201244577555,
    "total_throughput": 6264.563425009069,
    "itl": 32.274143685396396,
    "ttft": 33560.570853680874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.564088468221064,
    "arrivals": 48729,
    "finished_requests": 48438,
    "scheduler_time": 23.91065717448205
}
#Debug simulation 
Total elapsed time: 5.469122086185962. Arrivals time: 0.14514643093571067 Scheduler time: 5.017117367126048 Scheduler overhead time: 0.11707233032211661 Adapter cache time: 0.020793762058019638 Engine time: 0.11423230590298772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 270, 270, 4320, 270, 8640, 4320, 270, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 8640, 8640, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 145260 . Total input tokens: 32371057 . Total output tokens: 28510179
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.537545228842646,
    "estimated_duration": 3600.023164717175,
    "input_throughput": 3343.7270954187566,
    "output_throughput": 2923.784797597801,
    "total_throughput": 6267.511893016557,
    "itl": 32.34619811671394,
    "ttft": 32704.761625749627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8027303101262113,
    "arrivals": 48729,
    "finished_requests": 48458,
    "scheduler_time": 24.009083996509034
}
#Debug simulation 
Total elapsed time: 5.53767736395821. Arrivals time: 0.145986660849303 Scheduler time: 5.086286719888449 Scheduler overhead time: 0.11583492904901505 Adapter cache time: 0.020611020270735025 Engine time: 0.11428521620109677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 270, 270, 4320, 270, 8640, 4320, 270, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 8640, 8640, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 145260 . Total input tokens: 32371057 . Total output tokens: 28510179
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.5743078691884875,
    "estimated_duration": 3600.0026710021248,
    "input_throughput": 3343.6989080480867,
    "output_throughput": 2924.3561636217983,
    "total_throughput": 6268.0550716698845,
    "itl": 32.383167575323945,
    "ttft": 33112.28906076496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8548472211975926,
    "arrivals": 48729,
    "finished_requests": 48454,
    "scheduler_time": 24.01495459085215
}
#Debug simulation 
Total elapsed time: 5.574407005216926. Arrivals time: 0.14580158377066255 Scheduler time: 5.123326458502561 Scheduler overhead time: 0.11671383306384087 Adapter cache time: 0.020714327692985535 Engine time: 0.1138429157435894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 270, 270, 4320, 270, 8640, 4320, 270, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 8640, 8640, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 145260 . Total input tokens: 32371057 . Total output tokens: 28510179
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.520495114848018,
    "estimated_duration": 3600.014926617555,
    "input_throughput": 3343.687525015303,
    "output_throughput": 2924.3462081673756,
    "total_throughput": 6268.033733182679,
    "itl": 32.383853975288936,
    "ttft": 33077.35298153407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.507605904419899,
    "arrivals": 48729,
    "finished_requests": 48454,
    "scheduler_time": 24.008223582646863
}
#Debug simulation 
Total elapsed time: 5.520592786837369. Arrivals time: 0.1443990021944046 Scheduler time: 5.0735097280703485 Scheduler overhead time: 0.11579841980710626 Adapter cache time: 0.020226456224918365 Engine time: 0.11275025410577655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 270, 270, 4320, 270, 8640, 4320, 270, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 8640, 8640, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 145260 . Total input tokens: 32371057 . Total output tokens: 28510179
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.509617525152862,
    "estimated_duration": 3600.02784874851,
    "input_throughput": 3344.9680130075085,
    "output_throughput": 2924.0465469341893,
    "total_throughput": 6269.014559941698,
    "itl": 32.357613542779994,
    "ttft": 31736.065782922025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 527,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8760127113294183,
    "arrivals": 48729,
    "finished_requests": 48471,
    "scheduler_time": 24.017385508269033
}
#Debug simulation 
Total elapsed time: 5.50972561025992. Arrivals time: 0.14343263860791922 Scheduler time: 5.059704959858209 Scheduler overhead time: 0.1178105752915144 Adapter cache time: 0.02062746975570917 Engine time: 0.11358164064586163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 270, 270, 4320, 270, 8640, 4320, 270, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 8640, 8640, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 145260 . Total input tokens: 32371057 . Total output tokens: 28510179
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.51057102298364,
    "estimated_duration": 3600.019261645899,
    "input_throughput": 3344.9759917380297,
    "output_throughput": 2924.0535216434655,
    "total_throughput": 6269.029513381495,
    "itl": 32.35165486431107,
    "ttft": 31732.78206732542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 527,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.364326335680656,
    "arrivals": 48729,
    "finished_requests": 48471,
    "scheduler_time": 24.01330183499366
}
#Debug simulation 
Total elapsed time: 5.510666429065168. Arrivals time: 0.14424658054485917 Scheduler time: 5.063516036141664 Scheduler overhead time: 0.11531185870990157 Adapter cache time: 0.02041870402172208 Engine time: 0.11319084791466594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 270, 270, 4320, 270, 8640, 4320, 270, 270, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 8640, 8640, 270, 8640, 8640, 8640, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 145260 . Total input tokens: 32371057 . Total output tokens: 28510179
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.54192768689245,
    "estimated_duration": 3600.024373626379,
    "input_throughput": 3346.217622372747,
    "output_throughput": 2926.0263005911593,
    "total_throughput": 6272.243922963906,
    "itl": 32.38943241228334,
    "ttft": 30305.677332331077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.753315776996346,
    "arrivals": 48729,
    "finished_requests": 48493,
    "scheduler_time": 24.079170046250347
}
#Debug simulation 
Total elapsed time: 5.542103135958314. Arrivals time: 0.14250938966870308 Scheduler time: 5.094753127545118 Scheduler overhead time: 0.11634019343182445 Adapter cache time: 0.020385567098855972 Engine time: 0.11417238553985953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 135, 135, 4320, 135, 8640, 4320, 135, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 8640, 8640, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143910 . Total input tokens: 32086836 . Total output tokens: 28257754
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.984589183237404,
    "estimated_duration": 3599.9825597228846,
    "input_throughput": 3336.3222739958346,
    "output_throughput": 2877.51616241091,
    "total_throughput": 6213.838436406745,
    "itl": 31.972501147476308,
    "ttft": 25112.88983368868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6302125585405656,
    "arrivals": 48221,
    "finished_requests": 48013,
    "scheduler_time": 22.755189744535816
}
#Debug simulation 
Total elapsed time: 4.9846858801320195. Arrivals time: 0.1366387722082436 Scheduler time: 4.545926785096526 Scheduler overhead time: 0.11486474797129631 Adapter cache time: 0.020460149738937616 Engine time: 0.113284298684448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 135, 135, 4320, 135, 8640, 4320, 135, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 8640, 8640, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143910 . Total input tokens: 32086836 . Total output tokens: 28257754
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.007599757984281,
    "estimated_duration": 3599.964377153672,
    "input_throughput": 3336.2699576200025,
    "output_throughput": 2877.4079170722366,
    "total_throughput": 6213.677874692239,
    "itl": 31.968003836923526,
    "ttft": 25180.237147372587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.007308588712481,
    "arrivals": 48221,
    "finished_requests": 48012,
    "scheduler_time": 22.756693519482162
}
#Debug simulation 
Total elapsed time: 5.007725905627012. Arrivals time: 0.13573725055903196 Scheduler time: 4.570594005286694 Scheduler overhead time: 0.11513250088319182 Adapter cache time: 0.020504317246377468 Engine time: 0.1117275720462203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 135, 135, 4320, 135, 8640, 4320, 135, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 8640, 8640, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143910 . Total input tokens: 32086836 . Total output tokens: 28257754
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.0167582291178405,
    "estimated_duration": 3599.982430498787,
    "input_throughput": 3336.3223937556513,
    "output_throughput": 2877.5162657015335,
    "total_throughput": 6213.838659457185,
    "itl": 32.000596213287515,
    "ttft": 25251.35463678868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.072596271690016,
    "arrivals": 48221,
    "finished_requests": 48013,
    "scheduler_time": 22.776582266513543
}
#Debug simulation 
Total elapsed time: 5.016863108146936. Arrivals time: 0.13611625973135233 Scheduler time: 4.579516696278006 Scheduler overhead time: 0.11501040775328875 Adapter cache time: 0.02039057621732354 Engine time: 0.11226975964382291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 135, 135, 4320, 135, 8640, 4320, 135, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 8640, 8640, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143910 . Total input tokens: 32086836 . Total output tokens: 28257754
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.033264737110585,
    "estimated_duration": 3599.989841852102,
    "input_throughput": 3336.3155252184833,
    "output_throughput": 2877.5103417154523,
    "total_throughput": 6213.825866933936,
    "itl": 31.985534191989775,
    "ttft": 25252.700760805594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.666099037765518,
    "arrivals": 48221,
    "finished_requests": 48013,
    "scheduler_time": 22.773614631535395
}
#Debug simulation 
Total elapsed time: 5.03336276113987. Arrivals time: 0.1373831364326179 Scheduler time: 4.592787421308458 Scheduler overhead time: 0.11576443444937468 Adapter cache time: 0.02050230698660016 Engine time: 0.11300287116318941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 135, 135, 4320, 135, 8640, 4320, 135, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 8640, 8640, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143910 . Total input tokens: 32086836 . Total output tokens: 28257754
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.030566833913326,
    "estimated_duration": 3599.9681854010037,
    "input_throughput": 3336.3355956053033,
    "output_throughput": 2877.52765205232,
    "total_throughput": 6213.863247657623,
    "itl": 31.995143172480798,
    "ttft": 25254.529197276304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.014192220913282,
    "arrivals": 48221,
    "finished_requests": 48013,
    "scheduler_time": 22.7762065724433
}
#Debug simulation 
Total elapsed time: 5.030659295152873. Arrivals time: 0.13665301725268364 Scheduler time: 4.593965503387153 Scheduler overhead time: 0.11501907370984554 Adapter cache time: 0.020490783732384443 Engine time: 0.11098806746304035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 135, 135, 4320, 135, 8640, 4320, 135, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 8640, 8640, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143910 . Total input tokens: 32086836 . Total output tokens: 28257754
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.014392141718417,
    "estimated_duration": 3599.9944963816233,
    "input_throughput": 3336.3112116065818,
    "output_throughput": 2877.506621305089,
    "total_throughput": 6213.817832911671,
    "itl": 31.9610096963132,
    "ttft": 25249.2490833752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.479236912610925,
    "arrivals": 48221,
    "finished_requests": 48013,
    "scheduler_time": 22.771252328766646
}
#Debug simulation 
Total elapsed time: 5.014484817627817. Arrivals time: 0.13555197697132826 Scheduler time: 4.5775469448417425 Scheduler overhead time: 0.1159023093059659 Adapter cache time: 0.02043946785852313 Engine time: 0.11127029079943895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 135, 135, 4320, 135, 8640, 4320, 135, 135, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 8640, 8640, 135, 8640, 8640, 8640, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143910 . Total input tokens: 32086836 . Total output tokens: 28257754
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.992182192392647,
    "estimated_duration": 3599.993367733678,
    "input_throughput": 3336.3122575865073,
    "output_throughput": 2877.5075234434,
    "total_throughput": 6213.819781029907,
    "itl": 32.00018817609533,
    "ttft": 25259.575208174003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.982296175714618,
    "arrivals": 48221,
    "finished_requests": 48013,
    "scheduler_time": 22.77695417686022
}
#Debug simulation 
Total elapsed time: 4.992314060218632. Arrivals time: 0.13676405884325504 Scheduler time: 4.554194291122258 Scheduler overhead time: 0.11547872936353087 Adapter cache time: 0.02047046646475792 Engine time: 0.11159855360165238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 66, 66, 4320, 66, 8640, 4320, 66, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 8640, 8640, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143220 . Total input tokens: 31933121 . Total output tokens: 28124683
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.9800161379389465,
    "estimated_duration": 3599.9989390399273,
    "input_throughput": 3270.389297153463,
    "output_throughput": 2899.15696552494,
    "total_throughput": 6169.546262678403,
    "itl": 32.085253349161654,
    "ttft": 28197.798043957016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0152585185692016,
    "arrivals": 48032,
    "finished_requests": 47777,
    "scheduler_time": 23.12387960208536
}
#Debug simulation 
Total elapsed time: 4.98010915517807. Arrivals time: 0.13628138834610581 Scheduler time: 4.543459929525852 Scheduler overhead time: 0.11490709660574794 Adapter cache time: 0.019947597291320562 Engine time: 0.11154075432568789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 66, 66, 4320, 66, 8640, 4320, 66, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 8640, 8640, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143220 . Total input tokens: 31933121 . Total output tokens: 28124683
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.945054800249636,
    "estimated_duration": 3599.9933964914294,
    "input_throughput": 3270.3943322436116,
    "output_throughput": 2899.161429065929,
    "total_throughput": 6169.555761309541,
    "itl": 32.089024918281,
    "ttft": 28197.4269338039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3483575554937173,
    "arrivals": 48032,
    "finished_requests": 47777,
    "scheduler_time": 23.12627467465954
}
#Debug simulation 
Total elapsed time: 4.945162761025131. Arrivals time: 0.1352619589306414 Scheduler time: 4.510835983790457 Scheduler overhead time: 0.11445954209193587 Adapter cache time: 0.01986427465453744 Engine time: 0.11123283486813307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 66, 66, 4320, 66, 8640, 4320, 66, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 8640, 8640, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143220 . Total input tokens: 31933121 . Total output tokens: 28124683
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.968462091870606,
    "estimated_duration": 3599.9917373173625,
    "input_throughput": 3270.3958395119225,
    "output_throughput": 2899.162765239401,
    "total_throughput": 6169.558604751323,
    "itl": 32.08790112312215,
    "ttft": 28231.25466802345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.46255836408588,
    "arrivals": 48032,
    "finished_requests": 47777,
    "scheduler_time": 23.131396300434997
}
#Debug simulation 
Total elapsed time: 4.968551111873239. Arrivals time: 0.13401174684986472 Scheduler time: 4.533785895444453 Scheduler overhead time: 0.11552762938663363 Adapter cache time: 0.01984986150637269 Engine time: 0.11162878526374698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 66, 66, 4320, 66, 8640, 4320, 66, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 8640, 8640, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143220 . Total input tokens: 31933121 . Total output tokens: 28124683
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.974214083980769,
    "estimated_duration": 3599.9818890797937,
    "input_throughput": 3270.4047861222566,
    "output_throughput": 2899.170696291429,
    "total_throughput": 6169.575482413686,
    "itl": 32.074475231330275,
    "ttft": 28177.255105953092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1248445011069936,
    "arrivals": 48032,
    "finished_requests": 47777,
    "scheduler_time": 23.121563083155483
}
#Debug simulation 
Total elapsed time: 4.974305504933. Arrivals time: 0.13707223208621144 Scheduler time: 4.537508866749704 Scheduler overhead time: 0.11639631632715464 Adapter cache time: 0.01990111405029893 Engine time: 0.10946564516052604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 66, 66, 4320, 66, 8640, 4320, 66, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 8640, 8640, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143220 . Total input tokens: 31933121 . Total output tokens: 28124683
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.9647989477962255,
    "estimated_duration": 3600.0039668131285,
    "input_throughput": 3270.384729720811,
    "output_throughput": 2899.1529165561524,
    "total_throughput": 6169.537646276964,
    "itl": 32.10383928988311,
    "ttft": 28260.07249437228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3953550057066866,
    "arrivals": 48032,
    "finished_requests": 47777,
    "scheduler_time": 23.135178201928237
}
#Debug simulation 
Total elapsed time: 4.964904655702412. Arrivals time: 0.1370865977369249 Scheduler time: 4.52981903264299 Scheduler overhead time: 0.11448627710342407 Adapter cache time: 0.01977745583280921 Engine time: 0.11014376441016793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 66, 66, 4320, 66, 8640, 4320, 66, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 8640, 8640, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143220 . Total input tokens: 31933121 . Total output tokens: 28124683
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.99828679068014,
    "estimated_duration": 3599.979365321358,
    "input_throughput": 3269.5059625569043,
    "output_throughput": 2898.8649492074037,
    "total_throughput": 6168.370911764308,
    "itl": 32.097726889163425,
    "ttft": 29176.441731582516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9110679489001505,
    "arrivals": 48032,
    "finished_requests": 47766,
    "scheduler_time": 23.13484516410751
}
#Debug simulation 
Total elapsed time: 4.998378125950694. Arrivals time: 0.13836898701265454 Scheduler time: 4.560372688341886 Scheduler overhead time: 0.11508868308737874 Adapter cache time: 0.01991494046524167 Engine time: 0.11088569508865476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 66, 66, 4320, 66, 8640, 4320, 66, 66, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 8640, 8640, 66, 8640, 8640, 8640, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 143220 . Total input tokens: 31933121 . Total output tokens: 28124683
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.9814417152665555,
    "estimated_duration": 3599.999933705664,
    "input_throughput": 3270.388393557841,
    "output_throughput": 2899.1561644993426,
    "total_throughput": 6169.544558057184,
    "itl": 32.1037655001346,
    "ttft": 28257.639336673466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.36863688992339,
    "arrivals": 48032,
    "finished_requests": 47777,
    "scheduler_time": 23.134624995574352
}
#Debug simulation 
Total elapsed time: 4.9815832702443. Arrivals time: 0.1372834532521665 Scheduler time: 4.545791010838002 Scheduler overhead time: 0.11503855278715491 Adapter cache time: 0.01987943798303604 Engine time: 0.10952332848683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 33, 33, 4320, 33, 8640, 4320, 33, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 8640, 8640, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 142890 . Total input tokens: 31858323 . Total output tokens: 28054971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.833052197936922,
    "estimated_duration": 3599.9937576525153,
    "input_throughput": 3278.2159621564347,
    "output_throughput": 2883.3477774607627,
    "total_throughput": 6161.563739617198,
    "itl": 31.98541925005905,
    "ttft": 23398.42880487372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7507621572911956,
    "arrivals": 47900,
    "finished_requests": 47698,
    "scheduler_time": 22.700028543145386
}
#Debug simulation 
Total elapsed time: 4.83314423635602. Arrivals time: 0.13627856830134988 Scheduler time: 4.398222509305924 Scheduler overhead time: 0.11594474734738469 Adapter cache time: 0.01972125470638275 Engine time: 0.10931339720264077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 33, 33, 4320, 33, 8640, 4320, 33, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 8640, 8640, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 142890 . Total input tokens: 31858323 . Total output tokens: 28054971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.814023187849671,
    "estimated_duration": 3599.9878075504457,
    "input_throughput": 3278.2213804302246,
    "output_throughput": 2883.3525430917857,
    "total_throughput": 6161.573923522011,
    "itl": 32.03346115393786,
    "ttft": 23398.675292541033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.128527801567694,
    "arrivals": 47900,
    "finished_requests": 47698,
    "scheduler_time": 22.70419884257594
}
#Debug simulation 
Total elapsed time: 4.814113932661712. Arrivals time: 0.13510325783863664 Scheduler time: 4.379526517353952 Scheduler overhead time: 0.11514639388769865 Adapter cache time: 0.01984701119363308 Engine time: 0.11058633076027036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 33, 33, 4320, 33, 8640, 4320, 33, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 8640, 8640, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 142890 . Total input tokens: 31858323 . Total output tokens: 28054971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.816689794883132,
    "estimated_duration": 3599.9896591626257,
    "input_throughput": 3278.21969431576,
    "output_throughput": 2883.3510600734458,
    "total_throughput": 6161.570754389206,
    "itl": 31.992517589356577,
    "ttft": 23407.685814414577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1572344796872005,
    "arrivals": 47900,
    "finished_requests": 47698,
    "scheduler_time": 22.704235088540063
}
#Debug simulation 
Total elapsed time: 4.816775414161384. Arrivals time: 0.13493431638926268 Scheduler time: 4.383621474727988 Scheduler overhead time: 0.11521482421085238 Adapter cache time: 0.019607024732977152 Engine time: 0.10945749469101429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 33, 33, 4320, 33, 8640, 4320, 33, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 8640, 8640, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 142890 . Total input tokens: 31858323 . Total output tokens: 28054971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.800403218250722,
    "estimated_duration": 3599.985540261431,
    "input_throughput": 3278.2234450705514,
    "output_throughput": 2883.354359041731,
    "total_throughput": 6161.577804112283,
    "itl": 32.003679958809805,
    "ttft": 23387.79234158344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.863934779590922,
    "arrivals": 47900,
    "finished_requests": 47698,
    "scheduler_time": 22.700383014276262
}
#Debug simulation 
Total elapsed time: 4.800490015186369. Arrivals time: 0.13173669017851353 Scheduler time: 4.370345568284392 Scheduler overhead time: 0.11508266115561128 Adapter cache time: 0.019686500541865826 Engine time: 0.10979355266317725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 33, 33, 4320, 33, 8640, 4320, 33, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 8640, 8640, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 142890 . Total input tokens: 31858323 . Total output tokens: 28054971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.817227468360215,
    "estimated_duration": 3599.972873094255,
    "input_throughput": 3278.234980103143,
    "output_throughput": 2883.3645046547626,
    "total_throughput": 6161.599484757906,
    "itl": 31.985190950082032,
    "ttft": 23367.438563234708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1514292126847674,
    "arrivals": 47900,
    "finished_requests": 47698,
    "scheduler_time": 22.699219338461265
}
#Debug simulation 
Total elapsed time: 4.8173202173784375. Arrivals time: 0.13831138517707586 Scheduler time: 4.37975885020569 Scheduler overhead time: 0.11525362264364958 Adapter cache time: 0.0198188335634768 Engine time: 0.11047034291550517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 33, 33, 4320, 33, 8640, 4320, 33, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 8640, 8640, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 142890 . Total input tokens: 31858323 . Total output tokens: 28054971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.826893077697605,
    "estimated_duration": 3599.975199543787,
    "input_throughput": 3278.23286157515,
    "output_throughput": 2883.362641308037,
    "total_throughput": 6161.595502883187,
    "itl": 32.001273639211696,
    "ttft": 23388.658521711382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.687630715980183,
    "arrivals": 47900,
    "finished_requests": 47698,
    "scheduler_time": 22.69915183153048
}
#Debug simulation 
Total elapsed time: 4.826996103860438. Arrivals time: 0.13500545918941498 Scheduler time: 4.3939045504666865 Scheduler overhead time: 0.114406518638134 Adapter cache time: 0.01964588090777397 Engine time: 0.11029020650312304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 4320, 33, 33, 4320, 33, 8640, 4320, 33, 33, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 8640, 8640, 33, 8640, 8640, 8640, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640]
Prompts retrieved: 142890 . Total input tokens: 31858323 . Total output tokens: 28054971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.792812780942768,
    "estimated_duration": 3599.991865227701,
    "input_throughput": 3278.2176854317827,
    "output_throughput": 2883.349293163877,
    "total_throughput": 6161.56697859566,
    "itl": 32.025058866578526,
    "ttft": 23396.51397449082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1570153903961424,
    "arrivals": 47900,
    "finished_requests": 47698,
    "scheduler_time": 22.70410766373379
}
#Debug simulation 
Total elapsed time: 4.792937504127622. Arrivals time: 0.1353031573817134 Scheduler time: 4.357994886115193 Scheduler overhead time: 0.11442806385457516 Adapter cache time: 0.019671096932142973 Engine time: 0.11200462700799108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 540, 540, 1080, 540, 8640, 1080, 540, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 8640, 8640, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 112320 . Total input tokens: 25050381 . Total output tokens: 22051341
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 3.247242522891611,
    "estimated_duration": 3599.978081246098,
    "input_throughput": 2577.34291448474,
    "output_throughput": 2278.1085925837783,
    "total_throughput": 4855.451507068518,
    "itl": 29.02347407375224,
    "ttft": 14512.59617060067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.508644187944249,
    "arrivals": 37706,
    "finished_requests": 37581,
    "scheduler_time": 11.280724982223392
}
#Debug simulation 
Total elapsed time: 3.2473356290720403. Arrivals time: 0.11076108878478408 Scheduler time: 2.821381062269211 Scheduler overhead time: 0.12073590513318777 Adapter cache time: 0.02502446435391903 Engine time: 0.11305933352559805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 540, 540, 1080, 540, 8640, 1080, 540, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 8640, 8640, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 112320 . Total input tokens: 25050381 . Total output tokens: 22051341
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 3.2201333716511726,
    "estimated_duration": 3599.991141769916,
    "input_throughput": 2577.333564059365,
    "output_throughput": 2278.1003277602385,
    "total_throughput": 4855.433891819604,
    "itl": 29.033139608240674,
    "ttft": 14483.171037872233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.58298409958362,
    "arrivals": 37706,
    "finished_requests": 37581,
    "scheduler_time": 11.286632896750875
}
#Debug simulation 
Total elapsed time: 3.2202701666392386. Arrivals time: 0.11040235916152596 Scheduler time: 2.794679439160973 Scheduler overhead time: 0.1197860212996602 Adapter cache time: 0.02518569678068161 Engine time: 0.11403763573616743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 540, 540, 1080, 540, 8640, 1080, 540, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 8640, 8640, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 112320 . Total input tokens: 25050381 . Total output tokens: 22051341
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 3.266204747837037,
    "estimated_duration": 3599.971369904931,
    "input_throughput": 2577.347719363953,
    "output_throughput": 2278.1128396075487,
    "total_throughput": 4855.460558971502,
    "itl": 29.03542455967464,
    "ttft": 14483.650239961906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.85915777930514,
    "arrivals": 37706,
    "finished_requests": 37581,
    "scheduler_time": 11.288730058837976
}
#Debug simulation 
Total elapsed time: 3.2662948789075017. Arrivals time: 0.11194007424637675 Scheduler time: 2.8342940658330917 Scheduler overhead time: 0.12128379568457603 Adapter cache time: 0.025407683104276657 Engine time: 0.11697540758177638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 540, 540, 1080, 540, 8640, 1080, 540, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 8640, 8640, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 112320 . Total input tokens: 25050381 . Total output tokens: 22051341
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 3.221316314768046,
    "estimated_duration": 3599.9759754944944,
    "input_throughput": 2577.3444220625715,
    "output_throughput": 2278.1099251290107,
    "total_throughput": 4855.454347191582,
    "itl": 29.024438849828037,
    "ttft": 14482.177719331523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.798639249662028,
    "arrivals": 37706,
    "finished_requests": 37581,
    "scheduler_time": 11.280175469905487
}
#Debug simulation 
Total elapsed time: 3.221433222759515. Arrivals time: 0.10882757045328617 Scheduler time: 2.7980465018190444 Scheduler overhead time: 0.12013330217450857 Adapter cache time: 0.025323663372546434 Engine time: 0.11281423456966877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 540, 540, 1080, 540, 8640, 1080, 540, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 8640, 8640, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 112320 . Total input tokens: 25050381 . Total output tokens: 22051341
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 3.2014656718820333,
    "estimated_duration": 3599.9712953307176,
    "input_throughput": 2577.3477727542895,
    "output_throughput": 2278.112886799168,
    "total_throughput": 4855.460659553457,
    "itl": 29.029207410627983,
    "ttft": 14441.527833506421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.792507083066633,
    "arrivals": 37706,
    "finished_requests": 37581,
    "scheduler_time": 11.283594491075954
}
#Debug simulation 
Total elapsed time: 3.201568016782403. Arrivals time: 0.11061189696192741 Scheduler time: 2.778134285006672 Scheduler overhead time: 0.11965514067560434 Adapter cache time: 0.025032830890268087 Engine time: 0.11211488582193851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 540, 540, 1080, 540, 8640, 1080, 540, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 8640, 8640, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 112320 . Total input tokens: 25050381 . Total output tokens: 22051341
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 3.219452345278114,
    "estimated_duration": 3599.9664610630007,
    "input_throughput": 2577.3512337835155,
    "output_throughput": 2278.1159459964415,
    "total_throughput": 4855.4671797799565,
    "itl": 29.019807042635232,
    "ttft": 14511.970022560254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.186462233481114,
    "arrivals": 37706,
    "finished_requests": 37581,
    "scheduler_time": 11.278070266836616
}
#Debug simulation 
Total elapsed time: 3.219550653360784. Arrivals time: 0.11043652286753058 Scheduler time: 2.7966604814864695 Scheduler overhead time: 0.11994087463244796 Adapter cache time: 0.02501700073480606 Engine time: 0.11149607319384813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 540, 540, 1080, 540, 8640, 1080, 540, 540, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 8640, 8640, 540, 8640, 8640, 8640, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 112320 . Total input tokens: 25050381 . Total output tokens: 22051341
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 3.2568051419220865,
    "estimated_duration": 3599.978251438824,
    "input_throughput": 2577.380570644171,
    "output_throughput": 2278.1651518928265,
    "total_throughput": 4855.545722536997,
    "itl": 29.03497681982852,
    "ttft": 14387.949820811551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.665088984817165,
    "arrivals": 37706,
    "finished_requests": 37582,
    "scheduler_time": 11.287465177214985
}
#Debug simulation 
Total elapsed time: 3.256931841839105. Arrivals time: 0.11094634002074599 Scheduler time: 2.8296089614741504 Scheduler overhead time: 0.1201220634393394 Adapter cache time: 0.025202840566635132 Engine time: 0.11483634263277054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 270, 270, 1080, 270, 8640, 1080, 270, 270, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 8640, 8640, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 109620 . Total input tokens: 24434728 . Total output tokens: 21511302
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.9968802011571825,
    "estimated_duration": 3600.0112611361187,
    "input_throughput": 2520.920169881895,
    "output_throughput": 2194.7128569554934,
    "total_throughput": 4715.633026837389,
    "itl": 28.657272414190622,
    "ttft": 9199.86551628127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.115459582708135,
    "arrivals": 36751,
    "finished_requests": 36670,
    "scheduler_time": 9.663520361882277
}
#Debug simulation 
Total elapsed time: 2.9969651279971004. Arrivals time: 0.10522814141586423 Scheduler time: 2.5743556264787912 Scheduler overhead time: 0.12011607876047492 Adapter cache time: 0.026165536604821682 Engine time: 0.11452696612104774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 270, 270, 1080, 270, 8640, 1080, 270, 270, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 8640, 8640, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 109620 . Total input tokens: 24434728 . Total output tokens: 21511302
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 3.0031474148854613,
    "estimated_duration": 3600.0288671248204,
    "input_throughput": 2520.9078412885237,
    "output_throughput": 2194.702123683292,
    "total_throughput": 4715.609964971816,
    "itl": 28.667883476229125,
    "ttft": 9297.910450617726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.308390126782292,
    "arrivals": 36751,
    "finished_requests": 36670,
    "scheduler_time": 9.672872918857788
}
#Debug simulation 
Total elapsed time: 3.00323288096115. Arrivals time: 0.10362048959359527 Scheduler time: 2.582098400220275 Scheduler overhead time: 0.11981122381985188 Adapter cache time: 0.026161238085478544 Engine time: 0.11497224122285843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 270, 270, 1080, 270, 8640, 1080, 270, 270, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 8640, 8640, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 109620 . Total input tokens: 24434728 . Total output tokens: 21511302
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.975956085603684,
    "estimated_duration": 3600.030655642959,
    "input_throughput": 2520.906588885466,
    "output_throughput": 2194.701033341311,
    "total_throughput": 4715.6076222267775,
    "itl": 28.672606556830388,
    "ttft": 9297.308438754839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.623769248747061,
    "arrivals": 36751,
    "finished_requests": 36670,
    "scheduler_time": 9.675271136860506
}
#Debug simulation 
Total elapsed time: 2.9760405039414763. Arrivals time: 0.10412497399374843 Scheduler time: 2.5593745708465576 Scheduler overhead time: 0.1203065044246614 Adapter cache time: 0.025694094132632017 Engine time: 0.11041156109422445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 270, 270, 1080, 270, 8640, 1080, 270, 270, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 8640, 8640, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 109620 . Total input tokens: 24434728 . Total output tokens: 21511302
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.992831378709525,
    "estimated_duration": 3600.000805371896,
    "input_throughput": 2520.927491587735,
    "output_throughput": 2194.719231231892,
    "total_throughput": 4715.646722819627,
    "itl": 28.658234664443906,
    "ttft": 9200.21209144134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.347741213250163,
    "arrivals": 36751,
    "finished_requests": 36670,
    "scheduler_time": 9.665433926730598
}
#Debug simulation 
Total elapsed time: 2.9929179986938834. Arrivals time: 0.10578201059252024 Scheduler time: 2.5714233005419374 Scheduler overhead time: 0.12062806263566017 Adapter cache time: 0.02608225727453828 Engine time: 0.11274495115503669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 270, 270, 1080, 270, 8640, 1080, 270, 270, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 8640, 8640, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 109620 . Total input tokens: 24434728 . Total output tokens: 21511302
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 3.0148806888610125,
    "estimated_duration": 3600.0071763097308,
    "input_throughput": 2520.9230302987576,
    "output_throughput": 2194.7153472341383,
    "total_throughput": 4715.638377532896,
    "itl": 28.669146234462005,
    "ttft": 9203.095522369515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.509123726058549,
    "arrivals": 36751,
    "finished_requests": 36670,
    "scheduler_time": 9.674738620296878
}
#Debug simulation 
Total elapsed time: 3.0149707067757845. Arrivals time: 0.10661299526691437 Scheduler time: 2.5907398923300207 Scheduler overhead time: 0.11969678336754441 Adapter cache time: 0.026313713286072016 Engine time: 0.11495126644149423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 270, 270, 1080, 270, 8640, 1080, 270, 270, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 8640, 8640, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 109620 . Total input tokens: 24434728 . Total output tokens: 21511302
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.9889340940862894,
    "estimated_duration": 3600.010035180688,
    "input_throughput": 2520.9210283616612,
    "output_throughput": 2194.713604347895,
    "total_throughput": 4715.634632709556,
    "itl": 28.650661768774164,
    "ttft": 9197.53105274161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1692,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.801594231445675,
    "arrivals": 36751,
    "finished_requests": 36670,
    "scheduler_time": 9.660810397544914
}
#Debug simulation 
Total elapsed time: 2.9890249967575073. Arrivals time: 0.10550307622179389 Scheduler time: 2.5665148915722966 Scheduler overhead time: 0.12056146981194615 Adapter cache time: 0.026078049559146166 Engine time: 0.11412525689229369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 270, 270, 1080, 270, 8640, 1080, 270, 270, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 8640, 8640, 270, 8640, 8640, 8640, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 109620 . Total input tokens: 24434728 . Total output tokens: 21511302
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 3.0074715721420944,
    "estimated_duration": 3600.016318565897,
    "input_throughput": 2520.9166284044104,
    "output_throughput": 2194.7097737455365,
    "total_throughput": 4715.626402149947,
    "itl": 28.669852674452848,
    "ttft": 9297.725741961329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.396560209486438,
    "arrivals": 36751,
    "finished_requests": 36670,
    "scheduler_time": 9.673409574723967
}
#Debug simulation 
Total elapsed time: 3.0075876712799072. Arrivals time: 0.10557479877024889 Scheduler time: 2.5872443835251033 Scheduler overhead time: 0.12052547233179212 Adapter cache time: 0.026023760437965393 Engine time: 0.11180396704003215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 135, 135, 1080, 135, 8640, 1080, 135, 135, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 8640, 8640, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 108270 . Total input tokens: 24129533 . Total output tokens: 21261534
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.9128346676006913,
    "estimated_duration": 3599.913739812306,
    "input_throughput": 2501.195209324504,
    "output_throughput": 2158.9125633897033,
    "total_throughput": 4660.107772714207,
    "itl": 28.490217912015893,
    "ttft": 7822.71682704979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1773,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.72380121364755,
    "arrivals": 36327,
    "finished_requests": 36256,
    "scheduler_time": 8.953100801660824
}
#Debug simulation 
Total elapsed time: 2.9129221788607538. Arrivals time: 0.10522682592272758 Scheduler time: 2.4904598514549434 Scheduler overhead time: 0.12054386595264077 Adapter cache time: 0.026320808567106724 Engine time: 0.11409244267269969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 135, 135, 1080, 135, 8640, 1080, 135, 135, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 8640, 8640, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 108270 . Total input tokens: 24129533 . Total output tokens: 21261534
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.9075601086951792,
    "estimated_duration": 3599.924378326783,
    "input_throughput": 2501.187817779947,
    "output_throughput": 2158.9061833605288,
    "total_throughput": 4660.094001140476,
    "itl": 28.503353539425497,
    "ttft": 7824.390850758761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1770,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.058416037996993,
    "arrivals": 36327,
    "finished_requests": 36256,
    "scheduler_time": 8.963900071106966
}
#Debug simulation 
Total elapsed time: 2.907663146033883. Arrivals time: 0.10479468433186412 Scheduler time: 2.484440370928496 Scheduler overhead time: 0.11999891279265285 Adapter cache time: 0.02653623605147004 Engine time: 0.11564871529117227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 135, 135, 1080, 135, 8640, 1080, 135, 135, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 8640, 8640, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 108270 . Total input tokens: 24129533 . Total output tokens: 21261534
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.910223212093115,
    "estimated_duration": 3599.909748092526,
    "input_throughput": 2501.1979827469204,
    "output_throughput": 2158.9149572758242,
    "total_throughput": 4660.112940022745,
    "itl": 28.507740158355833,
    "ttft": 7828.414027986702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1765,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.394739888040093,
    "arrivals": 36327,
    "finished_requests": 36256,
    "scheduler_time": 8.966874430100946
}
#Debug simulation 
Total elapsed time: 2.9103151531890035. Arrivals time: 0.10428931005299091 Scheduler time: 2.4885825905948877 Scheduler overhead time: 0.12122720619663596 Adapter cache time: 0.026459996588528156 Engine time: 0.11329248873516917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 135, 135, 1080, 135, 8640, 1080, 135, 135, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 8640, 8640, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 108270 . Total input tokens: 24129533 . Total output tokens: 21261534
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.905520928092301,
    "estimated_duration": 3599.924013315406,
    "input_throughput": 2501.1880713858586,
    "output_throughput": 2158.9064022610714,
    "total_throughput": 4660.09447364693,
    "itl": 28.492607544825237,
    "ttft": 7822.655342883375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1774,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.992810036735827,
    "arrivals": 36327,
    "finished_requests": 36256,
    "scheduler_time": 8.955274383075334
}
#Debug simulation 
Total elapsed time: 2.9056099830195308. Arrivals time: 0.10494945617392659 Scheduler time: 2.483161191921681 Scheduler overhead time: 0.12028643721714616 Adapter cache time: 0.02662082016468048 Engine time: 0.11391519941389561 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 135, 135, 1080, 135, 8640, 1080, 135, 135, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 8640, 8640, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 108270 . Total input tokens: 24129533 . Total output tokens: 21261534
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.9173123380169272,
    "estimated_duration": 3599.9333958163725,
    "input_throughput": 2501.18155254317,
    "output_throughput": 2158.9007755065795,
    "total_throughput": 4660.082328049749,
    "itl": 28.50706801885272,
    "ttft": 7826.506861666053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1767,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.247683699615047,
    "arrivals": 36327,
    "finished_requests": 36256,
    "scheduler_time": 8.9657616579263
}
#Debug simulation 
Total elapsed time: 2.917430735193193. Arrivals time: 0.10460722167044878 Scheduler time: 2.493096860591322 Scheduler overhead time: 0.12270330032333732 Adapter cache time: 0.026469561737030745 Engine time: 0.11415047058835626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 135, 135, 1080, 135, 8640, 1080, 135, 135, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 8640, 8640, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 108270 . Total input tokens: 24129533 . Total output tokens: 21261534
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.8924947162158787,
    "estimated_duration": 3599.9333477511177,
    "input_throughput": 2501.1815859382127,
    "output_throughput": 2158.9008043315894,
    "total_throughput": 4660.082390269802,
    "itl": 28.485792674734242,
    "ttft": 7822.5692512616515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1771,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.305923985750814,
    "arrivals": 36327,
    "finished_requests": 36256,
    "scheduler_time": 8.949952451856511
}
#Debug simulation 
Total elapsed time: 2.892602596897632. Arrivals time: 0.10372758889570832 Scheduler time: 2.472819700371474 Scheduler overhead time: 0.11978829931467772 Adapter cache time: 0.026376033201813698 Engine time: 0.11365487426519394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 135, 135, 1080, 135, 8640, 1080, 135, 135, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 8640, 8640, 135, 8640, 8640, 8640, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 108270 . Total input tokens: 24129533 . Total output tokens: 21261534
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.909050215035677,
    "estimated_duration": 3599.9092505512485,
    "input_throughput": 2501.198328435979,
    "output_throughput": 2158.9152556581535,
    "total_throughput": 4660.113584094132,
    "itl": 28.504929607555976,
    "ttft": 7825.710677059274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1771,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.180571748334666,
    "arrivals": 36327,
    "finished_requests": 36256,
    "scheduler_time": 8.965109481253322
}
#Debug simulation 
Total elapsed time: 2.9091752860695124. Arrivals time: 0.10368480859324336 Scheduler time: 2.49133706279099 Scheduler overhead time: 0.11974194552749395 Adapter cache time: 0.02626789268106222 Engine time: 0.11153721902519464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 66, 66, 1080, 66, 8640, 1080, 66, 66, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 8640, 8640, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107580 . Total input tokens: 23981417 . Total output tokens: 21123930
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.8566702688112855,
    "estimated_duration": 3600.0127725109264,
    "input_throughput": 2469.310127974836,
    "output_throughput": 2168.3864734055765,
    "total_throughput": 4637.696601380412,
    "itl": 28.54860925800719,
    "ttft": 8237.516660299209,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.135296809803986,
    "arrivals": 36084,
    "finished_requests": 36007,
    "scheduler_time": 9.089430330046785
}
#Debug simulation 
Total elapsed time: 2.856759131886065. Arrivals time: 0.1018669125624001 Scheduler time: 2.440469814930111 Scheduler overhead time: 0.1191404527053237 Adapter cache time: 0.026008786167949438 Engine time: 0.11320408340543509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 66, 66, 1080, 66, 8640, 1080, 66, 66, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 8640, 8640, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107580 . Total input tokens: 23981417 . Total output tokens: 21123930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.8758949348703027,
    "estimated_duration": 3600.007611706303,
    "input_throughput": 2469.313667863775,
    "output_throughput": 2168.3895819042646,
    "total_throughput": 4637.703249768039,
    "itl": 28.562184557049566,
    "ttft": 8236.623102270883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.487480110022922,
    "arrivals": 36084,
    "finished_requests": 36007,
    "scheduler_time": 9.099784423311418
}
#Debug simulation 
Total elapsed time: 2.8759852875955403. Arrivals time: 0.10257414262741804 Scheduler time: 2.457384917419404 Scheduler overhead time: 0.11937208287417889 Adapter cache time: 0.02621155558153987 Engine time: 0.11418621707707644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 66, 66, 1080, 66, 8640, 1080, 66, 66, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 8640, 8640, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107580 . Total input tokens: 23981417 . Total output tokens: 21123930
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.8931777831166983,
    "estimated_duration": 3600.008636703927,
    "input_throughput": 2469.3129647986166,
    "output_throughput": 2168.3889645184763,
    "total_throughput": 4637.701929317092,
    "itl": 28.56735972699397,
    "ttft": 8238.040475539921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.896710313283586,
    "arrivals": 36084,
    "finished_requests": 36007,
    "scheduler_time": 9.103390749250055
}
#Debug simulation 
Total elapsed time: 2.8932616747915745. Arrivals time: 0.10082755889743567 Scheduler time: 2.4765273286029696 Scheduler overhead time: 0.11946037318557501 Adapter cache time: 0.025987739209085703 Engine time: 0.11426658276468515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 66, 66, 1080, 66, 8640, 1080, 66, 66, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 8640, 8640, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107580 . Total input tokens: 23981417 . Total output tokens: 21123930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.8558487980626523,
    "estimated_duration": 3600.0174757540262,
    "input_throughput": 2469.306901944435,
    "output_throughput": 2168.383640516907,
    "total_throughput": 4637.6905424613415,
    "itl": 28.551654969335466,
    "ttft": 8237.285734508714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.402996199061334,
    "arrivals": 36084,
    "finished_requests": 36007,
    "scheduler_time": 9.091433393722083
}
#Debug simulation 
Total elapsed time: 2.8559391451999545. Arrivals time: 0.100299880374223 Scheduler time: 2.4419461702927947 Scheduler overhead time: 0.11953672021627426 Adapter cache time: 0.02591573167592287 Engine time: 0.11220055539160967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 66, 66, 1080, 66, 8640, 1080, 66, 66, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 8640, 8640, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107580 . Total input tokens: 23981417 . Total output tokens: 21123930
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.8529168451204896,
    "estimated_duration": 3600.00282793959,
    "input_throughput": 2469.3169491446774,
    "output_throughput": 2168.3924633103074,
    "total_throughput": 4637.709412454985,
    "itl": 28.562294767174286,
    "ttft": 8233.389074989085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.723354732971393,
    "arrivals": 36084,
    "finished_requests": 36007,
    "scheduler_time": 9.10110998695588
}
#Debug simulation 
Total elapsed time: 2.853019948117435. Arrivals time: 0.10065335500985384 Scheduler time: 2.440735733602196 Scheduler overhead time: 0.11927171424031258 Adapter cache time: 0.025923075154423714 Engine time: 0.11069570388644934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 66, 66, 1080, 66, 8640, 1080, 66, 66, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 8640, 8640, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107580 . Total input tokens: 23981417 . Total output tokens: 21123930
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.8748669661581516,
    "estimated_duration": 3600.008293913149,
    "input_throughput": 2469.3131999252173,
    "output_throughput": 2168.389170991262,
    "total_throughput": 4637.70237091648,
    "itl": 28.543737839746605,
    "ttft": 8231.066385081598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.756906784861675,
    "arrivals": 36084,
    "finished_requests": 36007,
    "scheduler_time": 9.085716902216214
}
#Debug simulation 
Total elapsed time: 2.874961744993925. Arrivals time: 0.10298342257738113 Scheduler time: 2.4568840195424855 Scheduler overhead time: 0.11987887741997838 Adapter cache time: 0.026182725094258785 Engine time: 0.11305729066953063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 66, 66, 1080, 66, 8640, 1080, 66, 66, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 8640, 8640, 66, 8640, 8640, 8640, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107580 . Total input tokens: 23981417 . Total output tokens: 21123930
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.8682577810250223,
    "estimated_duration": 3600.01563313841,
    "input_throughput": 2469.308165823241,
    "output_throughput": 2168.3847503725196,
    "total_throughput": 4637.6929161957605,
    "itl": 28.56361088687237,
    "ttft": 8235.88339790097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.642056572232153,
    "arrivals": 36084,
    "finished_requests": 36007,
    "scheduler_time": 9.100447034209763
}
#Debug simulation 
Total elapsed time: 2.8683787803165615. Arrivals time: 0.10168588580563664 Scheduler time: 2.449502458795905 Scheduler overhead time: 0.11954987281933427 Adapter cache time: 0.026070523541420698 Engine time: 0.11544899549335241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 33, 33, 1080, 33, 8640, 1080, 33, 33, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 8640, 8640, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107250 . Total input tokens: 23904447 . Total output tokens: 21060636
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.8211176386103034,
    "estimated_duration": 3599.653891747245,
    "input_throughput": 2486.8246418143576,
    "output_throughput": 2143.2816131817503,
    "total_throughput": 4630.106254996108,
    "itl": 28.359932681727084,
    "ttft": 9511.279544022036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.128684400772036,
    "arrivals": 35993,
    "finished_requests": 35902,
    "scheduler_time": 8.60711075780844
}
#Debug simulation 
Total elapsed time: 2.8211999759078026. Arrivals time: 0.10075173014774919 Scheduler time: 2.4073291840031743 Scheduler overhead time: 0.120172212831676 Adapter cache time: 0.02580034453421831 Engine time: 0.11085976660251617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 33, 33, 1080, 33, 8640, 1080, 33, 33, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 8640, 8640, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107250 . Total input tokens: 23904447 . Total output tokens: 21060636
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.833237953018397,
    "estimated_duration": 3599.647388585145,
    "input_throughput": 2486.7135676637313,
    "output_throughput": 2143.2599271987033,
    "total_throughput": 4629.973494862435,
    "itl": 28.374102124059114,
    "ttft": 9612.132240987603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.56881848583901,
    "arrivals": 35993,
    "finished_requests": 35901,
    "scheduler_time": 8.618699759009894
}
#Debug simulation 
Total elapsed time: 2.8333233669400215. Arrivals time: 0.10067802667617798 Scheduler time: 2.4174847030080855 Scheduler overhead time: 0.11916717933490872 Adapter cache time: 0.025949282571673393 Engine time: 0.11422024620696902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 33, 33, 1080, 33, 8640, 1080, 33, 33, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 8640, 8640, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107250 . Total input tokens: 23904447 . Total output tokens: 21060636
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.840364752802998,
    "estimated_duration": 3599.6482399262068,
    "input_throughput": 2486.712979539218,
    "output_throughput": 2143.25942030329,
    "total_throughput": 4629.972399842508,
    "itl": 28.37863420121247,
    "ttft": 9612.209384000234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.971734766643216,
    "arrivals": 35993,
    "finished_requests": 35901,
    "scheduler_time": 8.622036534180545
}
#Debug simulation 
Total elapsed time: 2.840449979994446. Arrivals time: 0.10092683089897037 Scheduler time: 2.425128628499806 Scheduler overhead time: 0.1199126448482275 Adapter cache time: 0.02609892189502716 Engine time: 0.11216267244890332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 33, 33, 1080, 33, 8640, 1080, 33, 33, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 8640, 8640, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107250 . Total input tokens: 23904447 . Total output tokens: 21060636
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.8412276790477335,
    "estimated_duration": 3599.631162598638,
    "input_throughput": 2486.7247769735113,
    "output_throughput": 2143.2695883292713,
    "total_throughput": 4629.994365302783,
    "itl": 28.36333777666067,
    "ttft": 9609.303473539447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1695,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.494033166323966,
    "arrivals": 35993,
    "finished_requests": 35901,
    "scheduler_time": 8.609449238540888
}
#Debug simulation 
Total elapsed time: 2.841322197113186. Arrivals time: 0.10089013958349824 Scheduler time: 2.4167368337512016 Scheduler overhead time: 0.11938626784831285 Adapter cache time: 0.02735681924968958 Engine time: 0.11785648530349135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 33, 33, 1080, 33, 8640, 1080, 33, 33, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 8640, 8640, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107250 . Total input tokens: 23904447 . Total output tokens: 21060636
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.82322220178321,
    "estimated_duration": 3599.6399851317638,
    "input_throughput": 2486.7186821385253,
    "output_throughput": 2143.264335285351,
    "total_throughput": 4629.983017423876,
    "itl": 28.37730484004002,
    "ttft": 9609.85732485841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.88555755951414,
    "arrivals": 35993,
    "finished_requests": 35901,
    "scheduler_time": 8.620717512070767
}
#Debug simulation 
Total elapsed time: 2.8233171990141273. Arrivals time: 0.1012049070559442 Scheduler time: 2.404534128960222 Scheduler overhead time: 0.11906647961586714 Adapter cache time: 0.025906902737915516 Engine time: 0.1163475513458252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 33, 33, 1080, 33, 8640, 1080, 33, 33, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 8640, 8640, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107250 . Total input tokens: 23904447 . Total output tokens: 21060636
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.8462509918026626,
    "estimated_duration": 3599.653501061458,
    "input_throughput": 2486.82491172007,
    "output_throughput": 2143.2818458012684,
    "total_throughput": 4630.106757521338,
    "itl": 28.35649997544164,
    "ttft": 9510.86264527316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.769674626742818,
    "arrivals": 35993,
    "finished_requests": 35902,
    "scheduler_time": 8.604072977932095
}
#Debug simulation 
Total elapsed time: 2.8463585381396115. Arrivals time: 0.10112249758094549 Scheduler time: 2.413562531583011 Scheduler overhead time: 0.13555247941985726 Adapter cache time: 0.02593080746009946 Engine time: 0.11350503377616405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 1080, 33, 33, 1080, 33, 8640, 1080, 33, 33, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 8640, 8640, 33, 8640, 8640, 8640, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640]
Prompts retrieved: 107250 . Total input tokens: 23904447 . Total output tokens: 21060636
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.8210504427552223,
    "estimated_duration": 3599.645869730396,
    "input_throughput": 2486.7146169215885,
    "output_throughput": 2143.2608315378066,
    "total_throughput": 4629.975448459395,
    "itl": 28.375157292187012,
    "ttft": 9610.17719405606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.7848986116794,
    "arrivals": 35993,
    "finished_requests": 35901,
    "scheduler_time": 8.620093581822454
}
#Debug simulation 
Total elapsed time: 2.8211747808381915. Arrivals time: 0.10269666695967317 Scheduler time: 2.403819482307881 Scheduler overhead time: 0.11942588305100799 Adapter cache time: 0.026131702587008476 Engine time: 0.11289391061291099 
