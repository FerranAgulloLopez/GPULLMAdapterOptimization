INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 199.9267407269217,
    "estimated_duration": 3600.0258674814268,
    "input_throughput": 6736.366040882376,
    "output_throughput": 5840.8721975963645,
    "total_throughput": 12577.23823847874,
    "itl": 68.04509753686202,
    "ttft": 1150206.6975299406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1439467625273378,
    "arrivals": 128637,
    "finished_requests": 97753,
    "scheduler_time": 201.2689621062023
}
#Debug simulation 
Total elapsed time: 199.9269371512346. Arrivals time: 0.675533608533442 Scheduler time: 198.80594763346016 Scheduler overhead time: 0.1775989239104092 Adapter cache time: 0.033695563673973083 Engine time: 0.18142112391069531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 205.20760946068913,
    "estimated_duration": 3600.007702075512,
    "input_throughput": 6718.151460080598,
    "output_throughput": 5827.850309293568,
    "total_throughput": 12546.001769374167,
    "itl": 67.57857751358769,
    "ttft": 1145623.9920107895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2010555622912953,
    "arrivals": 128637,
    "finished_requests": 97533,
    "scheduler_time": 201.67030465055373
}
#Debug simulation 
Total elapsed time: 205.20781454257667. Arrivals time: 0.7170411925762892 Scheduler time: 204.02817309368402 Scheduler overhead time: 0.1820126147940755 Adapter cache time: 0.0381638677790761 Engine time: 0.18783708941191435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 200.43226403277367,
    "estimated_duration": 3600.088501394672,
    "input_throughput": 6703.003270795011,
    "output_throughput": 5808.098881985708,
    "total_throughput": 12511.10215278072,
    "itl": 66.7149259369925,
    "ttft": 1157673.8718405263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4250903317425427,
    "arrivals": 128637,
    "finished_requests": 97254,
    "scheduler_time": 202.48759804470933
}
#Debug simulation 
Total elapsed time: 200.4324539047666. Arrivals time: 0.7126036924310029 Scheduler time: 199.26878776354715 Scheduler overhead time: 0.17888779053464532 Adapter cache time: 0.0351383313536644 Engine time: 0.18266062019392848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 203.2651165178977,
    "estimated_duration": 3600.0067314727858,
    "input_throughput": 6724.5790926885675,
    "output_throughput": 5831.324096277928,
    "total_throughput": 12555.903188966495,
    "itl": 67.60187775553426,
    "ttft": 1150913.5546545328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1638379714498284,
    "arrivals": 128637,
    "finished_requests": 97583,
    "scheduler_time": 201.6626253916422
}
#Debug simulation 
Total elapsed time: 203.2652743179351. Arrivals time: 0.7105949292890728 Scheduler time: 202.100388109684 Scheduler overhead time: 0.17706256825476885 Adapter cache time: 0.03521702252328396 Engine time: 0.18912163702771068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 200.98608727101237,
    "estimated_duration": 3600.075016254587,
    "input_throughput": 6703.02837886573,
    "output_throughput": 5808.120637928765,
    "total_throughput": 12511.149016794496,
    "itl": 66.71457544698069,
    "ttft": 1157671.9047702975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4110063637327428,
    "arrivals": 128637,
    "finished_requests": 97254,
    "scheduler_time": 202.48758839454513
}
#Debug simulation 
Total elapsed time: 200.98625936591998. Arrivals time: 0.7251402880065143 Scheduler time: 199.80514653446153 Scheduler overhead time: 0.17839330807328224 Adapter cache time: 0.03714383766055107 Engine time: 0.18561439402401447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 204.09696645382792,
    "estimated_duration": 3600.0969499647613,
    "input_throughput": 6725.33721633163,
    "output_throughput": 5830.877137963041,
    "total_throughput": 12556.21435429467,
    "itl": 67.59798755690319,
    "ttft": 1148181.6024378452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1171861645998407,
    "arrivals": 128637,
    "finished_requests": 97608,
    "scheduler_time": 201.67417778427676
}
#Debug simulation 
Total elapsed time: 204.09714104095474. Arrivals time: 0.7114039650186896 Scheduler time: 202.92918330151588 Scheduler overhead time: 0.17957432614639401 Adapter cache time: 0.03592017572373152 Engine time: 0.1866795397363603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 214.13617819081992,
    "estimated_duration": 3600.0670735602585,
    "input_throughput": 6699.957947213027,
    "output_throughput": 5807.044583568111,
    "total_throughput": 12507.00253078114,
    "itl": 66.7125616106865,
    "ttft": 1157189.2762128327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3989935674890903,
    "arrivals": 128637,
    "finished_requests": 97234,
    "scheduler_time": 202.48503375056433
}
#Debug simulation 
Total elapsed time: 214.1363518498838. Arrivals time: 0.7156135179102421 Scheduler time: 212.96929128607735 Scheduler overhead time: 0.17861739732325077 Adapter cache time: 0.034384146332740784 Engine time: 0.1835014601238072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 211.9591337176971,
    "estimated_duration": 3600.0732318245737,
    "input_throughput": 6664.561372780747,
    "output_throughput": 5828.567545378889,
    "total_throughput": 12493.128918159637,
    "itl": 67.74371105933305,
    "ttft": 1151776.417521562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4216679418692348,
    "arrivals": 127906,
    "finished_requests": 97271,
    "scheduler_time": 200.4240426436657
}
#Debug simulation 
Total elapsed time: 211.95944232167676. Arrivals time: 0.7214194731786847 Scheduler time: 210.78676144592464 Scheduler overhead time: 0.1772773233242333 Adapter cache time: 0.03763074520975351 Engine time: 0.1812563929706812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 224.27944272011518,
    "estimated_duration": 3600.0697333578764,
    "input_throughput": 6651.900316848513,
    "output_throughput": 5818.3436853770945,
    "total_throughput": 12470.244002225607,
    "itl": 67.33512347031878,
    "ttft": 1152544.6920581418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.630459294770846,
    "arrivals": 127906,
    "finished_requests": 97107,
    "scheduler_time": 200.78131757178082
}
#Debug simulation 
Total elapsed time: 224.27960569504648. Arrivals time: 0.7329963915981352 Scheduler time: 223.08822714863345 Scheduler overhead time: 0.18120651971548796 Adapter cache time: 0.03658726764842868 Engine time: 0.18745797593146563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 218.53095362475142,
    "estimated_duration": 3600.0709417233875,
    "input_throughput": 6628.680208333954,
    "output_throughput": 5800.019871276291,
    "total_throughput": 12428.700079610246,
    "itl": 66.51152607455676,
    "ttft": 1157549.4858008318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6987409092485939,
    "arrivals": 127906,
    "finished_requests": 96763,
    "scheduler_time": 201.5503684838627
}
#Debug simulation 
Total elapsed time: 218.53121448401362. Arrivals time: 0.7153659029863775 Scheduler time: 217.3628471470438 Scheduler overhead time: 0.1824477929621935 Adapter cache time: 0.034089789260178804 Engine time: 0.1813035630621016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 226.29860763065517,
    "estimated_duration": 3600.083899825664,
    "input_throughput": 6650.714446171507,
    "output_throughput": 5818.496619207784,
    "total_throughput": 12469.211065379292,
    "itl": 67.35928478642614,
    "ttft": 1153062.753321959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.210470822933129,
    "arrivals": 127906,
    "finished_requests": 97087,
    "scheduler_time": 200.78377363626967
}
#Debug simulation 
Total elapsed time: 226.29877651389688. Arrivals time: 0.6844289251603186 Scheduler time: 225.18094175914302 Scheduler overhead time: 0.17238023784011602 Adapter cache time: 0.03231743490323424 Engine time: 0.17620367091149092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 223.04367341334,
    "estimated_duration": 3600.0550144329313,
    "input_throughput": 6628.709534806632,
    "output_throughput": 5800.0455316066955,
    "total_throughput": 12428.755066413327,
    "itl": 66.51109641593874,
    "ttft": 1157547.285080459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6821715351194177,
    "arrivals": 127906,
    "finished_requests": 96763,
    "scheduler_time": 201.550302648986
}
#Debug simulation 
Total elapsed time: 223.04385138303041. Arrivals time: 0.6573283509351313 Scheduler time: 221.95116408308968 Scheduler overhead time: 0.1738406983204186 Adapter cache time: 0.03234670776873827 Engine time: 0.17569231940433383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 227.8177207680419,
    "estimated_duration": 3600.0745008074286,
    "input_throughput": 6652.317888040575,
    "output_throughput": 5818.115429361888,
    "total_throughput": 12470.433317402463,
    "itl": 67.32335549617491,
    "ttft": 1152439.1955333024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.404462606925513,
    "arrivals": 127906,
    "finished_requests": 97096,
    "scheduler_time": 200.80209259048385
}
#Debug simulation 
Total elapsed time: 227.8178979740478. Arrivals time: 0.6708191144280136 Scheduler time: 226.7079837983474 Scheduler overhead time: 0.1752051254734397 Adapter cache time: 0.03166484599933028 Engine time: 0.17812903504818678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 229.95715926168486,
    "estimated_duration": 3600.0410498366955,
    "input_throughput": 6628.7352476390515,
    "output_throughput": 5800.0680300429285,
    "total_throughput": 12428.80327768198,
    "itl": 66.51073965307475,
    "ttft": 1157545.274152823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.667259098403159,
    "arrivals": 127906,
    "finished_requests": 96763,
    "scheduler_time": 201.55023917725438
}
#Debug simulation 
Total elapsed time: 229.9573281747289. Arrivals time: 0.6734243426471949 Scheduler time: 228.84747147886083 Scheduler overhead time: 0.1759205898270011 Adapter cache time: 0.03343883482739329 Engine time: 0.1721458276733756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 229.83132397197187,
    "estimated_duration": 3600.0635648026614,
    "input_throughput": 6683.020054208076,
    "output_throughput": 5802.856984039149,
    "total_throughput": 12485.877038247225,
    "itl": 67.29495248591778,
    "ttft": 1155374.4873771349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7589008024986812,
    "arrivals": 127669,
    "finished_requests": 97088,
    "scheduler_time": 200.27681498133364
}
#Debug simulation 
Total elapsed time: 229.83161358209327. Arrivals time: 0.6804309776052833 Scheduler time: 228.71670045284554 Scheduler overhead time: 0.17709350120276213 Adapter cache time: 0.03216083673760295 Engine time: 0.17272315826267004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 231.67496886989102,
    "estimated_duration": 3600.083177078479,
    "input_throughput": 6667.342619422138,
    "output_throughput": 5781.227259557369,
    "total_throughput": 12448.569878979508,
    "itl": 66.28706504311845,
    "ttft": 1147938.928248942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3787004584586258,
    "arrivals": 127669,
    "finished_requests": 96839,
    "scheduler_time": 201.23105406549706
}
#Debug simulation 
Total elapsed time: 231.67509806808084. Arrivals time: 0.6421211413107812 Scheduler time: 230.6037888797 Scheduler overhead time: 0.1714477869682014 Adapter cache time: 0.031200930941849947 Engine time: 0.17259895661845803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 229.6564038512297,
    "estimated_duration": 3600.0897417943906,
    "input_throughput": 6650.673654614536,
    "output_throughput": 5773.28247090876,
    "total_throughput": 12423.956125523297,
    "itl": 66.00605842774574,
    "ttft": 1159341.889476169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.109216775507673,
    "arrivals": 127669,
    "finished_requests": 96614,
    "scheduler_time": 201.45269525336286
}
#Debug simulation 
Total elapsed time: 229.6565358871594. Arrivals time: 0.6797218094579875 Scheduler time: 228.53420680155978 Scheduler overhead time: 0.17864008713513613 Adapter cache time: 0.03290590504184365 Engine time: 0.17622643150389194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 230.00889952806756,
    "estimated_duration": 3600.0366504517074,
    "input_throughput": 6675.111765038504,
    "output_throughput": 5793.92962496169,
    "total_throughput": 12469.041390000195,
    "itl": 66.90008615742093,
    "ttft": 1156792.0737010362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7972438432229658,
    "arrivals": 127669,
    "finished_requests": 96975,
    "scheduler_time": 200.6203261882471
}
#Debug simulation 
Total elapsed time: 230.00904584815726. Arrivals time: 0.6606746297329664 Scheduler time: 228.90909643936902 Scheduler overhead time: 0.17544711520895362 Adapter cache time: 0.0339628579095006 Engine time: 0.1742583280429244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 225.09549118811265,
    "estimated_duration": 3600.012103630686,
    "input_throughput": 6649.85042018511,
    "output_throughput": 5773.673365997188,
    "total_throughput": 12423.523786182299,
    "itl": 66.05115151736605,
    "ttft": 1158823.3533346308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.07069597368595,
    "arrivals": 127669,
    "finished_requests": 96625,
    "scheduler_time": 201.40206244230936
}
#Debug simulation 
Total elapsed time: 225.09562988206744. Arrivals time: 0.6555977379903197 Scheduler time: 224.00459460681304 Scheduler overhead time: 0.1743876156397164 Adapter cache time: 0.03345757257193327 Engine time: 0.17338343057781458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 229.40349184069782,
    "estimated_duration": 3600.0302613300582,
    "input_throughput": 6678.551638373489,
    "output_throughput": 5794.439347932886,
    "total_throughput": 12472.990986306375,
    "itl": 66.89192238791,
    "ttft": 1156550.3305192231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.685355128310615,
    "arrivals": 127669,
    "finished_requests": 97019,
    "scheduler_time": 200.63675984815123
}
#Debug simulation 
Total elapsed time: 229.40363281266764. Arrivals time: 0.6627637171186507 Scheduler time: 228.3076949333772 Scheduler overhead time: 0.17373601673170924 Adapter cache time: 0.03179169353097677 Engine time: 0.17269762326031923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 230.8373130070977,
    "estimated_duration": 3600.090808011075,
    "input_throughput": 6649.705042641901,
    "output_throughput": 5773.547143240854,
    "total_throughput": 12423.252185882755,
    "itl": 66.05070686320319,
    "ttft": 1158960.153167167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0539194823801585,
    "arrivals": 127669,
    "finished_requests": 96625,
    "scheduler_time": 201.40837768725464
}
#Debug simulation 
Total elapsed time: 230.8374607837759. Arrivals time: 0.6607963275164366 Scheduler time: 229.73589428327978 Scheduler overhead time: 0.1797872604802251 Adapter cache time: 0.03217880520969629 Engine time: 0.1736059682443738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 240.89600540697575,
    "estimated_duration": 3600.0708165522087,
    "input_throughput": 6665.28721870547,
    "output_throughput": 5797.747062095134,
    "total_throughput": 12463.034280800604,
    "itl": 66.87034312974798,
    "ttft": 1151375.2957223963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1291957082878814,
    "arrivals": 127143,
    "finished_requests": 96884,
    "scheduler_time": 199.58839160560495
}
#Debug simulation 
Total elapsed time: 240.8961573112756. Arrivals time: 0.6666088495403528 Scheduler time: 239.7932559554465 Scheduler overhead time: 0.17820927500724792 Adapter cache time: 0.032399857416749 Engine time: 0.17207676311954856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 239.53850266104564,
    "estimated_duration": 3600.0904178798196,
    "input_throughput": 6650.131863660102,
    "output_throughput": 5787.410198511167,
    "total_throughput": 12437.54206217127,
    "itl": 66.45954546181984,
    "ttft": 1152051.5848048995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.371853593974377,
    "arrivals": 127143,
    "finished_requests": 96696,
    "scheduler_time": 199.95027928958814
}
#Debug simulation 
Total elapsed time: 239.53869934007525. Arrivals time: 0.6642880802974105 Scheduler time: 238.43617113074288 Scheduler overhead time: 0.17515474651008844 Adapter cache time: 0.0344733209349215 Engine time: 0.17370954900979996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 239.35741339297965,
    "estimated_duration": 3600.0802051976507,
    "input_throughput": 6629.799237678266,
    "output_throughput": 5769.2756316965715,
    "total_throughput": 12399.074869374837,
    "itl": 65.65982479644903,
    "ttft": 1154254.4310472477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.565504358415507,
    "arrivals": 127143,
    "finished_requests": 96391,
    "scheduler_time": 200.7054989785725
}
#Debug simulation 
Total elapsed time: 239.35754248173907. Arrivals time: 0.6700110053643584 Scheduler time: 238.24642199650407 Scheduler overhead time: 0.17890379391610622 Adapter cache time: 0.0330724180676043 Engine time: 0.17448164569213986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 239.03717202693224,
    "estimated_duration": 3600.0039018605057,
    "input_throughput": 6651.016680183526,
    "output_throughput": 5787.90567122207,
    "total_throughput": 12438.922351405596,
    "itl": 66.47000315014343,
    "ttft": 1151917.393642908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.223030619025227,
    "arrivals": 127143,
    "finished_requests": 96704,
    "scheduler_time": 199.94200888106383
}
#Debug simulation 
Total elapsed time: 239.03730976721272. Arrivals time: 0.6607647552154958 Scheduler time: 237.94285691156983 Scheduler overhead time: 0.17441646801307797 Adapter cache time: 0.03235527500510216 Engine time: 0.17277125315740705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 237.3688479680568,
    "estimated_duration": 3600.053668793208,
    "input_throughput": 6629.848106681378,
    "output_throughput": 5769.318157682456,
    "total_throughput": 12399.166264363836,
    "itl": 65.65913175775432,
    "ttft": 1154250.8402692296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5392004769854397,
    "arrivals": 127143,
    "finished_requests": 96391,
    "scheduler_time": 200.7053675867724
}
#Debug simulation 
Total elapsed time: 237.36898687807843. Arrivals time: 0.6740528247319162 Scheduler time: 236.25470640789717 Scheduler overhead time: 0.17863159673288465 Adapter cache time: 0.03286903304979205 Engine time: 0.17278875783085823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 242.19996898202226,
    "estimated_duration": 3600.0884250770337,
    "input_throughput": 6651.827447679312,
    "output_throughput": 5788.5122639880965,
    "total_throughput": 12440.339711667408,
    "itl": 66.46999218371344,
    "ttft": 1151842.3001898297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.093926068507127,
    "arrivals": 127143,
    "finished_requests": 96716,
    "scheduler_time": 199.95415934867887
}
#Debug simulation 
Total elapsed time: 242.2001147530973. Arrivals time: 0.666721414308995 Scheduler time: 241.09060187451541 Scheduler overhead time: 0.17844944074749947 Adapter cache time: 0.03410933259874582 Engine time: 0.17504863534122705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 240.38450780091807,
    "estimated_duration": 3600.011900977189,
    "input_throughput": 6629.658361274187,
    "output_throughput": 5768.9537066148705,
    "total_throughput": 12398.612067889058,
    "itl": 65.64889636721104,
    "ttft": 1154322.2444382713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.524856026303038,
    "arrivals": 127143,
    "finished_requests": 96385,
    "scheduler_time": 200.71233927717486
}
#Debug simulation 
Total elapsed time: 240.3846501922235. Arrivals time: 0.6655045985244215 Scheduler time: 239.2757740537636 Scheduler overhead time: 0.1783022526651621 Adapter cache time: 0.0337663353420794 Engine time: 0.17601107573136687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.05435100896284,
    "estimated_duration": 3599.9063216116638,
    "input_throughput": 6345.897631517408,
    "output_throughput": 5469.053425586287,
    "total_throughput": 11814.951057103695,
    "itl": 70.82701006539476,
    "ttft": 616192.7721116396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.501016850252634,
    "arrivals": 101151,
    "finished_requests": 91571,
    "scheduler_time": 144.31375616918572
}
#Debug simulation 
Total elapsed time: 81.05452880822122. Arrivals time: 0.49363168701529503 Scheduler time: 80.2424929603003 Scheduler overhead time: 0.12638731952756643 Adapter cache time: 0.023804964032024145 Engine time: 0.12128322292119265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 71.04989316686988,
    "estimated_duration": 3600.0065847434757,
    "input_throughput": 6410.854940601321,
    "output_throughput": 5509.3249229190715,
    "total_throughput": 11920.179863520392,
    "itl": 71.14335410240224,
    "ttft": 566513.5664798977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7967245700303485,
    "arrivals": 101151,
    "finished_requests": 92572,
    "scheduler_time": 139.75875265279635
}
#Debug simulation 
Total elapsed time: 71.05004601320252. Arrivals time: 0.4495364907197654 Scheduler time: 70.2966090478003 Scheduler overhead time: 0.12117607705295086 Adapter cache time: 0.021755079738795757 Engine time: 0.11687041679397225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 120.70681297313422,
    "estimated_duration": 3599.9475717516166,
    "input_throughput": 6009.54418607646,
    "output_throughput": 5157.247051508164,
    "total_throughput": 11166.791237584624,
    "itl": 66.81295831750198,
    "ttft": 843510.9504762439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.289973849384116,
    "arrivals": 101151,
    "finished_requests": 86699,
    "scheduler_time": 165.99363989723932
}
#Debug simulation 
Total elapsed time: 120.70696410816163. Arrivals time: 0.550792483612895 Scheduler time: 119.79273584578186 Scheduler overhead time: 0.14738653879612684 Adapter cache time: 0.026821632869541645 Engine time: 0.14017508877441287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 93.8654317567125,
    "estimated_duration": 3599.940349641631,
    "input_throughput": 6277.172898781377,
    "output_throughput": 5391.476000965443,
    "total_throughput": 11668.648899746819,
    "itl": 70.21641851769026,
    "ttft": 688866.401579483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.442794565618968,
    "arrivals": 101151,
    "finished_requests": 90598,
    "scheduler_time": 152.239210176864
}
#Debug simulation 
Total elapsed time: 93.8655709316954. Arrivals time: 0.521151939406991 Scheduler time: 93.00889765331522 Scheduler overhead time: 0.13408100744709373 Adapter cache time: 0.024472512304782867 Engine time: 0.12968170968815684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 120.82739946292713,
    "estimated_duration": 3599.9961239634995,
    "input_throughput": 6008.85119181293,
    "output_throughput": 5156.3630517365,
    "total_throughput": 11165.21424354943,
    "itl": 66.80002169788662,
    "ttft": 843850.7792026008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.278168170317078,
    "arrivals": 101151,
    "finished_requests": 86688,
    "scheduler_time": 166.00586757630978
}
#Debug simulation 
Total elapsed time: 120.82769110100344. Arrivals time: 0.5421380852349102 Scheduler time: 119.91484816558659 Scheduler overhead time: 0.14937659446150064 Adapter cache time: 0.026805165223777294 Engine time: 0.14351094933226705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 78.13533751294017,
    "estimated_duration": 3599.9403542924906,
    "input_throughput": 6372.313633654208,
    "output_throughput": 5488.835662635139,
    "total_throughput": 11861.149296289348,
    "itl": 70.625388572709,
    "ttft": 595849.5078043101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.481069658212359,
    "arrivals": 101151,
    "finished_requests": 92051,
    "scheduler_time": 143.14586791584054
}
#Debug simulation 
Total elapsed time: 78.13549390388653. Arrivals time: 0.5107001974247396 Scheduler time: 77.31473694881424 Scheduler overhead time: 0.1240096278488636 Adapter cache time: 0.0230198185890913 Engine time: 0.11928918119519949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 143.31439351197332,
    "estimated_duration": 3600.0124182816703,
    "input_throughput": 5796.320838793104,
    "output_throughput": 4973.239789140968,
    "total_throughput": 10769.560627934072,
    "itl": 63.6113364933597,
    "ttft": 909356.9328999517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1843090407364083,
    "arrivals": 101151,
    "finished_requests": 83699,
    "scheduler_time": 167.64781048035647
}
#Debug simulation 
Total elapsed time: 143.31454852595925. Arrivals time: 0.5796147882938385 Scheduler time: 142.33901170780882 Scheduler overhead time: 0.16016416996717453 Adapter cache time: 0.02910660346969962 Engine time: 0.15367808239534497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 71.01937119290233,
    "estimated_duration": 3599.998876884626,
    "input_throughput": 6199.024156172037,
    "output_throughput": 5339.439721335233,
    "total_throughput": 11538.46387750727,
    "itl": 69.29859611999701,
    "ttft": 550245.35951695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6729394850833321,
    "arrivals": 97469,
    "finished_requests": 89248,
    "scheduler_time": 134.84292074401029
}
#Debug simulation 
Total elapsed time: 71.01956894993782. Arrivals time: 0.476649955380708 Scheduler time: 70.2368300370872 Scheduler overhead time: 0.12267323536798358 Adapter cache time: 0.02226808527484536 Engine time: 0.1163325347006321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 96.55745658325031,
    "estimated_duration": 3600.0238718993437,
    "input_throughput": 6014.223174741595,
    "output_throughput": 5166.5782399900845,
    "total_throughput": 11180.801414731679,
    "itl": 67.99361544107951,
    "ttft": 690798.3591550217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4678014985052885,
    "arrivals": 97469,
    "finished_requests": 86656,
    "scheduler_time": 147.97251059694958
}
#Debug simulation 
Total elapsed time: 96.55772102531046. Arrivals time: 0.5174124110490084 Scheduler time: 95.69509363919497 Scheduler overhead time: 0.13724684668704867 Adapter cache time: 0.02515236521139741 Engine time: 0.13323625223711133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.71845075720921,
    "estimated_duration": 3600.005784902121,
    "input_throughput": 6135.114308045619,
    "output_throughput": 5268.904311084521,
    "total_throughput": 11404.01861913014,
    "itl": 68.79738253391342,
    "ttft": 618762.9165014124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6710950763383914,
    "arrivals": 97469,
    "finished_requests": 88334,
    "scheduler_time": 144.05217385734662
}
#Debug simulation 
Total elapsed time: 81.71860518213362. Arrivals time: 0.49444815097376704 Scheduler time: 80.90386373689398 Scheduler overhead time: 0.12827283749356866 Adapter cache time: 0.022624389734119177 Engine time: 0.12345101591199636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 93.90460851090029,
    "estimated_duration": 3600.0350470373046,
    "input_throughput": 6039.2303730188405,
    "output_throughput": 5186.226732810618,
    "total_throughput": 11225.457105829459,
    "itl": 67.8546132595146,
    "ttft": 674133.3090753155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4355796076310785,
    "arrivals": 97469,
    "finished_requests": 87017,
    "scheduler_time": 146.7010813496688
}
#Debug simulation 
Total elapsed time: 93.9047648217529. Arrivals time: 0.5051911030896008 Scheduler time: 93.05894825141877 Scheduler overhead time: 0.13647129386663437 Adapter cache time: 0.02458698535338044 Engine time: 0.13118951208889484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 84.27911883592606,
    "estimated_duration": 3600.021373557472,
    "input_throughput": 6085.695535287978,
    "output_throughput": 5231.120053431915,
    "total_throughput": 11316.815588719892,
    "itl": 68.66647375592657,
    "ttft": 642624.3287602286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.638528803656812,
    "arrivals": 97469,
    "finished_requests": 87632,
    "scheduler_time": 144.73197021424116
}
#Debug simulation 
Total elapsed time: 84.27926533296704. Arrivals time: 0.4900088096037507 Scheduler time: 83.46674934960902 Scheduler overhead time: 0.12892430555075407 Adapter cache time: 0.023175830487161875 Engine time: 0.12399450596421957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 98.04385371087119,
    "estimated_duration": 3600.031750164023,
    "input_throughput": 5933.6765568877945,
    "output_throughput": 5092.341477034123,
    "total_throughput": 11026.018033921917,
    "itl": 66.89611470053579,
    "ttft": 736340.7516326534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.289552029995244,
    "arrivals": 97469,
    "finished_requests": 85497,
    "scheduler_time": 149.37459520004225
}
#Debug simulation 
Total elapsed time: 98.04399296967313. Arrivals time: 0.5055319033563137 Scheduler time: 97.18736049858853 Scheduler overhead time: 0.13988652965053916 Adapter cache time: 0.02596881240606308 Engine time: 0.13654111279174685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 87.35212941095233,
    "estimated_duration": 3600.054089818248,
    "input_throughput": 6070.452402870234,
    "output_throughput": 5216.81856200893,
    "total_throughput": 11287.270964879164,
    "itl": 66.97337896183872,
    "ttft": 656502.5273220565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6297765167430074,
    "arrivals": 97469,
    "finished_requests": 87409,
    "scheduler_time": 146.04929701367206
}
#Debug simulation 
Total elapsed time: 87.35227676387876. Arrivals time: 0.4975972636602819 Scheduler time: 86.52024241769686 Scheduler overhead time: 0.1330150868743658 Adapter cache time: 0.02498039836063981 Engine time: 0.12903108401224017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 88.96091157896444,
    "estimated_duration": 3600.030381675243,
    "input_throughput": 6007.0990261856205,
    "output_throughput": 5189.283150245718,
    "total_throughput": 11196.382176431338,
    "itl": 70.59352253387107,
    "ttft": 630506.2195934296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.322481806389986,
    "arrivals": 95579,
    "finished_requests": 86690,
    "scheduler_time": 146.59916650850352
}
#Debug simulation 
Total elapsed time: 88.961052659899. Arrivals time: 0.49024155177176 Scheduler time: 88.13669882528484 Scheduler overhead time: 0.13550443341955543 Adapter cache time: 0.02355952374637127 Engine time: 0.12821013992652297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 96.19768186006695,
    "estimated_duration": 3600.0043367415246,
    "input_throughput": 5896.3745635965515,
    "output_throughput": 5090.2341458250585,
    "total_throughput": 10986.60870942161,
    "itl": 66.92477687598586,
    "ttft": 666442.1425413999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.380088679431939,
    "arrivals": 95579,
    "finished_requests": 85048,
    "scheduler_time": 145.2041799954709
}
#Debug simulation 
Total elapsed time: 96.19783273898065. Arrivals time: 0.5033333441242576 Scheduler time: 95.33865558728576 Scheduler overhead time: 0.14129949128255248 Adapter cache time: 0.02696982678025961 Engine time: 0.13748164800927043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 72.41497373580933,
    "estimated_duration": 3600.074050528509,
    "input_throughput": 6083.132372453564,
    "output_throughput": 5240.619424822744,
    "total_throughput": 11323.751797276307,
    "itl": 68.71364084855348,
    "ttft": 558621.1759206865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9188086273288403,
    "arrivals": 95579,
    "finished_requests": 87808,
    "scheduler_time": 137.32811015272188
}
#Debug simulation 
Total elapsed time: 72.41511688474566. Arrivals time: 0.4669698509387672 Scheduler time: 71.64226057054475 Scheduler overhead time: 0.12151063559576869 Adapter cache time: 0.022157863713800907 Engine time: 0.11675155907869339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 69.10075977258384,
    "estimated_duration": 3600.0120011953977,
    "input_throughput": 6160.604740382984,
    "output_throughput": 5299.535666454708,
    "total_throughput": 11460.140406837692,
    "itl": 71.62191442443542,
    "ttft": 536600.7869085106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6934390340792003,
    "arrivals": 95579,
    "finished_requests": 88911,
    "scheduler_time": 139.7996374063636
}
#Debug simulation 
Total elapsed time: 69.10088990582153. Arrivals time: 0.3896270482800901 Scheduler time: 68.41700378246605 Scheduler overhead time: 0.11839802144095302 Adapter cache time: 0.02046911884099245 Engine time: 0.11031615035608411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 85.81152158789337,
    "estimated_duration": 3600.0242181619224,
    "input_throughput": 5949.665252789868,
    "output_throughput": 5133.926573815261,
    "total_throughput": 11083.591826605129,
    "itl": 67.12573611898013,
    "ttft": 629635.0105093757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.545501494561325,
    "arrivals": 95579,
    "finished_requests": 85862,
    "scheduler_time": 142.30568130522678
}
#Debug simulation 
Total elapsed time: 85.81166260689497. Arrivals time: 0.5009479797445238 Scheduler time: 84.97214420419186 Scheduler overhead time: 0.13543279096484184 Adapter cache time: 0.02351857302710414 Engine time: 0.13141445396468043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 71.20585070597008,
    "estimated_duration": 3600.0545228835717,
    "input_throughput": 6222.947418600673,
    "output_throughput": 5365.934564937351,
    "total_throughput": 11588.881983538024,
    "itl": 71.46276671865913,
    "ttft": 493269.8094297632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6534355236077625,
    "arrivals": 95579,
    "finished_requests": 89843,
    "scheduler_time": 138.103483659029
}
#Debug simulation 
Total elapsed time: 71.20599908102304. Arrivals time: 0.4593343702144921 Scheduler time: 70.43951213127002 Scheduler overhead time: 0.1223926767706871 Adapter cache time: 0.02187845902517438 Engine time: 0.11800535069778562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 109.01438369508833,
    "estimated_duration": 3600.077133100702,
    "input_throughput": 5719.5385650710805,
    "output_throughput": 4913.1772309460775,
    "total_throughput": 10632.715796017157,
    "itl": 66.16149831118368,
    "ttft": 812353.8544451285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.416233169101181,
    "arrivals": 95579,
    "finished_requests": 82487,
    "scheduler_time": 157.36896454523296
}
#Debug simulation 
Total elapsed time: 109.01458264095709. Arrivals time: 0.5194342723116279 Scheduler time: 108.12766221398488 Scheduler overhead time: 0.14887495012953877 Adapter cache time: 0.0264171683229506 Engine time: 0.1415886888280511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 60.29836197802797,
    "estimated_duration": 3600.0026646542005,
    "input_throughput": 6052.542186685566,
    "output_throughput": 5249.952225192431,
    "total_throughput": 11302.494411877997,
    "itl": 70.17739839142533,
    "ttft": 491333.1218501999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8118000747542806,
    "arrivals": 94646,
    "finished_requests": 87837,
    "scheduler_time": 128.08840653444864
}
#Debug simulation 
Total elapsed time: 60.29852972691879. Arrivals time: 0.44279512809589505 Scheduler time: 59.56680306792259 Scheduler overhead time: 0.11454336903989315 Adapter cache time: 0.020913225132972002 Engine time: 0.10938008734956384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 71.1087573999539,
    "estimated_duration": 3600.0397075012033,
    "input_throughput": 5935.225924169298,
    "output_throughput": 5158.159217329631,
    "total_throughput": 11093.38514149893,
    "itl": 68.86555060839795,
    "ttft": 588433.3613098067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7492606814997291,
    "arrivals": 94646,
    "finished_requests": 86259,
    "scheduler_time": 136.3818881695195
}
#Debug simulation 
Total elapsed time: 71.10888861631975. Arrivals time: 0.4488809434697032 Scheduler time: 70.34294343413785 Scheduler overhead time: 0.12624057102948427 Adapter cache time: 0.02215980878099799 Engine time: 0.1227152026258409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 81.25971346907318,
    "estimated_duration": 3600.015234508755,
    "input_throughput": 5984.527174630101,
    "output_throughput": 5211.520168069548,
    "total_throughput": 11196.047342699649,
    "itl": 70.0793077980354,
    "ttft": 600993.3849191995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6710950763383918,
    "arrivals": 94646,
    "finished_requests": 86922,
    "scheduler_time": 146.33898091811943
}
#Debug simulation 
Total elapsed time: 81.25988639798015. Arrivals time: 0.4732587542384863 Scheduler time: 80.46406558481976 Scheduler overhead time: 0.13035962963476777 Adapter cache time: 0.02295660274103284 Engine time: 0.12154308008030057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 63.973797677084804,
    "estimated_duration": 3600.092208523477,
    "input_throughput": 5932.195555835227,
    "output_throughput": 5155.332398447641,
    "total_throughput": 11087.527954282868,
    "itl": 68.74178894419468,
    "ttft": 557058.8438816575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7897550321137514,
    "arrivals": 94646,
    "finished_requests": 86213,
    "scheduler_time": 130.200562285411
}
#Debug simulation 
Total elapsed time: 63.973941603675485. Arrivals time: 0.4403147338889539 Scheduler time: 63.23579929443076 Scheduler overhead time: 0.11798073723912239 Adapter cache time: 0.02119932323694229 Engine time: 0.11421255907043815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 66.34126556944102,
    "estimated_duration": 3600.018424580402,
    "input_throughput": 5990.082676455587,
    "output_throughput": 5225.764365967854,
    "total_throughput": 11215.847042423442,
    "itl": 68.85507402368286,
    "ttft": 537939.9561356907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.866624795407994,
    "arrivals": 94646,
    "finished_requests": 86962,
    "scheduler_time": 132.26330041810994
}
#Debug simulation 
Total elapsed time: 66.34143073204905. Arrivals time: 0.4455569675192237 Scheduler time: 65.59231668477878 Scheduler overhead time: 0.12091817986220121 Adapter cache time: 0.021917349193245173 Engine time: 0.11556027038022876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 71.64982418902218,
    "estimated_duration": 3600.0425784465656,
    "input_throughput": 5934.165370126911,
    "output_throughput": 5162.22477791337,
    "total_throughput": 11096.39014804028,
    "itl": 68.63796906097657,
    "ttft": 574886.0222060406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5257571047963525,
    "arrivals": 94646,
    "finished_requests": 86174,
    "scheduler_time": 134.15518729438165
}
#Debug simulation 
Total elapsed time: 71.64996269857511. Arrivals time: 0.45552340615540743 Scheduler time: 70.87731189606711 Scheduler overhead time: 0.12400137446820736 Adapter cache time: 0.023544055875390768 Engine time: 0.12254543555900455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 78.73907495196909,
    "estimated_duration": 3600.011405210466,
    "input_throughput": 5797.474410717827,
    "output_throughput": 5033.074054647533,
    "total_throughput": 10830.548465365358,
    "itl": 65.68245474158532,
    "ttft": 637604.6929075365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6972332339733882,
    "arrivals": 94646,
    "finished_requests": 84096,
    "scheduler_time": 136.61658585101733
}
#Debug simulation 
Total elapsed time: 78.73932401789352. Arrivals time: 0.4783556116744876 Scheduler time: 77.93672441225499 Scheduler overhead time: 0.128836365416646 Adapter cache time: 0.023163278587162495 Engine time: 0.1254741526208818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 81.94380465475842,
    "estimated_duration": 3600.013372512963,
    "input_throughput": 5974.80780605699,
    "output_throughput": 5249.014390968615,
    "total_throughput": 11223.822197025604,
    "itl": 71.82084089786267,
    "ttft": 567330.0049502879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4348927599331347,
    "arrivals": 94180,
    "finished_requests": 87013,
    "scheduler_time": 142.48521426950086
}
#Debug simulation 
Total elapsed time: 81.94394785072654. Arrivals time: 0.47310870327055454 Scheduler time: 81.14473332604393 Scheduler overhead time: 0.1310330224223435 Adapter cache time: 0.022619099356234074 Engine time: 0.1261361539363861 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 90.64573641680181,
    "estimated_duration": 3600.089954310469,
    "input_throughput": 5724.734176523481,
    "output_throughput": 5010.41313659476,
    "total_throughput": 10735.147313118241,
    "itl": 69.5064903545462,
    "ttft": 718013.19266801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4747426033718527,
    "arrivals": 94180,
    "finished_requests": 83402,
    "scheduler_time": 148.07274413024854
}
#Debug simulation 
Total elapsed time: 90.64587674010545. Arrivals time: 0.48149470472708344 Scheduler time: 89.81647769501433 Scheduler overhead time: 0.13874391373246908 Adapter cache time: 0.02567160129547119 Engine time: 0.13539376389235258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 71.30001502577215,
    "estimated_duration": 3600.00078770726,
    "input_throughput": 5949.214531600145,
    "output_throughput": 5214.531914576487,
    "total_throughput": 11163.746446176632,
    "itl": 70.48720220681699,
    "ttft": 565679.3541435705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.882313831849028,
    "arrivals": 94180,
    "finished_requests": 86670,
    "scheduler_time": 138.94945012300903
}
#Debug simulation 
Total elapsed time: 71.30016934406012. Arrivals time: 0.4515593475662172 Scheduler time: 70.53778297547251 Scheduler overhead time: 0.12365216156467795 Adapter cache time: 0.0221310262568295 Engine time: 0.11937129776924849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 96.07936020474881,
    "estimated_duration": 3600.0218147317637,
    "input_throughput": 5842.042377059053,
    "output_throughput": 5132.4994544169795,
    "total_throughput": 10974.541831476032,
    "itl": 70.68964274306856,
    "ttft": 657821.9312896968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2720907743182024,
    "arrivals": 94180,
    "finished_requests": 85094,
    "scheduler_time": 149.58365034712725
}
#Debug simulation 
Total elapsed time: 96.07950930483639. Arrivals time: 0.4869643594138324 Scheduler time: 95.24989872146398 Scheduler overhead time: 0.13776465691626072 Adapter cache time: 0.024755734018981457 Engine time: 0.13180119264870882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 84.63397533306852,
    "estimated_duration": 3600.026379529047,
    "input_throughput": 5770.620770483823,
    "output_throughput": 5070.2859022904895,
    "total_throughput": 10840.906672774314,
    "itl": 68.1640409227469,
    "ttft": 650273.5722240253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5737686790013719,
    "arrivals": 94180,
    "finished_requests": 84072,
    "scheduler_time": 142.46080792818717
}
#Debug simulation 
Total elapsed time: 84.63411200093105. Arrivals time: 0.4777960670180619 Scheduler time: 83.8117761677131 Scheduler overhead time: 0.13372002355754375 Adapter cache time: 0.035649923142045736 Engine time: 0.1272397441789508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 89.98057856410742,
    "estimated_duration": 3600.0281430704417,
    "input_throughput": 5889.394237323088,
    "output_throughput": 5172.58484099457,
    "total_throughput": 11061.97907831766,
    "itl": 70.19504905251716,
    "ttft": 618583.0012340707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1874092949461161,
    "arrivals": 94180,
    "finished_requests": 85729,
    "scheduler_time": 145.39394717270545
}
#Debug simulation 
Total elapsed time: 89.98082380509004. Arrivals time: 0.48087774170562625 Scheduler time: 89.1652299631387 Scheduler overhead time: 0.13293884322047234 Adapter cache time: 0.024008563719689846 Engine time: 0.13017684873193502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 76.26568905962631,
    "estimated_duration": 3600.071154337959,
    "input_throughput": 5919.617998194547,
    "output_throughput": 5186.9083136035815,
    "total_throughput": 11106.526311798129,
    "itl": 70.06558442156302,
    "ttft": 592107.351117254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6923676562123042,
    "arrivals": 94180,
    "finished_requests": 86307,
    "scheduler_time": 140.805056487782
}
#Debug simulation 
Total elapsed time: 76.265874912031. Arrivals time: 0.46240364853292704 Scheduler time: 75.48784859990701 Scheduler overhead time: 0.12726148590445518 Adapter cache time: 0.02184304501861334 Engine time: 0.12091664550825953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 72.09089859575033,
    "estimated_duration": 3599.9970826356857,
    "input_throughput": 5708.8546263357675,
    "output_throughput": 5048.930202658,
    "total_throughput": 10757.784828993766,
    "itl": 66.34243708302373,
    "ttft": 601930.602021131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.408443123805335,
    "arrivals": 93946,
    "finished_requests": 83397,
    "scheduler_time": 128.09524850117424
}
#Debug simulation 
Total elapsed time: 72.09104460477829. Arrivals time: 0.45861369790509343 Scheduler time: 71.31988262059167 Scheduler overhead time: 0.1251601567491889 Adapter cache time: 0.02189782913774252 Engine time: 0.1188020664267242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 56.06642942689359,
    "estimated_duration": 3599.9933823885635,
    "input_throughput": 5915.828652404263,
    "output_throughput": 5236.674348410016,
    "total_throughput": 11152.503000814278,
    "itl": 68.22961991308203,
    "ttft": 488889.02677043393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8319778006058207,
    "arrivals": 93946,
    "finished_requests": 86544,
    "scheduler_time": 122.39628592780736
}
#Debug simulation 
Total elapsed time: 56.06658091722056. Arrivals time: 0.42919182078912854 Scheduler time: 55.354407356120646 Scheduler overhead time: 0.10934109427034855 Adapter cache time: 0.020725447218865156 Engine time: 0.1091307788155973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 61.043050806969404,
    "estimated_duration": 3600.0819606213145,
    "input_throughput": 5932.09577826231,
    "output_throughput": 5229.651215149208,
    "total_throughput": 11161.746993411518,
    "itl": 66.78470089296177,
    "ttft": 509434.3971475086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9481632666336426,
    "arrivals": 93946,
    "finished_requests": 86714,
    "scheduler_time": 127.14637215203302
}
#Debug simulation 
Total elapsed time: 61.0431812540628. Arrivals time: 0.4381698281504214 Scheduler time: 60.31176378764212 Scheduler overhead time: 0.11452609486877918 Adapter cache time: 0.020510585512965918 Engine time: 0.11357616120949388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 77.92114849574864,
    "estimated_duration": 3600.0450721925727,
    "input_throughput": 5729.457433553116,
    "output_throughput": 5035.1877925128265,
    "total_throughput": 10764.645226065943,
    "itl": 66.64635381689618,
    "ttft": 625619.0682340137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4661109913606187,
    "arrivals": 93946,
    "finished_requests": 83640,
    "scheduler_time": 132.77752728953772
}
#Debug simulation 
Total elapsed time: 77.92128608189523. Arrivals time: 0.45532712060958147 Scheduler time: 77.13853861484677 Scheduler overhead time: 0.13053133618086576 Adapter cache time: 0.02344080340117216 Engine time: 0.12631422746926546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 72.60111614875495,
    "estimated_duration": 3600.0330570707933,
    "input_throughput": 5605.066309146171,
    "output_throughput": 4937.868824589083,
    "total_throughput": 10542.935133735255,
    "itl": 63.84834346692608,
    "ttft": 648915.1896513082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7114724501781209,
    "arrivals": 93946,
    "finished_requests": 81903,
    "scheduler_time": 128.9002092582967
}
#Debug simulation 
Total elapsed time: 72.60126698808745. Arrivals time: 0.44686974259093404 Scheduler time: 71.83610997255892 Scheduler overhead time: 0.12726927362382412 Adapter cache time: 0.021834254264831543 Engine time: 0.12226623995229602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.65347175532952,
    "estimated_duration": 3600.096648403854,
    "input_throughput": 5728.961751386386,
    "output_throughput": 5035.158988866826,
    "total_throughput": 10764.12074025321,
    "itl": 66.64184002488378,
    "ttft": 625535.6504045783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3597751603415196,
    "arrivals": 93946,
    "finished_requests": 83639,
    "scheduler_time": 132.78361889269217
}
#Debug simulation 
Total elapsed time: 77.6536279222928. Arrivals time: 0.45898826885968447 Scheduler time: 76.86801746441051 Scheduler overhead time: 0.13050959585234523 Adapter cache time: 0.023431124165654182 Engine time: 0.12582774925976992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 73.0648441342637,
    "estimated_duration": 3600.0377587876583,
    "input_throughput": 5605.387596488889,
    "output_throughput": 4938.0029297210895,
    "total_throughput": 10543.390526209978,
    "itl": 63.849672511319596,
    "ttft": 648818.2777330228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6957315447554036,
    "arrivals": 93946,
    "finished_requests": 81906,
    "scheduler_time": 128.90018553600052
}
#Debug simulation 
Total elapsed time: 73.06497701397166. Arrivals time: 0.4483688506297767 Scheduler time: 72.29424632387236 Scheduler overhead time: 0.1272075898014009 Adapter cache time: 0.02324106078594923 Engine time: 0.12453014263883233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 45.95245731621981,
    "estimated_duration": 3600.016173744359,
    "input_throughput": 4886.375269170987,
    "output_throughput": 4248.94174408401,
    "total_throughput": 9135.317013254997,
    "itl": 51.89115989314143,
    "ttft": 363945.4806678795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.315869397358036,
    "arrivals": 74944,
    "finished_requests": 70769,
    "scheduler_time": 93.51708861853427
}
#Debug simulation 
Total elapsed time: 45.95261102821678. Arrivals time: 0.332418744917959 Scheduler time: 45.32498207362369 Scheduler overhead time: 0.11686441255733371 Adapter cache time: 0.019519755616784096 Engine time: 0.10972472280263901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 46.15708912629634,
    "estimated_duration": 3600.0367531966617,
    "input_throughput": 4886.191504678528,
    "output_throughput": 4248.850511433769,
    "total_throughput": 9135.042016112297,
    "itl": 51.817228906485774,
    "ttft": 364179.2576322624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4605865405173986,
    "arrivals": 74944,
    "finished_requests": 70767,
    "scheduler_time": 93.53949708323397
}
#Debug simulation 
Total elapsed time: 46.157218345906585. Arrivals time: 0.3294825730845332 Scheduler time: 45.53320822864771 Scheduler overhead time: 0.1162021798081696 Adapter cache time: 0.019801244605332613 Engine time: 0.10927342856302857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 34.560424311086535,
    "estimated_duration": 3600.00887767543,
    "input_throughput": 4893.739320825186,
    "output_throughput": 4263.931985054383,
    "total_throughput": 9157.67130587957,
    "itl": 52.997164006602155,
    "ttft": 323215.94697914773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2264723047241652,
    "arrivals": 74944,
    "finished_requests": 70938,
    "scheduler_time": 84.90909672445187
}
#Debug simulation 
Total elapsed time: 34.5605701059103. Arrivals time: 0.2958252406679094 Scheduler time: 33.982937570195645 Scheduler overhead time: 0.11126729473471642 Adapter cache time: 0.019189366605132818 Engine time: 0.10315650561824441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 34.76385749783367,
    "estimated_duration": 3600.01020268531,
    "input_throughput": 4899.887224442387,
    "output_throughput": 4267.41262803663,
    "total_throughput": 9167.299852479016,
    "itl": 52.92917685807483,
    "ttft": 317631.2262554444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1361583146825405,
    "arrivals": 74944,
    "finished_requests": 71069,
    "scheduler_time": 84.8985582863977
}
#Debug simulation 
Total elapsed time: 34.76396844908595. Arrivals time: 0.28785665007308125 Scheduler time: 34.192066804505885 Scheduler overhead time: 0.11193962208926678 Adapter cache time: 0.019684242084622383 Engine time: 0.10447525884956121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 33.246740197297186,
    "estimated_duration": 3600.0259707513796,
    "input_throughput": 4955.062587028189,
    "output_throughput": 4309.917518945458,
    "total_throughput": 9264.980105973647,
    "itl": 53.52885421552041,
    "ttft": 282209.8657513975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.234233033414939,
    "arrivals": 74944,
    "finished_requests": 71817,
    "scheduler_time": 84.34790165553191
}
#Debug simulation 
Total elapsed time: 33.246979475952685. Arrivals time: 0.28715214785188437 Scheduler time: 32.67831805348396 Scheduler overhead time: 0.11072170920670033 Adapter cache time: 0.019074555952101946 Engine time: 0.1036536660976708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 45.248112436849624,
    "estimated_duration": 3600.0180213421218,
    "input_throughput": 4900.173247861512,
    "output_throughput": 4266.44392026513,
    "total_throughput": 9166.617168126642,
    "itl": 53.544753112881814,
    "ttft": 355529.7340857845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.557676709499205,
    "arrivals": 74944,
    "finished_requests": 71050,
    "scheduler_time": 94.25081069672538
}
#Debug simulation 
Total elapsed time: 45.248250079806894. Arrivals time: 0.3263472397811711 Scheduler time: 44.62735610175878 Scheduler overhead time: 0.1163533073849976 Adapter cache time: 0.0196910104714334 Engine time: 0.10966965043917298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 41.032623117789626,
    "estimated_duration": 3600.0444919483093,
    "input_throughput": 4887.595150380337,
    "output_throughput": 4244.35837784068,
    "total_throughput": 9131.953528221016,
    "itl": 53.35252210301593,
    "ttft": 363815.6799749635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.927655673120176,
    "arrivals": 74944,
    "finished_requests": 70872,
    "scheduler_time": 93.37128355636875
}
#Debug simulation 
Total elapsed time: 41.032761903014034. Arrivals time: 0.3141426919028163 Scheduler time: 40.426597295794636 Scheduler overhead time: 0.11544492980465293 Adapter cache time: 0.019708518404513597 Engine time: 0.10824467474594712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 36.44523251801729,
    "estimated_duration": 3600.046914077529,
    "input_throughput": 4776.242479719447,
    "output_throughput": 4198.147513272801,
    "total_throughput": 8974.389992992248,
    "itl": 52.93334927065134,
    "ttft": 307616.27056435787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9242110282974294,
    "arrivals": 73090,
    "finished_requests": 69818,
    "scheduler_time": 85.07633704844332
}
#Debug simulation 
Total elapsed time: 36.445377070922405. Arrivals time: 0.2889470304362476 Scheduler time: 35.86821616999805 Scheduler overhead time: 0.11370305856689811 Adapter cache time: 0.019719503819942474 Engine time: 0.10622189566493034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 40.53136585606262,
    "estimated_duration": 3600.0739028022945,
    "input_throughput": 4726.429917662288,
    "output_throughput": 4158.4326889364265,
    "total_throughput": 8884.862606598714,
    "itl": 51.39600941991299,
    "ttft": 343705.4060583238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.669320004340263,
    "arrivals": 73090,
    "finished_requests": 69072,
    "scheduler_time": 87.09584112032009
}
#Debug simulation 
Total elapsed time: 40.53146968409419. Arrivals time: 0.3034436730667949 Scheduler time: 39.93628583336249 Scheduler overhead time: 0.11570164002478123 Adapter cache time: 0.0196590437553823 Engine time: 0.10699958447366953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 40.45765361888334,
    "estimated_duration": 3600.066905718061,
    "input_throughput": 4726.262162787848,
    "output_throughput": 4158.308551494567,
    "total_throughput": 8884.570714282416,
    "itl": 51.22125422944503,
    "ttft": 343852.8375260866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7068169826222621,
    "arrivals": 73090,
    "finished_requests": 69070,
    "scheduler_time": 87.10140967352405
}
#Debug simulation 
Total elapsed time: 40.45778152998537. Arrivals time: 0.30697389179840684 Scheduler time: 39.858873791992664 Scheduler overhead time: 0.11524612503126264 Adapter cache time: 0.019806881435215473 Engine time: 0.10770366108044982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 41.707675016950816,
    "estimated_duration": 3600.03660364042,
    "input_throughput": 4819.272665854515,
    "output_throughput": 4239.025510065138,
    "total_throughput": 9058.298175919654,
    "itl": 53.36279130608401,
    "ttft": 295481.90676003805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8003036159742607,
    "arrivals": 73090,
    "finished_requests": 70466,
    "scheduler_time": 90.35232007649064
}
#Debug simulation 
Total elapsed time: 41.707802690565586. Arrivals time: 0.31182696018368006 Scheduler time: 41.10501992003992 Scheduler overhead time: 0.1152005665935576 Adapter cache time: 0.019381079357117414 Engine time: 0.10750155709683895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 41.00562942912802,
    "estimated_duration": 3600.0139908956544,
    "input_throughput": 4726.1880212212645,
    "output_throughput": 4158.335783654988,
    "total_throughput": 8884.523804876251,
    "itl": 51.219340351868,
    "ttft": 343957.5381384051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6906618428463154,
    "arrivals": 73090,
    "finished_requests": 69067,
    "scheduler_time": 87.10035136321311
}
#Debug simulation 
Total elapsed time: 41.005746256094426. Arrivals time: 0.30805726209655404 Scheduler time: 40.40258758934215 Scheduler overhead time: 0.11622024374082685 Adapter cache time: 0.019847112242132425 Engine time: 0.10940297646448016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 36.022271792870015,
    "estimated_duration": 3600.020916523042,
    "input_throughput": 4814.197862144302,
    "output_throughput": 4251.7175191257065,
    "total_throughput": 9065.915381270008,
    "itl": 51.918612373826264,
    "ttft": 276525.0866107306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7683461005380314,
    "arrivals": 73090,
    "finished_requests": 70326,
    "scheduler_time": 84.71070459952072
}
#Debug simulation 
Total elapsed time: 36.022404099814594. Arrivals time: 0.2968593118712306 Scheduler time: 35.438777214381844 Scheduler overhead time: 0.11342900898307562 Adapter cache time: 0.019355827942490578 Engine time: 0.1052086977288127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 40.67376118293032,
    "estimated_duration": 3600.0187608172473,
    "input_throughput": 4726.1817591577055,
    "output_throughput": 4158.330273979354,
    "total_throughput": 8884.51203313706,
    "itl": 51.219174903880806,
    "ttft": 343956.9329476078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6751280546002127,
    "arrivals": 73090,
    "finished_requests": 69067,
    "scheduler_time": 87.10025867713738
}
#Debug simulation 
Total elapsed time: 40.673932468984276. Arrivals time: 0.30344822257757187 Scheduler time: 40.07884095842019 Scheduler overhead time: 0.11516942176967859 Adapter cache time: 0.019550228025764227 Engine time: 0.1077400567010045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 34.65307693276554,
    "estimated_duration": 3600.0814937880473,
    "input_throughput": 4699.991938848087,
    "output_throughput": 4052.615482503564,
    "total_throughput": 8752.607421351651,
    "itl": 48.953514464609,
    "ttft": 335586.2117027823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.86469934700988,
    "arrivals": 72131,
    "finished_requests": 68036,
    "scheduler_time": 80.66435584179519
}
#Debug simulation 
Total elapsed time: 34.65320474561304. Arrivals time: 0.2762585412710905 Scheduler time: 34.086429150309414 Scheduler overhead time: 0.11449320055544376 Adapter cache time: 0.019789328332990408 Engine time: 0.10685114189982414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.51315870601684,
    "estimated_duration": 3600.0447439600935,
    "input_throughput": 4710.254512377796,
    "output_throughput": 4068.5844320612596,
    "total_throughput": 8778.838944439056,
    "itl": 48.961749482261474,
    "ttft": 306588.9109275145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3668578940071194,
    "arrivals": 72131,
    "finished_requests": 68168,
    "scheduler_time": 75.7295771848404
}
#Debug simulation 
Total elapsed time: 29.513278680387884. Arrivals time: 0.2598538985475898 Scheduler time: 28.968522377777845 Scheduler overhead time: 0.11174993822351098 Adapter cache time: 0.019835020415484905 Engine time: 0.10418124590069056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.832893012091517,
    "estimated_duration": 3600.0697518463435,
    "input_throughput": 4720.66964571562,
    "output_throughput": 4074.8032708189926,
    "total_throughput": 8795.472916534613,
    "itl": 48.53916225485778,
    "ttft": 302230.3848664402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.427438221867208,
    "arrivals": 72131,
    "finished_requests": 68300,
    "scheduler_time": 76.0646857796827
}
#Debug simulation 
Total elapsed time: 29.833024467341602. Arrivals time: 0.2597456453368068 Scheduler time: 29.28755974723026 Scheduler overhead time: 0.11209229985252023 Adapter cache time: 0.01983073214069009 Engine time: 0.10452251927927136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 29.60914887022227,
    "estimated_duration": 3600.062122961707,
    "input_throughput": 4710.287884157262,
    "output_throughput": 4068.6211792228564,
    "total_throughput": 8778.909063380119,
    "itl": 48.9597658894933,
    "ttft": 306546.7436576887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2141535869427,
    "arrivals": 72131,
    "finished_requests": 68169,
    "scheduler_time": 75.7304211941812
}
#Debug simulation 
Total elapsed time: 29.609325582161546. Arrivals time: 0.2656990932300687 Scheduler time: 29.05701183900237 Scheduler overhead time: 0.11266088206321001 Adapter cache time: 0.019999767653644085 Engine time: 0.10458298539742827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 29.79299353994429,
    "estimated_duration": 3600.059931644876,
    "input_throughput": 4720.682522703188,
    "output_throughput": 4074.8143860198,
    "total_throughput": 8795.496908722987,
    "itl": 48.54023457141307,
    "ttft": 302231.60010417685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.404448215262976,
    "arrivals": 72131,
    "finished_requests": 68300,
    "scheduler_time": 76.06431968590695
}
#Debug simulation 
Total elapsed time: 29.79314763098955. Arrivals time: 0.26063737366348505 Scheduler time: 29.248164312914014 Scheduler overhead time: 0.11165148206055164 Adapter cache time: 0.019846910145133734 Engine time: 0.1037663952447474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.619758543092757,
    "estimated_duration": 3600.042258605962,
    "input_throughput": 4710.313874639448,
    "output_throughput": 4068.6436291089103,
    "total_throughput": 8778.95750374836,
    "itl": 48.95653433069008,
    "ttft": 306542.33313374553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.068390384744845,
    "arrivals": 72131,
    "finished_requests": 68169,
    "scheduler_time": 75.72995259169106
}
#Debug simulation 
Total elapsed time: 29.61986203538254. Arrivals time: 0.26163232466205955 Scheduler time: 29.0717393476516 Scheduler overhead time: 0.11256170691922307 Adapter cache time: 0.019995539914816618 Engine time: 0.10458573140203953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.92514651408419,
    "estimated_duration": 3600.039236841941,
    "input_throughput": 4720.582438681385,
    "output_throughput": 4074.73031123634,
    "total_throughput": 8795.312749917724,
    "itl": 48.539481232302336,
    "ttft": 302271.1465942212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.382493794541817,
    "arrivals": 72131,
    "finished_requests": 68298,
    "scheduler_time": 76.06380304608481
}
#Debug simulation 
Total elapsed time: 29.92530821915716. Arrivals time: 0.26202251529321074 Scheduler time: 29.377216421533376 Scheduler overhead time: 0.11278359265998006 Adapter cache time: 0.019710818771272898 Engine time: 0.10416421154513955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 36.674455841071904,
    "estimated_duration": 3600.0077384222773,
    "input_throughput": 4750.34784438956,
    "output_throughput": 4091.2073168083775,
    "total_throughput": 8841.555161197937,
    "itl": 49.68995075175929,
    "ttft": 295696.85359823407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4282803509011848,
    "arrivals": 71695,
    "finished_requests": 68535,
    "scheduler_time": 82.080124488618
}
#Debug simulation 
Total elapsed time: 36.67457318911329. Arrivals time: 0.28368768421933055 Scheduler time: 36.10184082854539 Scheduler overhead time: 0.11392808705568314 Adapter cache time: 0.01969151059165597 Engine time: 0.10624710423871875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 31.82594910589978,
    "estimated_duration": 3600.007118531086,
    "input_throughput": 4745.95367105035,
    "output_throughput": 4093.509127841118,
    "total_throughput": 8839.462798891467,
    "itl": 50.1995615194117,
    "ttft": 282365.3234354143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9876944920048139,
    "arrivals": 71695,
    "finished_requests": 68447,
    "scheduler_time": 77.39908007462884
}
#Debug simulation 
Total elapsed time: 31.82607710827142. Arrivals time: 0.27116385381668806 Scheduler time: 31.26856578886509 Scheduler overhead time: 0.11266559734940529 Adapter cache time: 0.019227120094001293 Engine time: 0.10541568463668227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 37.111217700876296,
    "estimated_duration": 3600.0085830443863,
    "input_throughput": 4748.974233150935,
    "output_throughput": 4090.0071931352004,
    "total_throughput": 8838.981426286136,
    "itl": 49.61412398403018,
    "ttft": 296425.7922798681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6248154010903126,
    "arrivals": 71695,
    "finished_requests": 68522,
    "scheduler_time": 82.08468043646799
}
#Debug simulation 
Total elapsed time: 37.111336771864444. Arrivals time: 0.28984523518010974 Scheduler time: 36.52922205347568 Scheduler overhead time: 0.115776008926332 Adapter cache time: 0.019573412835597992 Engine time: 0.10742230666801333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 36.721336774062365,
    "estimated_duration": 3600.0170273603203,
    "input_throughput": 4749.877811699728,
    "output_throughput": 4090.5142637054823,
    "total_throughput": 8840.39207540521,
    "itl": 49.66449435189036,
    "ttft": 296013.5731873453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4761023912951332,
    "arrivals": 71695,
    "finished_requests": 68529,
    "scheduler_time": 82.08016799963974
}
#Debug simulation 
Total elapsed time: 36.72151092486456. Arrivals time: 0.2856780416332185 Scheduler time: 36.14617371466011 Scheduler overhead time: 0.1142812897451222 Adapter cache time: 0.019502263516187668 Engine time: 0.10642356099560857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 36.85391895566136,
    "estimated_duration": 3600.016687046869,
    "input_throughput": 4748.963542728551,
    "output_throughput": 4089.99798611442,
    "total_throughput": 8838.96152884297,
    "itl": 49.61280689284306,
    "ttft": 296426.33270790434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6090744956675953,
    "arrivals": 71695,
    "finished_requests": 68522,
    "scheduler_time": 82.08507527842103
}
#Debug simulation 
Total elapsed time: 36.85403164988384. Arrivals time: 0.2916797585785389 Scheduler time: 36.271435922477394 Scheduler overhead time: 0.11440725810825825 Adapter cache time: 0.019641463179141283 Engine time: 0.10753308096900582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 36.85207737702876,
    "estimated_duration": 3600.0192371100293,
    "input_throughput": 4749.8748961483325,
    "output_throughput": 4090.511752882037,
    "total_throughput": 8840.38664903037,
    "itl": 49.664210309183225,
    "ttft": 296013.89568795107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.378926923163231,
    "arrivals": 71695,
    "finished_requests": 68529,
    "scheduler_time": 82.08142920508445
}
#Debug simulation 
Total elapsed time: 36.852229638025165. Arrivals time: 0.2901057889685035 Scheduler time: 36.273517889901996 Scheduler overhead time: 0.1132140546105802 Adapter cache time: 0.01934690959751606 Engine time: 0.10674590524286032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 36.71590599324554,
    "estimated_duration": 3600.0050410893014,
    "input_throughput": 4748.978905548124,
    "output_throughput": 4090.0112171911696,
    "total_throughput": 8838.990122739293,
    "itl": 49.61386499930506,
    "ttft": 296425.78272179863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.594576293304566,
    "arrivals": 71695,
    "finished_requests": 68522,
    "scheduler_time": 82.08501445113507
}
#Debug simulation 
Total elapsed time: 36.71604938711971. Arrivals time: 0.28995501762256026 Scheduler time: 36.13671519048512 Scheduler overhead time: 0.11446289578452706 Adapter cache time: 0.019284884445369244 Engine time: 0.10622283723205328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.796717218123376,
    "estimated_duration": 3599.9671658573684,
    "input_throughput": 4610.547050933189,
    "output_throughput": 4024.862264694909,
    "total_throughput": 8635.409315628098,
    "itl": 48.0469650813848,
    "ttft": 331490.4491968409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8382497108820803,
    "arrivals": 71480,
    "finished_requests": 66892,
    "scheduler_time": 74.80511687475668
}
#Debug simulation 
Total elapsed time: 30.79683106020093. Arrivals time: 0.26464360812678933 Scheduler time: 30.240511421579868 Scheduler overhead time: 0.1157714482396841 Adapter cache time: 0.019362890627235174 Engine time: 0.10659826872870326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 32.63413706095889,
    "estimated_duration": 3599.969211514721,
    "input_throughput": 4660.784860696132,
    "output_throughput": 4071.3070414935873,
    "total_throughput": 8732.091902189719,
    "itl": 49.620179975368835,
    "ttft": 310428.1792989401,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9566059243492804,
    "arrivals": 71480,
    "finished_requests": 67630,
    "scheduler_time": 78.0472605490974
}
#Debug simulation 
Total elapsed time: 32.63426499022171. Arrivals time: 0.2701498242095113 Scheduler time: 32.07572918385267 Scheduler overhead time: 0.11367555195465684 Adapter cache time: 0.01961360778659582 Engine time: 0.10569667536765337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 31.26677760714665,
    "estimated_duration": 3599.973635528979,
    "input_throughput": 4700.133587926604,
    "output_throughput": 4107.670915714118,
    "total_throughput": 8807.804503640722,
    "itl": 49.32160081975183,
    "ttft": 275188.02013515064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0730269106244728,
    "arrivals": 71480,
    "finished_requests": 68266,
    "scheduler_time": 76.47112476567796
}
#Debug simulation 
Total elapsed time: 31.266917815897614. Arrivals time: 0.26106135081499815 Scheduler time: 30.719966291449964 Scheduler overhead time: 0.11267737299203873 Adapter cache time: 0.01919433381408453 Engine time: 0.10478095477446914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 28.43017772724852,
    "estimated_duration": 3599.96387753187,
    "input_throughput": 4738.16782064336,
    "output_throughput": 4131.531178084252,
    "total_throughput": 8869.698998727612,
    "itl": 49.98532478956519,
    "ttft": 243950.93429124018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0809222842380377,
    "arrivals": 71480,
    "finished_requests": 68769,
    "scheduler_time": 74.28260096489353
}
#Debug simulation 
Total elapsed time: 28.430294025223702. Arrivals time: 0.25828641280531883 Scheduler time: 27.88847738923505 Scheduler overhead time: 0.11166270449757576 Adapter cache time: 0.019330414943397045 Engine time: 0.10379551677033305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 29.449143861886114,
    "estimated_duration": 3599.9450037451047,
    "input_throughput": 4664.833207876707,
    "output_throughput": 4074.6803033768283,
    "total_throughput": 8739.513511253535,
    "itl": 48.3591053944432,
    "ttft": 293332.60542212916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.208971082675279,
    "arrivals": 71480,
    "finished_requests": 67701,
    "scheduler_time": 74.29457668538767
}
#Debug simulation 
Total elapsed time: 29.44924806803465. Arrivals time: 0.25776587799191475 Scheduler time: 28.9033295228146 Scheduler overhead time: 0.11361845070496202 Adapter cache time: 0.019378223922103643 Engine time: 0.10581556847319007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.85912405513227,
    "estimated_duration": 3599.9775139769913,
    "input_throughput": 4759.305004956069,
    "output_throughput": 4158.212083792387,
    "total_throughput": 8917.517088748455,
    "itl": 50.319091020148655,
    "ttft": 237800.72811260037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.736426495835179,
    "arrivals": 71480,
    "finished_requests": 69086,
    "scheduler_time": 76.8211505882202
}
#Debug simulation 
Total elapsed time: 30.859249687287956. Arrivals time: 0.2661310904659331 Scheduler time: 30.308980074245483 Scheduler overhead time: 0.11185236508026719 Adapter cache time: 0.019186556339263916 Engine time: 0.10422871867194772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.50970119284466,
    "estimated_duration": 3599.9726197858763,
    "input_throughput": 4664.824367747233,
    "output_throughput": 4074.725713017366,
    "total_throughput": 8739.550080764599,
    "itl": 48.358945621790376,
    "ttft": 293281.4262222718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.188052247837194,
    "arrivals": 71480,
    "finished_requests": 67702,
    "scheduler_time": 74.29538963668743
}
#Debug simulation 
Total elapsed time: 29.509826206136495. Arrivals time: 0.2536517772823572 Scheduler time: 28.970524738542736 Scheduler overhead time: 0.1120818667113781 Adapter cache time: 0.019517093896865845 Engine time: 0.10498795751482248 
