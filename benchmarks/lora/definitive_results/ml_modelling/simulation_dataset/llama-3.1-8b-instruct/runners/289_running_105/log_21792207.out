INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.450642466079444,
    "estimated_duration": 3600.0387075685867,
    "input_throughput": 4443.96082918928,
    "output_throughput": 3874.466396896168,
    "total_throughput": 8318.427226085449,
    "itl": 124.15235681269628,
    "ttft": 2240953.3402075185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.145269996630887,
    "arrivals": 1486263,
    "finished_requests": 64789,
    "scheduler_time": 157.71326773845743
}
#Debug simulation 
Total elapsed time: 7.450817142147571. Arrivals time: 0.25155151868239045 Scheduler time: 7.061329870950431 Scheduler overhead time: 0.04420689446851611 Adapter cache time: 0.028096059802919626 Engine time: 0.0450064386241138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.181355447042733,
    "estimated_duration": 3600.082281251672,
    "input_throughput": 3914.6816375263797,
    "output_throughput": 3421.393745400046,
    "total_throughput": 7336.075382926426,
    "itl": 100.56134963050943,
    "ttft": 2297782.50916914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2027,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.27314702309171,
    "arrivals": 1486263,
    "finished_requests": 57130,
    "scheduler_time": 174.8628509637873
}
#Debug simulation 
Total elapsed time: 5.181531799957156. Arrivals time: 0.22356665460392833 Scheduler time: 4.788550144992769 Scheduler overhead time: 0.051304330583661795 Adapter cache time: 0.041535905096679926 Engine time: 0.0524152172729373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 7.444233826827258,
    "estimated_duration": 3600.1284318386274,
    "input_throughput": 4445.777227960135,
    "output_throughput": 3875.4636852981566,
    "total_throughput": 8321.24091325829,
    "itl": 124.1307487072063,
    "ttft": 2240955.6957922215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.558600231162316,
    "arrivals": 1486263,
    "finished_requests": 64808,
    "scheduler_time": 157.7418329694317
}
#Debug simulation 
Total elapsed time: 7.44434966519475. Arrivals time: 0.25441431207582355 Scheduler time: 7.052853831555694 Scheduler overhead time: 0.04414611775428057 Adapter cache time: 0.027403208892792463 Engine time: 0.044802956748753786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 5.168928157072514,
    "estimated_duration": 3600.0582950247654,
    "input_throughput": 3914.7079977778667,
    "output_throughput": 3421.417096779336,
    "total_throughput": 7336.125094557203,
    "itl": 100.5569041646316,
    "ttft": 2297842.268236962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2027,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.139970678528464,
    "arrivals": 1486263,
    "finished_requests": 57131,
    "scheduler_time": 174.86796874964284
}
#Debug simulation 
Total elapsed time: 5.169202052988112. Arrivals time: 0.22175130620598793 Scheduler time: 4.778416727203876 Scheduler overhead time: 0.051438139751553535 Adapter cache time: 0.041206992231309414 Engine time: 0.05223677819594741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.466507165692747,
    "estimated_duration": 3600.1299609703933,
    "input_throughput": 4446.892799304602,
    "output_throughput": 3876.5767212020855,
    "total_throughput": 8323.469520506687,
    "itl": 124.10108643339898,
    "ttft": 2241054.9196491805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.880034028333688,
    "arrivals": 1486263,
    "finished_requests": 64829,
    "scheduler_time": 157.76993258532838
}
#Debug simulation 
Total elapsed time: 7.466642691753805. Arrivals time: 0.2520476314239204 Scheduler time: 7.076720166485757 Scheduler overhead time: 0.044079090002924204 Adapter cache time: 0.028071969747543335 Engine time: 0.045126888900995255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_384_slots_96_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 270, 33, 34560, 34560, 33, 270, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 34560, 34560, 270, 270, 33, 34560, 33, 34560, 270, 270, 270, 33, 33, 270, 33, 34560, 33, 34560, 270, 34560, 270, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 34560, 34560, 33, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 270, 33, 33, 33, 270, 33, 270, 33, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 34560, 34560, 270, 33, 33, 34560, 270, 270, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 34560, 34560, 33, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 34560, 33, 33, 34560, 33, 34560, 270, 33, 34560, 270, 270, 33, 34560, 34560, 34560, 33, 270, 34560, 33, 34560, 270, 270, 34560, 34560, 33, 270, 34560, 33, 270, 270, 33, 270, 33, 270, 34560, 34560, 270, 33, 34560, 33, 34560, 270, 270, 270, 270, 270, 33, 33, 34560, 33, 34560, 33, 270, 34560, 270, 34560, 270, 34560, 270, 34560, 33, 270, 270, 270, 270, 270, 270, 270, 270, 34560, 33, 33, 33, 33, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33, 270, 34560, 34560, 270, 270, 33, 34560, 270, 34560, 33, 270, 270, 33, 33, 270, 33, 34560, 34560, 33, 34560, 34560, 33, 270, 34560, 33, 33, 270, 270, 33, 33, 34560, 34560, 34560, 34560, 270, 33, 270, 270, 33, 33, 33, 33, 33, 33, 33, 33, 270, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 270, 33, 34560, 34560, 34560, 270, 33, 34560, 34560, 270, 270, 34560, 34560, 33, 34560, 34560, 270, 33, 34560, 270, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 270, 34560, 33, 270, 270, 270, 270, 33, 270, 33, 270, 270, 33, 34560, 33, 33, 33, 270, 33, 270, 270, 33, 34560, 270, 34560, 33, 34560, 33, 270, 270, 270, 270, 33, 270, 34560, 34560, 33, 34560, 34560, 33, 34560, 270, 34560, 270, 270, 270, 33, 270, 33, 33, 34560, 33, 270, 33, 33, 270, 34560, 270, 34560, 33, 34560, 34560, 270, 34560, 270, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4462464 . Total input tokens: 995789657 . Total output tokens: 876302610
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.1647052289918065,
    "estimated_duration": 3600.013555623495,
    "input_throughput": 3914.796092360587,
    "output_throughput": 3421.625727147195,
    "total_throughput": 7336.421819507782,
    "itl": 100.55300941217591,
    "ttft": 2297783.1759049417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2027,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.98628973348036,
    "arrivals": 1486263,
    "finished_requests": 57133,
    "scheduler_time": 174.87300672323437
}
#Debug simulation 
Total elapsed time: 5.164880897849798. Arrivals time: 0.22360981767997146 Scheduler time: 4.773224600125104 Scheduler overhead time: 0.05084554851055145 Adapter cache time: 0.04087977297604084 Engine time: 0.052088483702391386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 10.068844969850034,
    "estimated_duration": 3600.030646396026,
    "input_throughput": 4711.068784088982,
    "output_throughput": 4095.3081926564664,
    "total_throughput": 8806.376976745449,
    "itl": 133.81848604056646,
    "ttft": 2216052.0581931323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1091,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.214138253857547,
    "arrivals": 1482016,
    "finished_requests": 68639,
    "scheduler_time": 153.79259334887396
}
#Debug simulation 
Total elapsed time: 10.06898265099153. Arrivals time: 0.2746374309062958 Scheduler time: 9.666772430296987 Scheduler overhead time: 0.04271717369556427 Adapter cache time: 0.021524672396481037 Engine time: 0.043800335843116045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.576409668661654,
    "estimated_duration": 3600.0557765108897,
    "input_throughput": 4485.581058316459,
    "output_throughput": 3895.024930305433,
    "total_throughput": 8380.605988621892,
    "itl": 123.7881706330186,
    "ttft": 2238294.488413957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.225956948138736,
    "arrivals": 1482016,
    "finished_requests": 65320,
    "scheduler_time": 158.3028203761396
}
#Debug simulation 
Total elapsed time: 7.576571052893996. Arrivals time: 0.25535014225170016 Scheduler time: 7.1860998519696295 Scheduler overhead time: 0.044182153418660164 Adapter cache time: 0.025336310733109713 Engine time: 0.04500051075592637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.16343409428373,
    "estimated_duration": 3600.0318431591645,
    "input_throughput": 3933.506317981178,
    "output_throughput": 3427.3327396939058,
    "total_throughput": 7360.8390576750835,
    "itl": 100.63117964225047,
    "ttft": 2295934.853986576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1877,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.18647888384731,
    "arrivals": 1482016,
    "finished_requests": 57340,
    "scheduler_time": 174.7497919247579
}
#Debug simulation 
Total elapsed time: 5.16354413703084. Arrivals time: 0.22824539989233017 Scheduler time: 4.766951909288764 Scheduler overhead time: 0.051926789339631796 Adapter cache time: 0.03956788731738925 Engine time: 0.05226427270099521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 7.596564270090312,
    "estimated_duration": 3600.069165188101,
    "input_throughput": 4486.371860901763,
    "output_throughput": 3895.8376510155713,
    "total_throughput": 8382.209511917335,
    "itl": 123.76845161621813,
    "ttft": 2238523.0620505004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.673445000760266,
    "arrivals": 1482016,
    "finished_requests": 65333,
    "scheduler_time": 158.3260292239035
}
#Debug simulation 
Total elapsed time: 7.596738483291119. Arrivals time: 0.263170865830034 Scheduler time: 7.19781631603837 Scheduler overhead time: 0.04422725411131978 Adapter cache time: 0.025008582044392824 Engine time: 0.0458630146458745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 5.197314992081374,
    "estimated_duration": 3600.023510939973,
    "input_throughput": 3933.5154220430636,
    "output_throughput": 3427.3406722219966,
    "total_throughput": 7360.85609426506,
    "itl": 100.62780705444072,
    "ttft": 2295904.2238861383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1877,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.063244163761563,
    "arrivals": 1482016,
    "finished_requests": 57340,
    "scheduler_time": 174.75505296055746
}
#Debug simulation 
Total elapsed time: 5.19742190791294. Arrivals time: 0.23602471593767405 Scheduler time: 4.79401889629662 Scheduler overhead time: 0.05134544102475047 Adapter cache time: 0.03961161803454161 Engine time: 0.05225357972085476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.489816673099995,
    "estimated_duration": 3600.1297347371615,
    "input_throughput": 4477.904738946017,
    "output_throughput": 3883.829481223076,
    "total_throughput": 8361.734220169092,
    "itl": 123.71086842008695,
    "ttft": 2240740.2268374492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.541686218483404,
    "arrivals": 1482016,
    "finished_requests": 65180,
    "scheduler_time": 158.1977958710698
}
#Debug simulation 
Total elapsed time: 7.489994370844215. Arrivals time: 0.26819203700870275 Scheduler time: 7.084484662394971 Scheduler overhead time: 0.04405957506969571 Adapter cache time: 0.027337954845279455 Engine time: 0.045037202537059784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 135, 66, 34560, 34560, 66, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 66, 34560, 66, 34560, 135, 135, 135, 66, 66, 135, 66, 34560, 66, 34560, 135, 34560, 135, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 34560, 34560, 66, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 135, 66, 66, 66, 135, 66, 135, 66, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 34560, 34560, 135, 66, 66, 34560, 135, 135, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 66, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 34560, 66, 66, 34560, 66, 34560, 135, 66, 34560, 135, 135, 66, 34560, 34560, 34560, 66, 135, 34560, 66, 34560, 135, 135, 34560, 34560, 66, 135, 34560, 66, 135, 135, 66, 135, 66, 135, 34560, 34560, 135, 66, 34560, 66, 34560, 135, 135, 135, 135, 135, 66, 66, 34560, 66, 34560, 66, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 66, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 66, 66, 66, 66, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66, 135, 34560, 34560, 135, 135, 66, 34560, 135, 34560, 66, 135, 135, 66, 66, 135, 66, 34560, 34560, 66, 34560, 34560, 66, 135, 34560, 66, 66, 135, 135, 66, 66, 34560, 34560, 34560, 34560, 135, 66, 135, 135, 66, 66, 66, 66, 66, 66, 66, 66, 135, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 135, 66, 34560, 34560, 34560, 135, 66, 34560, 34560, 135, 135, 34560, 34560, 66, 34560, 34560, 135, 66, 34560, 135, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 135, 34560, 66, 135, 135, 135, 135, 66, 135, 66, 135, 135, 66, 34560, 66, 66, 66, 135, 66, 135, 135, 66, 34560, 135, 34560, 66, 34560, 66, 135, 135, 135, 135, 66, 135, 34560, 34560, 66, 34560, 34560, 66, 34560, 135, 34560, 135, 135, 135, 66, 135, 66, 66, 34560, 66, 135, 66, 66, 135, 34560, 135, 34560, 66, 34560, 34560, 135, 34560, 135, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 4449408 . Total input tokens: 992886182 . Total output tokens: 873731726
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.693556576035917,
    "estimated_duration": 3600.1061862776924,
    "input_throughput": 3934.02118359282,
    "output_throughput": 3427.6333423261344,
    "total_throughput": 7361.654525918954,
    "itl": 100.62550841095326,
    "ttft": 2295820.8083328647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1877,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.915569616835285,
    "arrivals": 1482016,
    "finished_requests": 57348,
    "scheduler_time": 174.76578239178377
}
#Debug simulation 
Total elapsed time: 5.693632512353361. Arrivals time: 0.6518611814826727 Scheduler time: 4.874284137971699 Scheduler overhead time: 0.05143199162557721 Adapter cache time: 0.039499673526734114 Engine time: 0.05250564590096474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 9.346631030086428,
    "estimated_duration": 3600.062000147008,
    "input_throughput": 4739.228379762153,
    "output_throughput": 4094.1544893943847,
    "total_throughput": 8833.382869156538,
    "itl": 133.9071897331862,
    "ttft": 2222012.3095440078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.762968203509409,
    "arrivals": 1480643,
    "finished_requests": 68763,
    "scheduler_time": 153.6512069563954
}
#Debug simulation 
Total elapsed time: 9.346837711986154. Arrivals time: 0.2747597061097622 Scheduler time: 8.945217869244516 Scheduler overhead time: 0.04224999761208892 Adapter cache time: 0.021975793410092592 Engine time: 0.042972135823220015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.264102547895163,
    "estimated_duration": 3600.019303617556,
    "input_throughput": 4496.83310968151,
    "output_throughput": 3890.998858235036,
    "total_throughput": 8387.831967916547,
    "itl": 123.84967656641136,
    "ttft": 2241254.0959800747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.820794918830439,
    "arrivals": 1480643,
    "finished_requests": 65297,
    "scheduler_time": 158.3126335970821
}
#Debug simulation 
Total elapsed time: 7.264246811158955. Arrivals time: 0.2573200548067689 Scheduler time: 6.871015128213912 Scheduler overhead time: 0.043962156400084496 Adapter cache time: 0.02682444453239441 Engine time: 0.0445155818015337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.179714560974389,
    "estimated_duration": 3600.0038495014364,
    "input_throughput": 3951.0204973724776,
    "output_throughput": 3426.8093912478685,
    "total_throughput": 7377.8298886203465,
    "itl": 100.76177348402233,
    "ttft": 2301390.8229151187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.525339812407946,
    "arrivals": 1480643,
    "finished_requests": 57404,
    "scheduler_time": 174.77395877847
}
#Debug simulation 
Total elapsed time: 5.179881350137293. Arrivals time: 0.22465956816449761 Scheduler time: 4.789562827907503 Scheduler overhead time: 0.05154099315404892 Adapter cache time: 0.03748489776626229 Engine time: 0.0524400370195508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 7.26381949102506,
    "estimated_duration": 3600.0733790793456,
    "input_throughput": 4497.809154140333,
    "output_throughput": 3891.726507969939,
    "total_throughput": 8389.535662110271,
    "itl": 123.828961179276,
    "ttft": 2241281.773707711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.22052627443339,
    "arrivals": 1480643,
    "finished_requests": 65307,
    "scheduler_time": 158.34080251166318
}
#Debug simulation 
Total elapsed time: 7.263929419685155. Arrivals time: 0.2548580449074507 Scheduler time: 6.872520532459021 Scheduler overhead time: 0.04409223282709718 Adapter cache time: 0.026874235831201077 Engine time: 0.044968086294829845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 5.171339537017047,
    "estimated_duration": 3600.10776763873,
    "input_throughput": 3950.9522820005095,
    "output_throughput": 3426.819361046974,
    "total_throughput": 7377.771643047484,
    "itl": 100.7586861627402,
    "ttft": 2301376.8777746693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.406247435854498,
    "arrivals": 1480643,
    "finished_requests": 57405,
    "scheduler_time": 174.78460558834146
}
#Debug simulation 
Total elapsed time: 5.171532666776329. Arrivals time: 0.22661549458280206 Scheduler time: 4.779746927320957 Scheduler overhead time: 0.051015141885727644 Adapter cache time: 0.037751256953924894 Engine time: 0.05203460156917572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.425078600179404,
    "estimated_duration": 3600.0793126361546,
    "input_throughput": 4501.983315509812,
    "output_throughput": 3894.7941926681387,
    "total_throughput": 8396.777508177951,
    "itl": 124.01519946087691,
    "ttft": 2241120.1516069137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.005436859475408,
    "arrivals": 1480643,
    "finished_requests": 65348,
    "scheduler_time": 158.16465251237
}
#Debug simulation 
Total elapsed time: 7.425194843206555. Arrivals time: 0.26731372345238924 Scheduler time: 7.021958127152175 Scheduler overhead time: 0.04443838493898511 Adapter cache time: 0.025564696174114943 Engine time: 0.045284456573426723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_384_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 135, 33, 34560, 34560, 33, 135, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 34560, 34560, 135, 135, 33, 34560, 33, 34560, 135, 135, 135, 33, 33, 135, 33, 34560, 33, 34560, 135, 34560, 135, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 34560, 34560, 33, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 135, 33, 33, 33, 135, 33, 135, 33, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 34560, 34560, 135, 33, 33, 34560, 135, 135, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 34560, 34560, 33, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 34560, 33, 33, 34560, 33, 34560, 135, 33, 34560, 135, 135, 33, 34560, 34560, 34560, 33, 135, 34560, 33, 34560, 135, 135, 34560, 34560, 33, 135, 34560, 33, 135, 135, 33, 135, 33, 135, 34560, 34560, 135, 33, 34560, 33, 34560, 135, 135, 135, 135, 135, 33, 33, 34560, 33, 34560, 33, 135, 34560, 135, 34560, 135, 34560, 135, 34560, 33, 135, 135, 135, 135, 135, 135, 135, 135, 34560, 33, 33, 33, 33, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33, 135, 34560, 34560, 135, 135, 33, 34560, 135, 34560, 33, 135, 135, 33, 33, 135, 33, 34560, 34560, 33, 34560, 34560, 33, 135, 34560, 33, 33, 135, 135, 33, 33, 34560, 34560, 34560, 34560, 135, 33, 135, 135, 33, 33, 33, 33, 33, 33, 33, 33, 135, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 135, 33, 34560, 34560, 34560, 135, 33, 34560, 34560, 135, 135, 34560, 34560, 33, 34560, 34560, 135, 33, 34560, 135, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 135, 34560, 33, 135, 135, 135, 135, 33, 135, 33, 135, 135, 33, 34560, 33, 33, 33, 135, 33, 135, 135, 33, 34560, 135, 34560, 33, 34560, 33, 135, 135, 135, 135, 33, 135, 34560, 34560, 33, 34560, 34560, 33, 34560, 135, 34560, 135, 135, 135, 33, 135, 33, 33, 34560, 33, 135, 33, 33, 135, 34560, 135, 34560, 33, 34560, 34560, 135, 34560, 135, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4445184 . Total input tokens: 991937976 . Total output tokens: 872898079
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.1732496889308095,
    "estimated_duration": 3600.079364936179,
    "input_throughput": 3951.137616170905,
    "output_throughput": 3427.0897247896423,
    "total_throughput": 7378.227340960548,
    "itl": 100.75525263216245,
    "ttft": 2301367.9026323794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.266650458816194,
    "arrivals": 1480643,
    "finished_requests": 57407,
    "scheduler_time": 174.7897023916485
}
#Debug simulation 
Total elapsed time: 5.1734141311608255. Arrivals time: 0.22755287028849125 Scheduler time: 4.780107092577964 Scheduler overhead time: 0.05142148490995169 Adapter cache time: 0.03771491954103112 Engine time: 0.05244998214766383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 9.949426726903766,
    "estimated_duration": 3600.047183971971,
    "input_throughput": 4706.287205187895,
    "output_throughput": 4104.392594015246,
    "total_throughput": 8810.67979920314,
    "itl": 133.6769970583047,
    "ttft": 2216171.291960275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 909,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.010679810042619,
    "arrivals": 1477777,
    "finished_requests": 68629,
    "scheduler_time": 153.95412072756062
}
#Debug simulation 
Total elapsed time: 9.949537762906402. Arrivals time: 0.27687213756144047 Scheduler time: 9.546533283777535 Scheduler overhead time: 0.04300621058791876 Adapter cache time: 0.019296562764793634 Engine time: 0.04414015123620629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.2227987460792065,
    "estimated_duration": 3600.0327704790693,
    "input_throughput": 4451.519755990281,
    "output_throughput": 3893.5526128932206,
    "total_throughput": 8345.0723688835,
    "itl": 123.58449884129895,
    "ttft": 2239928.656319074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.805713578546397,
    "arrivals": 1477777,
    "finished_requests": 65005,
    "scheduler_time": 158.56142337562804
}
#Debug simulation 
Total elapsed time: 7.222990158945322. Arrivals time: 0.2546678544022143 Scheduler time: 6.834613884333521 Scheduler overhead time: 0.04415656719356775 Adapter cache time: 0.023746755439788103 Engine time: 0.04510778794065118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.114428103901446,
    "estimated_duration": 3600.012423423224,
    "input_throughput": 3914.5581577186867,
    "output_throughput": 3427.316228055549,
    "total_throughput": 7341.874385774236,
    "itl": 100.77256012712382,
    "ttft": 2298807.713656153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.126988618913792,
    "arrivals": 1477777,
    "finished_requests": 57099,
    "scheduler_time": 174.77133778083862
}
#Debug simulation 
Total elapsed time: 5.114541062153876. Arrivals time: 0.232016792986542 Scheduler time: 4.717741418629885 Scheduler overhead time: 0.051383902318775654 Adapter cache time: 0.03654536930844188 Engine time: 0.05259723262861371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 7.21521576307714,
    "estimated_duration": 3600.039418948856,
    "input_throughput": 4452.061251229232,
    "output_throughput": 3893.808752819977,
    "total_throughput": 8345.870004049208,
    "itl": 123.56724273207334,
    "ttft": 2239820.3316020425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.274024945767602,
    "arrivals": 1477777,
    "finished_requests": 65012,
    "scheduler_time": 158.58390179814486
}
#Debug simulation 
Total elapsed time: 7.215384454932064. Arrivals time: 0.25523387361317873 Scheduler time: 6.826329398434609 Scheduler overhead time: 0.04405707400292158 Adapter cache time: 0.024197036866098642 Engine time: 0.04492126917466521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 5.087833872996271,
    "estimated_duration": 3600.013469205914,
    "input_throughput": 3914.5570205626186,
    "output_throughput": 3427.3152324403895,
    "total_throughput": 7341.872253003008,
    "itl": 100.76947898455701,
    "ttft": 2298768.152234766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.013488406128936,
    "arrivals": 1477777,
    "finished_requests": 57099,
    "scheduler_time": 174.77657983444914
}
#Debug simulation 
Total elapsed time: 5.087949214037508. Arrivals time: 0.2239392907358706 Scheduler time: 4.6997889298945665 Scheduler overhead time: 0.05124671244993806 Adapter cache time: 0.03665254591032863 Engine time: 0.05209203204140067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 7.194678356871009,
    "estimated_duration": 3600.082425353792,
    "input_throughput": 4452.854158866825,
    "output_throughput": 3894.4869432044075,
    "total_throughput": 8347.341102071232,
    "itl": 123.54626123217385,
    "ttft": 2239631.779600445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.64793728680346,
    "arrivals": 1477777,
    "finished_requests": 65022,
    "scheduler_time": 158.6119696483243
}
#Debug simulation 
Total elapsed time: 7.19499142980203. Arrivals time: 0.2530314917676151 Scheduler time: 6.808063766453415 Scheduler overhead time: 0.04397496860474348 Adapter cache time: 0.02423484204337001 Engine time: 0.04487291956320405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_384_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 66, 33, 34560, 34560, 33, 66, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 34560, 34560, 66, 66, 33, 34560, 33, 34560, 66, 66, 66, 33, 33, 66, 33, 34560, 33, 34560, 66, 34560, 66, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 34560, 34560, 33, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 66, 33, 33, 33, 66, 33, 66, 33, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 34560, 34560, 66, 33, 33, 34560, 66, 66, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 34560, 34560, 33, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 34560, 33, 33, 34560, 33, 34560, 66, 33, 34560, 66, 66, 33, 34560, 34560, 34560, 33, 66, 34560, 33, 34560, 66, 66, 34560, 34560, 33, 66, 34560, 33, 66, 66, 33, 66, 33, 66, 34560, 34560, 66, 33, 34560, 33, 34560, 66, 66, 66, 66, 66, 33, 33, 34560, 33, 34560, 33, 66, 34560, 66, 34560, 66, 34560, 66, 34560, 33, 66, 66, 66, 66, 66, 66, 66, 66, 34560, 33, 33, 33, 33, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33, 66, 34560, 34560, 66, 66, 33, 34560, 66, 34560, 33, 66, 66, 33, 33, 66, 33, 34560, 34560, 33, 34560, 34560, 33, 66, 34560, 33, 33, 66, 66, 33, 33, 34560, 34560, 34560, 34560, 66, 33, 66, 66, 33, 33, 33, 33, 33, 33, 33, 33, 66, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 66, 33, 34560, 34560, 34560, 66, 33, 34560, 34560, 66, 66, 34560, 34560, 33, 34560, 34560, 66, 33, 34560, 66, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 66, 34560, 33, 66, 66, 66, 66, 33, 66, 33, 66, 66, 33, 34560, 33, 33, 33, 66, 33, 66, 66, 33, 34560, 66, 34560, 33, 34560, 33, 66, 66, 66, 66, 33, 66, 34560, 34560, 33, 34560, 34560, 33, 34560, 66, 34560, 66, 66, 66, 33, 66, 33, 33, 34560, 33, 66, 33, 33, 66, 34560, 66, 34560, 33, 34560, 34560, 66, 34560, 66, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 4436352 . Total input tokens: 989988399 . Total output tokens: 871154464
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 5.072619513142854,
    "estimated_duration": 3600.100792678291,
    "input_throughput": 3914.46206968998,
    "output_throughput": 3427.232100027087,
    "total_throughput": 7341.694169717067,
    "itl": 100.76562532490237,
    "ttft": 2298686.341764874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.871820257324485,
    "arrivals": 1477777,
    "finished_requests": 57099,
    "scheduler_time": 174.78730109513046
}
#Debug simulation 
Total elapsed time: 5.072732912842184. Arrivals time: 0.2260902808047831 Scheduler time: 4.6828893893398345 Scheduler overhead time: 0.05082623613998294 Adapter cache time: 0.03666978050023317 Engine time: 0.052117050625383854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 21.01684081600979,
    "estimated_duration": 3600.0219035480172,
    "input_throughput": 4622.414097980792,
    "output_throughput": 4022.168861175338,
    "total_throughput": 8644.582959156129,
    "itl": 135.7892934639176,
    "ttft": 2213265.810348626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.660729595679102,
    "arrivals": 1289312,
    "finished_requests": 67458,
    "scheduler_time": 151.09394964779955
}
#Debug simulation 
Total elapsed time: 21.01700366474688. Arrivals time: 0.3713196571916342 Scheduler time: 20.49414362059906 Scheduler overhead time: 0.049052915535867214 Adapter cache time: 0.033106219954788685 Engine time: 0.048607099801301956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 13.798176274169236,
    "estimated_duration": 3600.105612393811,
    "input_throughput": 4436.077359791917,
    "output_throughput": 3867.6282029241993,
    "total_throughput": 8303.705562716117,
    "itl": 125.03566590655213,
    "ttft": 2230930.626062804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.21132724474155,
    "arrivals": 1289312,
    "finished_requests": 64794,
    "scheduler_time": 156.7860795127217
}
#Debug simulation 
Total elapsed time: 13.798313301056623. Arrivals time: 0.3095619650557637 Scheduler time: 13.328563195187598 Scheduler overhead time: 0.047419043723493814 Adapter cache time: 0.043661502189934254 Engine time: 0.04783705249428749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 8.47821173677221,
    "estimated_duration": 3600.025813393813,
    "input_throughput": 3907.838090399004,
    "output_throughput": 3415.748285540094,
    "total_throughput": 7323.586375939098,
    "itl": 101.25344574606127,
    "ttft": 2289559.5214440166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.378822922386494,
    "arrivals": 1289312,
    "finished_requests": 57156,
    "scheduler_time": 173.74749862226324
}
#Debug simulation 
Total elapsed time: 8.478325864765793. Arrivals time: 0.26960695814341307 Scheduler time: 8.009561921469867 Scheduler overhead time: 0.052675295155495405 Adapter cache time: 0.06933046784251928 Engine time: 0.05279028043150902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 13.858661927748471,
    "estimated_duration": 3600.0188262949655,
    "input_throughput": 4437.106240480312,
    "output_throughput": 3868.2086599896606,
    "total_throughput": 8305.314900469974,
    "itl": 125.00090534714906,
    "ttft": 2230644.874477252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.182655503517307,
    "arrivals": 1289312,
    "finished_requests": 64804,
    "scheduler_time": 156.82597189228022
}
#Debug simulation 
Total elapsed time: 13.85880419658497. Arrivals time: 0.34171332512050867 Scheduler time: 13.355889029800892 Scheduler overhead time: 0.04798338748514652 Adapter cache time: 0.043781688902527094 Engine time: 0.04811113374307752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 8.536024569068104,
    "estimated_duration": 3600.0825913783688,
    "input_throughput": 3908.18089387585,
    "output_throughput": 3415.914965242953,
    "total_throughput": 7324.095859118803,
    "itl": 101.24548893451673,
    "ttft": 2289710.162822941,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.10998482714059,
    "arrivals": 1289312,
    "finished_requests": 57160,
    "scheduler_time": 173.7632603692714
}
#Debug simulation 
Total elapsed time: 8.536168657243252. Arrivals time: 0.3608133988454938 Scheduler time: 7.975656334776431 Scheduler overhead time: 0.05291220638900995 Adapter cache time: 0.06951617542654276 Engine time: 0.05281997658312321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 14.368348248302937,
    "estimated_duration": 3600.086167876226,
    "input_throughput": 4440.468715067048,
    "output_throughput": 3866.9604978406805,
    "total_throughput": 8307.42921290773,
    "itl": 124.74627301869852,
    "ttft": 2232150.3080775733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.565831998713085,
    "arrivals": 1289312,
    "finished_requests": 64822,
    "scheduler_time": 157.0085043321802
}
#Debug simulation 
Total elapsed time: 14.371034348383546. Arrivals time: 0.31101192999631166 Scheduler time: 13.897732640150934 Scheduler overhead time: 0.048295341432094574 Adapter cache time: 0.04209590097889304 Engine time: 0.0480277044698596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 17280, 8640, 4320, 17280, 17280, 4320, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 4320, 17280, 8640, 17280, 8640, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 17280, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 8640, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 4320, 17280, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 17280, 4320, 17280, 4320, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 17280, 4320, 8640, 17280, 4320, 4320, 8640, 8640, 4320, 4320, 17280, 17280, 17280, 17280, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 17280, 17280, 4320, 17280, 17280, 8640, 4320, 17280, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 8640, 17280, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 17280, 8640, 17280, 4320, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 4320]
Prompts retrieved: 3870720 . Total input tokens: 863684042 . Total output tokens: 760046049
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 8.464329176116735,
    "estimated_duration": 3600.0297547595487,
    "input_throughput": 3908.555750517628,
    "output_throughput": 3416.0903764034038,
    "total_throughput": 7324.646126921031,
    "itl": 101.23659205477497,
    "ttft": 2289702.7652370096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.84011114601161,
    "arrivals": 1289312,
    "finished_requests": 57164,
    "scheduler_time": 173.7736716225129
}
#Debug simulation 
Total elapsed time: 8.464445712044835. Arrivals time: 0.2611650303006172 Scheduler time: 8.004073209129274 Scheduler overhead time: 0.05261307908222079 Adapter cache time: 0.06935520749539137 Engine time: 0.05283755622804165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 19.570151370950043,
    "estimated_duration": 3600.1368269374084,
    "input_throughput": 4641.961904046973,
    "output_throughput": 4026.1622534859293,
    "total_throughput": 8668.124157532902,
    "itl": 136.12046923676976,
    "ttft": 2199258.0697698677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.41454422532142,
    "arrivals": 1150725,
    "finished_requests": 67780,
    "scheduler_time": 150.81739417526285
}
#Debug simulation 
Total elapsed time: 19.57026685960591. Arrivals time: 0.3454622211866081 Scheduler time: 19.07533909007907 Scheduler overhead time: 0.0474393698386848 Adapter cache time: 0.034365853760391474 Engine time: 0.04714188305661082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 14.488196218851954,
    "estimated_duration": 3600.1343463582357,
    "input_throughput": 4446.229073698907,
    "output_throughput": 3857.8049216549975,
    "total_throughput": 8304.033995353906,
    "itl": 124.31744881963083,
    "ttft": 2219569.2515956745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.570696937297184,
    "arrivals": 1150725,
    "finished_requests": 64936,
    "scheduler_time": 157.0825000390732
}
#Debug simulation 
Total elapsed time: 14.488336947746575. Arrivals time: 0.30364286061376333 Scheduler time: 14.02368824928999 Scheduler overhead time: 0.048169602639973164 Adapter cache time: 0.04339849902316928 Engine time: 0.0482091074809432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 8.128642862662673,
    "estimated_duration": 3600.077593285582,
    "input_throughput": 3928.955872056939,
    "output_throughput": 3405.8432581753946,
    "total_throughput": 7334.7991302323335,
    "itl": 100.77263580353527,
    "ttft": 2281008.0507008773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.690832032641556,
    "arrivals": 1150725,
    "finished_requests": 57365,
    "scheduler_time": 174.0336008772818
}
#Debug simulation 
Total elapsed time: 8.128720494918525. Arrivals time: 0.6228017648681998 Scheduler time: 7.309089488349855 Scheduler overhead time: 0.0524670472368598 Adapter cache time: 0.06748855067417026 Engine time: 0.05280758859589696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 14.05662282416597,
    "estimated_duration": 3600.0368310444433,
    "input_throughput": 4464.878209408955,
    "output_throughput": 3871.7234445504837,
    "total_throughput": 8336.601653959438,
    "itl": 125.09293173221708,
    "ttft": 2219022.731906417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.46014859169749,
    "arrivals": 1150725,
    "finished_requests": 65202,
    "scheduler_time": 156.63439553715793
}
#Debug simulation 
Total elapsed time: 14.056747948285192. Arrivals time: 0.32400812255218625 Scheduler time: 13.572919149883091 Scheduler overhead time: 0.04777592606842518 Adapter cache time: 0.04281926620751619 Engine time: 0.04813056578859687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 7.737187194172293,
    "estimated_duration": 3600.0293422669383,
    "input_throughput": 3929.321029114855,
    "output_throughput": 3406.1258490405867,
    "total_throughput": 7335.4468781554415,
    "itl": 100.76509308336756,
    "ttft": 2281000.2984472597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.42655051528118,
    "arrivals": 1150725,
    "finished_requests": 57369,
    "scheduler_time": 174.04401813662048
}
#Debug simulation 
Total elapsed time: 7.737326523289084. Arrivals time: 0.2513659414835274 Scheduler time: 7.289391810074449 Scheduler overhead time: 0.052373758517205715 Adapter cache time: 0.06714536622166634 Engine time: 0.052708398550748825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 14.01306051807478,
    "estimated_duration": 3600.0083476819354,
    "input_throughput": 4465.573811891711,
    "output_throughput": 3872.392131806154,
    "total_throughput": 8337.965943697865,
    "itl": 125.06211626577304,
    "ttft": 2218681.2673400505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.482841026485657,
    "arrivals": 1150725,
    "finished_requests": 65210,
    "scheduler_time": 156.67464477405622
}
#Debug simulation 
Total elapsed time: 14.013191592879593. Arrivals time: 0.29861424677073956 Scheduler time: 13.554350706748664 Scheduler overhead time: 0.04764665802940726 Adapter cache time: 0.04270388511940837 Engine time: 0.04853152437135577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 8640, 1080, 17280, 17280, 1080, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 1080, 17280, 8640, 17280, 8640, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 8640, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 1080, 17280, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 17280, 1080, 17280, 1080, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 8640, 17280, 1080, 1080, 8640, 8640, 1080, 1080, 17280, 17280, 17280, 17280, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 17280, 17280, 1080, 17280, 17280, 8640, 1080, 17280, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 8640, 17280, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 17280, 8640, 17280, 1080, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 3456000 . Total input tokens: 771185707 . Total output tokens: 678835930
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 8.071257496252656,
    "estimated_duration": 3600.085383339559,
    "input_throughput": 3929.326250278482,
    "output_throughput": 3406.3419875403956,
    "total_throughput": 7335.668237818877,
    "itl": 100.7580548874679,
    "ttft": 2280964.5837599104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.158333771565122,
    "arrivals": 1150725,
    "finished_requests": 57371,
    "scheduler_time": 174.05977171308368
}
#Debug simulation 
Total elapsed time: 8.071338563226163. Arrivals time: 0.6116044367663562 Scheduler time: 7.263963351026177 Scheduler overhead time: 0.05215704347938299 Adapter cache time: 0.06690482003614306 Engine time: 0.05249829404056072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 16.516797902993858,
    "estimated_duration": 3600.08990921901,
    "input_throughput": 4636.180879054993,
    "output_throughput": 4025.5297410458,
    "total_throughput": 8661.710620100794,
    "itl": 136.26699348012716,
    "ttft": 2203012.7509362013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.393180762050042,
    "arrivals": 1127715,
    "finished_requests": 67522,
    "scheduler_time": 150.7184269094685
}
#Debug simulation 
Total elapsed time: 16.51690589496866. Arrivals time: 0.37303579365834594 Scheduler time: 15.999347447417676 Scheduler overhead time: 0.04495125450193882 Adapter cache time: 0.034811653196811676 Engine time: 0.04498792300000787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 12.999209930188954,
    "estimated_duration": 3600.0034434507925,
    "input_throughput": 4450.113521181408,
    "output_throughput": 3867.1365788070784,
    "total_throughput": 8317.250099988487,
    "itl": 124.96302052684301,
    "ttft": 2222983.962568504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2029,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.839567891149523,
    "arrivals": 1127715,
    "finished_requests": 64846,
    "scheduler_time": 156.73098946168784
}
#Debug simulation 
Total elapsed time: 12.999373365193605. Arrivals time: 0.2727789324708283 Scheduler time: 12.573469678405672 Scheduler overhead time: 0.04634849168360233 Adapter cache time: 0.04010564927011728 Engine time: 0.045836581848561764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.222881295718253,
    "estimated_duration": 3600.0668153260376,
    "input_throughput": 3925.032429910691,
    "output_throughput": 3412.7886037268736,
    "total_throughput": 7337.821033637564,
    "itl": 101.16513314517537,
    "ttft": 2282573.0391680636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3890,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.278208623925465,
    "arrivals": 1127715,
    "finished_requests": 57160,
    "scheduler_time": 173.67977892026872
}
#Debug simulation 
Total elapsed time: 7.22301294375211. Arrivals time: 0.23346038395538926 Scheduler time: 6.795451891142875 Scheduler overhead time: 0.05183814512565732 Adapter cache time: 0.06652237148955464 Engine time: 0.051713877357542515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 13.021651026792824,
    "estimated_duration": 3600.0298239278522,
    "input_throughput": 4451.5958988680795,
    "output_throughput": 3868.257675933933,
    "total_throughput": 8319.853574802011,
    "itl": 124.93195550864716,
    "ttft": 2222906.354843033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2029,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.89696585027058,
    "arrivals": 1127715,
    "finished_requests": 64867,
    "scheduler_time": 156.77182066648598
}
#Debug simulation 
Total elapsed time: 13.021771982777864. Arrivals time: 0.2681813342496753 Scheduler time: 12.600968352984637 Scheduler overhead time: 0.04635666310787201 Adapter cache time: 0.039671854581683874 Engine time: 0.0458491463214159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 7.233735276851803,
    "estimated_duration": 3600.0146503591673,
    "input_throughput": 3925.6581910270925,
    "output_throughput": 3413.077499219103,
    "total_throughput": 7338.735690246195,
    "itl": 101.15761908077107,
    "ttft": 2282478.784047445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3890,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.008127825619873,
    "arrivals": 1127715,
    "finished_requests": 57166,
    "scheduler_time": 173.69022513456088
}
#Debug simulation 
Total elapsed time: 7.233838647138327. Arrivals time: 0.23360582534223795 Scheduler time: 6.806676829699427 Scheduler overhead time: 0.05169619666412473 Adapter cache time: 0.06630757357925177 Engine time: 0.051581543404608965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 13.116766033228487,
    "estimated_duration": 3600.1310128659297,
    "input_throughput": 4439.998417522944,
    "output_throughput": 3864.2543702667417,
    "total_throughput": 8304.252787789685,
    "itl": 124.52557752215287,
    "ttft": 2223311.9878435927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2064,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.17641282133823,
    "arrivals": 1127715,
    "finished_requests": 64751,
    "scheduler_time": 157.03478634527602
}
#Debug simulation 
Total elapsed time: 13.116846906021237. Arrivals time: 0.6281683067791164 Scheduler time: 12.335861627478153 Scheduler overhead time: 0.046019889414310455 Adapter cache time: 0.03954078443348408 Engine time: 0.04616421042010188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 8640, 540, 17280, 17280, 540, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 540, 540, 8640, 540, 17280, 540, 17280, 8640, 17280, 8640, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 17280, 17280, 540, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 17280, 8640, 8640, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 540, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 8640, 540, 17280, 8640, 8640, 540, 17280, 17280, 17280, 540, 8640, 17280, 540, 17280, 8640, 8640, 17280, 17280, 540, 8640, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 17280, 17280, 8640, 540, 17280, 540, 17280, 8640, 8640, 8640, 8640, 8640, 540, 540, 17280, 540, 17280, 540, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 17280, 540, 8640, 8640, 540, 540, 8640, 540, 17280, 17280, 540, 17280, 17280, 540, 8640, 17280, 540, 540, 8640, 8640, 540, 540, 17280, 17280, 17280, 17280, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 8640, 540, 17280, 17280, 17280, 8640, 540, 17280, 17280, 8640, 8640, 17280, 17280, 540, 17280, 17280, 8640, 540, 17280, 8640, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 8640, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 17280, 540, 540, 540, 8640, 540, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 540, 8640, 8640, 8640, 8640, 540, 8640, 17280, 17280, 540, 17280, 17280, 540, 17280, 8640, 17280, 8640, 8640, 8640, 540, 8640, 540, 540, 17280, 540, 8640, 540, 540, 8640, 17280, 8640, 17280, 540, 17280, 17280, 8640, 17280, 8640, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 3386880 . Total input tokens: 755683179 . Total output tokens: 665337276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.307013458106667,
    "estimated_duration": 3600.0665119702485,
    "input_throughput": 3926.1460734136595,
    "output_throughput": 3413.451379062064,
    "total_throughput": 7339.597452475724,
    "itl": 101.14836077000912,
    "ttft": 2282550.339531383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3890,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.73162639483923,
    "arrivals": 1127715,
    "finished_requests": 57174,
    "scheduler_time": 173.7060336577893
}
#Debug simulation 
Total elapsed time: 7.307119847275317. Arrivals time: 0.31122720893472433 Scheduler time: 6.801494761370122 Scheduler overhead time: 0.05198712693527341 Adapter cache time: 0.06682398030534387 Engine time: 0.051585094537585974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 16.971433840226382,
    "estimated_duration": 3600.1022225196816,
    "input_throughput": 4623.309553791151,
    "output_throughput": 4013.7043636185263,
    "total_throughput": 8637.013917409677,
    "itl": 135.37700656156784,
    "ttft": 2203362.9052519724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.614442732455451,
    "arrivals": 1116080,
    "finished_requests": 67415,
    "scheduler_time": 151.13765128845515
}
#Debug simulation 
Total elapsed time: 16.971569283865392. Arrivals time: 0.3746308679692447 Scheduler time: 16.454405878670514 Scheduler overhead time: 0.04584974097087979 Adapter cache time: 0.03156658820807934 Engine time: 0.04509132448583841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 11.722168942913413,
    "estimated_duration": 3600.106896342103,
    "input_throughput": 4445.203006682967,
    "output_throughput": 3867.1765591587664,
    "total_throughput": 8312.379565841733,
    "itl": 124.83684196749627,
    "ttft": 2224628.9794925135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.867048979811102,
    "arrivals": 1116080,
    "finished_requests": 64867,
    "scheduler_time": 156.71837701227648
}
#Debug simulation 
Total elapsed time: 11.722282465081662. Arrivals time: 0.6097118244506419 Scheduler time: 10.960706365760416 Scheduler overhead time: 0.045867336448282 Adapter cache time: 0.03943834966048598 Engine time: 0.04559400398284197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.0239700470119715,
    "estimated_duration": 3600.102178967387,
    "input_throughput": 3906.518287776466,
    "output_throughput": 3397.2141322688813,
    "total_throughput": 7303.732420045347,
    "itl": 100.19001874326943,
    "ttft": 2288920.9222012074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.81954392835587,
    "arrivals": 1116080,
    "finished_requests": 56965,
    "scheduler_time": 174.50232714968277
}
#Debug simulation 
Total elapsed time: 7.024074550718069. Arrivals time: 0.23593098670244217 Scheduler time: 6.597737553995103 Scheduler overhead time: 0.052075766026973724 Adapter cache time: 0.06266174744814634 Engine time: 0.051607683300971985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 11.394952815026045,
    "estimated_duration": 3600.005730378029,
    "input_throughput": 4446.024867387995,
    "output_throughput": 3868.014676337134,
    "total_throughput": 8314.03954372513,
    "itl": 124.80373958805917,
    "ttft": 2224511.992413326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.918894055038917,
    "arrivals": 1116080,
    "finished_requests": 64880,
    "scheduler_time": 156.7534378279139
}
#Debug simulation 
Total elapsed time: 11.395057246088982. Arrivals time: 0.2703499896451831 Scheduler time: 10.974177835509181 Scheduler overhead time: 0.04546288773417473 Adapter cache time: 0.03928191540762782 Engine time: 0.04507995443418622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 7.012263425625861,
    "estimated_duration": 3600.085985808557,
    "input_throughput": 3907.36583944138,
    "output_throughput": 3397.489128930607,
    "total_throughput": 7304.8549683719875,
    "itl": 100.18253749811852,
    "ttft": 2288994.878260944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.57783818324649,
    "arrivals": 1116080,
    "finished_requests": 56972,
    "scheduler_time": 174.5129446077631
}
#Debug simulation 
Total elapsed time: 7.012358808889985. Arrivals time: 0.23158500995486975 Scheduler time: 6.590726959053427 Scheduler overhead time: 0.05189055111259222 Adapter cache time: 0.062316711992025375 Engine time: 0.05172865744680166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 11.354673257097602,
    "estimated_duration": 3600.0394863857923,
    "input_throughput": 4447.287609079372,
    "output_throughput": 3868.8095096375405,
    "total_throughput": 8316.097118716913,
    "itl": 124.77434825494629,
    "ttft": 2224163.6060011517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.972127351239946,
    "arrivals": 1116080,
    "finished_requests": 64895,
    "scheduler_time": 156.79433343652016
}
#Debug simulation 
Total elapsed time: 11.354771688114852. Arrivals time: 0.2670871517620981 Scheduler time: 10.93625809205696 Scheduler overhead time: 0.045435893815010786 Adapter cache time: 0.039767296984791756 Engine time: 0.04535696282982826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 8640, 270, 17280, 17280, 270, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 270, 270, 8640, 270, 17280, 270, 17280, 8640, 17280, 8640, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 17280, 17280, 270, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 17280, 8640, 8640, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 270, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 8640, 270, 17280, 8640, 8640, 270, 17280, 17280, 17280, 270, 8640, 17280, 270, 17280, 8640, 8640, 17280, 17280, 270, 8640, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 17280, 17280, 8640, 270, 17280, 270, 17280, 8640, 8640, 8640, 8640, 8640, 270, 270, 17280, 270, 17280, 270, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 17280, 270, 8640, 8640, 270, 270, 8640, 270, 17280, 17280, 270, 17280, 17280, 270, 8640, 17280, 270, 270, 8640, 8640, 270, 270, 17280, 17280, 17280, 17280, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 8640, 270, 17280, 17280, 17280, 8640, 270, 17280, 17280, 8640, 8640, 17280, 17280, 270, 17280, 17280, 8640, 270, 17280, 8640, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 8640, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 17280, 270, 270, 270, 8640, 270, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 270, 8640, 8640, 8640, 8640, 270, 8640, 17280, 17280, 270, 17280, 17280, 270, 17280, 8640, 17280, 8640, 8640, 8640, 270, 8640, 270, 270, 17280, 270, 8640, 270, 270, 8640, 17280, 8640, 17280, 270, 17280, 17280, 8640, 17280, 8640, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 3352320 . Total input tokens: 747931339 . Total output tokens: 658544234
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.023687021806836,
    "estimated_duration": 3600.0564541122762,
    "input_throughput": 3907.520945659392,
    "output_throughput": 3397.7739393579272,
    "total_throughput": 7305.29488501732,
    "itl": 100.17531486836155,
    "ttft": 2289054.775972842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.322462704480547,
    "arrivals": 1116080,
    "finished_requests": 56976,
    "scheduler_time": 174.52355573881883
}
#Debug simulation 
Total elapsed time: 7.02378640556708. Arrivals time: 0.2327982448041439 Scheduler time: 6.600388573948294 Scheduler overhead time: 0.051814448554068804 Adapter cache time: 0.06301299016922712 Engine time: 0.0516751310788095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 16.07519878819585,
    "estimated_duration": 3600.148055361919,
    "input_throughput": 4626.25084965448,
    "output_throughput": 4021.9759791363276,
    "total_throughput": 8648.226828790806,
    "itl": 135.6650360860242,
    "ttft": 2201633.896280674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.297047098921844,
    "arrivals": 1110230,
    "finished_requests": 67330,
    "scheduler_time": 150.98200403589323
}
#Debug simulation 
Total elapsed time: 16.07530022226274. Arrivals time: 0.28648516023531556 Scheduler time: 15.649828767869622 Scheduler overhead time: 0.04556329781189561 Adapter cache time: 0.029084902722388506 Engine time: 0.044671001844108105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 11.39775337697938,
    "estimated_duration": 3600.0309993862056,
    "input_throughput": 4353.649455427535,
    "output_throughput": 3782.879925845614,
    "total_throughput": 8136.529381273149,
    "itl": 119.16308768461425,
    "ttft": 2229491.1570814317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1742,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.742232375303113,
    "arrivals": 1110230,
    "finished_requests": 63279,
    "scheduler_time": 160.30500442714813
}
#Debug simulation 
Total elapsed time: 11.397851853165776. Arrivals time: 0.2626787284389138 Scheduler time: 10.98493775492534 Scheduler overhead time: 0.047049143351614475 Adapter cache time: 0.03475493472069502 Engine time: 0.04683200968429446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.087721988093108,
    "estimated_duration": 3600.012211730465,
    "input_throughput": 3922.708915815563,
    "output_throughput": 3409.1459356746436,
    "total_throughput": 7331.854851490207,
    "itl": 100.66144043449974,
    "ttft": 2280681.1689298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.86689332626838,
    "arrivals": 1110230,
    "finished_requests": 57001,
    "scheduler_time": 174.08864167800243
}
#Debug simulation 
Total elapsed time: 7.087817148771137. Arrivals time: 0.23521673260256648 Scheduler time: 6.664637862239033 Scheduler overhead time: 0.05160343227908015 Adapter cache time: 0.06069969339296222 Engine time: 0.051713728811591864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 11.390001574065536,
    "estimated_duration": 3600.019839044126,
    "input_throughput": 4354.753779402115,
    "output_throughput": 3783.6399822776207,
    "total_throughput": 8138.393761679736,
    "itl": 119.13712231270763,
    "ttft": 2229440.1652315194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1742,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.930123105915422,
    "arrivals": 1110230,
    "finished_requests": 63291,
    "scheduler_time": 160.33891579897917
}
#Debug simulation 
Total elapsed time: 11.39010332711041. Arrivals time: 0.26416708854958415 Scheduler time: 10.97668891493231 Scheduler overhead time: 0.04681561794131994 Adapter cache time: 0.0345359961502254 Engine time: 0.04664960550144315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 7.050665638875216,
    "estimated_duration": 3600.0034535297855,
    "input_throughput": 3922.9720699719746,
    "output_throughput": 3409.283673871468,
    "total_throughput": 7332.255743843442,
    "itl": 100.65376762245404,
    "ttft": 2280736.466554022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.64196257595049,
    "arrivals": 1110230,
    "finished_requests": 57005,
    "scheduler_time": 174.09903222664903
}
#Debug simulation 
Total elapsed time: 7.050758828874677. Arrivals time: 0.23291428294032812 Scheduler time: 6.631219362374395 Scheduler overhead time: 0.051209183409810066 Adapter cache time: 0.06037871399894357 Engine time: 0.05101811978965998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 11.374814925715327,
    "estimated_duration": 3600.01101617643,
    "input_throughput": 4355.159728553347,
    "output_throughput": 3784.075642765304,
    "total_throughput": 8139.235371318651,
    "itl": 119.11180249990858,
    "ttft": 2229274.9045004025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1742,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.120790278474244,
    "arrivals": 1110230,
    "finished_requests": 63299,
    "scheduler_time": 160.3728944880947
}
#Debug simulation 
Total elapsed time: 11.374913766048849. Arrivals time: 0.2631284692324698 Scheduler time: 10.963294812478125 Scheduler overhead time: 0.0466727246530354 Adapter cache time: 0.03425827110186219 Engine time: 0.04624937754124403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 8640, 135, 17280, 17280, 135, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 135, 135, 8640, 135, 17280, 135, 17280, 8640, 17280, 8640, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 17280, 17280, 135, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 17280, 8640, 8640, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 135, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 8640, 135, 17280, 8640, 8640, 135, 17280, 17280, 17280, 135, 8640, 17280, 135, 17280, 8640, 8640, 17280, 17280, 135, 8640, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 17280, 17280, 8640, 135, 17280, 135, 17280, 8640, 8640, 8640, 8640, 8640, 135, 135, 17280, 135, 17280, 135, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 17280, 135, 8640, 8640, 135, 135, 8640, 135, 17280, 17280, 135, 17280, 17280, 135, 8640, 17280, 135, 135, 8640, 8640, 135, 135, 17280, 17280, 17280, 17280, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 8640, 135, 17280, 17280, 17280, 8640, 135, 17280, 17280, 8640, 8640, 17280, 17280, 135, 17280, 17280, 8640, 135, 17280, 8640, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 8640, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 17280, 135, 135, 135, 8640, 135, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 135, 8640, 8640, 8640, 8640, 135, 8640, 17280, 17280, 135, 17280, 17280, 135, 17280, 8640, 17280, 8640, 8640, 8640, 135, 8640, 135, 135, 17280, 135, 8640, 135, 135, 8640, 17280, 8640, 17280, 135, 17280, 17280, 8640, 17280, 8640, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 3335040 . Total input tokens: 744104716 . Total output tokens: 655166751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.073504852131009,
    "estimated_duration": 3600.0797748122072,
    "input_throughput": 3923.240562283583,
    "output_throughput": 3409.596666685059,
    "total_throughput": 7332.837228968642,
    "itl": 100.64757238951313,
    "ttft": 2280639.692066713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.3959073701322,
    "arrivals": 1110230,
    "finished_requests": 57010,
    "scheduler_time": 174.1147608527811
}
#Debug simulation 
Total elapsed time: 7.0736009338870645. Arrivals time: 0.23093964299187064 Scheduler time: 6.655932962428778 Scheduler overhead time: 0.051372861955314875 Adapter cache time: 0.06029943795874715 Engine time: 0.05123618012294173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 16.263502629008144,
    "estimated_duration": 3600.137477571655,
    "input_throughput": 4593.1140416208955,
    "output_throughput": 4005.6059219513454,
    "total_throughput": 8598.71996357224,
    "itl": 134.48239613992442,
    "ttft": 2199675.9446356436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.637332431902356,
    "arrivals": 1107373,
    "finished_requests": 66946,
    "scheduler_time": 151.79023471439325
}
#Debug simulation 
Total elapsed time: 16.263603621162474. Arrivals time: 0.2994561195373535 Scheduler time: 15.827770629432052 Scheduler overhead time: 0.045709265395998955 Adapter cache time: 0.02573490748181939 Engine time: 0.04493157146498561 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 12.025519373826683,
    "estimated_duration": 3600.0766444060873,
    "input_throughput": 4441.454052053338,
    "output_throughput": 3872.608662835841,
    "total_throughput": 8314.06271488918,
    "itl": 125.08607102567015,
    "ttft": 2216401.368437934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.808479933151839,
    "arrivals": 1107373,
    "finished_requests": 64710,
    "scheduler_time": 156.80938602555904
}
#Debug simulation 
Total elapsed time: 12.025628752075136. Arrivals time: 0.2886007269844413 Scheduler time: 11.590295935515314 Scheduler overhead time: 0.046218839939683676 Adapter cache time: 0.03306589415296912 Engine time: 0.04657089849933982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.032372137065977,
    "estimated_duration": 3600.067470501786,
    "input_throughput": 3914.0766431346833,
    "output_throughput": 3421.5417074622546,
    "total_throughput": 7335.6183505969375,
    "itl": 101.3764913048218,
    "ttft": 2276798.5251944726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.607047005384878,
    "arrivals": 1107373,
    "finished_requests": 57225,
    "scheduler_time": 173.68708354789126
}
#Debug simulation 
Total elapsed time: 7.032470017205924. Arrivals time: 0.2389832423068583 Scheduler time: 6.60845793876797 Scheduler overhead time: 0.051649834495037794 Adapter cache time: 0.05868009012192488 Engine time: 0.05090218689292669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 11.744996543042362,
    "estimated_duration": 3600.092195740617,
    "input_throughput": 4441.286814520234,
    "output_throughput": 3873.9327333062643,
    "total_throughput": 8315.219547826498,
    "itl": 125.14092574517684,
    "ttft": 2216845.2399592223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.647294657314333,
    "arrivals": 1107373,
    "finished_requests": 64762,
    "scheduler_time": 156.76454516754353
}
#Debug simulation 
Total elapsed time: 11.745101084932685. Arrivals time: 0.27734068408608437 Scheduler time: 11.321562624536455 Scheduler overhead time: 0.045665419194847345 Adapter cache time: 0.0341803845949471 Engine time: 0.045541425701230764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 7.06067720009014,
    "estimated_duration": 3600.057158009649,
    "input_throughput": 3914.561458738436,
    "output_throughput": 3422.087333416438,
    "total_throughput": 7336.648792154874,
    "itl": 101.37058079234814,
    "ttft": 2276751.6476684534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.387917032526502,
    "arrivals": 1107373,
    "finished_requests": 57232,
    "scheduler_time": 173.69737577706314
}
#Debug simulation 
Total elapsed time: 7.060778352897614. Arrivals time: 0.24388954369351268 Scheduler time: 6.632776110898703 Scheduler overhead time: 0.0514748333953321 Adapter cache time: 0.05790868354961276 Engine time: 0.05086652748286724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 11.699147333856672,
    "estimated_duration": 3600.086588994548,
    "input_throughput": 4442.262041387866,
    "output_throughput": 3874.652360485523,
    "total_throughput": 8316.91440187339,
    "itl": 125.11356934546319,
    "ttft": 2216471.0631669248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.814362073326818,
    "arrivals": 1107373,
    "finished_requests": 64775,
    "scheduler_time": 156.79901292289526
}
#Debug simulation 
Total elapsed time: 11.699251954909414. Arrivals time: 0.2788576534949243 Scheduler time: 11.274420706089586 Scheduler overhead time: 0.04565069219097495 Adapter cache time: 0.03414460737258196 Engine time: 0.04533907352015376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 8640, 66, 17280, 17280, 66, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 66, 66, 8640, 66, 17280, 66, 17280, 8640, 17280, 8640, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 17280, 17280, 66, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 17280, 8640, 8640, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 66, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 8640, 66, 17280, 8640, 8640, 66, 17280, 17280, 17280, 66, 8640, 17280, 66, 17280, 8640, 8640, 17280, 17280, 66, 8640, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 17280, 17280, 8640, 66, 17280, 66, 17280, 8640, 8640, 8640, 8640, 8640, 66, 66, 17280, 66, 17280, 66, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 17280, 66, 8640, 8640, 66, 66, 8640, 66, 17280, 17280, 66, 17280, 17280, 66, 8640, 17280, 66, 66, 8640, 8640, 66, 66, 17280, 17280, 17280, 17280, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 8640, 66, 17280, 17280, 17280, 8640, 66, 17280, 17280, 8640, 8640, 17280, 17280, 66, 17280, 17280, 8640, 66, 17280, 8640, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 8640, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 17280, 66, 66, 66, 8640, 66, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 66, 8640, 8640, 8640, 8640, 66, 8640, 17280, 17280, 66, 17280, 17280, 66, 17280, 8640, 17280, 8640, 8640, 8640, 66, 8640, 66, 66, 17280, 66, 8640, 66, 66, 8640, 17280, 8640, 17280, 66, 17280, 17280, 8640, 17280, 8640, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 3326208 . Total input tokens: 742175848 . Total output tokens: 653399524
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.035032014362514,
    "estimated_duration": 3600.0304910141676,
    "input_throughput": 3914.849620073547,
    "output_throughput": 3422.3957354675395,
    "total_throughput": 7337.245355541087,
    "itl": 101.36461862903306,
    "ttft": 2276832.6922017327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.152010568362332,
    "arrivals": 1107373,
    "finished_requests": 57236,
    "scheduler_time": 173.70758370935158
}
#Debug simulation 
Total elapsed time: 7.03512525325641. Arrivals time: 0.23843775736168027 Scheduler time: 6.612651992589235 Scheduler overhead time: 0.05123536754399538 Adapter cache time: 0.057914883363991976 Engine time: 0.05092871421948075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 14.285709410905838,
    "estimated_duration": 3600.147911812701,
    "input_throughput": 4613.479892173969,
    "output_throughput": 4033.90287169833,
    "total_throughput": 8647.3827638723,
    "itl": 136.52681911398395,
    "ttft": 2201724.5988383996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.039163146675788,
    "arrivals": 1105887,
    "finished_requests": 67439,
    "scheduler_time": 150.77888333682316
}
#Debug simulation 
Total elapsed time: 14.285843899007887. Arrivals time: 0.2875089203007519 Scheduler time: 13.862272608559579 Scheduler overhead time: 0.04434303939342499 Adapter cache time: 0.028362158220261335 Engine time: 0.04375186515972018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 11.167109346017241,
    "estimated_duration": 3600.0196616874446,
    "input_throughput": 4434.647168709289,
    "output_throughput": 3874.443283863092,
    "total_throughput": 8309.090452572382,
    "itl": 125.32825436628725,
    "ttft": 2220173.259021708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.216512644789026,
    "arrivals": 1105887,
    "finished_requests": 64713,
    "scheduler_time": 156.69608245934305
}
#Debug simulation 
Total elapsed time: 11.167221021838486. Arrivals time: 0.27196167036890984 Scheduler time: 10.750766352750361 Scheduler overhead time: 0.045632222667336464 Adapter cache time: 0.033343675546348095 Engine time: 0.04496499942615628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.949821728747338,
    "estimated_duration": 3600.0110276567716,
    "input_throughput": 3916.515502780915,
    "output_throughput": 3421.1103536581227,
    "total_throughput": 7337.625856439038,
    "itl": 101.41908412550714,
    "ttft": 2281029.3980381815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.191941901078202,
    "arrivals": 1105887,
    "finished_requests": 57097,
    "scheduler_time": 173.65928035289622
}
#Debug simulation 
Total elapsed time: 6.949931615963578. Arrivals time: 0.2364485813304782 Scheduler time: 6.527705939952284 Scheduler overhead time: 0.051108489744365215 Adapter cache time: 0.05923984572291374 Engine time: 0.05139103764668107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 11.207619443070143,
    "estimated_duration": 3600.1064957404433,
    "input_throughput": 4436.022384030997,
    "output_throughput": 3875.38841323373,
    "total_throughput": 8311.410797264727,
    "itl": 125.3015601160036,
    "ttft": 2220199.469818903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.486308412826741,
    "arrivals": 1105887,
    "finished_requests": 64732,
    "scheduler_time": 156.73089464348556
}
#Debug simulation 
Total elapsed time: 11.207732561975718. Arrivals time: 0.2716057295911014 Scheduler time: 10.790643900632858 Scheduler overhead time: 0.046110743191093206 Adapter cache time: 0.033388884738087654 Engine time: 0.045307112857699394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 6.959603693336248,
    "estimated_duration": 3600.0133182223576,
    "input_throughput": 3916.513010835793,
    "output_throughput": 3421.108176922386,
    "total_throughput": 7337.621187758179,
    "itl": 101.41293077913323,
    "ttft": 2280952.2459224425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.96949805339399,
    "arrivals": 1105887,
    "finished_requests": 57097,
    "scheduler_time": 173.66996344604075
}
#Debug simulation 
Total elapsed time: 6.959697778336704. Arrivals time: 0.2352813216857612 Scheduler time: 6.537860410287976 Scheduler overhead time: 0.051572644617408514 Adapter cache time: 0.0596974641084671 Engine time: 0.05138611886650324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 11.793536860961467,
    "estimated_duration": 3600.0285805615317,
    "input_throughput": 4437.855878774163,
    "output_throughput": 3877.1622745902896,
    "total_throughput": 8315.018153364454,
    "itl": 125.24807833326301,
    "ttft": 2221357.7215456567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1663,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.616460524169105,
    "arrivals": 1105887,
    "finished_requests": 64806,
    "scheduler_time": 156.76273394613435
}
#Debug simulation 
Total elapsed time: 11.793662648648024. Arrivals time: 0.2717173914425075 Scheduler time: 11.376931821461767 Scheduler overhead time: 0.04532265616580844 Adapter cache time: 0.03367122448980808 Engine time: 0.04533647187054157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_384_slots_96_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [128 128 128]
Adapter prompts. [33, 33, 17280, 8640, 33, 17280, 17280, 33, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 8640, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 33, 33, 8640, 33, 17280, 33, 17280, 8640, 17280, 8640, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 17280, 17280, 33, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 17280, 8640, 8640, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 17280, 17280, 33, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 8640, 33, 17280, 8640, 8640, 33, 17280, 17280, 17280, 33, 8640, 17280, 33, 17280, 8640, 8640, 17280, 17280, 33, 8640, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 17280, 17280, 8640, 33, 17280, 33, 17280, 8640, 8640, 8640, 8640, 8640, 33, 33, 17280, 33, 17280, 33, 8640, 17280, 8640, 17280, 8640, 17280, 8640, 17280, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 17280, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 17280, 33, 8640, 8640, 33, 33, 8640, 33, 17280, 17280, 33, 17280, 17280, 33, 8640, 17280, 33, 33, 8640, 8640, 33, 33, 17280, 17280, 17280, 17280, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 33, 33, 8640, 33, 17280, 17280, 17280, 8640, 33, 17280, 17280, 8640, 8640, 17280, 17280, 33, 17280, 17280, 8640, 33, 17280, 8640, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 33, 17280, 33, 8640, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 17280, 33, 33, 33, 8640, 33, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 33, 8640, 8640, 8640, 8640, 33, 8640, 17280, 17280, 33, 17280, 17280, 33, 17280, 8640, 17280, 8640, 8640, 8640, 33, 8640, 33, 33, 17280, 33, 8640, 33, 33, 8640, 17280, 8640, 17280, 33, 17280, 17280, 8640, 17280, 8640, 33, 17280, 33, 17280, 17280, 33, 33, 33]
Prompts retrieved: 3321984 . Total input tokens: 741254487 . Total output tokens: 652564258
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.946524908300489,
    "estimated_duration": 3600.10086949304,
    "input_throughput": 3916.7758102249086,
    "output_throughput": 3421.3508027997245,
    "total_throughput": 7338.126613024633,
    "itl": 101.40677194164195,
    "ttft": 2280883.251458798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.721578792986175,
    "arrivals": 1105887,
    "finished_requests": 57102,
    "scheduler_time": 173.68596023228437
}
#Debug simulation 
Total elapsed time: 6.946619043126702. Arrivals time: 0.23071350902318954 Scheduler time: 6.53067062003538 Scheduler overhead time: 0.05145442998036742 Adapter cache time: 0.058788498397916555 Engine time: 0.05119555816054344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 18.829505336936563,
    "estimated_duration": 3600.1406472948615,
    "input_throughput": 4578.179192078123,
    "output_throughput": 4024.6083193686122,
    "total_throughput": 8602.787511446735,
    "itl": 135.98153897136248,
    "ttft": 2195446.47559579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.826039821477856,
    "arrivals": 966726,
    "finished_requests": 67047,
    "scheduler_time": 150.66446280919246
}
#Debug simulation 
Total elapsed time: 18.829640358686447. Arrivals time: 0.2947223586961627 Scheduler time: 18.391207894310355 Scheduler overhead time: 0.04614834953099489 Adapter cache time: 0.03189285472035408 Engine time: 0.04580252757295966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 13.118200177326798,
    "estimated_duration": 3600.0379153488575,
    "input_throughput": 4396.914524847748,
    "output_throughput": 3863.178757286033,
    "total_throughput": 8260.093282133781,
    "itl": 124.69033756651265,
    "ttft": 2217029.520997082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.390266121499907,
    "arrivals": 966726,
    "finished_requests": 64336,
    "scheduler_time": 156.6480960686003
}
#Debug simulation 
Total elapsed time: 13.118299254216254. Arrivals time: 0.33713149605318904 Scheduler time: 12.627089145127684 Scheduler overhead time: 0.04609516728669405 Adapter cache time: 0.041291818022727966 Engine time: 0.0458889021538198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.441154042258859,
    "estimated_duration": 3600.0296737969456,
    "input_throughput": 3861.6573361012042,
    "output_throughput": 3402.51223181748,
    "total_throughput": 7264.169567918684,
    "itl": 100.59511999956653,
    "ttft": 2279807.9329460696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.73259514989907,
    "arrivals": 966726,
    "finished_requests": 56490,
    "scheduler_time": 173.9254265089102
}
#Debug simulation 
Total elapsed time: 7.441252523101866. Arrivals time: 0.23417505202814937 Scheduler time: 7.012993630487472 Scheduler overhead time: 0.05191315012052655 Adapter cache time: 0.06674369191750884 Engine time: 0.05134492926299572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 13.564863408915699,
    "estimated_duration": 3600.023871329785,
    "input_throughput": 4388.317012510387,
    "output_throughput": 3863.256327481712,
    "total_throughput": 8251.5733399921,
    "itl": 124.57757595701545,
    "ttft": 2218129.483355116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2030,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.886691119531424,
    "arrivals": 966726,
    "finished_requests": 64374,
    "scheduler_time": 156.72928011950663
}
#Debug simulation 
Total elapsed time: 13.564965795725584. Arrivals time: 0.2755284318700433 Scheduler time: 13.134485445450991 Scheduler overhead time: 0.04672739701345563 Adapter cache time: 0.041658911388367414 Engine time: 0.045946897473186255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 7.4455635021440685,
    "estimated_duration": 3600.0951423605434,
    "input_throughput": 3861.6834973101095,
    "output_throughput": 3402.4514674265984,
    "total_throughput": 7264.134964736708,
    "itl": 100.58606737995672,
    "ttft": 2279791.3831262533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.46624246077255,
    "arrivals": 966726,
    "finished_requests": 56492,
    "scheduler_time": 173.9413180107031
}
#Debug simulation 
Total elapsed time: 7.445659315213561. Arrivals time: 0.23528385814279318 Scheduler time: 7.015775708481669 Scheduler overhead time: 0.051912566646933556 Adapter cache time: 0.06699515413492918 Engine time: 0.05161904124543071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 13.600243898574263,
    "estimated_duration": 3600.078605559188,
    "input_throughput": 4389.651374721958,
    "output_throughput": 3864.437564923406,
    "total_throughput": 8254.088939645364,
    "itl": 124.54809556525571,
    "ttft": 2218212.536693522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.978511272180517,
    "arrivals": 966726,
    "finished_requests": 64396,
    "scheduler_time": 156.76978077803128
}
#Debug simulation 
Total elapsed time: 13.600363438948989. Arrivals time: 0.2743280488066375 Scheduler time: 13.170508571900427 Scheduler overhead time: 0.04684922285377979 Adapter cache time: 0.04150887252762914 Engine time: 0.04622758133336902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 17280, 4320, 1080, 17280, 17280, 1080, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 1080, 17280, 4320, 17280, 4320, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 17280, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 4320, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 1080, 17280, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 17280, 1080, 17280, 1080, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 17280, 1080, 4320, 17280, 1080, 1080, 4320, 4320, 1080, 1080, 17280, 17280, 17280, 17280, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 17280, 17280, 1080, 17280, 17280, 4320, 1080, 17280, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 17280, 17280, 1080, 17280, 17280, 1080, 17280, 4320, 17280, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 17280, 4320, 17280, 1080, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 1080]
Prompts retrieved: 2903040 . Total input tokens: 647816560 . Total output tokens: 570179533
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 7.50791248306632,
    "estimated_duration": 3600.0540993204495,
    "input_throughput": 3861.856410053484,
    "output_throughput": 3402.5855340096773,
    "total_throughput": 7264.441944063162,
    "itl": 100.57925411067178,
    "ttft": 2279711.8121945956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.205067701061388,
    "arrivals": 966726,
    "finished_requests": 56494,
    "scheduler_time": 173.95178621218457
}
#Debug simulation 
Total elapsed time: 7.5080062802881. Arrivals time: 0.2359107118099928 Scheduler time: 7.07794728083536 Scheduler overhead time: 0.05174187244847417 Adapter cache time: 0.06702192313969135 Engine time: 0.051285638473927975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 15.961985798086971,
    "estimated_duration": 3600.1532595827293,
    "input_throughput": 4590.049314154835,
    "output_throughput": 4018.2092141479225,
    "total_throughput": 8608.258528302757,
    "itl": 135.66734631125482,
    "ttft": 2192008.934707125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.87741285755793,
    "arrivals": 943309,
    "finished_requests": 66811,
    "scheduler_time": 150.78317616274833
}
#Debug simulation 
Total elapsed time: 15.962086894083768. Arrivals time: 0.3622999698854983 Scheduler time: 15.455434346571565 Scheduler overhead time: 0.04565931856632233 Adapter cache time: 0.03411750681698322 Engine time: 0.04468477191403508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 11.58435020269826,
    "estimated_duration": 3600.0543618587767,
    "input_throughput": 4397.115823502938,
    "output_throughput": 3858.1206292753354,
    "total_throughput": 8255.236452778274,
    "itl": 124.52001949658164,
    "ttft": 2210962.095475571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.514988485625654,
    "arrivals": 943309,
    "finished_requests": 64014,
    "scheduler_time": 156.6837922104987
}
#Debug simulation 
Total elapsed time: 11.584477589000016. Arrivals time: 0.36082438100129366 Scheduler time: 11.067693553864956 Scheduler overhead time: 0.0460368855856359 Adapter cache time: 0.04396133776754141 Engine time: 0.04520117212086916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.947284623980522,
    "estimated_duration": 3600.028374096905,
    "input_throughput": 3867.48062881385,
    "output_throughput": 3396.5982290535285,
    "total_throughput": 7264.078857867378,
    "itl": 100.53241035601756,
    "ttft": 2274335.743209314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.144482989084317,
    "arrivals": 943309,
    "finished_requests": 56297,
    "scheduler_time": 173.89135167412846
}
#Debug simulation 
Total elapsed time: 6.947395766153932. Arrivals time: 0.22515857312828302 Scheduler time: 6.526776860933751 Scheduler overhead time: 0.051624213345348835 Adapter cache time: 0.06850184686481953 Engine time: 0.051337799057364464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 11.537350337021053,
    "estimated_duration": 3600.1166719359226,
    "input_throughput": 4398.216069893478,
    "output_throughput": 3859.2929802268623,
    "total_throughput": 8257.50905012034,
    "itl": 124.48657691172492,
    "ttft": 2210825.5199770695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.475210976614926,
    "arrivals": 943309,
    "finished_requests": 64031,
    "scheduler_time": 156.7307045720786
}
#Debug simulation 
Total elapsed time: 11.537444564979523. Arrivals time: 0.32904080767184496 Scheduler time: 11.052611660212278 Scheduler overhead time: 0.046046541072428226 Adapter cache time: 0.04334441106766462 Engine time: 0.04552668146789074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 6.938004873692989,
    "estimated_duration": 3600.0788007538386,
    "input_throughput": 3867.9656114983313,
    "output_throughput": 3397.243137411055,
    "total_throughput": 7265.2087489093865,
    "itl": 100.52499478104798,
    "ttft": 2274378.3311438086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.869224261363346,
    "arrivals": 943309,
    "finished_requests": 56305,
    "scheduler_time": 173.90719523599066
}
#Debug simulation 
Total elapsed time: 6.938101675827056. Arrivals time: 0.2257218318991363 Scheduler time: 6.517178964801133 Scheduler overhead time: 0.051520487293601036 Adapter cache time: 0.06866912310943007 Engine time: 0.0509673161432147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 11.58720734110102,
    "estimated_duration": 3600.0324093478916,
    "input_throughput": 4398.581512455981,
    "output_throughput": 3859.6288644292777,
    "total_throughput": 8258.21037688526,
    "itl": 124.45340240363312,
    "ttft": 2210428.797880549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.434045246630793,
    "arrivals": 943309,
    "finished_requests": 64036,
    "scheduler_time": 156.77093219070744
}
#Debug simulation 
Total elapsed time: 11.587301819119602. Arrivals time: 0.32959682820364833 Scheduler time: 11.101573723368347 Scheduler overhead time: 0.04596746526658535 Adapter cache time: 0.04373277956619859 Engine time: 0.04579396639019251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 17280, 4320, 540, 17280, 17280, 540, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 540, 540, 4320, 540, 17280, 540, 17280, 4320, 17280, 4320, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 17280, 17280, 540, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 17280, 4320, 4320, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 540, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 4320, 540, 17280, 4320, 4320, 540, 17280, 17280, 17280, 540, 4320, 17280, 540, 17280, 4320, 4320, 17280, 17280, 540, 4320, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 17280, 17280, 4320, 540, 17280, 540, 17280, 4320, 4320, 4320, 4320, 4320, 540, 540, 17280, 540, 17280, 540, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 17280, 540, 4320, 4320, 540, 540, 4320, 540, 17280, 17280, 540, 17280, 17280, 540, 4320, 17280, 540, 540, 4320, 4320, 540, 540, 17280, 17280, 17280, 17280, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 540, 540, 4320, 540, 17280, 17280, 17280, 4320, 540, 17280, 17280, 4320, 4320, 17280, 17280, 540, 17280, 17280, 4320, 540, 17280, 4320, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 17280, 540, 4320, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 17280, 540, 540, 540, 4320, 540, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 540, 4320, 4320, 4320, 4320, 540, 4320, 17280, 17280, 540, 17280, 17280, 540, 17280, 4320, 17280, 4320, 4320, 4320, 540, 4320, 540, 540, 17280, 540, 4320, 540, 540, 4320, 17280, 4320, 17280, 540, 17280, 17280, 4320, 17280, 4320, 540, 17280, 540, 17280, 17280, 540, 540, 540]
Prompts retrieved: 2833920 . Total input tokens: 632417434 . Total output tokens: 556558944
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.979788653086871,
    "estimated_duration": 3600.0346789987725,
    "input_throughput": 3868.1788487306812,
    "output_throughput": 3397.4833829660924,
    "total_throughput": 7265.662231696774,
    "itl": 100.517754424571,
    "ttft": 2274338.615252818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4020,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.607425157093733,
    "arrivals": 943309,
    "finished_requests": 56308,
    "scheduler_time": 173.91757172966723
}
#Debug simulation 
Total elapsed time: 6.979894790332764. Arrivals time: 0.29807721776887774 Scheduler time: 6.48619243549183 Scheduler overhead time: 0.051282322965562344 Adapter cache time: 0.06884285528212786 Engine time: 0.05137344775721431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 15.441117680165917,
    "estimated_duration": 3600.121223968317,
    "input_throughput": 4676.894180091136,
    "output_throughput": 4028.4426822755327,
    "total_throughput": 8705.336862366668,
    "itl": 136.5118143442211,
    "ttft": 2182911.538651805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1616,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.685652995631376,
    "arrivals": 931969,
    "finished_requests": 67813,
    "scheduler_time": 150.35238973804738
}
#Debug simulation 
Total elapsed time: 15.441257399972528. Arrivals time: 0.2937898742966354 Scheduler time: 15.003873087931424 Scheduler overhead time: 0.04558161273598671 Adapter cache time: 0.03380516730248928 Engine time: 0.0443993229418993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 13.148752672132105,
    "estimated_duration": 3600.025842584916,
    "input_throughput": 4471.533456671373,
    "output_throughput": 3860.060901680102,
    "total_throughput": 8331.594358351475,
    "itl": 124.27782671415501,
    "ttft": 2205652.3184162555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.004813648597171,
    "arrivals": 931969,
    "finished_requests": 64964,
    "scheduler_time": 156.90928678563475
}
#Debug simulation 
Total elapsed time: 13.14885199116543. Arrivals time: 0.276611543726176 Scheduler time: 12.721557367127389 Scheduler overhead time: 0.04667177563533187 Adapter cache time: 0.03669637814164162 Engine time: 0.04638664145022631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.938961676321924,
    "estimated_duration": 3600.070009172492,
    "input_throughput": 3951.0876076739287,
    "output_throughput": 3410.212570511088,
    "total_throughput": 7361.300178185016,
    "itl": 100.91111096230831,
    "ttft": 2268303.262449567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3806,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.61861533087787,
    "arrivals": 931969,
    "finished_requests": 57371,
    "scheduler_time": 173.56861644347796
}
#Debug simulation 
Total elapsed time: 6.939054941292852. Arrivals time: 0.2958757453598082 Scheduler time: 6.449767295271158 Scheduler overhead time: 0.05170412827283144 Adapter cache time: 0.06592447683215141 Engine time: 0.05167026072740555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 13.192456395365298,
    "estimated_duration": 3600.061898653879,
    "input_throughput": 4480.135468234811,
    "output_throughput": 3869.9309601341565,
    "total_throughput": 8350.066428368968,
    "itl": 124.93206597570119,
    "ttft": 2204247.8497013836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1881,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.896616712133591,
    "arrivals": 931969,
    "finished_requests": 65107,
    "scheduler_time": 156.51257927098123
}
#Debug simulation 
Total elapsed time: 13.192566353362054. Arrivals time: 0.284916864708066 Scheduler time: 12.756480100564659 Scheduler overhead time: 0.0469634011387825 Adapter cache time: 0.03712699515745044 Engine time: 0.046232196502387524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 6.900684035848826,
    "estimated_duration": 3600.0320277580718,
    "input_throughput": 3951.3437353663294,
    "output_throughput": 3410.4168811092245,
    "total_throughput": 7361.760616475553,
    "itl": 100.90427367135241,
    "ttft": 2268288.1444235328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3806,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.358061922696553,
    "arrivals": 931969,
    "finished_requests": 57373,
    "scheduler_time": 173.57916225649345
}
#Debug simulation 
Total elapsed time: 6.900794525630772. Arrivals time: 0.23975897021591663 Scheduler time: 6.4678187388926744 Scheduler overhead time: 0.05173823703080416 Adapter cache time: 0.06578249717131257 Engine time: 0.05148575780913234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 13.232350339647382,
    "estimated_duration": 3600.138379586301,
    "input_throughput": 4481.404406975587,
    "output_throughput": 3871.097588643653,
    "total_throughput": 8352.501995619241,
    "itl": 124.90326149842842,
    "ttft": 2204124.2455839314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1881,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.008155289213667,
    "arrivals": 931969,
    "finished_requests": 65124,
    "scheduler_time": 156.55274554548637
}
#Debug simulation 
Total elapsed time: 13.232480405829847. Arrivals time: 0.33815727988258004 Scheduler time: 12.742784459143877 Scheduler overhead time: 0.04701974196359515 Adapter cache time: 0.03697704756632447 Engine time: 0.04674301389604807 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 17280, 4320, 270, 17280, 17280, 270, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 270, 270, 4320, 270, 17280, 270, 17280, 4320, 17280, 4320, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 17280, 17280, 270, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 270, 17280, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 17280, 4320, 4320, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 270, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 4320, 270, 17280, 4320, 4320, 270, 17280, 17280, 17280, 270, 4320, 17280, 270, 17280, 4320, 4320, 17280, 17280, 270, 4320, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 17280, 17280, 4320, 270, 17280, 270, 17280, 4320, 4320, 4320, 4320, 4320, 270, 270, 17280, 270, 17280, 270, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 17280, 270, 4320, 4320, 270, 270, 4320, 270, 17280, 17280, 270, 17280, 17280, 270, 4320, 17280, 270, 270, 4320, 4320, 270, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 270, 270, 4320, 270, 17280, 17280, 17280, 4320, 270, 17280, 17280, 4320, 4320, 17280, 17280, 270, 17280, 17280, 4320, 270, 17280, 4320, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 17280, 270, 4320, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 17280, 270, 270, 270, 4320, 270, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 270, 4320, 4320, 4320, 4320, 270, 4320, 17280, 17280, 270, 17280, 17280, 270, 17280, 4320, 17280, 4320, 4320, 4320, 270, 4320, 270, 270, 17280, 270, 4320, 270, 270, 4320, 17280, 4320, 17280, 270, 17280, 17280, 4320, 17280, 4320, 270, 17280, 270, 17280, 17280, 270, 270, 270]
Prompts retrieved: 2799360 . Total input tokens: 624712026 . Total output tokens: 549802560
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.87044075364247,
    "estimated_duration": 3600.1104696076654,
    "input_throughput": 3951.382636752884,
    "output_throughput": 3410.5464550753286,
    "total_throughput": 7361.929091828212,
    "itl": 100.89574724403434,
    "ttft": 2268313.2412615586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.099887189642637,
    "arrivals": 931969,
    "finished_requests": 57376,
    "scheduler_time": 173.59512376298946
}
#Debug simulation 
Total elapsed time: 6.870537705719471. Arrivals time: 0.2310713091865182 Scheduler time: 6.446731010451913 Scheduler overhead time: 0.05141064338386059 Adapter cache time: 0.06576128816232085 Engine time: 0.05150392418727279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 32.63386113103479,
    "estimated_duration": 3600.0552125079676,
    "input_throughput": 4053.7239399262967,
    "output_throughput": 3534.7510659801683,
    "total_throughput": 7588.475005906465,
    "itl": 105.26837289829933,
    "ttft": 2250495.10389778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 470,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1078322450165037,
    "arrivals": 926257,
    "finished_requests": 58941,
    "scheduler_time": 170.69923205654155
}
#Debug simulation 
Total elapsed time: 32.634012481197715. Arrivals time: 0.31546403001993895 Scheduler time: 32.15327596385032 Scheduler overhead time: 0.06260997243225574 Adapter cache time: 0.017369522713124752 Engine time: 0.05997683200985193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 10.917171807028353,
    "estimated_duration": 3600.017679417589,
    "input_throughput": 4417.646082941706,
    "output_throughput": 3853.727463428583,
    "total_throughput": 8271.373546370289,
    "itl": 123.82167288397346,
    "ttft": 2209005.9065552363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1931,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.1237092757083,
    "arrivals": 926257,
    "finished_requests": 64321,
    "scheduler_time": 157.1795799059388
}
#Debug simulation 
Total elapsed time: 10.917269031051546. Arrivals time: 0.26860934030264616 Scheduler time: 10.4975447030738 Scheduler overhead time: 0.045702587347477674 Adapter cache time: 0.03886929573491216 Engine time: 0.04555501649156213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.561248007696122,
    "estimated_duration": 3600.0081962637537,
    "input_throughput": 3889.5747555609482,
    "output_throughput": 3401.8183660553973,
    "total_throughput": 7291.3931216163455,
    "itl": 100.70904641256777,
    "ttft": 2271428.9226393905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3875,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.138270652979823,
    "arrivals": 926257,
    "finished_requests": 56733,
    "scheduler_time": 173.79514709198043
}
#Debug simulation 
Total elapsed time: 6.561346181668341. Arrivals time: 0.229502335190773 Scheduler time: 6.138814254198223 Scheduler overhead time: 0.0512817264534533 Adapter cache time: 0.06633142940700054 Engine time: 0.051559639628976583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 10.944905368145555,
    "estimated_duration": 3600.1142526228773,
    "input_throughput": 4419.475850914525,
    "output_throughput": 3854.7212744399817,
    "total_throughput": 8274.197125354507,
    "itl": 123.79262143577931,
    "ttft": 2208953.6761849527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.241631773729075,
    "arrivals": 926257,
    "finished_requests": 64342,
    "scheduler_time": 157.22062864963303
}
#Debug simulation 
Total elapsed time: 10.945018942002207. Arrivals time: 0.27668092818930745 Scheduler time: 10.518458559643477 Scheduler overhead time: 0.04569951444864273 Adapter cache time: 0.03815517062321305 Engine time: 0.04518486047163606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 6.576258106157184,
    "estimated_duration": 3600.0837467099686,
    "input_throughput": 3889.9986737248037,
    "output_throughput": 3402.2589088916443,
    "total_throughput": 7292.2575826164475,
    "itl": 100.70111238280062,
    "ttft": 2271445.5930039976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.88455061511249,
    "arrivals": 926257,
    "finished_requests": 56740,
    "scheduler_time": 173.81101924935595
}
#Debug simulation 
Total elapsed time: 6.576365488115698. Arrivals time: 0.23395521845668554 Scheduler time: 6.149628325831145 Scheduler overhead time: 0.05147449253126979 Adapter cache time: 0.06604874273762107 Engine time: 0.05129779316484928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 10.879856838844717,
    "estimated_duration": 3600.049499595246,
    "input_throughput": 4420.311721210816,
    "output_throughput": 3855.6242078228593,
    "total_throughput": 8275.935929033676,
    "itl": 123.76333303917873,
    "ttft": 2208724.903916522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.333735257182807,
    "arrivals": 926257,
    "finished_requests": 64353,
    "scheduler_time": 157.25555674901682
}
#Debug simulation 
Total elapsed time: 10.879957394208759. Arrivals time: 0.26995639503002167 Scheduler time: 10.459488762076944 Scheduler overhead time: 0.04584580846130848 Adapter cache time: 0.03859427710995078 Engine time: 0.045203575398772955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_384_slots_96_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 17280, 4320, 135, 17280, 17280, 135, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 135, 135, 4320, 135, 17280, 135, 17280, 4320, 17280, 4320, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 17280, 17280, 135, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 135, 17280, 4320, 4320, 135, 135, 135, 4320, 135, 4320, 135, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 17280, 4320, 4320, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 135, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 4320, 135, 17280, 4320, 4320, 135, 17280, 17280, 17280, 135, 4320, 17280, 135, 17280, 4320, 4320, 17280, 17280, 135, 4320, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 17280, 17280, 4320, 135, 17280, 135, 17280, 4320, 4320, 4320, 4320, 4320, 135, 135, 17280, 135, 17280, 135, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 135, 135, 135, 135, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 17280, 135, 4320, 4320, 135, 135, 4320, 135, 17280, 17280, 135, 17280, 17280, 135, 4320, 17280, 135, 135, 4320, 4320, 135, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 135, 135, 135, 135, 135, 135, 135, 135, 4320, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 135, 135, 4320, 135, 17280, 17280, 17280, 4320, 135, 17280, 17280, 4320, 4320, 17280, 17280, 135, 17280, 17280, 4320, 135, 17280, 4320, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 17280, 135, 4320, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 17280, 135, 135, 135, 4320, 135, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 135, 4320, 4320, 4320, 4320, 135, 4320, 17280, 17280, 135, 17280, 17280, 135, 17280, 4320, 17280, 4320, 4320, 4320, 135, 4320, 135, 135, 17280, 135, 4320, 135, 135, 4320, 17280, 4320, 17280, 135, 17280, 17280, 4320, 17280, 4320, 135, 17280, 135, 17280, 17280, 135, 135, 135]
Prompts retrieved: 2782080 . Total input tokens: 620844137 . Total output tokens: 546415449
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 6.580841471906751,
    "estimated_duration": 3600.0247710400863,
    "input_throughput": 3890.4104529129772,
    "output_throughput": 3402.6385314179274,
    "total_throughput": 7293.048984330905,
    "itl": 100.69421781368949,
    "ttft": 2271417.0313402517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.604942426682626,
    "arrivals": 926257,
    "finished_requests": 56745,
    "scheduler_time": 173.82148196492298
}
#Debug simulation 
Total elapsed time: 6.580941341817379. Arrivals time: 0.23150421353057027 Scheduler time: 6.1551451571285725 Scheduler overhead time: 0.05152658233419061 Adapter cache time: 0.0671097724698484 Engine time: 0.05155832786113024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 384,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_384_slots_96_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 17280, 4320, 66, 17280, 17280, 66, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 4320, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 66, 66, 4320, 66, 17280, 66, 17280, 4320, 17280, 4320, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 17280, 17280, 66, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 66, 17280, 4320, 4320, 66, 66, 66, 4320, 66, 4320, 66, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 17280, 4320, 4320, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 17280, 17280, 66, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 4320, 66, 17280, 4320, 4320, 66, 17280, 17280, 17280, 66, 4320, 17280, 66, 17280, 4320, 4320, 17280, 17280, 66, 4320, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 17280, 17280, 4320, 66, 17280, 66, 17280, 4320, 4320, 4320, 4320, 4320, 66, 66, 17280, 66, 17280, 66, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 17280, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 17280, 66, 66, 66, 66, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 17280, 66, 4320, 4320, 66, 66, 4320, 66, 17280, 17280, 66, 17280, 17280, 66, 4320, 17280, 66, 66, 4320, 4320, 66, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 66, 66, 66, 66, 66, 66, 66, 66, 4320, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 66, 66, 4320, 66, 17280, 17280, 17280, 4320, 66, 17280, 17280, 4320, 4320, 17280, 17280, 66, 17280, 17280, 4320, 66, 17280, 4320, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 17280, 66, 4320, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 17280, 66, 66, 66, 4320, 66, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 66, 4320, 4320, 4320, 4320, 66, 4320, 17280, 17280, 66, 17280, 17280, 66, 17280, 4320, 17280, 4320, 4320, 4320, 66, 4320, 66, 66, 17280, 66, 4320, 66, 66, 4320, 17280, 4320, 17280, 66, 17280, 17280, 4320, 17280, 4320, 66, 17280, 66, 17280, 17280, 66, 66, 66]
Prompts retrieved: 2773248 . Total input tokens: 618871582 . Total output tokens: 544686778
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 14.467879652045667,
    "estimated_duration": 3600.0393303912488,
    "input_throughput": 4644.609257139088,
    "output_throughput": 4024.183535357527,
    "total_throughput": 8668.792792496615,
    "itl": 135.966950860276,
    "ttft": 2188497.783137294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.116985818883663,
    "arrivals": 923309,
    "finished_requests": 67664,
    "scheduler_time": 150.55891756622805
}
#Debug simulation 
Total elapsed time: 14.467995140701532. Arrivals time: 0.30731248296797276 Scheduler time: 14.020128356758505 Scheduler overhead time: 0.04479095386341214 Adapter cache time: 0.03198604704812169 Engine time: 0.0440747169777751 
