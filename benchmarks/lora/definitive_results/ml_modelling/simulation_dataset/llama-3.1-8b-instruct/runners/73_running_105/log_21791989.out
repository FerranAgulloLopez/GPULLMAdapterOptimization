INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.679823806043714,
    "estimated_duration": 3600.0912670178636,
    "input_throughput": 5442.788125821915,
    "output_throughput": 4709.224223096863,
    "total_throughput": 10152.012348918777,
    "itl": 116.52606005158395,
    "ttft": 1976220.4570267843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 426676,
    "finished_requests": 79131,
    "scheduler_time": 35.5521092299973
}
#Debug simulation 
Total elapsed time: 5.679931217804551. Arrivals time: 0.29884815122932196 Scheduler time: 5.228836595080793 Scheduler overhead time: 0.04601065069437027 Adapter cache time: 0.03713006107136607 Engine time: 0.04750544996932149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.61077637784183,
    "estimated_duration": 3600.04512709236,
    "input_throughput": 5315.831975544297,
    "output_throughput": 4603.719235426906,
    "total_throughput": 9919.551210971204,
    "itl": 104.78314058533694,
    "ttft": 1991124.9916906143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867943,
    "arrivals": 426676,
    "finished_requests": 77310,
    "scheduler_time": 30.19205177165133
}
#Debug simulation 
Total elapsed time: 5.610884773079306. Arrivals time: 0.2825803509913385 Scheduler time: 5.159274386242032 Scheduler overhead time: 0.04992047883570194 Adapter cache time: 0.04171400377526879 Engine time: 0.05121953412890434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.326474992092699,
    "estimated_duration": 3600.0316720069977,
    "input_throughput": 4924.593896729901,
    "output_throughput": 4272.293802189111,
    "total_throughput": 9196.887698919012,
    "itl": 80.77459732179747,
    "ttft": 2041747.3651400208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 426676,
    "finished_requests": 71684,
    "scheduler_time": 14.093764530782495
}
#Debug simulation 
Total elapsed time: 5.326611946802586. Arrivals time: 0.2746970816515386 Scheduler time: 4.841996813658625 Scheduler overhead time: 0.06176169775426388 Adapter cache time: 0.05503493640571833 Engine time: 0.06367893004789948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.591084424871951,
    "estimated_duration": 3600.003283678958,
    "input_throughput": 5316.016262197017,
    "output_throughput": 4603.896633967083,
    "total_throughput": 9919.9128961641,
    "itl": 104.77858149335503,
    "ttft": 1991016.8702777396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 426676,
    "finished_requests": 77312,
    "scheduler_time": 30.189883423179786
}
#Debug simulation 
Total elapsed time: 5.591178340837359. Arrivals time: 0.28443434834480286 Scheduler time: 5.14112201705575 Scheduler overhead time: 0.049777820240706205 Adapter cache time: 0.04104356048628688 Engine time: 0.05113325035199523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.316504021175206,
    "estimated_duration": 3600.0066276362036,
    "input_throughput": 4924.71593354844,
    "output_throughput": 4272.332690148108,
    "total_throughput": 9197.048623696548,
    "itl": 80.77598764441676,
    "ttft": 2041743.087067663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 426676,
    "finished_requests": 71685,
    "scheduler_time": 14.094570352664832
}
#Debug simulation 
Total elapsed time: 5.316604500170797. Arrivals time: 0.29662249656394124 Scheduler time: 4.810785525012761 Scheduler overhead time: 0.061540997587144375 Adapter cache time: 0.054507472552359104 Engine time: 0.0638471320271492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.665211770217866,
    "estimated_duration": 3600.0571608344962,
    "input_throughput": 5315.732263418851,
    "output_throughput": 4603.485516923962,
    "total_throughput": 9919.217780342813,
    "itl": 104.77869126127524,
    "ttft": 1991079.0332518101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 426676,
    "finished_requests": 77309,
    "scheduler_time": 30.190311326882533
}
#Debug simulation 
Total elapsed time: 5.665350889787078. Arrivals time: 0.28454318223521113 Scheduler time: 5.213332476560026 Scheduler overhead time: 0.05034572118893266 Adapter cache time: 0.04144800407811999 Engine time: 0.051711464300751686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 1080, 4320]
Prompts retrieved: 1278720 . Total input tokens: 284919212 . Total output tokens: 251216072
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.337883557192981,
    "estimated_duration": 3600.023551001154,
    "input_throughput": 4924.6050057282855,
    "output_throughput": 4272.303439715767,
    "total_throughput": 9196.908445444053,
    "itl": 80.77687401477463,
    "ttft": 2041769.3428284242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 426676,
    "finished_requests": 71684,
    "scheduler_time": 14.095417748854093
}
#Debug simulation 
Total elapsed time: 5.338001507800072. Arrivals time: 0.2753870035521686 Scheduler time: 4.852198145352304 Scheduler overhead time: 0.0619300133548677 Adapter cache time: 0.05498442333191633 Engine time: 0.06392543902620673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.891054376959801,
    "estimated_duration": 3600.1068124186413,
    "input_throughput": 5604.355940329746,
    "output_throughput": 4879.459670308598,
    "total_throughput": 10483.815610638343,
    "itl": 112.22269787987491,
    "ttft": 1955156.9080823604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 420888,
    "finished_requests": 81523,
    "scheduler_time": 36.801692226039265
}
#Debug simulation 
Total elapsed time: 5.891182960942388. Arrivals time: 0.2963426155038178 Scheduler time: 5.443461033049971 Scheduler overhead time: 0.04740408156067133 Adapter cache time: 0.03211900219321251 Engine time: 0.04932573111727834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.79271928826347,
    "estimated_duration": 3600.0428258613574,
    "input_throughput": 5459.005892602915,
    "output_throughput": 4757.974232126442,
    "total_throughput": 10216.980124729358,
    "itl": 101.11819709542021,
    "ttft": 1972835.6309635956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867947,
    "arrivals": 420888,
    "finished_requests": 79431,
    "scheduler_time": 31.14051294561231
}
#Debug simulation 
Total elapsed time: 5.792856498155743. Arrivals time: 0.3012948832474649 Scheduler time: 5.3248744029551744 Scheduler overhead time: 0.05186075251549482 Adapter cache time: 0.03672395134344697 Engine time: 0.05348063679412007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.442839742172509,
    "estimated_duration": 3600.0784919097355,
    "input_throughput": 5045.261385499649,
    "output_throughput": 4394.451130871806,
    "total_throughput": 9439.712516371454,
    "itl": 78.56625069910137,
    "ttft": 2027923.442128196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 420888,
    "finished_requests": 73361,
    "scheduler_time": 14.576087699507134
}
#Debug simulation 
Total elapsed time: 5.442961914930493. Arrivals time: 0.27972045727074146 Scheduler time: 4.956699451897293 Scheduler overhead time: 0.06318553211167455 Adapter cache time: 0.04717484675347805 Engine time: 0.06578424666076899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.043226034846157,
    "estimated_duration": 3600.109052654777,
    "input_throughput": 5462.698410621132,
    "output_throughput": 4761.566594033896,
    "total_throughput": 10224.265004655028,
    "itl": 101.43247644979013,
    "ttft": 1971898.7481480776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407829,
    "arrivals": 420888,
    "finished_requests": 79492,
    "scheduler_time": 31.31665075694489
}
#Debug simulation 
Total elapsed time: 6.043343244120479. Arrivals time: 0.5695401667617261 Scheduler time: 5.308201729785651 Scheduler overhead time: 0.05166232539340854 Adapter cache time: 0.03620674088597298 Engine time: 0.05314701236784458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.453996071126312,
    "estimated_duration": 3600.047416301608,
    "input_throughput": 5045.36104656636,
    "output_throughput": 4394.524341085672,
    "total_throughput": 9439.885387652032,
    "itl": 78.56593594011855,
    "ttft": 2027949.4940514807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 420888,
    "finished_requests": 73362,
    "scheduler_time": 14.576375331638232
}
#Debug simulation 
Total elapsed time: 5.45413850620389. Arrivals time: 0.2994046388193965 Scheduler time: 4.947757111862302 Scheduler overhead time: 0.06330527132377028 Adapter cache time: 0.04752703383564949 Engine time: 0.0655475640669465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.891285558231175,
    "estimated_duration": 3600.1057189736866,
    "input_throughput": 5462.96512803705,
    "output_throughput": 4761.74266484792,
    "total_throughput": 10224.70779288497,
    "itl": 101.42794895933366,
    "ttft": 1971863.1656019948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 420888,
    "finished_requests": 79495,
    "scheduler_time": 31.315736260782188
}
#Debug simulation 
Total elapsed time: 5.891399409156293. Arrivals time: 0.284722164273262 Scheduler time: 5.440693737939 Scheduler overhead time: 0.05158891761675477 Adapter cache time: 0.03615104639902711 Engine time: 0.05353236058726907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 4320, 4320, 540, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 34560, 540, 540, 4320, 540, 34560, 540, 540, 34560, 540, 4320, 4320, 540, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 540, 540, 4320]
Prompts retrieved: 1261440 . Total input tokens: 281027078 . Total output tokens: 247815829
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.464976790826768,
    "estimated_duration": 3600.0533325035135,
    "input_throughput": 5045.762193575023,
    "output_throughput": 4394.774337689499,
    "total_throughput": 9440.53653126452,
    "itl": 78.56421460936527,
    "ttft": 2028001.0129830004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 420888,
    "finished_requests": 73369,
    "scheduler_time": 14.577092476794643
}
#Debug simulation 
Total elapsed time: 5.465086545795202. Arrivals time: 0.2897767350077629 Scheduler time: 4.965128904674202 Scheduler overhead time: 0.06317438744008541 Adapter cache time: 0.0472457897849381 Engine time: 0.06712595373392105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.2846805192530155,
    "estimated_duration": 3600.087872298718,
    "input_throughput": 5686.9681313993215,
    "output_throughput": 4968.4537251528045,
    "total_throughput": 10655.421856552126,
    "itl": 110.70155641763051,
    "ttft": 1940576.724840119,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 418034,
    "finished_requests": 83078,
    "scheduler_time": 37.623475279299434
}
#Debug simulation 
Total elapsed time: 6.284781419206411. Arrivals time: 0.263819916639477 Scheduler time: 5.872802523896098 Scheduler overhead time: 0.04822190152481198 Adapter cache time: 0.026525722816586494 Engine time: 0.0506227551959455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.8787799146957695,
    "estimated_duration": 3600.015855751846,
    "input_throughput": 5541.895591410763,
    "output_throughput": 4833.2834346270165,
    "total_throughput": 10375.179026037778,
    "itl": 99.91476234710537,
    "ttft": 1959714.6327531864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867944,
    "arrivals": 418034,
    "finished_requests": 80831,
    "scheduler_time": 31.80102464766718
}
#Debug simulation 
Total elapsed time: 5.878872944042087. Arrivals time: 0.25719711603596807 Scheduler time: 5.459929803386331 Scheduler overhead time: 0.05239983834326267 Adapter cache time: 0.030538029968738556 Engine time: 0.05401996383443475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.505652981810272,
    "estimated_duration": 3600.0690893987744,
    "input_throughput": 5085.179907771531,
    "output_throughput": 4443.955269389516,
    "total_throughput": 9529.135177161048,
    "itl": 77.68002803456301,
    "ttft": 2017971.0832294808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 418034,
    "finished_requests": 74269,
    "scheduler_time": 14.755137671920126
}
#Debug simulation 
Total elapsed time: 5.505746182054281. Arrivals time: 0.24441140377894044 Scheduler time: 5.057999642100185 Scheduler overhead time: 0.06398382224142551 Adapter cache time: 0.0424733585678041 Engine time: 0.06607919884845614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.887905809096992,
    "estimated_duration": 3600.012648174917,
    "input_throughput": 5541.734696436171,
    "output_throughput": 4833.091075060748,
    "total_throughput": 10374.825771496919,
    "itl": 99.91448471061118,
    "ttft": 1959743.0307902072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407829,
    "arrivals": 418034,
    "finished_requests": 80831,
    "scheduler_time": 31.800628145047405
}
#Debug simulation 
Total elapsed time: 5.888046133797616. Arrivals time: 0.26088192174211144 Scheduler time: 5.464406651444733 Scheduler overhead time: 0.052509518805891275 Adapter cache time: 0.031191890593618155 Engine time: 0.05397243611514568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.791331426706165,
    "estimated_duration": 3600.022790913448,
    "input_throughput": 5085.325305775306,
    "output_throughput": 4444.105476326871,
    "total_throughput": 9529.430782102178,
    "itl": 77.67961010070017,
    "ttft": 2017915.5340660908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 418034,
    "finished_requests": 74271,
    "scheduler_time": 14.752741407212145
}
#Debug simulation 
Total elapsed time: 5.791426268871874. Arrivals time: 0.24677586695179343 Scheduler time: 5.339194861706346 Scheduler overhead time: 0.06413288041949272 Adapter cache time: 0.0427689072676003 Engine time: 0.06767722312361002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.85053826821968,
    "estimated_duration": 3600.0136603855217,
    "input_throughput": 5541.9673040528,
    "output_throughput": 4833.342492966163,
    "total_throughput": 10375.309797018963,
    "itl": 99.91155237176574,
    "ttft": 1959835.7234604496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 418034,
    "finished_requests": 80833,
    "scheduler_time": 31.800475428530472
}
#Debug simulation 
Total elapsed time: 5.850660335272551. Arrivals time: 0.2573498752899468 Scheduler time: 5.43155003990978 Scheduler overhead time: 0.05221640458330512 Adapter cache time: 0.030598717276006937 Engine time: 0.05409696884453297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 4320, 4320, 270, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 34560, 270, 270, 4320, 270, 34560, 270, 270, 34560, 270, 4320, 4320, 270, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 270, 270, 4320]
Prompts retrieved: 1252800 . Total input tokens: 279112825 . Total output tokens: 246135763
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.559379896149039,
    "estimated_duration": 3600.008890848754,
    "input_throughput": 5085.344940824241,
    "output_throughput": 4444.12263554939,
    "total_throughput": 9529.46757637363,
    "itl": 77.67693014845871,
    "ttft": 2017882.9777190213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 418034,
    "finished_requests": 74271,
    "scheduler_time": 14.753263968235478
}
#Debug simulation 
Total elapsed time: 5.559516659006476. Arrivals time: 0.24626099551096559 Scheduler time: 5.108793809544295 Scheduler overhead time: 0.06429492961615324 Adapter cache time: 0.042173344641923904 Engine time: 0.06704092491418123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.046243139076978,
    "estimated_duration": 3600.0189174732086,
    "input_throughput": 5719.615222036743,
    "output_throughput": 5023.276936748091,
    "total_throughput": 10742.892158784834,
    "itl": 109.88491059359826,
    "ttft": 1934354.878973077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 416602,
    "finished_requests": 83715,
    "scheduler_time": 38.15960679762758
}
#Debug simulation 
Total elapsed time: 6.04636659193784. Arrivals time: 0.3004076601937413 Scheduler time: 5.599360222928226 Scheduler overhead time: 0.04849216854199767 Adapter cache time: 0.02528179297223687 Engine time: 0.04976040171459317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.936403390951455,
    "estimated_duration": 3600.043047877676,
    "input_throughput": 5556.586055767549,
    "output_throughput": 4887.938773502664,
    "total_throughput": 10444.524829270213,
    "itl": 99.33520941997824,
    "ttft": 1953897.5596321218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867945,
    "arrivals": 416602,
    "finished_requests": 81389,
    "scheduler_time": 32.320358567816264
}
#Debug simulation 
Total elapsed time: 5.936524560209364. Arrivals time: 0.25847856095060706 Scheduler time: 5.516388176009059 Scheduler overhead time: 0.0526522439904511 Adapter cache time: 0.029826756101101637 Engine time: 0.05407976498827338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.558276728261262,
    "estimated_duration": 3600.038416158047,
    "input_throughput": 5088.208758491155,
    "output_throughput": 4478.851372149395,
    "total_throughput": 9567.06013064055,
    "itl": 77.45023245587811,
    "ttft": 2012360.112708992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 416602,
    "finished_requests": 74498,
    "scheduler_time": 15.07883458975301
}
#Debug simulation 
Total elapsed time: 5.5584481889382005. Arrivals time: 0.28513542376458645 Scheduler time: 5.071230488363653 Scheduler overhead time: 0.06412020698189735 Adapter cache time: 0.04056297428905964 Engine time: 0.06642746040597558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.197581687010825,
    "estimated_duration": 3600.029500001215,
    "input_throughput": 5556.37113529021,
    "output_throughput": 4887.941334923522,
    "total_throughput": 10444.312470213732,
    "itl": 99.33298308861868,
    "ttft": 1953933.3669307877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407829,
    "arrivals": 416602,
    "finished_requests": 81388,
    "scheduler_time": 32.320667445194694
}
#Debug simulation 
Total elapsed time: 6.19767685001716. Arrivals time: 0.5292623941786587 Scheduler time: 5.506529403850436 Scheduler overhead time: 0.05265092849731445 Adapter cache time: 0.02990811550989747 Engine time: 0.054213167168200016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.59431172395125,
    "estimated_duration": 3600.005349347064,
    "input_throughput": 5088.008272910523,
    "output_throughput": 4478.714456056104,
    "total_throughput": 9566.722728966626,
    "itl": 77.45445333017716,
    "ttft": 2012440.3356128964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 416602,
    "finished_requests": 74495,
    "scheduler_time": 15.080630129420133
}
#Debug simulation 
Total elapsed time: 5.594408229924738. Arrivals time: 0.2633225517347455 Scheduler time: 5.128587923012674 Scheduler overhead time: 0.06427485216408968 Adapter cache time: 0.040668460074812174 Engine time: 0.06652157241478562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.956255671102554,
    "estimated_duration": 3600.0796690654274,
    "input_throughput": 5556.635641119876,
    "output_throughput": 4888.144879462717,
    "total_throughput": 10444.780520582592,
    "itl": 99.3301090854593,
    "ttft": 1953924.8301279368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 416602,
    "finished_requests": 81393,
    "scheduler_time": 32.32185923761572
}
#Debug simulation 
Total elapsed time: 5.956422240007669. Arrivals time: 0.2621711348183453 Scheduler time: 5.532407291699201 Scheduler overhead time: 0.05267189024016261 Adapter cache time: 0.02985455049201846 Engine time: 0.05403147032484412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 4320, 4320, 135, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 34560, 135, 135, 4320, 135, 34560, 135, 135, 34560, 135, 4320, 4320, 135, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 135, 135, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 135, 135, 4320]
Prompts retrieved: 1248480 . Total input tokens: 278169548 . Total output tokens: 245281139
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.567488728091121,
    "estimated_duration": 3600.0663926053894,
    "input_throughput": 5088.032275633587,
    "output_throughput": 4478.666569349386,
    "total_throughput": 9566.698844982973,
    "itl": 77.4538323177373,
    "ttft": 2012364.6625927316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 416602,
    "finished_requests": 74496,
    "scheduler_time": 15.081343946137752
}
#Debug simulation 
Total elapsed time: 5.56758248526603. Arrivals time: 0.2845647041685879 Scheduler time: 5.081004260107875 Scheduler overhead time: 0.06433677533641458 Adapter cache time: 0.03995556104928255 Engine time: 0.06699107307940722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.1556760226376355,
    "estimated_duration": 3600.0004139053126,
    "input_throughput": 5781.608501933757,
    "output_throughput": 5060.6569181575605,
    "total_throughput": 10842.265420091317,
    "itl": 108.72060015185866,
    "ttft": 1926831.0080971145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 415862,
    "finished_requests": 84598,
    "scheduler_time": 38.35786459796422
}
#Debug simulation 
Total elapsed time: 6.155772041995078. Arrivals time: 0.30627476517111063 Scheduler time: 5.702221876010299 Scheduler overhead time: 0.049402070697396994 Adapter cache time: 0.023919439874589443 Engine time: 0.05058332299813628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.962574075907469,
    "estimated_duration": 3600.030080990518,
    "input_throughput": 5613.071709234205,
    "output_throughput": 4917.307800697409,
    "total_throughput": 10530.379509931614,
    "itl": 98.36290621605372,
    "ttft": 1946064.7723390225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867946,
    "arrivals": 415862,
    "finished_requests": 82130,
    "scheduler_time": 32.41764462387612
}
#Debug simulation 
Total elapsed time: 5.962717341724783. Arrivals time: 0.30930693494156003 Scheduler time: 5.492068577557802 Scheduler overhead time: 0.05316467769443989 Adapter cache time: 0.028057216200977564 Engine time: 0.05467558465898037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.570147016085684,
    "estimated_duration": 3600.0644472224953,
    "input_throughput": 5144.903729234624,
    "output_throughput": 4502.528562372208,
    "total_throughput": 9647.432291606832,
    "itl": 76.78424685561546,
    "ttft": 2007199.8687358703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 415862,
    "finished_requests": 75228,
    "scheduler_time": 15.035616588396195
}
#Debug simulation 
Total elapsed time: 5.570275123231113. Arrivals time: 0.28332916693761945 Scheduler time: 5.085219549946487 Scheduler overhead time: 0.06442925985902548 Adapter cache time: 0.039630936458706856 Engine time: 0.06678848573938012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.9592057932168245,
    "estimated_duration": 3600.0557096962893,
    "input_throughput": 5613.041194216614,
    "output_throughput": 4917.3186271313125,
    "total_throughput": 10530.359821347927,
    "itl": 98.36165231282834,
    "ttft": 1946058.06000844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 415862,
    "finished_requests": 82130,
    "scheduler_time": 32.41885036568688
}
#Debug simulation 
Total elapsed time: 5.959300264250487. Arrivals time: 0.2958092144690454 Scheduler time: 5.502909655217081 Scheduler overhead time: 0.05306013813242316 Adapter cache time: 0.027851268649101257 Engine time: 0.05426783300936222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.607149420771748,
    "estimated_duration": 3600.043806669305,
    "input_throughput": 5144.651841649467,
    "output_throughput": 4502.126604674628,
    "total_throughput": 9646.778446324095,
    "itl": 76.78553988750083,
    "ttft": 2007203.017179637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 415862,
    "finished_requests": 75224,
    "scheduler_time": 15.037203701988282
}
#Debug simulation 
Total elapsed time: 5.607295224908739. Arrivals time: 0.288423418533057 Scheduler time: 5.116213814355433 Scheduler overhead time: 0.06479257298633456 Adapter cache time: 0.039510325994342566 Engine time: 0.06712154811248183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.005444761831313,
    "estimated_duration": 3600.054917189663,
    "input_throughput": 5613.042429856749,
    "output_throughput": 4917.31970961691,
    "total_throughput": 10530.362139473658,
    "itl": 98.36024176302656,
    "ttft": 1946078.9778944317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 415862,
    "finished_requests": 82130,
    "scheduler_time": 32.41864772707624
}
#Debug simulation 
Total elapsed time: 6.005553038790822. Arrivals time: 0.3072169697843492 Scheduler time: 5.5364793771877885 Scheduler overhead time: 0.05309096956625581 Adapter cache time: 0.02798238256946206 Engine time: 0.05547724151983857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 4320, 4320, 66, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 34560, 66, 66, 4320, 66, 34560, 66, 66, 34560, 66, 4320, 4320, 66, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 66, 66, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 66, 66, 4320]
Prompts retrieved: 1246272 . Total input tokens: 277666438 . Total output tokens: 244836926
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.575354504864663,
    "estimated_duration": 3600.02871389498,
    "input_throughput": 5144.510355852755,
    "output_throughput": 4502.170756983806,
    "total_throughput": 9646.681112836563,
    "itl": 76.78537437163047,
    "ttft": 2007223.170156208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 415862,
    "finished_requests": 75223,
    "scheduler_time": 15.036284061362203
}
#Debug simulation 
Total elapsed time: 5.5754760298877954. Arrivals time: 0.2815456078387797 Scheduler time: 5.090822207275778 Scheduler overhead time: 0.0646066376939416 Adapter cache time: 0.039729784708470106 Engine time: 0.067739128600806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.379083706066012,
    "estimated_duration": 3600.0716030348626,
    "input_throughput": 5803.068189640585,
    "output_throughput": 5077.78983745479,
    "total_throughput": 10880.858027095375,
    "itl": 108.70489664654401,
    "ttft": 1923126.635848197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6083416309393936,
    "arrivals": 415494,
    "finished_requests": 84960,
    "scheduler_time": 38.567965032495565
}
#Debug simulation 
Total elapsed time: 6.379193800035864. Arrivals time: 0.2672603684477508 Scheduler time: 5.9670488042756915 Scheduler overhead time: 0.04895226750522852 Adapter cache time: 0.02235555090010166 Engine time: 0.05032563488930464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.013345798011869,
    "estimated_duration": 3600.0032184251613,
    "input_throughput": 5633.574685767078,
    "output_throughput": 4930.614758662611,
    "total_throughput": 10564.18944442969,
    "itl": 98.42133476715163,
    "ttft": 1943636.7820614392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6706139849312608,
    "arrivals": 415494,
    "finished_requests": 82495,
    "scheduler_time": 32.60981382136908
}
#Debug simulation 
Total elapsed time: 6.013438455760479. Arrivals time: 0.26060991268604994 Scheduler time: 5.59271720610559 Scheduler overhead time: 0.05332691501826048 Adapter cache time: 0.026257495395839214 Engine time: 0.05506677273660898 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.621461026836187,
    "estimated_duration": 3600.02332355214,
    "input_throughput": 5156.21826074293,
    "output_throughput": 4510.714387254944,
    "total_throughput": 9666.932647997875,
    "itl": 76.87871453493321,
    "ttft": 2006037.4421619524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6883169457502665,
    "arrivals": 415494,
    "finished_requests": 75476,
    "scheduler_time": 15.181173724549497
}
#Debug simulation 
Total elapsed time: 5.6215546117164195. Arrivals time: 0.28539255214855075 Scheduler time: 5.135127944871783 Scheduler overhead time: 0.06498663127422333 Adapter cache time: 0.03750621760264039 Engine time: 0.06743920175358653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.051190169993788,
    "estimated_duration": 3600.037190234369,
    "input_throughput": 5633.669300699541,
    "output_throughput": 4930.777673117421,
    "total_throughput": 10564.446973816963,
    "itl": 98.4167357014855,
    "ttft": 1943612.0016359703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6289673557318747,
    "arrivals": 415494,
    "finished_requests": 82498,
    "scheduler_time": 32.60771777863388
}
#Debug simulation 
Total elapsed time: 6.051352479029447. Arrivals time: 0.2723509925417602 Scheduler time: 5.619310871232301 Scheduler overhead time: 0.05317087285220623 Adapter cache time: 0.02604099875316024 Engine time: 0.05490613868460059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.608671905938536,
    "estimated_duration": 3600.067557860501,
    "input_throughput": 5156.180460966583,
    "output_throughput": 4510.612297967667,
    "total_throughput": 9666.79275893425,
    "itl": 76.8775965659655,
    "ttft": 2005981.0655292622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6821034304518254,
    "arrivals": 415494,
    "finished_requests": 75476,
    "scheduler_time": 15.181337181608074
}
#Debug simulation 
Total elapsed time: 5.608764945995063. Arrivals time: 0.2862772084772587 Scheduler time: 5.121097447350621 Scheduler overhead time: 0.06499122316017747 Adapter cache time: 0.037852730602025986 Engine time: 0.06744803348556161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.254200831986964,
    "estimated_duration": 3600.075634161948,
    "input_throughput": 5633.780803808418,
    "output_throughput": 4930.815850520951,
    "total_throughput": 10564.596654329369,
    "itl": 98.4165024465927,
    "ttft": 1943600.2008055053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5873207265324892,
    "arrivals": 415494,
    "finished_requests": 82499,
    "scheduler_time": 32.6081952590761
}
#Debug simulation 
Total elapsed time: 6.254268380813301. Arrivals time: 0.5781404078006744 Scheduler time: 5.517003365792334 Scheduler overhead time: 0.05313290748745203 Adapter cache time: 0.02604656759649515 Engine time: 0.05462128762155771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 4320, 4320, 33, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 34560, 33, 33, 4320, 33, 34560, 33, 33, 34560, 33, 4320, 4320, 33, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 33, 33, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 33, 33, 4320]
Prompts retrieved: 1245216 . Total input tokens: 277432167 . Total output tokens: 244628990
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.582762913778424,
    "estimated_duration": 3600.0294444770375,
    "input_throughput": 5156.032828697159,
    "output_throughput": 4510.4842197641565,
    "total_throughput": 9666.517048461315,
    "itl": 76.87906636663934,
    "ttft": 2005926.4553584151,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6758899151533844,
    "arrivals": 415494,
    "finished_requests": 75474,
    "scheduler_time": 15.181698244449965
}
#Debug simulation 
Total elapsed time: 5.5829331749118865. Arrivals time: 0.2849055523984134 Scheduler time: 5.096477813087404 Scheduler overhead time: 0.06497115176171064 Adapter cache time: 0.03769090352579951 Engine time: 0.06767084542661905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.289462673012167,
    "estimated_duration": 3600.100544110873,
    "input_throughput": 6012.17875300913,
    "output_throughput": 5278.011479730165,
    "total_throughput": 11290.190232739295,
    "itl": 104.60093458404307,
    "ttft": 1872479.2418585499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 386239,
    "finished_requests": 87927,
    "scheduler_time": 40.20956839378309
}
#Debug simulation 
Total elapsed time: 6.289558949880302. Arrivals time: 0.26892590476199985 Scheduler time: 5.856041193008423 Scheduler overhead time: 0.0507983616553247 Adapter cache time: 0.036905886605381966 Engine time: 0.05288237100467086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.272784585133195,
    "estimated_duration": 3600.089441830645,
    "input_throughput": 5872.302436255556,
    "output_throughput": 5160.599285153531,
    "total_throughput": 11032.901721409087,
    "itl": 94.03871114174056,
    "ttft": 1888652.2448095726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867939,
    "arrivals": 386239,
    "finished_requests": 85909,
    "scheduler_time": 34.23156073529664
}
#Debug simulation 
Total elapsed time: 6.272878908086568. Arrivals time: 0.2683951314538717 Scheduler time: 5.824148517567664 Scheduler overhead time: 0.05602117860689759 Adapter cache time: 0.0394115224480629 Engine time: 0.058271059300750494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.865669467952102,
    "estimated_duration": 3600.061082512493,
    "input_throughput": 5423.708251742487,
    "output_throughput": 4768.793530585997,
    "total_throughput": 10192.501782328485,
    "itl": 72.71092467110367,
    "ttft": 1945688.1866339648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 386239,
    "finished_requests": 79374,
    "scheduler_time": 16.17628574944367
}
#Debug simulation 
Total elapsed time: 5.865818616002798. Arrivals time: 0.25803702091798186 Scheduler time: 5.39214654546231 Scheduler overhead time: 0.06857642717659473 Adapter cache time: 0.042784558609128 Engine time: 0.0713969049975276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.247568130027503,
    "estimated_duration": 3600.0293358539748,
    "input_throughput": 5872.548812157107,
    "output_throughput": 5160.895722421308,
    "total_throughput": 11033.444534578415,
    "itl": 94.03955009503939,
    "ttft": 1888563.9505750102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407824,
    "arrivals": 386239,
    "finished_requests": 85913,
    "scheduler_time": 34.23234220617695
}
#Debug simulation 
Total elapsed time: 6.247686794027686. Arrivals time: 0.28469131235033274 Scheduler time: 5.784178949892521 Scheduler overhead time: 0.055904606357216835 Adapter cache time: 0.03880751691758633 Engine time: 0.05743201356381178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.920479231048375,
    "estimated_duration": 3600.076626322113,
    "input_throughput": 5423.939550961346,
    "output_throughput": 4768.98765833765,
    "total_throughput": 10192.927209298996,
    "itl": 72.71047430672957,
    "ttft": 1945525.306901381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 386239,
    "finished_requests": 79377,
    "scheduler_time": 16.17562693394214
}
#Debug simulation 
Total elapsed time: 5.920580129139125. Arrivals time: 0.2591353836469352 Scheduler time: 5.443468704819679 Scheduler overhead time: 0.069107661023736 Adapter cache time: 0.04434117674827576 Engine time: 0.07136858673766255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.187786882277578,
    "estimated_duration": 3600.0252565150636,
    "input_throughput": 5872.465189442912,
    "output_throughput": 5160.771293583912,
    "total_throughput": 11033.236483026823,
    "itl": 94.0384740763062,
    "ttft": 1888578.315298847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 386239,
    "finished_requests": 85910,
    "scheduler_time": 34.232954420704665
}
#Debug simulation 
Total elapsed time: 6.187969060149044. Arrivals time: 0.2664532894268632 Scheduler time: 5.743649897631258 Scheduler overhead time: 0.055877366568893194 Adapter cache time: 0.03815775131806731 Engine time: 0.05718345195055008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 1080, 1080, 540, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 34560, 540, 540, 1080, 540, 34560, 540, 540, 34560, 540, 1080, 1080, 540, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 540, 540, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 540, 540, 1080]
Prompts retrieved: 1157760 . Total input tokens: 257831444 . Total output tokens: 227556113
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.872476891614497,
    "estimated_duration": 3600.0708631876655,
    "input_throughput": 5423.859902221632,
    "output_throughput": 4768.941404891739,
    "total_throughput": 10192.801307113372,
    "itl": 72.71175047807209,
    "ttft": 1945670.9078713246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 386239,
    "finished_requests": 79376,
    "scheduler_time": 16.177112043478186
}
#Debug simulation 
Total elapsed time: 5.872605617623776. Arrivals time: 0.255686170887202 Scheduler time: 5.400957454927266 Scheduler overhead time: 0.0686443978920579 Adapter cache time: 0.04342808714136481 Engine time: 0.0708979363553226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.539699602872133,
    "estimated_duration": 3600.0300698667934,
    "input_throughput": 6217.798342120029,
    "output_throughput": 5421.100274510242,
    "total_throughput": 11638.898616630271,
    "itl": 101.27497916710793,
    "ttft": 1851901.1630931979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 383390,
    "finished_requests": 90589,
    "scheduler_time": 41.14001379362856
}
#Debug simulation 
Total elapsed time: 6.5398186021484435. Arrivals time: 0.27913594944402575 Scheduler time: 6.095991919748485 Scheduler overhead time: 0.052771486807614565 Adapter cache time: 0.0325619550421834 Engine time: 0.054156935308128595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.393071106635034,
    "estimated_duration": 3600.070810979238,
    "input_throughput": 6060.5985675224065,
    "output_throughput": 5291.87257703356,
    "total_throughput": 11352.471144555968,
    "itl": 91.21515526811139,
    "ttft": 1870624.1515700698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867946,
    "arrivals": 383390,
    "finished_requests": 88340,
    "scheduler_time": 34.923833531134655
}
#Debug simulation 
Total elapsed time: 6.393240013625473. Arrivals time: 0.28777361614629626 Scheduler time: 5.928711537271738 Scheduler overhead time: 0.0571563015691936 Adapter cache time: 0.03325799806043506 Engine time: 0.05897115170955658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.972317470237613,
    "estimated_duration": 3600.0558064492616,
    "input_throughput": 5564.162078853129,
    "output_throughput": 4858.147745562314,
    "total_throughput": 10422.309824415443,
    "itl": 71.02981820140606,
    "ttft": 1932236.2114484264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 383390,
    "finished_requests": 81183,
    "scheduler_time": 16.33458580042243
}
#Debug simulation 
Total elapsed time: 5.972430739086121. Arrivals time: 0.26897937525063753 Scheduler time: 5.490127810277045 Scheduler overhead time: 0.06982043571770191 Adapter cache time: 0.037303800228983164 Engine time: 0.07249666284769773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.395200157072395,
    "estimated_duration": 3600.0371414007536,
    "input_throughput": 6060.738859908095,
    "output_throughput": 5292.12762304642,
    "total_throughput": 11352.866482954514,
    "itl": 91.21646206964753,
    "ttft": 1870583.6157508912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 383390,
    "finished_requests": 88342,
    "scheduler_time": 34.923763891788134
}
#Debug simulation 
Total elapsed time: 6.395307471975684. Arrivals time: 0.2803019555285573 Scheduler time: 5.936423808801919 Scheduler overhead time: 0.057470121420919895 Adapter cache time: 0.03430768661201 Engine time: 0.0594002865254879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.938439772929996,
    "estimated_duration": 3600.007264857154,
    "input_throughput": 5564.697937017884,
    "output_throughput": 4858.669084017541,
    "total_throughput": 10423.367021035425,
    "itl": 71.02846443746178,
    "ttft": 1932250.316812205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 383390,
    "finished_requests": 81189,
    "scheduler_time": 16.332940913147013
}
#Debug simulation 
Total elapsed time: 5.938581766095012. Arrivals time: 0.26298380037769675 Scheduler time: 5.458794847596437 Scheduler overhead time: 0.07027133414521813 Adapter cache time: 0.037456966470927 Engine time: 0.07556182844564319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.376424411311746,
    "estimated_duration": 3600.029142205666,
    "input_throughput": 6060.946491847445,
    "output_throughput": 5292.106604539145,
    "total_throughput": 11353.053096386591,
    "itl": 91.21369252359912,
    "ttft": 1870593.9146307006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 383390,
    "finished_requests": 88344,
    "scheduler_time": 34.923946475878296
}
#Debug simulation 
Total elapsed time: 6.376517990138382. Arrivals time: 0.2774149547331035 Scheduler time: 5.921718489378691 Scheduler overhead time: 0.05740418704226613 Adapter cache time: 0.03370758844539523 Engine time: 0.058863036800175905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 1080, 1080, 270, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 34560, 270, 270, 1080, 270, 34560, 270, 270, 34560, 270, 1080, 1080, 270, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 270, 270, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 270, 270, 1080]
Prompts retrieved: 1149120 . Total input tokens: 255902896 . Total output tokens: 225838714
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.944871452637017,
    "estimated_duration": 3600.0783770846288,
    "input_throughput": 5564.381911102235,
    "output_throughput": 4858.3231163327655,
    "total_throughput": 10422.705027435,
    "itl": 71.02753507041227,
    "ttft": 1932185.8171259053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 383390,
    "finished_requests": 81187,
    "scheduler_time": 16.333713866009525
}
#Debug simulation 
Total elapsed time: 5.9449631427414715. Arrivals time: 0.26666746847331524 Scheduler time: 5.465896417852491 Scheduler overhead time: 0.0695497845299542 Adapter cache time: 0.037148778326809406 Engine time: 0.07234360603615642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.508132399059832,
    "estimated_duration": 3600.059103620214,
    "input_throughput": 6291.769759341575,
    "output_throughput": 5470.79490450436,
    "total_throughput": 11762.564663845935,
    "itl": 100.45938544135323,
    "ttft": 1842893.753013002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 381970,
    "finished_requests": 91599,
    "scheduler_time": 41.55838421203237
}
#Debug simulation 
Total elapsed time: 6.508273807354271. Arrivals time: 0.2781862863339484 Scheduler time: 6.06885623652488 Scheduler overhead time: 0.053087218664586544 Adapter cache time: 0.028931701090186834 Engine time: 0.05413043079897761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.370112520176917,
    "estimated_duration": 3600.052473099042,
    "input_throughput": 6131.738680185816,
    "output_throughput": 5331.792284537606,
    "total_throughput": 11463.530964723423,
    "itl": 90.62781471565818,
    "ttft": 1862791.6992318875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867945,
    "arrivals": 381970,
    "finished_requests": 89262,
    "scheduler_time": 35.23801654369342
}
#Debug simulation 
Total elapsed time: 6.370206956285983. Arrivals time: 0.2779258624650538 Scheduler time: 5.917580183129758 Scheduler overhead time: 0.05754228914156556 Adapter cache time: 0.030765749514102936 Engine time: 0.059077602345496416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.016127468086779,
    "estimated_duration": 3600.040258067161,
    "input_throughput": 5604.784545057601,
    "output_throughput": 4885.025093981023,
    "total_throughput": 10489.809639038624,
    "itl": 70.72207630534791,
    "ttft": 1924948.877110349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 381970,
    "finished_requests": 81684,
    "scheduler_time": 16.487942062374294
}
#Debug simulation 
Total elapsed time: 6.0162220019847155. Arrivals time: 0.26454916782677174 Scheduler time: 5.540297299157828 Scheduler overhead time: 0.07023723470047116 Adapter cache time: 0.03451734548434615 Engine time: 0.07286831829696894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.444086110219359,
    "estimated_duration": 3600.002023962464,
    "input_throughput": 6131.513775012853,
    "output_throughput": 5331.663946920084,
    "total_throughput": 11463.177721932936,
    "itl": 90.62747761868577,
    "ttft": 1862792.1788035613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407827,
    "arrivals": 381970,
    "finished_requests": 89258,
    "scheduler_time": 35.238618989344204
}
#Debug simulation 
Total elapsed time: 6.444258950185031. Arrivals time: 0.276887986343354 Scheduler time: 5.991530997212976 Scheduler overhead time: 0.057662063743919134 Adapter cache time: 0.03097157133743167 Engine time: 0.059561952482908964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.9444510336034,
    "estimated_duration": 3600.061585326463,
    "input_throughput": 5604.892450241295,
    "output_throughput": 4885.04059810555,
    "total_throughput": 10489.933048346846,
    "itl": 70.72205142003644,
    "ttft": 1924975.264922883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 381970,
    "finished_requests": 81685,
    "scheduler_time": 16.489346681948746
}
#Debug simulation 
Total elapsed time: 5.9445455386303365. Arrivals time: 0.25988000771030784 Scheduler time: 5.473003795836121 Scheduler overhead time: 0.07010732917115092 Adapter cache time: 0.03453371301293373 Engine time: 0.07259837863966823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.407002729829401,
    "estimated_duration": 3600.0031602822214,
    "input_throughput": 6131.554617376932,
    "output_throughput": 5331.709763970118,
    "total_throughput": 11463.26438134705,
    "itl": 90.62476052072105,
    "ttft": 1862693.5008541923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 381970,
    "finished_requests": 89259,
    "scheduler_time": 35.23870204234925
}
#Debug simulation 
Total elapsed time: 6.407098337076604. Arrivals time: 0.277445022482425 Scheduler time: 5.953821074683219 Scheduler overhead time: 0.05805331701412797 Adapter cache time: 0.03076078137382865 Engine time: 0.05957424221560359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 1080, 1080, 135, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 34560, 135, 135, 1080, 135, 34560, 135, 135, 34560, 135, 1080, 1080, 135, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 135, 135, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 135, 135, 1080]
Prompts retrieved: 1144800 . Total input tokens: 254958843 . Total output tokens: 224983646
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.028792970813811,
    "estimated_duration": 3600.02575723453,
    "input_throughput": 5604.931842348892,
    "output_throughput": 4885.03143752766,
    "total_throughput": 10489.963279876552,
    "itl": 70.72266960350507,
    "ttft": 1924998.605150736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 381970,
    "finished_requests": 81684,
    "scheduler_time": 16.487933223716613
}
#Debug simulation 
Total elapsed time: 6.028955569956452. Arrivals time: 0.27371443435549736 Scheduler time: 5.543158492073417 Scheduler overhead time: 0.07022490911185741 Adapter cache time: 0.034941073041409254 Engine time: 0.0728819565847516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.565834952984005,
    "estimated_duration": 3600.007784026546,
    "input_throughput": 6268.252835487093,
    "output_throughput": 5491.8756253039855,
    "total_throughput": 11760.128460791078,
    "itl": 100.41601195495821,
    "ttft": 1845169.3432421517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 381260,
    "finished_requests": 91605,
    "scheduler_time": 41.78130860466803
}
#Debug simulation 
Total elapsed time: 6.5659301481209695. Arrivals time: 0.27754081040620804 Scheduler time: 6.128370281308889 Scheduler overhead time: 0.05292531428858638 Adapter cache time: 0.027664173394441605 Engine time: 0.054429834708571434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.499232206027955,
    "estimated_duration": 3600.015726992544,
    "input_throughput": 6098.228637001026,
    "output_throughput": 5348.075247461238,
    "total_throughput": 11446.303884462264,
    "itl": 90.66267426093196,
    "ttft": 1865003.016813741,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867945,
    "arrivals": 381260,
    "finished_requests": 89194,
    "scheduler_time": 35.468445238967924
}
#Debug simulation 
Total elapsed time: 6.499327667988837. Arrivals time: 0.2939105946570635 Scheduler time: 6.030864814762026 Scheduler overhead time: 0.05775726353749633 Adapter cache time: 0.029663051944226027 Engine time: 0.059430510737001896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.9981114119291306,
    "estimated_duration": 3600.0509635792664,
    "input_throughput": 5595.060515469422,
    "output_throughput": 4898.815094124622,
    "total_throughput": 10493.875609594044,
    "itl": 70.73804072247444,
    "ttft": 1929457.9720104535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 381260,
    "finished_requests": 81744,
    "scheduler_time": 16.640244432683893
}
#Debug simulation 
Total elapsed time: 5.9982577548362315. Arrivals time: 0.26008107932284474 Scheduler time: 5.529239030554891 Scheduler overhead time: 0.06993309454992414 Adapter cache time: 0.03299156157299876 Engine time: 0.07256877236068249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.4499976160004735,
    "estimated_duration": 3600.0233590687635,
    "input_throughput": 6098.280708289332,
    "output_throughput": 5348.231686191189,
    "total_throughput": 11446.51239448052,
    "itl": 90.66139872219618,
    "ttft": 1864959.4212212975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407827,
    "arrivals": 381260,
    "finished_requests": 89196,
    "scheduler_time": 35.46946321053867
}
#Debug simulation 
Total elapsed time: 6.450088998768479. Arrivals time: 0.2759864591062069 Scheduler time: 6.000572693534195 Scheduler overhead time: 0.05756446998566389 Adapter cache time: 0.0293187340721488 Engine time: 0.059374043717980385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.9907815982587636,
    "estimated_duration": 3600.0327303229174,
    "input_throughput": 5595.026075830502,
    "output_throughput": 4898.991292898059,
    "total_throughput": 10494.01736872856,
    "itl": 70.7380875280063,
    "ttft": 1929392.175992166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 381260,
    "finished_requests": 81744,
    "scheduler_time": 16.638935448723913
}
#Debug simulation 
Total elapsed time: 5.990897265262902. Arrivals time: 0.26842925138771534 Scheduler time: 5.514619801659137 Scheduler overhead time: 0.0697510871104896 Adapter cache time: 0.03259311756119132 Engine time: 0.07205998059362173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.450134390965104,
    "estimated_duration": 3600.0628206525344,
    "input_throughput": 6098.158586027324,
    "output_throughput": 5348.1094522984395,
    "total_throughput": 11446.268038325763,
    "itl": 90.6595539391011,
    "ttft": 1865056.1106413489,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 381260,
    "finished_requests": 89196,
    "scheduler_time": 35.470229464298065
}
#Debug simulation 
Total elapsed time: 6.450277249328792. Arrivals time: 0.32347124349325895 Scheduler time: 5.953152222093195 Scheduler overhead time: 0.057547389063984156 Adapter cache time: 0.029282338451594114 Engine time: 0.059442717116326094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 66, 34560, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 34560, 1080, 1080, 66, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 34560, 66, 66, 1080, 66, 34560, 66, 66, 34560, 66, 1080, 1080, 66, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 66, 66, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 34560, 66, 66, 34560, 66, 66, 1080]
Prompts retrieved: 1142592 . Total input tokens: 254479005 . Total output tokens: 224547271
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.009202099405229,
    "estimated_duration": 3600.004015396838,
    "input_throughput": 5595.118759271619,
    "output_throughput": 4898.75231376818,
    "total_throughput": 10493.871073039798,
    "itl": 70.73714325003674,
    "ttft": 1929387.558649115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 381260,
    "finished_requests": 81743,
    "scheduler_time": 16.638875274234753
}
#Debug simulation 
Total elapsed time: 6.009322472382337. Arrivals time: 0.26473034219816327 Scheduler time: 5.53593033971265 Scheduler overhead time: 0.06993623403832316 Adapter cache time: 0.03283613082021475 Engine time: 0.07243497809395194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.670710699632764,
    "estimated_duration": 3600.103938413033,
    "input_throughput": 6377.698642256756,
    "output_throughput": 5540.176711895734,
    "total_throughput": 11917.87535415249,
    "itl": 99.41363383883954,
    "ttft": 1839486.1341280201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6281788580352434,
    "arrivals": 380865,
    "finished_requests": 92894,
    "scheduler_time": 42.17071157175174
}
#Debug simulation 
Total elapsed time: 6.670808719936758. Arrivals time: 0.2902568653225899 Scheduler time: 6.2202984457835555 Scheduler overhead time: 0.053475954569876194 Adapter cache time: 0.026647794526070356 Engine time: 0.05474895890802145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.507068358827382,
    "estimated_duration": 3600.0493337978237,
    "input_throughput": 6205.943288124588,
    "output_throughput": 5391.691668714802,
    "total_throughput": 11597.63495683939,
    "itl": 89.74901904073755,
    "ttft": 1859876.9909137685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6939304106729112,
    "arrivals": 380865,
    "finished_requests": 90365,
    "scheduler_time": 35.73549829588922
}
#Debug simulation 
Total elapsed time: 6.507222115062177. Arrivals time: 0.27984074922278523 Scheduler time: 6.051587691530585 Scheduler overhead time: 0.05830031633377075 Adapter cache time: 0.029816713649779558 Engine time: 0.05984496418386698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.06693432899192,
    "estimated_duration": 3600.05513626977,
    "input_throughput": 5659.37665641165,
    "output_throughput": 4929.120896294592,
    "total_throughput": 10588.497552706243,
    "itl": 70.21483323349575,
    "ttft": 1924044.0828333597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7044690924976021,
    "arrivals": 380865,
    "finished_requests": 82467,
    "scheduler_time": 16.72754765247551
}
#Debug simulation 
Total elapsed time: 6.067045922856778. Arrivals time: 0.27032589353621006 Scheduler time: 5.586218048352748 Scheduler overhead time: 0.07083544693887234 Adapter cache time: 0.03163166530430317 Engine time: 0.07421125052496791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.53061727900058,
    "estimated_duration": 3600.0311586990956,
    "input_throughput": 6206.076562979947,
    "output_throughput": 5391.685278361331,
    "total_throughput": 11597.761841341278,
    "itl": 89.74932836014973,
    "ttft": 1859874.1897290377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6495073395268994,
    "arrivals": 380865,
    "finished_requests": 90365,
    "scheduler_time": 35.73612435166021
}
#Debug simulation 
Total elapsed time: 6.530717559158802. Arrivals time: 0.3126114886254072 Scheduler time: 6.043520159088075 Scheduler overhead time: 0.05827365070581436 Adapter cache time: 0.028492705430835485 Engine time: 0.06005736021324992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.986149962991476,
    "estimated_duration": 3600.0487779470104,
    "input_throughput": 5659.408040470692,
    "output_throughput": 4929.152657236911,
    "total_throughput": 10588.560697707602,
    "itl": 70.21378192369566,
    "ttft": 1924053.058438682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6980484600225465,
    "arrivals": 380865,
    "finished_requests": 82468,
    "scheduler_time": 16.725058535065504
}
#Debug simulation 
Total elapsed time: 5.986306356731802. Arrivals time: 0.25866901874542236 Scheduler time: 5.519050936214626 Scheduler overhead time: 0.07023165514692664 Adapter cache time: 0.03182565560564399 Engine time: 0.07272889418527484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.54086325597018,
    "estimated_duration": 3600.077089232394,
    "input_throughput": 6206.122104114125,
    "output_throughput": 5391.769542395758,
    "total_throughput": 11597.891646509885,
    "itl": 89.7490722720958,
    "ttft": 1859889.3295612603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6064724893542007,
    "arrivals": 380865,
    "finished_requests": 90368,
    "scheduler_time": 35.73753326756084
}
#Debug simulation 
Total elapsed time: 6.540957184042782. Arrivals time: 0.2743904576636851 Scheduler time: 6.092637423891574 Scheduler overhead time: 0.058289946522563696 Adapter cache time: 0.028133034706115723 Engine time: 0.059889542404562235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 1080, 33, 34560, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 34560, 1080, 1080, 33, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 34560, 33, 33, 1080, 33, 34560, 33, 33, 34560, 33, 1080, 1080, 33, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 33, 33, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 34560, 33, 33, 34560, 33, 33, 1080]
Prompts retrieved: 1141536 . Total input tokens: 254248456 . Total output tokens: 224347751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.063626035116613,
    "estimated_duration": 3600.0444555199983,
    "input_throughput": 5659.414835491834,
    "output_throughput": 4929.158575470104,
    "total_throughput": 10588.573410961937,
    "itl": 70.2157457299812,
    "ttft": 1924048.0128198906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6916278275474907,
    "arrivals": 380865,
    "finished_requests": 82468,
    "scheduler_time": 16.72587883851903
}
#Debug simulation 
Total elapsed time: 6.063720147125423. Arrivals time: 0.2618284919299185 Scheduler time: 5.592861487995833 Scheduler overhead time: 0.07054437324404716 Adapter cache time: 0.031685019843280315 Engine time: 0.07290096208453178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.0139447138644755,
    "estimated_duration": 3600.0186188380217,
    "input_throughput": 6391.062779399257,
    "output_throughput": 5620.127599931821,
    "total_throughput": 12011.190379331078,
    "itl": 98.01532499532111,
    "ttft": 1827801.3363192072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 377686,
    "finished_requests": 93353,
    "scheduler_time": 42.72169738245183
}
#Debug simulation 
Total elapsed time: 7.0140821798704565. Arrivals time: 0.33383601857349277 Scheduler time: 6.517240555025637 Scheduler overhead time: 0.054308418184518814 Adapter cache time: 0.027273652609437704 Engine time: 0.055615768767893314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.624508098233491,
    "estimated_duration": 3600.0411854755666,
    "input_throughput": 6216.366382220625,
    "output_throughput": 5472.5035034279645,
    "total_throughput": 11688.86988564859,
    "itl": 88.50996738374798,
    "ttft": 1847764.758217614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867944,
    "arrivals": 377686,
    "finished_requests": 90824,
    "scheduler_time": 36.216420883376244
}
#Debug simulation 
Total elapsed time: 6.624631135258824. Arrivals time: 0.27742158016189933 Scheduler time: 6.169982612133026 Scheduler overhead time: 0.05924232164397836 Adapter cache time: 0.028657308779656887 Engine time: 0.06124637834727764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.162130414042622,
    "estimated_duration": 3600.0127280843835,
    "input_throughput": 5684.08407013842,
    "output_throughput": 5002.202314318567,
    "total_throughput": 10686.286384456987,
    "itl": 69.25659550177865,
    "ttft": 1914559.8869103612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 377686,
    "finished_requests": 83012,
    "scheduler_time": 16.92397145716003
}
#Debug simulation 
Total elapsed time: 6.162226261105388. Arrivals time: 0.2672841511666775 Scheduler time: 5.682848108932376 Scheduler overhead time: 0.07214285060763359 Adapter cache time: 0.030725876335054636 Engine time: 0.07480380870401859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.622218846343458,
    "estimated_duration": 3600.0054504819186,
    "input_throughput": 6216.530310253875,
    "output_throughput": 5472.7561585670865,
    "total_throughput": 11689.286468820961,
    "itl": 88.50768312585312,
    "ttft": 1847725.4376331307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 377686,
    "finished_requests": 90825,
    "scheduler_time": 36.216362505814715
}
#Debug simulation 
Total elapsed time: 6.622362392023206. Arrivals time: 0.2806427935138345 Scheduler time: 6.163352379575372 Scheduler overhead time: 0.060328188352286816 Adapter cache time: 0.028592825401574373 Engine time: 0.061263762414455414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.137926212977618,
    "estimated_duration": 3600.0725554142246,
    "input_throughput": 5684.095441139105,
    "output_throughput": 5002.190017786453,
    "total_throughput": 10686.285458925557,
    "itl": 69.25598154552384,
    "ttft": 1914502.0906568826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 377686,
    "finished_requests": 83013,
    "scheduler_time": 16.92338901315308
}
#Debug simulation 
Total elapsed time: 6.138016253709793. Arrivals time: 0.2692581955343485 Scheduler time: 5.656277013476938 Scheduler overhead time: 0.0716820415109396 Adapter cache time: 0.03067046031355858 Engine time: 0.07584594376385212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.910160050727427,
    "estimated_duration": 3600.0905649292076,
    "input_throughput": 6216.314727749097,
    "output_throughput": 5472.435941451769,
    "total_throughput": 11688.750669200866,
    "itl": 88.50865074914722,
    "ttft": 1847753.7564276685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 377686,
    "finished_requests": 90824,
    "scheduler_time": 36.21833272813791
}
#Debug simulation 
Total elapsed time: 6.910245630890131. Arrivals time: 0.29249667655676603 Scheduler time: 6.440687097143382 Scheduler overhead time: 0.05923833092674613 Adapter cache time: 0.028930285945534706 Engine time: 0.060781868640333414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 270, 34560, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 270, 540, 540, 540, 270, 270, 34560, 540, 540, 270, 34560, 34560, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 34560, 270, 270, 540, 270, 34560, 270, 270, 34560, 270, 540, 540, 270, 540, 34560, 34560, 34560, 540, 34560, 34560, 270, 270, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 34560, 270, 270, 34560, 270, 270, 540]
Prompts retrieved: 1131840 . Total input tokens: 252058092 . Total output tokens: 222460538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.126272439956665,
    "estimated_duration": 3600.043218557573,
    "input_throughput": 5684.025651280595,
    "output_throughput": 5002.102448985368,
    "total_throughput": 10686.128100265963,
    "itl": 69.25802563889349,
    "ttft": 1914583.5241209993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 377686,
    "finished_requests": 83011,
    "scheduler_time": 16.927943602712226
}
#Debug simulation 
Total elapsed time: 6.126446641050279. Arrivals time: 0.26798859518021345 Scheduler time: 5.647266497369856 Scheduler overhead time: 0.07176639465615153 Adapter cache time: 0.030687290243804455 Engine time: 0.07469587679952383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.854529828298837,
    "estimated_duration": 3600.0570651794646,
    "input_throughput": 6515.540330422704,
    "output_throughput": 5700.295753222178,
    "total_throughput": 12215.836083644883,
    "itl": 96.74692032003611,
    "ttft": 1816515.55257619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 376204,
    "finished_requests": 94887,
    "scheduler_time": 43.41866475794679
}
#Debug simulation 
Total elapsed time: 6.854625120293349. Arrivals time: 0.33352770330384374 Scheduler time: 6.358446836471558 Scheduler overhead time: 0.05501704290509224 Adapter cache time: 0.025122300256043673 Engine time: 0.05635076388716698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.63875723304227,
    "estimated_duration": 3600.0878195797573,
    "input_throughput": 6328.589784973507,
    "output_throughput": 5541.494541188379,
    "total_throughput": 11870.084326161887,
    "itl": 87.52227504642507,
    "ttft": 1837768.0101594594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867941,
    "arrivals": 376204,
    "finished_requests": 92233,
    "scheduler_time": 36.79010277839983
}
#Debug simulation 
Total elapsed time: 6.6388829168863595. Arrivals time: 0.3128449539653957 Scheduler time: 6.152231874875724 Scheduler overhead time: 0.059281228575855494 Adapter cache time: 0.025331769604235888 Engine time: 0.06094415532425046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.140618308912963,
    "estimated_duration": 3600.075768934769,
    "input_throughput": 5752.098102681687,
    "output_throughput": 5043.649402238179,
    "total_throughput": 10795.747504919866,
    "itl": 68.64724382010172,
    "ttft": 1906070.4230079653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 376204,
    "finished_requests": 83845,
    "scheduler_time": 17.135549301406364
}
#Debug simulation 
Total elapsed time: 6.140793086960912. Arrivals time: 0.2986718430183828 Scheduler time: 5.6334259528666735 Scheduler overhead time: 0.07237513596192002 Adapter cache time: 0.027844642288982868 Engine time: 0.07426541857421398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.634812287054956,
    "estimated_duration": 3600.0778396886853,
    "input_throughput": 6328.590106810075,
    "output_throughput": 5541.527958108028,
    "total_throughput": 11870.118064918102,
    "itl": 87.52060092095583,
    "ttft": 1837764.9124087289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 376204,
    "finished_requests": 92233,
    "scheduler_time": 36.78931814681861
}
#Debug simulation 
Total elapsed time: 6.634941205382347. Arrivals time: 0.31524953385815024 Scheduler time: 6.145720780827105 Scheduler overhead time: 0.059453537687659264 Adapter cache time: 0.025602870155125856 Engine time: 0.060784085653722286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.272571619134396,
    "estimated_duration": 3600.03170959952,
    "input_throughput": 5750.825178787964,
    "output_throughput": 5042.990580218977,
    "total_throughput": 10793.81575900694,
    "itl": 68.60811907804535,
    "ttft": 1905982.0358657693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 376204,
    "finished_requests": 83831,
    "scheduler_time": 17.07969668215945
}
#Debug simulation 
Total elapsed time: 6.2726798742078245. Arrivals time: 0.31077257823199034 Scheduler time: 5.751698435284197 Scheduler overhead time: 0.07213836116716266 Adapter cache time: 0.028276133351027966 Engine time: 0.07506647985428572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.672792681958526,
    "estimated_duration": 3600.015889606411,
    "input_throughput": 6328.594011425563,
    "output_throughput": 5541.628318251986,
    "total_throughput": 11870.222329677548,
    "itl": 87.52110809369266,
    "ttft": 1837805.5864751455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 376204,
    "finished_requests": 92232,
    "scheduler_time": 36.78921504562212
}
#Debug simulation 
Total elapsed time: 6.672962179873139. Arrivals time: 0.31463327864184976 Scheduler time: 6.18301926413551 Scheduler overhead time: 0.059669609647244215 Adapter cache time: 0.02585915569216013 Engine time: 0.0615334645844996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 135, 34560, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 135, 540, 540, 540, 135, 135, 34560, 540, 540, 135, 34560, 34560, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 34560, 135, 135, 540, 135, 34560, 135, 135, 34560, 135, 540, 540, 135, 540, 34560, 34560, 34560, 540, 34560, 34560, 135, 135, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 34560, 135, 135, 34560, 135, 135, 540]
Prompts retrieved: 1127520 . Total input tokens: 251133005 . Total output tokens: 221605852
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.147163451183587,
    "estimated_duration": 3600.037946360553,
    "input_throughput": 5752.4604764057485,
    "output_throughput": 5043.935166949319,
    "total_throughput": 10796.395643355067,
    "itl": 68.660163154702,
    "ttft": 1906062.475550865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 376204,
    "finished_requests": 83850,
    "scheduler_time": 17.15298798457154
}
#Debug simulation 
Total elapsed time: 6.147255716379732. Arrivals time: 0.2990361866541207 Scheduler time: 5.64046905329451 Scheduler overhead time: 0.07170254969969392 Adapter cache time: 0.02771160425618291 Engine time: 0.0740700913593173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.901799666695297,
    "estimated_duration": 3600.0074013957437,
    "input_throughput": 6566.119278209089,
    "output_throughput": 5730.680440268384,
    "total_throughput": 12296.799718477472,
    "itl": 96.0494170837795,
    "ttft": 1808812.7700058895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 375496,
    "finished_requests": 95547,
    "scheduler_time": 43.600520228464674
}
#Debug simulation 
Total elapsed time: 6.901900586672127. Arrivals time: 0.2907323339022696 Scheduler time: 6.448691111523658 Scheduler overhead time: 0.05532622430473566 Adapter cache time: 0.023196538910269737 Engine time: 0.05768711771816015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.646644437219948,
    "estimated_duration": 3600.0766830550706,
    "input_throughput": 6375.4961409662,
    "output_throughput": 5565.139791133033,
    "total_throughput": 11940.635932099234,
    "itl": 86.9025549283104,
    "ttft": 1830059.9304731556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867945,
    "arrivals": 375496,
    "finished_requests": 92740,
    "scheduler_time": 36.81904338172785
}
#Debug simulation 
Total elapsed time: 6.646791159175336. Arrivals time: 0.31589509127661586 Scheduler time: 6.15750884776935 Scheduler overhead time: 0.05964928539469838 Adapter cache time: 0.023900993168354034 Engine time: 0.061650997027754784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.1545216660015285,
    "estimated_duration": 3600.001533006582,
    "input_throughput": 5802.826417841363,
    "output_throughput": 5060.298956257887,
    "total_throughput": 10863.12537409925,
    "itl": 68.2450537927744,
    "ttft": 1900396.752980843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 375496,
    "finished_requests": 84355,
    "scheduler_time": 17.06298148380581
}
#Debug simulation 
Total elapsed time: 6.154612368904054. Arrivals time: 0.26517401123419404 Scheduler time: 5.6833811472170055 Scheduler overhead time: 0.07182152476161718 Adapter cache time: 0.025881619658321142 Engine time: 0.07427886873483658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.644482561852783,
    "estimated_duration": 3600.0546782777396,
    "input_throughput": 6375.555387669936,
    "output_throughput": 5565.100474969606,
    "total_throughput": 11940.655862639542,
    "itl": 86.90096578779182,
    "ttft": 1830017.7903663705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 375496,
    "finished_requests": 92740,
    "scheduler_time": 36.819391969614266
}
#Debug simulation 
Total elapsed time: 6.644579359795898. Arrivals time: 0.31361534958705306 Scheduler time: 6.157183396164328 Scheduler overhead time: 0.05956189474090934 Adapter cache time: 0.023932649288326502 Engine time: 0.062029223423451185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.188900507055223,
    "estimated_duration": 3600.0760374597885,
    "input_throughput": 5802.834102009808,
    "output_throughput": 5060.304507583191,
    "total_throughput": 10863.138609593,
    "itl": 68.24313882660518,
    "ttft": 1900372.718770283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 375496,
    "finished_requests": 84358,
    "scheduler_time": 17.061474156658214
}
#Debug simulation 
Total elapsed time: 6.189058498945087. Arrivals time: 0.3022955493070185 Scheduler time: 5.679249783046544 Scheduler overhead time: 0.07239911984652281 Adapter cache time: 0.026021515484899282 Engine time: 0.07451149029657245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.722258217167109,
    "estimated_duration": 3600.022832781377,
    "input_throughput": 6375.545674599848,
    "output_throughput": 5565.22192514013,
    "total_throughput": 11940.767599739978,
    "itl": 86.90203765573501,
    "ttft": 1829971.2205203313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 375496,
    "finished_requests": 92738,
    "scheduler_time": 36.81992373623842
}
#Debug simulation 
Total elapsed time: 6.722350253723562. Arrivals time: 0.3292922042310238 Scheduler time: 6.219309485051781 Scheduler overhead time: 0.05991054978221655 Adapter cache time: 0.023811686784029007 Engine time: 0.061580452136695385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 66, 34560, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 66, 540, 540, 540, 66, 66, 34560, 540, 540, 66, 34560, 34560, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 34560, 66, 66, 540, 66, 34560, 66, 66, 34560, 66, 540, 540, 66, 540, 34560, 34560, 34560, 540, 34560, 34560, 66, 66, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 34560, 66, 66, 34560, 66, 66, 540]
Prompts retrieved: 1125312 . Total input tokens: 250628906 . Total output tokens: 221180151
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.466803076211363,
    "estimated_duration": 3600.0064754107207,
    "input_throughput": 5802.994284230167,
    "output_throughput": 5060.353397814822,
    "total_throughput": 10863.347682044989,
    "itl": 68.24228444012371,
    "ttft": 1900404.5655951314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 375496,
    "finished_requests": 84358,
    "scheduler_time": 17.061991819511793
}
#Debug simulation 
Total elapsed time: 6.466879183892161. Arrivals time: 0.5882519241422415 Scheduler time: 5.670815313700587 Scheduler overhead time: 0.07215265743434429 Adapter cache time: 0.026358139235526323 Engine time: 0.07470430061221123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.860082762315869,
    "estimated_duration": 3600.0399959342294,
    "input_throughput": 6619.4556246356115,
    "output_throughput": 5756.335213887606,
    "total_throughput": 12375.790838523219,
    "itl": 95.44013098420669,
    "ttft": 1805136.1226749807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 375142,
    "finished_requests": 96108,
    "scheduler_time": 43.72891560307473
}
#Debug simulation 
Total elapsed time: 6.860222202260047. Arrivals time: 0.28784648794680834 Scheduler time: 6.4109492199495435 Scheduler overhead time: 0.05577366938814521 Adapter cache time: 0.022096457425504923 Engine time: 0.057287263218313456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.724176510237157,
    "estimated_duration": 3600.0754896223825,
    "input_throughput": 6419.00650878405,
    "output_throughput": 5590.198610560513,
    "total_throughput": 12009.205119344562,
    "itl": 86.43129359115612,
    "ttft": 1827502.666559178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867945,
    "arrivals": 375142,
    "finished_requests": 93272,
    "scheduler_time": 36.95402926478594
}
#Debug simulation 
Total elapsed time: 6.7242708071134984. Arrivals time: 0.28143688244745135 Scheduler time: 6.268897315487266 Scheduler overhead time: 0.060226615983992815 Adapter cache time: 0.023043439257889986 Engine time: 0.06222009425982833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.221064812969416,
    "estimated_duration": 3600.023022055312,
    "input_throughput": 5824.093588165353,
    "output_throughput": 5071.6509000479045,
    "total_throughput": 10895.744488213257,
    "itl": 68.06602804471342,
    "ttft": 1898216.1425710823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7044690924976023,
    "arrivals": 375142,
    "finished_requests": 84657,
    "scheduler_time": 17.112070598757008
}
#Debug simulation 
Total elapsed time: 6.221186961978674. Arrivals time: 0.2680651368573308 Scheduler time: 5.7449126918800175 Scheduler overhead time: 0.07251466484740376 Adapter cache time: 0.025323326233774424 Engine time: 0.0755389230325818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.695125306025147,
    "estimated_duration": 3600.0165886857285,
    "input_throughput": 6419.017643592673,
    "output_throughput": 5590.189796138419,
    "total_throughput": 12009.207439731093,
    "itl": 86.43094144084037,
    "ttft": 1827538.5674540969,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407827,
    "arrivals": 375142,
    "finished_requests": 93270,
    "scheduler_time": 36.955178315543
}
#Debug simulation 
Total elapsed time: 6.695259243249893. Arrivals time: 0.3134908205829561 Scheduler time: 6.2079984727315605 Scheduler overhead time: 0.06027546711266041 Adapter cache time: 0.0229314430616796 Engine time: 0.06206256523728371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.20790033088997,
    "estimated_duration": 3600.0264982208632,
    "input_throughput": 5824.123797522578,
    "output_throughput": 5071.801279524867,
    "total_throughput": 10895.925077047445,
    "itl": 68.06589432528669,
    "ttft": 1898307.3005698454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6978413428459318,
    "arrivals": 375142,
    "finished_requests": 84658,
    "scheduler_time": 17.112243120766962
}
#Debug simulation 
Total elapsed time: 6.208019230980426. Arrivals time: 0.26951227942481637 Scheduler time: 5.730548398569226 Scheduler overhead time: 0.07270639948546886 Adapter cache time: 0.0254145385697484 Engine time: 0.07527081668376923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.734296432696283,
    "estimated_duration": 3600.00561864051,
    "input_throughput": 6419.1308147809905,
    "output_throughput": 5590.241275123309,
    "total_throughput": 12009.372089904298,
    "itl": 86.42974563220474,
    "ttft": 1827455.4905657968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 375142,
    "finished_requests": 93271,
    "scheduler_time": 36.954525488707716
}
#Debug simulation 
Total elapsed time: 6.734391560778022. Arrivals time: 0.2847731406800449 Scheduler time: 6.275590140372515 Scheduler overhead time: 0.06017950503155589 Adapter cache time: 0.02315695583820343 Engine time: 0.06178312422707677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_96_slots_96_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 34560, 540, 540, 540, 540, 34560, 540, 34560, 540, 33, 34560, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 33, 540, 540, 540, 33, 33, 34560, 540, 540, 33, 34560, 34560, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 34560, 33, 33, 540, 33, 34560, 33, 33, 34560, 33, 540, 540, 33, 540, 34560, 34560, 34560, 540, 34560, 34560, 33, 33, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 34560, 33, 33, 34560, 33, 33, 540]
Prompts retrieved: 1124256 . Total input tokens: 250394443 . Total output tokens: 220969785
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.201953735202551,
    "estimated_duration": 3600.0314764367163,
    "input_throughput": 5823.968800615169,
    "output_throughput": 5071.9095428778455,
    "total_throughput": 10895.878343493016,
    "itl": 68.06376518215355,
    "ttft": 1898242.0726491467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6916278275474907,
    "arrivals": 375142,
    "finished_requests": 84658,
    "scheduler_time": 17.111229737819702
}
#Debug simulation 
Total elapsed time: 6.202088790014386. Arrivals time: 0.2673580148257315 Scheduler time: 5.7269607139751315 Scheduler overhead time: 0.07271666685119271 Adapter cache time: 0.025303330272436142 Engine time: 0.07524056127294898 
