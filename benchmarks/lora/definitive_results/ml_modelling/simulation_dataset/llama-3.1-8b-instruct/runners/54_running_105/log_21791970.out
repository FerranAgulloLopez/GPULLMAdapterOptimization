INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 10.33705882821232,
    "estimated_duration": 3599.984936414389,
    "input_throughput": 3729.0242145765283,
    "output_throughput": 3228.468786754434,
    "total_throughput": 6957.493001330962,
    "itl": 36.010599959907864,
    "ttft": 153203.98929973473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.721767513142181,
    "arrivals": 55037,
    "finished_requests": 53748,
    "scheduler_time": 47.09436779299867
}
#Debug simulation 
Total elapsed time: 10.3371731643565. Arrivals time: 0.17598330276086926 Scheduler time: 9.843789029400796 Scheduler overhead time: 0.12402589060366154 Adapter cache time: 0.025430822279304266 Engine time: 0.11425143340602517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.106407416984439,
    "estimated_duration": 3600.023390376844,
    "input_throughput": 3717.928898956104,
    "output_throughput": 3215.2588316345214,
    "total_throughput": 6933.1877305906255,
    "itl": 36.06359633458012,
    "ttft": 162153.73674904162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.942079939017944,
    "arrivals": 55037,
    "finished_requests": 53585,
    "scheduler_time": 46.59438524111218
}
#Debug simulation 
Total elapsed time: 10.106545463204384. Arrivals time: 0.1729982178658247 Scheduler time: 9.616870713885874 Scheduler overhead time: 0.12424773164093494 Adapter cache time: 0.025418045930564404 Engine time: 0.11308428086340427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.137140053790063,
    "estimated_duration": 3599.9937157878157,
    "input_throughput": 3729.572899285404,
    "output_throughput": 3230.6667506098215,
    "total_throughput": 6960.239649895226,
    "itl": 36.084967233845845,
    "ttft": 147973.99791377143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.129850605493337,
    "arrivals": 55037,
    "finished_requests": 53810,
    "scheduler_time": 46.737419177978545
}
#Debug simulation 
Total elapsed time: 10.137234557885677. Arrivals time: 0.1746941194869578 Scheduler time: 9.645082061178982 Scheduler overhead time: 0.12514613987877965 Adapter cache time: 0.025352023541927338 Engine time: 0.1135772499255836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 10.257476444821805,
    "estimated_duration": 3599.996995203499,
    "input_throughput": 3728.2278340460975,
    "output_throughput": 3229.4971399949186,
    "total_throughput": 6957.724974041016,
    "itl": 36.023043786330675,
    "ttft": 153633.87561514278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.163609214527568,
    "arrivals": 55037,
    "finished_requests": 53736,
    "scheduler_time": 46.93838303698679
}
#Debug simulation 
Total elapsed time: 10.257584244944155. Arrivals time: 0.1652872390113771 Scheduler time: 9.774633212946355 Scheduler overhead time: 0.12608768604695797 Adapter cache time: 0.024932149332016706 Engine time: 0.11314974306151271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 10.249965580645949,
    "estimated_duration": 3599.9812307106636,
    "input_throughput": 3731.9650128697554,
    "output_throughput": 3231.247402282614,
    "total_throughput": 6963.212415152369,
    "itl": 36.08381366347757,
    "ttft": 150159.95109110005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.876012312024757,
    "arrivals": 55037,
    "finished_requests": 53791,
    "scheduler_time": 47.003986179370465
}
#Debug simulation 
Total elapsed time: 10.250087451655418. Arrivals time: 0.18305101059377193 Scheduler time: 9.747107996605337 Scheduler overhead time: 0.12611440010368824 Adapter cache time: 0.02555143553763628 Engine time: 0.11421167105436325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 10.32092653401196,
    "estimated_duration": 3599.9741761932087,
    "input_throughput": 3729.03536052463,
    "output_throughput": 3228.4784365564933,
    "total_throughput": 6957.513797081123,
    "itl": 36.00538149262192,
    "ttft": 153201.28286994458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.420391720612548,
    "arrivals": 55037,
    "finished_requests": 53748,
    "scheduler_time": 47.093624923657494
}
#Debug simulation 
Total elapsed time: 10.321046275086701. Arrivals time: 0.17347089992836118 Scheduler time: 9.829862323589623 Scheduler overhead time: 0.12441918021067977 Adapter cache time: 0.025241343770176172 Engine time: 0.11420096037909389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 10.21070803469047,
    "estimated_duration": 3599.990122008479,
    "input_throughput": 3728.7141200573506,
    "output_throughput": 3226.5871867224646,
    "total_throughput": 6955.301306779816,
    "itl": 36.191924190148214,
    "ttft": 151087.1342524265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.967934208083827,
    "arrivals": 55037,
    "finished_requests": 53760,
    "scheduler_time": 46.68553412896727
}
#Debug simulation 
Total elapsed time: 10.210807624738663. Arrivals time: 0.175747771281749 Scheduler time: 9.718702329788357 Scheduler overhead time: 0.12436098419129848 Adapter cache time: 0.02533324994146824 Engine time: 0.11330607580021024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.32019381178543,
    "estimated_duration": 3600.0299766595954,
    "input_throughput": 3621.857619113069,
    "output_throughput": 3146.31852330018,
    "total_throughput": 6768.176142413249,
    "itl": 35.24986293029525,
    "ttft": 131440.87131537506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.442520097624747,
    "arrivals": 53479,
    "finished_requests": 52412,
    "scheduler_time": 42.72169176647588
}
#Debug simulation 
Total elapsed time: 9.320305159781128. Arrivals time: 0.16788134211674333 Scheduler time: 8.8350562392734 Scheduler overhead time: 0.12414892949163914 Adapter cache time: 0.025535054970532656 Engine time: 0.11411293037235737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.280149623285979,
    "estimated_duration": 3600.003337348507,
    "input_throughput": 3619.436366855401,
    "output_throughput": 3145.332917479968,
    "total_throughput": 6764.769284335369,
    "itl": 35.35257733335086,
    "ttft": 132918.14028216846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.496942832288255,
    "arrivals": 53479,
    "finished_requests": 52370,
    "scheduler_time": 42.4571180300722
}
#Debug simulation 
Total elapsed time: 9.280267521273345. Arrivals time: 0.16865343460813165 Scheduler time: 8.79133984213695 Scheduler overhead time: 0.1256140684708953 Adapter cache time: 0.025896138977259398 Engine time: 0.1147073688916862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.427514540031552,
    "estimated_duration": 3600.009862840202,
    "input_throughput": 3614.549819520202,
    "output_throughput": 3138.1180692341522,
    "total_throughput": 6752.667888754355,
    "itl": 35.277073634620365,
    "ttft": 140954.9592438166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.5266560205956,
    "arrivals": 53479,
    "finished_requests": 52278,
    "scheduler_time": 42.85240042401412
}
#Debug simulation 
Total elapsed time: 9.427614511922002. Arrivals time: 0.1680810316465795 Scheduler time: 8.941221738234162 Scheduler overhead time: 0.12472286028787494 Adapter cache time: 0.0256825708784163 Engine time: 0.11425802949815989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 9.316204462666065,
    "estimated_duration": 3600.0198607900807,
    "input_throughput": 3616.050050663617,
    "output_throughput": 3140.2212868678375,
    "total_throughput": 6756.271337531455,
    "itl": 35.21815214424708,
    "ttft": 137539.97624315578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.806732633113926,
    "arrivals": 53479,
    "finished_requests": 52314,
    "scheduler_time": 42.60227134893412
}
#Debug simulation 
Total elapsed time: 9.31631378410384. Arrivals time: 0.17409687722101808 Scheduler time: 8.821304305456579 Scheduler overhead time: 0.12543777795508504 Adapter cache time: 0.026104172226041555 Engine time: 0.11526891542598605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 9.223896705079824,
    "estimated_duration": 3600.013282796792,
    "input_throughput": 3618.6344262261814,
    "output_throughput": 3141.6553527862106,
    "total_throughput": 6760.2897790123925,
    "itl": 35.30015540829672,
    "ttft": 134361.02794428536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.743288167714974,
    "arrivals": 53479,
    "finished_requests": 52348,
    "scheduler_time": 42.39533798522798
}
#Debug simulation 
Total elapsed time: 9.224030251149088. Arrivals time: 0.16786272497847676 Scheduler time: 8.735546031501144 Scheduler overhead time: 0.12553747091442347 Adapter cache time: 0.026011180132627487 Engine time: 0.11498449370265007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 9.348410358652472,
    "estimated_duration": 3600.016360797159,
    "input_throughput": 3621.871317582788,
    "output_throughput": 3146.3304232017085,
    "total_throughput": 6768.201740784497,
    "itl": 35.24669621264059,
    "ttft": 131437.71859593678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.116239103134829,
    "arrivals": 53479,
    "finished_requests": 52412,
    "scheduler_time": 42.72035737094286
}
#Debug simulation 
Total elapsed time: 9.348567768000066. Arrivals time: 0.1741256290115416 Scheduler time: 8.856303206644952 Scheduler overhead time: 0.12402017321437597 Adapter cache time: 0.02598776062950492 Engine time: 0.11361452145501971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 9.363479886204004,
    "estimated_duration": 3600.024598063794,
    "input_throughput": 3622.3369159792937,
    "output_throughput": 3145.9965596044253,
    "total_throughput": 6768.333475583719,
    "itl": 35.247841729846556,
    "ttft": 131029.53643288671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.447100218310865,
    "arrivals": 53479,
    "finished_requests": 52426,
    "scheduler_time": 42.915341321154244
}
#Debug simulation 
Total elapsed time: 9.363586985040456. Arrivals time: 0.17188502149656415 Scheduler time: 8.874565854202956 Scheduler overhead time: 0.12419972335919738 Adapter cache time: 0.02584570925682783 Engine time: 0.11334951594471931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.95589844416827,
    "estimated_duration": 3600.0095633241076,
    "input_throughput": 3591.340181902739,
    "output_throughput": 3102.0273150853877,
    "total_throughput": 6693.367496988127,
    "itl": 34.83621803630489,
    "ttft": 117536.08261212654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.171411327314791,
    "arrivals": 52821,
    "finished_requests": 51896,
    "scheduler_time": 40.41408950772462
}
#Debug simulation 
Total elapsed time: 8.955997151322663. Arrivals time: 0.17454498913139105 Scheduler time: 8.458254722878337 Scheduler overhead time: 0.12737090047448874 Adapter cache time: 0.02595013612881303 Engine time: 0.11480038892477751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.951567686162889,
    "estimated_duration": 3600.0123236536538,
    "input_throughput": 3599.1682347492315,
    "output_throughput": 3107.407418163116,
    "total_throughput": 6706.575652912347,
    "itl": 34.93600288836133,
    "ttft": 109706.36922271064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.02839505943464,
    "arrivals": 52821,
    "finished_requests": 52016,
    "scheduler_time": 40.552625986753505
}
#Debug simulation 
Total elapsed time: 8.951687130145729. Arrivals time: 0.16789314290508628 Scheduler time: 8.46349034132436 Scheduler overhead time: 0.12514666002243757 Adapter cache time: 0.025896944105625153 Engine time: 0.11447607073932886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.767692396882921,
    "estimated_duration": 3600.0216313601027,
    "input_throughput": 3594.615067663067,
    "output_throughput": 3103.9782935354947,
    "total_throughput": 6698.593361198562,
    "itl": 34.87280847457442,
    "ttft": 112334.07751728821,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.540468374346288,
    "arrivals": 52821,
    "finished_requests": 51951,
    "scheduler_time": 40.05861055827028
}
#Debug simulation 
Total elapsed time: 8.767823501024395. Arrivals time: 0.17159583466127515 Scheduler time: 8.277424732223153 Scheduler overhead time: 0.12490679416805506 Adapter cache time: 0.025798579677939415 Engine time: 0.11377351963892579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 8.842566252686083,
    "estimated_duration": 3600.0247412175067,
    "input_throughput": 3592.9561405251766,
    "output_throughput": 3102.843397743502,
    "total_throughput": 6695.799538268679,
    "itl": 34.795638669308595,
    "ttft": 113893.59976379907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.720408035013879,
    "arrivals": 52821,
    "finished_requests": 51935,
    "scheduler_time": 40.15684545377403
}
#Debug simulation 
Total elapsed time: 8.842661809641868. Arrivals time: 0.16922733932733536 Scheduler time: 8.352419241797179 Scheduler overhead time: 0.1246724626980722 Adapter cache time: 0.026057947892695665 Engine time: 0.11614952003583312 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 8.96151928184554,
    "estimated_duration": 3600.0052118141325,
    "input_throughput": 3594.13007446169,
    "output_throughput": 3101.994398050488,
    "total_throughput": 6696.124472512178,
    "itl": 34.749643328184106,
    "ttft": 114579.23191412642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.32338571778024,
    "arrivals": 52821,
    "finished_requests": 51937,
    "scheduler_time": 40.376258499247754
}
#Debug simulation 
Total elapsed time: 8.961621431168169. Arrivals time: 0.17300246190279722 Scheduler time: 8.466691778972745 Scheduler overhead time: 0.1259345728904009 Adapter cache time: 0.02615684876218438 Engine time: 0.1148838521912694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.978850794024765,
    "estimated_duration": 3600.0180962352233,
    "input_throughput": 3590.8850051389686,
    "output_throughput": 3102.7230145540266,
    "total_throughput": 6693.608019692995,
    "itl": 34.884295834533894,
    "ttft": 117967.4759848614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.873650107393116,
    "arrivals": 52821,
    "finished_requests": 51896,
    "scheduler_time": 40.53175697793261
}
#Debug simulation 
Total elapsed time: 8.97897223988548. Arrivals time: 0.17172517394647002 Scheduler time: 8.48241121834144 Scheduler overhead time: 0.12871345784515142 Adapter cache time: 0.025926931761205196 Engine time: 0.11519307969138026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.84759760927409,
    "estimated_duration": 3600.029193705645,
    "input_throughput": 3592.096148167334,
    "output_throughput": 3101.701236068643,
    "total_throughput": 6693.797384235977,
    "itl": 34.95114732898175,
    "ttft": 114557.97003267895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.447401079628525,
    "arrivals": 52821,
    "finished_requests": 51927,
    "scheduler_time": 40.202678277423495
}
#Debug simulation 
Total elapsed time: 8.847755194175988. Arrivals time: 0.1727267811074853 Scheduler time: 8.350906533189118 Scheduler overhead time: 0.12688890099525452 Adapter cache time: 0.026290059555321932 Engine time: 0.11558624915778637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.453804078977555,
    "estimated_duration": 3600.039507334475,
    "input_throughput": 3571.84413498863,
    "output_throughput": 3096.095744863845,
    "total_throughput": 6667.939879852474,
    "itl": 34.58099211746704,
    "ttft": 104422.21909096143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.15157410021894,
    "arrivals": 52471,
    "finished_requests": 51665,
    "scheduler_time": 38.86274307540875
}
#Debug simulation 
Total elapsed time: 8.453898920677602. Arrivals time: 0.16815208084881306 Scheduler time: 7.966745796613395 Scheduler overhead time: 0.12492231884971261 Adapter cache time: 0.025870942045003176 Engine time: 0.11373228440061212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.531210100278258,
    "estimated_duration": 3600.002060915373,
    "input_throughput": 3576.8598967762364,
    "output_throughput": 3102.971834732679,
    "total_throughput": 6679.831731508915,
    "itl": 34.614379050258094,
    "ttft": 100884.56550180513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.833250996856913,
    "arrivals": 52471,
    "finished_requests": 51738,
    "scheduler_time": 39.273858006944366
}
#Debug simulation 
Total elapsed time: 8.531315511092544. Arrivals time: 0.1718071522191167 Scheduler time: 8.039851907175034 Scheduler overhead time: 0.12472064606845379 Adapter cache time: 0.025907511357218027 Engine time: 0.11403479985892773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.4953993097879,
    "estimated_duration": 3600.0295194651485,
    "input_throughput": 3570.634887991074,
    "output_throughput": 3098.7798682432435,
    "total_throughput": 6669.414756234318,
    "itl": 34.67281842221012,
    "ttft": 106212.46607750856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.291655878154486,
    "arrivals": 52471,
    "finished_requests": 51638,
    "scheduler_time": 38.845166918602196
}
#Debug simulation 
Total elapsed time: 8.495499581098557. Arrivals time: 0.17018750170245767 Scheduler time: 8.004284454975277 Scheduler overhead time: 0.1255164695903659 Adapter cache time: 0.025892214383929968 Engine time: 0.1153160915710032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 8.444223849102855,
    "estimated_duration": 3600.036680979144,
    "input_throughput": 3570.373620888799,
    "output_throughput": 3098.717871109555,
    "total_throughput": 6669.091491998353,
    "itl": 34.65391013346462,
    "ttft": 106476.92712478533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.44283018413471,
    "arrivals": 52471,
    "finished_requests": 51634,
    "scheduler_time": 38.841019932282045
}
#Debug simulation 
Total elapsed time: 8.44436282897368. Arrivals time: 0.16626802226528525 Scheduler time: 7.955917569343001 Scheduler overhead time: 0.12565985741093755 Adapter cache time: 0.02602513413876295 Engine time: 0.11546114645898342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 8.525343381799757,
    "estimated_duration": 3600.0210228498418,
    "input_throughput": 3576.8410568354316,
    "output_throughput": 3102.955490842403,
    "total_throughput": 6679.7965476778345,
    "itl": 34.61661146972985,
    "ttft": 100883.24697213391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.993915867065011,
    "arrivals": 52471,
    "finished_requests": 51738,
    "scheduler_time": 39.27405177274864
}
#Debug simulation 
Total elapsed time: 8.525461686775088. Arrivals time: 0.171174640301615 Scheduler time: 8.036629094276577 Scheduler overhead time: 0.12398541532456875 Adapter cache time: 0.025527713354676962 Engine time: 0.11414918163791299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.534278831910342,
    "estimated_duration": 3600.0172752470025,
    "input_throughput": 3570.7270318912624,
    "output_throughput": 3098.461242587974,
    "total_throughput": 6669.1882744792365,
    "itl": 34.53496345494708,
    "ttft": 105159.35563692584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.701284241997689,
    "arrivals": 52471,
    "finished_requests": 51659,
    "scheduler_time": 38.940710239094656
}
#Debug simulation 
Total elapsed time: 8.534411725122482. Arrivals time: 0.16885990090668201 Scheduler time: 8.043253256008029 Scheduler overhead time: 0.127570740878582 Adapter cache time: 0.025665948633104563 Engine time: 0.11454393435269594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.473567777778953,
    "estimated_duration": 3600.0168416945935,
    "input_throughput": 3574.196890129878,
    "output_throughput": 3100.8510490037925,
    "total_throughput": 6675.04793913367,
    "itl": 34.63056523576801,
    "ttft": 103318.18082273219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.068910513166271,
    "arrivals": 52471,
    "finished_requests": 51683,
    "scheduler_time": 38.90448688465207
}
#Debug simulation 
Total elapsed time: 8.47367287101224. Arrivals time: 0.16965896682813764 Scheduler time: 7.984328871127218 Scheduler overhead time: 0.12420196225866675 Adapter cache time: 0.0258572306483984 Engine time: 0.11517999926581979 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.471112412866205,
    "estimated_duration": 3599.9236270571023,
    "input_throughput": 3434.432304917308,
    "output_throughput": 3012.5058538715434,
    "total_throughput": 6446.938158788852,
    "itl": 34.35459351262981,
    "ttft": 119347.72388433894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.5549310511679,
    "arrivals": 50642,
    "finished_requests": 49685,
    "scheduler_time": 37.73087834512943
}
#Debug simulation 
Total elapsed time: 8.471242411993444. Arrivals time: 0.16874287277460098 Scheduler time: 7.977052041795105 Scheduler overhead time: 0.12668055202811956 Adapter cache time: 0.02666277438402176 Engine time: 0.11682424461469054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.496569177135825,
    "estimated_duration": 3599.92394116026,
    "input_throughput": 3442.0946671462943,
    "output_throughput": 3019.656297654004,
    "total_throughput": 6461.750964800298,
    "itl": 34.289556019082035,
    "ttft": 111332.73830747134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.612967777070498,
    "arrivals": 50642,
    "finished_requests": 49807,
    "scheduler_time": 37.935040950389386
}
#Debug simulation 
Total elapsed time: 8.496673779096454. Arrivals time: 0.1677705510519445 Scheduler time: 8.005195174366236 Scheduler overhead time: 0.12662159092724323 Adapter cache time: 0.02654193714261055 Engine time: 0.1149886129423976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.447411020752043,
    "estimated_duration": 3599.912307210219,
    "input_throughput": 3442.3157961876323,
    "output_throughput": 3018.989928791438,
    "total_throughput": 6461.30572497907,
    "itl": 34.38459036897072,
    "ttft": 111919.20129333784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.831674974397655,
    "arrivals": 50642,
    "finished_requests": 49802,
    "scheduler_time": 38.0036088028506
}
#Debug simulation 
Total elapsed time: 8.447544633876532. Arrivals time: 0.16536049311980605 Scheduler time: 7.9604540332220495 Scheduler overhead time: 0.12621085857972503 Adapter cache time: 0.026285492349416018 Engine time: 0.11473206803202629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 8.358688379172236,
    "estimated_duration": 3599.9110242897627,
    "input_throughput": 3447.3599253605007,
    "output_throughput": 3023.4208363934067,
    "total_throughput": 6470.780761753907,
    "itl": 34.543527368972136,
    "ttft": 106426.346033818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.01157528713818,
    "arrivals": 50642,
    "finished_requests": 49861,
    "scheduler_time": 37.697887895010126
}
#Debug simulation 
Total elapsed time: 8.358826245181262. Arrivals time: 0.1680611534975469 Scheduler time: 7.867756558116525 Scheduler overhead time: 0.12762938579544425 Adapter cache time: 0.02627120353281498 Engine time: 0.11453834921121597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 8.370427021291107,
    "estimated_duration": 3599.922706212371,
    "input_throughput": 3445.118412847489,
    "output_throughput": 3023.54769484817,
    "total_throughput": 6468.666107695659,
    "itl": 34.3978322178458,
    "ttft": 109225.93023342623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.995710193007143,
    "arrivals": 50642,
    "finished_requests": 49815,
    "scheduler_time": 37.542089222057015
}
#Debug simulation 
Total elapsed time: 8.370566045399755. Arrivals time: 0.16707715671509504 Scheduler time: 7.878261220641434 Scheduler overhead time: 0.12729414412751794 Adapter cache time: 0.026588916778564453 Engine time: 0.11618810705840588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 8.419372776988894,
    "estimated_duration": 3599.924364862554,
    "input_throughput": 3435.5744028172676,
    "output_throughput": 3014.1258260614263,
    "total_throughput": 6449.700228878693,
    "itl": 34.335167810495925,
    "ttft": 117238.37513136624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.250301442886828,
    "arrivals": 50642,
    "finished_requests": 49715,
    "scheduler_time": 37.71489551906547
}
#Debug simulation 
Total elapsed time: 8.419469540007412. Arrivals time: 0.1677043465897441 Scheduler time: 7.925568896345794 Scheduler overhead time: 0.12811336340382695 Adapter cache time: 0.026603969279676676 Engine time: 0.11586524546146393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 8.447294311132282,
    "estimated_duration": 3599.9006824443804,
    "input_throughput": 3442.116906288136,
    "output_throughput": 3019.675807449989,
    "total_throughput": 6461.792713738125,
    "itl": 34.29570451031252,
    "ttft": 111324.9448615899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.696984968501974,
    "arrivals": 50642,
    "finished_requests": 49807,
    "scheduler_time": 37.93351667199132
}
#Debug simulation 
Total elapsed time: 8.447415884118527. Arrivals time: 0.16548451129347086 Scheduler time: 7.965842814184725 Scheduler overhead time: 0.12198120914399624 Adapter cache time: 0.025817808229476213 Engine time: 0.1141824615187943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.471917247865349,
    "estimated_duration": 3599.9376538167867,
    "input_throughput": 3394.601844574594,
    "output_throughput": 2951.5561161828846,
    "total_throughput": 6346.157960757479,
    "itl": 33.65722474935774,
    "ttft": 100984.43960793468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.269071226618516,
    "arrivals": 49929,
    "finished_requests": 49116,
    "scheduler_time": 34.18175523383702
}
#Debug simulation 
Total elapsed time: 7.472024395130575. Arrivals time: 0.16266585141420364 Scheduler time: 6.989731926470995 Scheduler overhead time: 0.12426820024847984 Adapter cache time: 0.026669586077332497 Engine time: 0.11419941112399101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.557271279860288,
    "estimated_duration": 3599.9150294279084,
    "input_throughput": 3401.1841668234565,
    "output_throughput": 2958.7001117892073,
    "total_throughput": 6359.884278612664,
    "itl": 33.78552462467985,
    "ttft": 95753.3446414797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1538,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.185868065310537,
    "arrivals": 49929,
    "finished_requests": 49198,
    "scheduler_time": 34.382853904264884
}
#Debug simulation 
Total elapsed time: 7.557370537891984. Arrivals time: 0.15936236316338181 Scheduler time: 7.077955977991223 Scheduler overhead time: 0.12390222446992993 Adapter cache time: 0.026525115594267845 Engine time: 0.11523595917969942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.461140374653041,
    "estimated_duration": 3599.9297308413475,
    "input_throughput": 3393.3951252834527,
    "output_throughput": 2955.708526430937,
    "total_throughput": 6349.10365171439,
    "itl": 33.706604218675814,
    "ttft": 101515.15306876082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.506158440988369,
    "arrivals": 49929,
    "finished_requests": 49110,
    "scheduler_time": 34.21572128300314
}
#Debug simulation 
Total elapsed time: 7.46123629482463. Arrivals time: 0.15925802197307348 Scheduler time: 6.98005355335772 Scheduler overhead time: 0.12611571233719587 Adapter cache time: 0.026711488142609596 Engine time: 0.11401893384754658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 7.480498178862035,
    "estimated_duration": 3599.906566478698,
    "input_throughput": 3393.416960804415,
    "output_throughput": 2955.7275455646086,
    "total_throughput": 6349.1445063690235,
    "itl": 33.693643611999626,
    "ttft": 101513.37986825535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.541724056010226,
    "arrivals": 49929,
    "finished_requests": 49110,
    "scheduler_time": 34.2117794558591
}
#Debug simulation 
Total elapsed time: 7.480640823021531. Arrivals time: 0.1603088052943349 Scheduler time: 6.995391977485269 Scheduler overhead time: 0.12778420373797417 Adapter cache time: 0.026910822838544846 Engine time: 0.11502523673698306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 7.4131044410169125,
    "estimated_duration": 3599.9301247496546,
    "input_throughput": 3393.1236376010083,
    "output_throughput": 2951.227560490171,
    "total_throughput": 6344.351198091179,
    "itl": 33.67486840445961,
    "ttft": 102762.89933731404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.478803649782066,
    "arrivals": 49929,
    "finished_requests": 49091,
    "scheduler_time": 34.17437336478156
}
#Debug simulation 
Total elapsed time: 7.413206621073186. Arrivals time: 0.15973768197000027 Scheduler time: 6.936013395432383 Scheduler overhead time: 0.12357793422415853 Adapter cache time: 0.02657190104946494 Engine time: 0.11321602435782552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.445749599952251,
    "estimated_duration": 3599.901743209591,
    "input_throughput": 3397.081607315414,
    "output_throughput": 2958.454635626963,
    "total_throughput": 6355.536242942378,
    "itl": 33.70409358203218,
    "ttft": 99382.08302255822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1532,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.780166880954253,
    "arrivals": 49929,
    "finished_requests": 49147,
    "scheduler_time": 34.339392900577906
}
#Debug simulation 
Total elapsed time: 7.445847759954631. Arrivals time: 0.1552263810299337 Scheduler time: 6.974240506067872 Scheduler overhead time: 0.12251346558332443 Adapter cache time: 0.026608861982822418 Engine time: 0.11267829639837146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.435694721993059,
    "estimated_duration": 3599.91728582029,
    "input_throughput": 3398.325302691611,
    "output_throughput": 2955.5523516912112,
    "total_throughput": 6353.877654382822,
    "itl": 33.898935442606756,
    "ttft": 98205.90428815318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.25291869258495,
    "arrivals": 49929,
    "finished_requests": 49166,
    "scheduler_time": 34.44353394539151
}
#Debug simulation 
Total elapsed time: 7.435798239894211. Arrivals time: 0.15314223570749164 Scheduler time: 6.964775443077087 Scheduler overhead time: 0.12216795329004526 Adapter cache time: 0.026505746878683567 Engine time: 0.11471033841371536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.346168543212116,
    "estimated_duration": 3600.0026920290966,
    "input_throughput": 3367.790259391842,
    "output_throughput": 2954.4608462515102,
    "total_throughput": 6322.251105643352,
    "itl": 33.58013167682139,
    "ttft": 93331.37586807994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.495419369880349,
    "arrivals": 49606,
    "finished_requests": 48889,
    "scheduler_time": 33.75939935495168
}
#Debug simulation 
Total elapsed time: 7.346270677167922. Arrivals time: 0.15539689920842648 Scheduler time: 6.87210984621197 Scheduler overhead time: 0.12369786063209176 Adapter cache time: 0.02624036744236946 Engine time: 0.11351153161376715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.389985415153205,
    "estimated_duration": 3599.986285778325,
    "input_throughput": 3372.7356262344524,
    "output_throughput": 2956.134872524706,
    "total_throughput": 6328.870498759159,
    "itl": 33.587554782397454,
    "ttft": 90256.35923500548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.27264612927096,
    "arrivals": 49606,
    "finished_requests": 48939,
    "scheduler_time": 33.93727068005942
}
#Debug simulation 
Total elapsed time: 7.390089466236532. Arrivals time: 0.1586295384913683 Scheduler time: 6.91411124356091 Scheduler overhead time: 0.12422454496845603 Adapter cache time: 0.026135633699595928 Engine time: 0.11316081043332815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.341672312933952,
    "estimated_duration": 3599.994753838987,
    "input_throughput": 3372.8874151973496,
    "output_throughput": 2954.3262496864418,
    "total_throughput": 6327.213664883791,
    "itl": 33.77065010141516,
    "ttft": 88679.95682482282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.715639167567627,
    "arrivals": 49606,
    "finished_requests": 48949,
    "scheduler_time": 33.76004596973928
}
#Debug simulation 
Total elapsed time: 7.341771793086082. Arrivals time: 0.15598847763612866 Scheduler time: 6.867544527165592 Scheduler overhead time: 0.12364978902041912 Adapter cache time: 0.02597325947135687 Engine time: 0.1142514985986054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 7.414975335355848,
    "estimated_duration": 3600.031944361318,
    "input_throughput": 3369.4942676826818,
    "output_throughput": 2951.7657521467463,
    "total_throughput": 6321.2600198294285,
    "itl": 33.6466131351864,
    "ttft": 92946.91845356823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.680980663835136,
    "arrivals": 49606,
    "finished_requests": 48907,
    "scheduler_time": 33.999034430587876
}
#Debug simulation 
Total elapsed time: 7.415082874242216. Arrivals time: 0.15227262629196048 Scheduler time: 6.942051290068775 Scheduler overhead time: 0.12432139879092574 Adapter cache time: 0.02628265507519245 Engine time: 0.11542732315137982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 7.390451597049832,
    "estimated_duration": 3600.0124770069456,
    "input_throughput": 3367.091941325128,
    "output_throughput": 2952.84559925693,
    "total_throughput": 6319.937540582058,
    "itl": 33.46612552469001,
    "ttft": 94445.53757862687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1431,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.533549617263402,
    "arrivals": 49606,
    "finished_requests": 48878,
    "scheduler_time": 33.844759111166546
}
#Debug simulation 
Total elapsed time: 7.390552721917629. Arrivals time: 0.15641375724226236 Scheduler time: 6.916303555015475 Scheduler overhead time: 0.12398967845365405 Adapter cache time: 0.026355677749961615 Engine time: 0.11274179490283132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.374604844953865,
    "estimated_duration": 3600.0023255828273,
    "input_throughput": 3374.8089310006403,
    "output_throughput": 2958.3886444506034,
    "total_throughput": 6333.197575451243,
    "itl": 33.61002083228227,
    "ttft": 86058.83755122713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.192846154421686,
    "arrivals": 49606,
    "finished_requests": 48990,
    "scheduler_time": 33.825361117876724
}
#Debug simulation 
Total elapsed time: 7.3747034929692745. Arrivals time: 0.15727679803967476 Scheduler time: 6.89652160089463 Scheduler overhead time: 0.12686129985377192 Adapter cache time: 0.02587078418582678 Engine time: 0.1133446954190731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.3552625039592385,
    "estimated_duration": 3599.988765223974,
    "input_throughput": 3374.359697270953,
    "output_throughput": 2959.2203461605,
    "total_throughput": 6333.580043431453,
    "itl": 33.56672022420883,
    "ttft": 87399.43083185793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1431,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.445059072617356,
    "arrivals": 49606,
    "finished_requests": 48975,
    "scheduler_time": 33.90062173107735
}
#Debug simulation 
Total elapsed time: 7.3553509139455855. Arrivals time: 0.15270743146538734 Scheduler time: 6.885998006444424 Scheduler overhead time: 0.1237153229303658 Adapter cache time: 0.025939518585801125 Engine time: 0.1125985374674201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 7.023984326981008,
    "estimated_duration": 3599.9624924973064,
    "input_throughput": 3311.895339145376,
    "output_throughput": 2899.1785391519074,
    "total_throughput": 6211.073878297284,
    "itl": 33.114435676247226,
    "ttft": 87345.13507304042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.832652230509806,
    "arrivals": 48532,
    "finished_requests": 47867,
    "scheduler_time": 31.85488249088763
}
#Debug simulation 
Total elapsed time: 7.024079148191959. Arrivals time: 0.1493489588610828 Scheduler time: 6.555954959243536 Scheduler overhead time: 0.12400098843500018 Adapter cache time: 0.02658715471625328 Engine time: 0.11314265243709087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 7.091263885144144,
    "estimated_duration": 3599.9650404719023,
    "input_throughput": 3311.1996549937157,
    "output_throughput": 2898.628704081007,
    "total_throughput": 6209.828359074722,
    "itl": 33.21572787009452,
    "ttft": 89441.28433229076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.719549028160904,
    "arrivals": 48532,
    "finished_requests": 47848,
    "scheduler_time": 32.033720073883494
}
#Debug simulation 
Total elapsed time: 7.091358779929578. Arrivals time: 0.15153442742303014 Scheduler time: 6.6222501513548195 Scheduler overhead time: 0.12420522281900048 Adapter cache time: 0.02631774451583624 Engine time: 0.11213547550141811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.958584817592055,
    "estimated_duration": 3599.9351180984463,
    "input_throughput": 3313.390827527078,
    "output_throughput": 2903.0323206270095,
    "total_throughput": 6216.423148154087,
    "itl": 33.255624023614466,
    "ttft": 84972.2779493595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.227076513701936,
    "arrivals": 48532,
    "finished_requests": 47887,
    "scheduler_time": 31.666866367062884
}
#Debug simulation 
Total elapsed time: 6.958679188974202. Arrivals time: 0.15239625284448266 Scheduler time: 6.48848736891523 Scheduler overhead time: 0.12348375376313925 Adapter cache time: 0.026691142469644547 Engine time: 0.11285475408658385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.998709871899337,
    "estimated_duration": 3599.9406986505264,
    "input_throughput": 3312.681235129894,
    "output_throughput": 2900.989453497056,
    "total_throughput": 6213.67068862695,
    "itl": 33.169304951481934,
    "ttft": 85738.80466472056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.358809606754308,
    "arrivals": 48532,
    "finished_requests": 47879,
    "scheduler_time": 31.70678399301482
}
#Debug simulation 
Total elapsed time: 6.998807104770094. Arrivals time: 0.15062858583405614 Scheduler time: 6.530896307900548 Scheduler overhead time: 0.12301069870591164 Adapter cache time: 0.026570418383926153 Engine time: 0.1122697638347745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 7.0106782810762525,
    "estimated_duration": 3599.958642586273,
    "input_throughput": 3314.540578007222,
    "output_throughput": 2899.828035941061,
    "total_throughput": 6214.368613948283,
    "itl": 33.13319812653314,
    "ttft": 85799.32548195357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.20779965113371,
    "arrivals": 48532,
    "finished_requests": 47882,
    "scheduler_time": 31.763874897536184
}
#Debug simulation 
Total elapsed time: 7.010769482702017. Arrivals time: 0.14742764038965106 Scheduler time: 6.542152112815529 Scheduler overhead time: 0.12470857938751578 Adapter cache time: 0.026715871412307024 Engine time: 0.11418559867888689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.981101912911981,
    "estimated_duration": 3599.9473800767973,
    "input_throughput": 3308.768085311818,
    "output_throughput": 2898.086804751417,
    "total_throughput": 6206.854890063235,
    "itl": 33.098963164749286,
    "ttft": 89493.25336148289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.563113568974826,
    "arrivals": 48532,
    "finished_requests": 47833,
    "scheduler_time": 31.739981797857368
}
#Debug simulation 
Total elapsed time: 6.981202865950763. Arrivals time: 0.15008322428911924 Scheduler time: 6.51125950505957 Scheduler overhead time: 0.12479852000251412 Adapter cache time: 0.026617951225489378 Engine time: 0.1127593475393951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.960517790168524,
    "estimated_duration": 3599.9265435767065,
    "input_throughput": 3315.94182700735,
    "output_throughput": 2902.7847856077615,
    "total_throughput": 6218.726612615112,
    "itl": 33.156486439124066,
    "ttft": 83068.80194877279,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.14539617415506,
    "arrivals": 48532,
    "finished_requests": 47915,
    "scheduler_time": 31.7041446318302
}
#Debug simulation 
Total elapsed time: 6.960609873291105. Arrivals time: 0.1527113188058138 Scheduler time: 6.4908571797423065 Scheduler overhead time: 0.12333688139915466 Adapter cache time: 0.02640600921586156 Engine time: 0.11293206829577684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.707799742929637,
    "estimated_duration": 3600.0279142130194,
    "input_throughput": 3278.1090261573177,
    "output_throughput": 2832.611647187522,
    "total_throughput": 6110.720673344839,
    "itl": 32.68433116399307,
    "ttft": 86367.8192496084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.336721553113545,
    "arrivals": 48139,
    "finished_requests": 47450,
    "scheduler_time": 29.87136202755911
}
#Debug simulation 
Total elapsed time: 6.707901103887707. Arrivals time: 0.1465433337725699 Scheduler time: 6.2501013106666505 Scheduler overhead time: 0.11988857854157686 Adapter cache time: 0.02529508015140891 Engine time: 0.11146361054852605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.654115477111191,
    "estimated_duration": 3600.036268419181,
    "input_throughput": 3285.6396764008164,
    "output_throughput": 2836.9869741575585,
    "total_throughput": 6122.626650558374,
    "itl": 32.825759274682035,
    "ttft": 80269.76722993616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.34759110646317,
    "arrivals": 48139,
    "finished_requests": 47520,
    "scheduler_time": 29.69467491554241
}
#Debug simulation 
Total elapsed time: 6.654222380835563. Arrivals time: 0.14510294375941157 Scheduler time: 6.197127623017877 Scheduler overhead time: 0.11959645617753267 Adapter cache time: 0.025247578974813223 Engine time: 0.11252640141174197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.603308624122292,
    "estimated_duration": 3600.0029351742146,
    "input_throughput": 3288.424263306082,
    "output_throughput": 2837.854908444847,
    "total_throughput": 6126.279171750929,
    "itl": 32.87609773792734,
    "ttft": 75710.74807212806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.727132853730497,
    "arrivals": 48139,
    "finished_requests": 47574,
    "scheduler_time": 29.594057246441995
}
#Debug simulation 
Total elapsed time: 6.603412692900747. Arrivals time: 0.146283068228513 Scheduler time: 6.139983053319156 Scheduler overhead time: 0.12347677210345864 Adapter cache time: 0.02598059130832553 Engine time: 0.11288342392072082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.475908624008298,
    "estimated_duration": 3600.0322892636186,
    "input_throughput": 3286.296635528224,
    "output_throughput": 2838.4917631086614,
    "total_throughput": 6124.788398636885,
    "itl": 32.60435265963187,
    "ttft": 74672.64030778341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.162843984812584,
    "arrivals": 48139,
    "finished_requests": 47572,
    "scheduler_time": 29.26767269479429
}
#Debug simulation 
Total elapsed time: 6.476011671125889. Arrivals time: 0.14875864842906594 Scheduler time: 6.010515126399696 Scheduler overhead time: 0.12326972838491201 Adapter cache time: 0.02622609445825219 Engine time: 0.11255631083622575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.687390123959631,
    "estimated_duration": 3600.037947023923,
    "input_throughput": 3285.63814439187,
    "output_throughput": 2836.985651343783,
    "total_throughput": 6122.623795735653,
    "itl": 32.829426965156486,
    "ttft": 80270.83626681249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.51745119476217,
    "arrivals": 48139,
    "finished_requests": 47520,
    "scheduler_time": 29.695730533012977
}
#Debug simulation 
Total elapsed time: 6.687484691850841. Arrivals time: 0.14746703300625086 Scheduler time: 6.207138099242002 Scheduler overhead time: 0.1280719256028533 Adapter cache time: 0.02680256962776184 Engine time: 0.12164594838395715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.529289877973497,
    "estimated_duration": 3600.026699847023,
    "input_throughput": 3293.8164043349675,
    "output_throughput": 2843.8214084453984,
    "total_throughput": 6137.6378127803655,
    "itl": 32.678687350312664,
    "ttft": 69155.95225641763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1489,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.505658280509683,
    "arrivals": 48139,
    "finished_requests": 47650,
    "scheduler_time": 29.397535288974122
}
#Debug simulation 
Total elapsed time: 6.529386749956757. Arrivals time: 0.15054490556940436 Scheduler time: 6.060805760789663 Scheduler overhead time: 0.1238465802744031 Adapter cache time: 0.026332268491387367 Engine time: 0.11315815756097436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.675622737035155,
    "estimated_duration": 3600.03066245933,
    "input_throughput": 3276.511259474101,
    "output_throughput": 2830.789222511276,
    "total_throughput": 6107.300481985377,
    "itl": 32.740699621990906,
    "ttft": 87870.8814073405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.431183359548374,
    "arrivals": 48139,
    "finished_requests": 47417,
    "scheduler_time": 29.63301589466184
}
#Debug simulation 
Total elapsed time: 6.675711729098111. Arrivals time: 0.14720957865938544 Scheduler time: 6.206435346510261 Scheduler overhead time: 0.1253938591107726 Adapter cache time: 0.026291233953088522 Engine time: 0.11439486453309655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.279997192788869,
    "estimated_duration": 3600.0321835266686,
    "input_throughput": 3248.9348438385578,
    "output_throughput": 2852.6250534626433,
    "total_throughput": 6101.559897301201,
    "itl": 32.72129908638551,
    "ttft": 56443.835231461984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.734992331206081,
    "arrivals": 47462,
    "finished_requests": 47109,
    "scheduler_time": 28.670398864113853
}
#Debug simulation 
Total elapsed time: 6.2800816600210965. Arrivals time: 0.13865811144933105 Scheduler time: 5.830916775856167 Scheduler overhead time: 0.1193174198269844 Adapter cache time: 0.024809463880956173 Engine time: 0.11150035914033651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.316131113097072,
    "estimated_duration": 3600.024098780789,
    "input_throughput": 3246.667988683291,
    "output_throughput": 2851.5336893096414,
    "total_throughput": 6098.201677992933,
    "itl": 32.75437947354303,
    "ttft": 59255.43413966014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1308,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.495450893249323,
    "arrivals": 47462,
    "finished_requests": 47075,
    "scheduler_time": 28.722520282586764
}
#Debug simulation 
Total elapsed time: 6.316220021340996. Arrivals time: 0.1423901212401688 Scheduler time: 5.866290621925145 Scheduler overhead time: 0.11932397913187742 Adapter cache time: 0.024661300238221884 Engine time: 0.10964574152603745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.407653308939189,
    "estimated_duration": 3600.025787967225,
    "input_throughput": 3246.8875748248993,
    "output_throughput": 2852.1973465634187,
    "total_throughput": 6099.084921388318,
    "itl": 32.681276404925704,
    "ttft": 59498.57409105194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.697770117297715,
    "arrivals": 47462,
    "finished_requests": 47075,
    "scheduler_time": 28.773009433026527
}
#Debug simulation 
Total elapsed time: 6.407811724115163. Arrivals time: 0.1423783702775836 Scheduler time: 5.9520544139668345 Scheduler overhead time: 0.12113512167707086 Adapter cache time: 0.024951898027211428 Engine time: 0.11249135341495275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.321409086696804,
    "estimated_duration": 3600.013886114769,
    "input_throughput": 3250.4380177901767,
    "output_throughput": 2855.263430967862,
    "total_throughput": 6105.701448758038,
    "itl": 32.7236280729276,
    "ttft": 53963.494236246945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.070060180756292,
    "arrivals": 47462,
    "finished_requests": 47141,
    "scheduler_time": 28.68792386219046
}
#Debug simulation 
Total elapsed time: 6.321500648744404. Arrivals time: 0.1464837328530848 Scheduler time: 5.854237436782569 Scheduler overhead time: 0.12620576051995158 Adapter cache time: 0.0256466306746006 Engine time: 0.11289559956640005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.2568160318769515,
    "estimated_duration": 3600.0102917719255,
    "input_throughput": 3247.7529374631936,
    "output_throughput": 2852.651844766064,
    "total_throughput": 6100.404782229258,
    "itl": 32.690588849058905,
    "ttft": 57063.318923985666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.855078696585199,
    "arrivals": 47462,
    "finished_requests": 47094,
    "scheduler_time": 28.537223900470458
}
#Debug simulation 
Total elapsed time: 6.256910334806889. Arrivals time: 0.1423646486364305 Scheduler time: 5.798520900309086 Scheduler overhead time: 0.12313035177066922 Adapter cache time: 0.02535413671284914 Engine time: 0.11305354256182909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.368011028971523,
    "estimated_duration": 3600.0017129984167,
    "input_throughput": 3248.6942874977303,
    "output_throughput": 2852.736142574307,
    "total_throughput": 6101.430430072037,
    "itl": 32.689432186481284,
    "ttft": 56872.02378758612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.356552511206834,
    "arrivals": 47462,
    "finished_requests": 47108,
    "scheduler_time": 28.76162329541969
}
#Debug simulation 
Total elapsed time: 6.368097519967705. Arrivals time: 0.14007122302427888 Scheduler time: 5.912760987877846 Scheduler overhead time: 0.1232369034551084 Adapter cache time: 0.02524976758286357 Engine time: 0.11191450245678425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.4-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.331538354977965,
    "estimated_duration": 3600.022396930133,
    "input_throughput": 3246.669523491533,
    "output_throughput": 2851.5350373247215,
    "total_throughput": 6098.204560816254,
    "itl": 32.75467260645668,
    "ttft": 59256.14788565227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1308,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.567278813458922,
    "arrivals": 47462,
    "finished_requests": 47075,
    "scheduler_time": 28.723023364757807
}
#Debug simulation 
Total elapsed time: 6.331628926098347. Arrivals time: 0.14179092412814498 Scheduler time: 5.878168996423483 Scheduler overhead time: 0.12211402039974928 Adapter cache time: 0.02486999100074172 Engine time: 0.11031838646158576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 2.127354145050049,
    "estimated_duration": 3599.9792392814084,
    "input_throughput": 1383.4565893202596,
    "output_throughput": 1204.7636143773243,
    "total_throughput": 2588.220203697584,
    "itl": 25.494712074717608,
    "ttft": 44513.6366032042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4833,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.957772851410724,
    "arrivals": 20310,
    "finished_requests": 20170,
    "scheduler_time": 0.10291176244159941
}
#Debug simulation 
Total elapsed time: 2.1274484270252287. Arrivals time: 0.06612058961763978 Scheduler time: 1.7138262819498777 Scheduler overhead time: 0.1222549406811595 Adapter cache time: 0.04178508138284087 Engine time: 0.12180957989767194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 2.1283016852103174,
    "estimated_duration": 3599.976561704419,
    "input_throughput": 1383.5017852566054,
    "output_throughput": 1204.6064538672567,
    "total_throughput": 2588.1082391238624,
    "itl": 25.514777541046723,
    "ttft": 44895.8752394422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4834,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.19667614734639,
    "arrivals": 20310,
    "finished_requests": 20168,
    "scheduler_time": 0.11278816050551224
}
#Debug simulation 
Total elapsed time: 2.128409954253584. Arrivals time: 0.06591378292068839 Scheduler time: 1.7129031298682094 Scheduler overhead time: 0.1269037900492549 Adapter cache time: 0.041373285464942455 Engine time: 0.119713569059968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 2.1436767880804837,
    "estimated_duration": 3599.9801424576885,
    "input_throughput": 1383.3645750612727,
    "output_throughput": 1204.7230341218399,
    "total_throughput": 2588.0876091831124,
    "itl": 25.519838490217815,
    "ttft": 44641.94854379029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4844,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.193314068099234,
    "arrivals": 20310,
    "finished_requests": 20169,
    "scheduler_time": 0.09908079779460309
}
#Debug simulation 
Total elapsed time: 2.1437720833346248. Arrivals time: 0.06810248782858253 Scheduler time: 1.728589574340731 Scheduler overhead time: 0.12311726529151201 Adapter cache time: 0.041907842736691236 Engine time: 0.1205678740516305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 2.136879009194672,
    "estimated_duration": 3599.9743085325586,
    "input_throughput": 1383.6812635561478,
    "output_throughput": 1204.4635956770146,
    "total_throughput": 2588.1448592331626,
    "itl": 25.50032143405262,
    "ttft": 45356.56362257604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4828,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.02328876474977,
    "arrivals": 20310,
    "finished_requests": 20166,
    "scheduler_time": 0.10135004885316154
}
#Debug simulation 
Total elapsed time: 2.1370248780585825. Arrivals time: 0.06348458072170615 Scheduler time: 1.7245208197273314 Scheduler overhead time: 0.12625014875084162 Adapter cache time: 0.04153619706630707 Engine time: 0.11969826556742191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 2.121051446069032,
    "estimated_duration": 3599.9625825204957,
    "input_throughput": 1383.0102079878088,
    "output_throughput": 1204.3975181998478,
    "total_throughput": 2587.4077261876564,
    "itl": 25.519074356339875,
    "ttft": 45376.517820742774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4807,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.60733300227287,
    "arrivals": 20310,
    "finished_requests": 20166,
    "scheduler_time": 0.09869505663008757
}
#Debug simulation 
Total elapsed time: 2.121176145039499. Arrivals time: 0.0645411522127688 Scheduler time: 1.7124426709488034 Scheduler overhead time: 0.1224808320403099 Adapter cache time: 0.0414129588752985 Engine time: 0.11955289542675018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 2.1231250977143645,
    "estimated_duration": 3599.9777142187445,
    "input_throughput": 1383.5713427689711,
    "output_throughput": 1204.7457913058815,
    "total_throughput": 2588.3171340748527,
    "itl": 25.478767607322943,
    "ttft": 44477.57731150238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4805,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.674740119444397,
    "arrivals": 20310,
    "finished_requests": 20171,
    "scheduler_time": 0.09256947719107159
}
#Debug simulation 
Total elapsed time: 2.123222265858203. Arrivals time: 0.06561566283926368 Scheduler time: 1.7106452151201665 Scheduler overhead time: 0.12244343338534236 Adapter cache time: 0.04162020469084382 Engine time: 0.12234037183225155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 2.144192809704691,
    "estimated_duration": 3599.984168925183,
    "input_throughput": 1383.0933044037724,
    "output_throughput": 1204.3861296463685,
    "total_throughput": 2587.479434050141,
    "itl": 25.519441691512167,
    "ttft": 45686.396795253655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4802,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.241364433766286,
    "arrivals": 20310,
    "finished_requests": 20164,
    "scheduler_time": 0.12066878058558426
}
#Debug simulation 
Total elapsed time: 2.1442902418784797. Arrivals time: 0.06699602445587516 Scheduler time: 1.7307371995411813 Scheduler overhead time: 0.12280017556622624 Adapter cache time: 0.04169294098392129 Engine time: 0.12031602952629328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.9591948753222823,
    "estimated_duration": 3599.705842329038,
    "input_throughput": 1268.693665548284,
    "output_throughput": 1122.0247367175873,
    "total_throughput": 2390.718402265871,
    "itl": 25.235421658173294,
    "ttft": 40118.23633353836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.86875906164481,
    "arrivals": 18897,
    "finished_requests": 18770,
    "scheduler_time": 0.0056562831213301384
}
#Debug simulation 
Total elapsed time: 1.9592812363989651. Arrivals time: 0.05973087344318628 Scheduler time: 1.55525594484061 Scheduler overhead time: 0.12225678656250238 Adapter cache time: 0.042775413021445274 Engine time: 0.11842491151764989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.9629088076762855,
    "estimated_duration": 3599.693476433775,
    "input_throughput": 1269.006383433948,
    "output_throughput": 1122.2127735170561,
    "total_throughput": 2391.219156951004,
    "itl": 25.25959298700852,
    "ttft": 40663.739018154345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5081,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.85402143611951,
    "arrivals": 18897,
    "finished_requests": 18767,
    "scheduler_time": 0.007127206537599144
}
#Debug simulation 
Total elapsed time: 1.9630064126104116. Arrivals time: 0.060706378892064095 Scheduler time: 1.5534072699956596 Scheduler overhead time: 0.12290069041773677 Adapter cache time: 0.042695567943155766 Engine time: 0.12045172089710832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.9765513939782977,
    "estimated_duration": 3599.7095319015284,
    "input_throughput": 1269.2262971525179,
    "output_throughput": 1122.0874807269015,
    "total_throughput": 2391.3137778794194,
    "itl": 25.271954460778918,
    "ttft": 39137.735174023735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5094,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.884241583752804,
    "arrivals": 18897,
    "finished_requests": 18775,
    "scheduler_time": 0.010350674300170785
}
#Debug simulation 
Total elapsed time: 1.9766554231755435. Arrivals time: 0.06311690248548985 Scheduler time: 1.5653441725298762 Scheduler overhead time: 0.12213946785777807 Adapter cache time: 0.04289307165890932 Engine time: 0.12239377619698644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.9837507540360093,
    "estimated_duration": 3599.6891899053535,
    "input_throughput": 1269.0465090182163,
    "output_throughput": 1121.9890904264967,
    "total_throughput": 2391.0355994447127,
    "itl": 25.242411180761533,
    "ttft": 40054.11770233199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5095,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.79720670454788,
    "arrivals": 18897,
    "finished_requests": 18770,
    "scheduler_time": 0.010708038603570995
}
#Debug simulation 
Total elapsed time: 1.9838397428393364. Arrivals time: 0.06042133364826441 Scheduler time: 1.5709586795419455 Scheduler overhead time: 0.12644232204183936 Adapter cache time: 0.04274757578969002 Engine time: 0.12123310333117843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.9752448503859341,
    "estimated_duration": 3599.6897924373884,
    "input_throughput": 1268.6293162244565,
    "output_throughput": 1121.6658192277355,
    "total_throughput": 2390.295135452192,
    "itl": 25.261029481034896,
    "ttft": 40705.101392342636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.525677723372795,
    "arrivals": 18897,
    "finished_requests": 18767,
    "scheduler_time": 0.00958285462009415
}
#Debug simulation 
Total elapsed time: 1.9753366662189364. Arrivals time: 0.06063641840592027 Scheduler time: 1.5662497077137232 Scheduler overhead time: 0.12311931559816003 Adapter cache time: 0.04292651591822505 Engine time: 0.12074746936559677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.9820469771511853,
    "estimated_duration": 3599.6908278647616,
    "input_throughput": 1269.10871473644,
    "output_throughput": 1121.9888576919018,
    "total_throughput": 2391.097572428342,
    "itl": 25.221740769144468,
    "ttft": 40218.09308285433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5091,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 32.50054150844781,
    "arrivals": 18897,
    "finished_requests": 18769,
    "scheduler_time": 0.008856068111883282
}
#Debug simulation 
Total elapsed time: 1.9821405238471925. Arrivals time: 0.06324362382292747 Scheduler time: 1.5666733048856258 Scheduler overhead time: 0.12348477821797132 Adapter cache time: 0.043340607080608606 Engine time: 0.12274690950289369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.9801962650381029,
    "estimated_duration": 3599.6893612921426,
    "input_throughput": 1268.629468171873,
    "output_throughput": 1121.665953572907,
    "total_throughput": 2390.29542174478,
    "itl": 25.25972138890973,
    "ttft": 40789.1923339379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5092,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.20950868435254,
    "arrivals": 18897,
    "finished_requests": 18767,
    "scheduler_time": 0.008591883575681499
}
#Debug simulation 
Total elapsed time: 1.9803535412065685. Arrivals time: 0.060418044216930866 Scheduler time: 1.567002228461206 Scheduler overhead time: 0.12630286673083901 Adapter cache time: 0.042750485707074404 Engine time: 0.1218752427957952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.8732776381075382,
    "estimated_duration": 3599.671302138707,
    "input_throughput": 1240.7963464181578,
    "output_throughput": 1076.6702497801168,
    "total_throughput": 2317.4665961982746,
    "itl": 25.057027822047093,
    "ttft": 32988.03497991663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.580865536589666,
    "arrivals": 18219,
    "finished_requests": 18125,
    "scheduler_time": 0.004809532697168421
}
#Debug simulation 
Total elapsed time: 1.8733622897416353. Arrivals time: 0.05919037526473403 Scheduler time: 1.4684605859220028 Scheduler overhead time: 0.12375268712639809 Adapter cache time: 0.04116593534126878 Engine time: 0.11881693499162793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8640668261796236,
    "estimated_duration": 3599.680163649386,
    "input_throughput": 1241.1924940215827,
    "output_throughput": 1076.701768990137,
    "total_throughput": 2317.8942630117194,
    "itl": 25.08515570409156,
    "ttft": 30282.6189517261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.32177554620821,
    "arrivals": 18219,
    "finished_requests": 18129,
    "scheduler_time": 0.0012108594949724076
}
#Debug simulation 
Total elapsed time: 1.8641675300896168. Arrivals time: 0.05876607075333595 Scheduler time: 1.4549641679041088 Scheduler overhead time: 0.12312654033303261 Adapter cache time: 0.04363602725788951 Engine time: 0.12256442662328482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8783904192969203,
    "estimated_duration": 3599.6560735951525,
    "input_throughput": 1240.6663049727458,
    "output_throughput": 1076.6564696080134,
    "total_throughput": 2317.322774580759,
    "itl": 25.087005216592882,
    "ttft": 31167.785450188054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 39.050692288717514,
    "arrivals": 18219,
    "finished_requests": 18125,
    "scheduler_time": 0.0003525149336169954
}
#Debug simulation 
Total elapsed time: 1.8784832642413676. Arrivals time: 0.060916298534721136 Scheduler time: 1.4663640661165118 Scheduler overhead time: 0.12347452156245708 Adapter cache time: 0.043753225822001696 Engine time: 0.12259755609557033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.8803422059863806,
    "estimated_duration": 3599.6788885790347,
    "input_throughput": 1240.975692074407,
    "output_throughput": 1076.8157716229289,
    "total_throughput": 2317.791463697336,
    "itl": 25.07326696607828,
    "ttft": 30645.35808025065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.25747200352736,
    "arrivals": 18219,
    "finished_requests": 18127,
    "scheduler_time": 0.0001683372675767815
}
#Debug simulation 
Total elapsed time: 1.8804543670266867. Arrivals time: 0.061490957625210285 Scheduler time: 1.4628892350010574 Scheduler overhead time: 0.12777646258473396 Adapter cache time: 0.04376544151455164 Engine time: 0.12122461386024952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.849863903131336,
    "estimated_duration": 3599.6831420423932,
    "input_throughput": 1241.1914670536805,
    "output_throughput": 1076.7008781225543,
    "total_throughput": 2317.892345176235,
    "itl": 25.091067944344797,
    "ttft": 30286.612778101262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.93285114680446,
    "arrivals": 18219,
    "finished_requests": 18129,
    "scheduler_time": 0.0012095359336646155
}
#Debug simulation 
Total elapsed time: 1.849969550035894. Arrivals time: 0.06009408459067345 Scheduler time: 1.4432907407172024 Scheduler overhead time: 0.12330211466178298 Adapter cache time: 0.043090999126434326 Engine time: 0.11905841156840324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.873713559936732,
    "estimated_duration": 3599.680837860456,
    "input_throughput": 1240.8889013213006,
    "output_throughput": 1076.4723803411305,
    "total_throughput": 2317.361281662431,
    "itl": 25.04240140783506,
    "ttft": 33083.83487333943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4763,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.4066154399404,
    "arrivals": 18219,
    "finished_requests": 18125,
    "scheduler_time": 0.003391992537223983
}
#Debug simulation 
Total elapsed time: 1.873801609966904. Arrivals time: 0.06043763877823949 Scheduler time: 1.4666274827905 Scheduler overhead time: 0.12340614711865783 Adapter cache time: 0.04102817224338651 Engine time: 0.12119967630133033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8804343417286873,
    "estimated_duration": 3599.679566561411,
    "input_throughput": 1240.6582078821293,
    "output_throughput": 1076.6494429119855,
    "total_throughput": 2317.3076507941146,
    "itl": 25.08532968438057,
    "ttft": 31053.729607481444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.6830498736355,
    "arrivals": 18219,
    "finished_requests": 18125,
    "scheduler_time": 0.0001036893029416039
}
#Debug simulation 
Total elapsed time: 1.8805275158956647. Arrivals time: 0.059470409993082285 Scheduler time: 1.4666481534950435 Scheduler overhead time: 0.12624412961304188 Adapter cache time: 0.04370085569098592 Engine time: 0.12200886337086558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.8314254581928253,
    "estimated_duration": 3599.9700357594793,
    "input_throughput": 1207.106158338684,
    "output_throughput": 1061.1410545238289,
    "total_throughput": 2268.247212862513,
    "itl": 25.05091852909463,
    "ttft": 28085.0302922063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.847395598373694,
    "arrivals": 17838,
    "finished_requests": 17754,
    "scheduler_time": 0.00032619445880504867
}
#Debug simulation 
Total elapsed time: 1.8315683407709002. Arrivals time: 0.058489381801337004 Scheduler time: 1.4223738862201571 Scheduler overhead time: 0.12327918829396367 Adapter cache time: 0.043383820448070765 Engine time: 0.12166466191411018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8085644240491092,
    "estimated_duration": 3599.9725572655652,
    "input_throughput": 1207.1053128529265,
    "output_throughput": 1061.140311275489,
    "total_throughput": 2268.2456241284153,
    "itl": 25.068994299236167,
    "ttft": 28191.310704013453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.62641900382879,
    "arrivals": 17838,
    "finished_requests": 17754,
    "scheduler_time": 0.0003229522384076589
}
#Debug simulation 
Total elapsed time: 1.8086528698913753. Arrivals time: 0.05664527555927634 Scheduler time: 1.4075332027859986 Scheduler overhead time: 0.1220244006253779 Adapter cache time: 0.0427488861605525 Engine time: 0.11906002694740891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8173347120173275,
    "estimated_duration": 3599.978111730701,
    "input_throughput": 1207.1034503903873,
    "output_throughput": 1061.138674024739,
    "total_throughput": 2268.242124415126,
    "itl": 25.080013965688487,
    "ttft": 28123.030048441862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.77683643239121,
    "arrivals": 17838,
    "finished_requests": 17754,
    "scheduler_time": 0.00027161179480229006
}
#Debug simulation 
Total elapsed time: 1.8174248686991632. Arrivals time: 0.05692309886217117 Scheduler time: 1.410750234965235 Scheduler overhead time: 0.12360313953831792 Adapter cache time: 0.04341285675764084 Engine time: 0.1212497684173286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.8075018306262791,
    "estimated_duration": 3599.9709359400285,
    "input_throughput": 1207.0941897383116,
    "output_throughput": 1061.2146786686287,
    "total_throughput": 2268.3088684069403,
    "itl": 25.055180966763768,
    "ttft": 27695.503659525708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.859978398067355,
    "arrivals": 17838,
    "finished_requests": 17756,
    "scheduler_time": 0.0017178134289972486
}
#Debug simulation 
Total elapsed time: 1.8075988460332155. Arrivals time: 0.05902870977297425 Scheduler time: 1.400535776745528 Scheduler overhead time: 0.12321687024086714 Adapter cache time: 0.04308134410530329 Engine time: 0.12039058189839125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8173425658605993,
    "estimated_duration": 3599.975555523178,
    "input_throughput": 1207.0409737458626,
    "output_throughput": 1060.98331532835,
    "total_throughput": 2268.0242890742124,
    "itl": 25.077672620269727,
    "ttft": 28072.51093459602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.51482386613771,
    "arrivals": 17838,
    "finished_requests": 17754,
    "scheduler_time": 0.00027994197953336444
}
#Debug simulation 
Total elapsed time: 1.8174350848421454. Arrivals time: 0.05948780197650194 Scheduler time: 1.4072775337845087 Scheduler overhead time: 0.12365074269473553 Adapter cache time: 0.043692699167877436 Engine time: 0.12139983009546995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.8289783443324268,
    "estimated_duration": 3599.9741675436703,
    "input_throughput": 1207.105050691252,
    "output_throughput": 1061.2328928478778,
    "total_throughput": 2268.33794353913,
    "itl": 25.044294992705822,
    "ttft": 27878.1965525835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.7071025662158,
    "arrivals": 17838,
    "finished_requests": 17755,
    "scheduler_time": 0.0003359436918381405
}
#Debug simulation 
Total elapsed time: 1.8291134391911328. Arrivals time: 0.05975980730727315 Scheduler time: 1.4189390633255243 Scheduler overhead time: 0.12352227326482534 Adapter cache time: 0.04380091652274132 Engine time: 0.12052413634955883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.1-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8262042119167745,
    "estimated_duration": 3599.9910273131954,
    "input_throughput": 1207.0991197006508,
    "output_throughput": 1061.1348670085608,
    "total_throughput": 2268.233986709212,
    "itl": 25.070460369729446,
    "ttft": 28164.149377403624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.93526043975776,
    "arrivals": 17838,
    "finished_requests": 17754,
    "scheduler_time": 0.0003184965489927943
}
#Debug simulation 
Total elapsed time: 1.8263091347180307. Arrivals time: 0.06003756541758776 Scheduler time: 1.4184653931297362 Scheduler overhead time: 0.12189503433182836 Adapter cache time: 0.04327654559165239 Engine time: 0.1216313629411161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6533347913064063,
    "estimated_duration": 3600.0137728324557,
    "input_throughput": 1094.9349776788895,
    "output_throughput": 953.7607955589889,
    "total_throughput": 2048.6957732378783,
    "itl": 24.799572073675453,
    "ttft": 24328.189210678298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 40.335695094893794,
    "arrivals": 16094,
    "finished_requests": 16023,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6534239472821355. Arrivals time: 0.05346902506425977 Scheduler time: 1.2477727229706943 Scheduler overhead time: 0.12365253595635295 Adapter cache time: 0.046508414670825005 Engine time: 0.12075136369094253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.677275000140071,
    "estimated_duration": 3600.0061559655114,
    "input_throughput": 1094.9909053538197,
    "output_throughput": 953.9405909929505,
    "total_throughput": 2048.9314963467705,
    "itl": 24.827802190087056,
    "ttft": 23898.479872455864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6090,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.16720043639546,
    "arrivals": 16094,
    "finished_requests": 16025,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6774258599616587. Arrivals time: 0.05506934318691492 Scheduler time: 1.2643771693110466 Scheduler overhead time: 0.12685615150257945 Adapter cache time: 0.046840489376336336 Engine time: 0.1215830878354609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6758506218902767,
    "estimated_duration": 3600.023613763058,
    "input_throughput": 1094.9855953526665,
    "output_throughput": 953.9359649950417,
    "total_throughput": 2048.9215603477082,
    "itl": 24.834716343030458,
    "ttft": 23914.93596959769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6092,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.30336041269726,
    "arrivals": 16094,
    "finished_requests": 16025,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6759369848296046. Arrivals time: 0.05374837666749954 Scheduler time: 1.2673615398816764 Scheduler overhead time: 0.12379184365272522 Adapter cache time: 0.046980577521026134 Engine time: 0.12220368068665266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.6814576163887978,
    "estimated_duration": 3600.000979241393,
    "input_throughput": 1094.9388688307047,
    "output_throughput": 953.7641850096197,
    "total_throughput": 2048.703053840324,
    "itl": 24.80977293701433,
    "ttft": 24370.461830487537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6098,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.590369501441394,
    "arrivals": 16094,
    "finished_requests": 16023,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6815542741678655. Arrivals time: 0.05445328960195184 Scheduler time: 1.2741739177145064 Scheduler overhead time: 0.12320890696719289 Adapter cache time: 0.04689475987106562 Engine time: 0.12112155277282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6589486757293344,
    "estimated_duration": 3600.0131557101813,
    "input_throughput": 1094.98877629028,
    "output_throughput": 953.9387361829044,
    "total_throughput": 2048.9275124731844,
    "itl": 24.829962498038395,
    "ttft": 23923.73454194008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6088,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.88007075873579,
    "arrivals": 16094,
    "finished_requests": 16025,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6590413246303797. Arrivals time: 0.053457105066627264 Scheduler time: 1.2504727272316813 Scheduler overhead time: 0.12359860772266984 Adapter cache time: 0.04664592444896698 Engine time: 0.12211858993396163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6656170100905001,
    "estimated_duration": 3600.022757300958,
    "input_throughput": 1094.9322450826028,
    "output_throughput": 953.7584152868617,
    "total_throughput": 2048.6906603694647,
    "itl": 24.78970002266816,
    "ttft": 24334.384424935975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.98022126312777,
    "arrivals": 16094,
    "finished_requests": 16023,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6657052808441222. Arrivals time: 0.05245661502704024 Scheduler time: 1.2601410089991987 Scheduler overhead time: 0.12279612943530083 Adapter cache time: 0.04668675269931555 Engine time: 0.12258563097566366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.1-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6777034997940063,
    "estimated_duration": 3600.024579557021,
    "input_throughput": 1094.9853015962062,
    "output_throughput": 953.9357090785679,
    "total_throughput": 2048.921010674774,
    "itl": 24.831894312212743,
    "ttft": 23911.044689069997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6087,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.475986643674865,
    "arrivals": 16094,
    "finished_requests": 16025,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6777913798578084. Arrivals time: 0.0549029977992177 Scheduler time: 1.2692523864097893 Scheduler overhead time: 0.1231643888168037 Adapter cache time: 0.04663506196811795 Engine time: 0.12254906352609396 
