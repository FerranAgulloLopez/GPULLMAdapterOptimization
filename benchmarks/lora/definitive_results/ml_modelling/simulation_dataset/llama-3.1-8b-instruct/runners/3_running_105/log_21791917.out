INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 17280, 270, 17280, 270, 4320, 4320, 270, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 126630 . Total input tokens: 28223824 . Total output tokens: 24809644
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.087657731026411,
    "estimated_duration": 3600.018764364613,
    "input_throughput": 2913.7414793034704,
    "output_throughput": 2537.599828764325,
    "total_throughput": 5451.341308067796,
    "itl": 28.411422685838808,
    "ttft": 7854.506638288951,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 42420,
    "finished_requests": 42328,
    "scheduler_time": 14.294493423788976
}
#Debug simulation 
Total elapsed time: 3.087774028070271. Arrivals time: 0.107803909573704 Scheduler time: 2.668999807909131 Scheduler overhead time: 0.11693261610344052 Adapter cache time: 0.020545366685837507 Engine time: 0.1177841629832983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 17280, 270, 17280, 270, 4320, 4320, 270, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 126630 . Total input tokens: 28223824 . Total output tokens: 24809644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.0967910401523113,
    "estimated_duration": 3600.0120189012023,
    "input_throughput": 2913.746938878726,
    "output_throughput": 2537.6045835503387,
    "total_throughput": 5451.351522429065,
    "itl": 28.4115693717102,
    "ttft": 7854.655426119374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 42420,
    "finished_requests": 42328,
    "scheduler_time": 14.294562185525612
}
#Debug simulation 
Total elapsed time: 3.09688841085881. Arrivals time: 0.10011801868677139 Scheduler time: 2.6917046257294714 Scheduler overhead time: 0.11679173214361072 Adapter cache time: 0.020200539380311966 Engine time: 0.11270464258268476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 17280, 270, 17280, 270, 4320, 4320, 270, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 126630 . Total input tokens: 28223824 . Total output tokens: 24809644
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.101183947175741,
    "estimated_duration": 3600.0172342300552,
    "input_throughput": 2913.742717746578,
    "output_throughput": 2537.6009073339374,
    "total_throughput": 5451.343625080515,
    "itl": 28.41152132700341,
    "ttft": 7854.569440366265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 42420,
    "finished_requests": 42328,
    "scheduler_time": 14.294614768030199
}
#Debug simulation 
Total elapsed time: 3.1013041981495917. Arrivals time: 0.10147566441446543 Scheduler time: 2.6905585494823754 Scheduler overhead time: 0.11698936577886343 Adapter cache time: 0.020327793899923563 Engine time: 0.11626716796308756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 17280, 270, 17280, 270, 4320, 4320, 270, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 126630 . Total input tokens: 28223824 . Total output tokens: 24809644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.0972036379389465,
    "estimated_duration": 3600.031048357587,
    "input_throughput": 2913.731537061479,
    "output_throughput": 2537.591169989429,
    "total_throughput": 5451.322707050907,
    "itl": 28.411318354672964,
    "ttft": 7854.546110124073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 42420,
    "finished_requests": 42328,
    "scheduler_time": 14.294607512400187
}
#Debug simulation 
Total elapsed time: 3.0972986528649926. Arrivals time: 0.10796782607212663 Scheduler time: 2.680369504261762 Scheduler overhead time: 0.11657753027975559 Adapter cache time: 0.02023422811180353 Engine time: 0.11650945944711566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 17280, 270, 17280, 270, 4320, 4320, 270, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 126630 . Total input tokens: 28223824 . Total output tokens: 24809644
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.052903070114553,
    "estimated_duration": 3600.015003604536,
    "input_throughput": 2913.744523147071,
    "output_throughput": 2537.6024796710904,
    "total_throughput": 5451.347002818161,
    "itl": 28.411539625408594,
    "ttft": 7854.565669300703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 42420,
    "finished_requests": 42328,
    "scheduler_time": 14.294582409565841
}
#Debug simulation 
Total elapsed time: 3.0530310650356114. Arrivals time: 0.10576223209500313 Scheduler time: 2.6427578167058527 Scheduler overhead time: 0.11618905514478683 Adapter cache time: 0.020434546750038862 Engine time: 0.11269382340833545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 17280, 270, 17280, 270, 4320, 4320, 270, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 126630 . Total input tokens: 28223824 . Total output tokens: 24809644
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.0787428729236126,
    "estimated_duration": 3600.017986754973,
    "input_throughput": 2913.742108676288,
    "output_throughput": 2537.6003768899445,
    "total_throughput": 5451.342485566232,
    "itl": 28.411421681369475,
    "ttft": 7854.536444009819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 42420,
    "finished_requests": 42328,
    "scheduler_time": 14.294493423788948
}
#Debug simulation 
Total elapsed time: 3.0788348456844687. Arrivals time: 0.1063650450669229 Scheduler time: 2.665671736933291 Scheduler overhead time: 0.11606273241341114 Adapter cache time: 0.020434020552784204 Engine time: 0.11492333188652992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [5 5 6]
Adapter prompts. [4320, 270, 270, 4320, 17280, 270, 17280, 270, 4320, 4320, 270, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 126630 . Total input tokens: 28223824 . Total output tokens: 24809644
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.12042395491153,
    "estimated_duration": 3600.0124335610653,
    "input_throughput": 2913.746603264911,
    "output_throughput": 2537.6042912616904,
    "total_throughput": 5451.350894526601,
    "itl": 28.411605163240708,
    "ttft": 7854.622319834773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 42420,
    "finished_requests": 42328,
    "scheduler_time": 14.294562185525596
}
#Debug simulation 
Total elapsed time: 3.1205406449735165. Arrivals time: 0.10852957563474774 Scheduler time: 2.6975459684617817 Scheduler overhead time: 0.11945180501788855 Adapter cache time: 0.022080649621784687 Engine time: 0.11685369163751602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 17280, 135, 17280, 135, 4320, 4320, 135, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125955 . Total input tokens: 28075492 . Total output tokens: 24680928
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.0471488260664046,
    "estimated_duration": 3599.98878648087,
    "input_throughput": 2923.030771517076,
    "output_throughput": 2509.103926634689,
    "total_throughput": 5432.1346981517645,
    "itl": 28.11563072893957,
    "ttft": 5677.283714136961,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 42197,
    "finished_requests": 42131,
    "scheduler_time": 13.687724852209076
}
#Debug simulation 
Total elapsed time: 3.0472306851297617. Arrivals time: 0.09989432152360678 Scheduler time: 2.640884353313595 Scheduler overhead time: 0.1172233889810741 Adapter cache time: 0.020350344944745302 Engine time: 0.11318210558965802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 17280, 135, 17280, 135, 4320, 4320, 135, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125955 . Total input tokens: 28075492 . Total output tokens: 24680928
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.0686946003697813,
    "estimated_duration": 3599.9904532212577,
    "input_throughput": 2923.0294181986424,
    "output_throughput": 2509.1027649580387,
    "total_throughput": 5432.132183156681,
    "itl": 28.12138738358947,
    "ttft": 5677.331211180082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 42197,
    "finished_requests": 42131,
    "scheduler_time": 13.692223721113002
}
#Debug simulation 
Total elapsed time: 3.068784872069955. Arrivals time: 0.10738042648881674 Scheduler time: 2.650445722974837 Scheduler overhead time: 0.11748383846133947 Adapter cache time: 0.020644718315452337 Engine time: 0.11688335938379169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 17280, 135, 17280, 135, 4320, 4320, 135, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125955 . Total input tokens: 28075492 . Total output tokens: 24680928
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.0717411381192505,
    "estimated_duration": 3599.9980342192753,
    "input_throughput": 2923.023262784108,
    "output_throughput": 2509.097481204296,
    "total_throughput": 5432.120743988404,
    "itl": 28.11806906034286,
    "ttft": 5677.399103621216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 42197,
    "finished_requests": 42131,
    "scheduler_time": 13.68968228905695
}
#Debug simulation 
Total elapsed time: 3.071837336756289. Arrivals time: 0.1067517651244998 Scheduler time: 2.653646184131503 Scheduler overhead time: 0.11742801545187831 Adapter cache time: 0.02054999116808176 Engine time: 0.11736235348507762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 17280, 135, 17280, 135, 4320, 4320, 135, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125955 . Total input tokens: 28075492 . Total output tokens: 24680928
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.043814711738378,
    "estimated_duration": 3599.9943273363237,
    "input_throughput": 2923.026272595823,
    "output_throughput": 2509.1000648002214,
    "total_throughput": 5432.126337396044,
    "itl": 28.115675329916446,
    "ttft": 5677.279432661863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 42197,
    "finished_requests": 42131,
    "scheduler_time": 13.687756376687354
}
#Debug simulation 
Total elapsed time: 3.043908413965255. Arrivals time: 0.10631450172513723 Scheduler time: 2.62924899533391 Scheduler overhead time: 0.11785770859569311 Adapter cache time: 0.020570813212543726 Engine time: 0.11423214012756944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 17280, 135, 17280, 135, 4320, 4320, 135, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125955 . Total input tokens: 28075492 . Total output tokens: 24680928
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.0576268890872598,
    "estimated_duration": 3599.991798903369,
    "input_throughput": 2923.028325566043,
    "output_throughput": 2509.101827051817,
    "total_throughput": 5432.1301526178595,
    "itl": 28.121335800338905,
    "ttft": 5677.360214080842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 42197,
    "finished_requests": 42131,
    "scheduler_time": 13.692195407456627
}
#Debug simulation 
Total elapsed time: 3.0577189307659864. Arrivals time: 0.10678880382329226 Scheduler time: 2.640471590682864 Scheduler overhead time: 0.11733377631753683 Adapter cache time: 0.020527055021375418 Engine time: 0.1167715466581285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 17280, 135, 17280, 135, 4320, 4320, 135, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125955 . Total input tokens: 28075492 . Total output tokens: 24680928
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.117600231897086,
    "estimated_duration": 3599.984576496509,
    "input_throughput": 2923.0341898411198,
    "output_throughput": 2509.106860894008,
    "total_throughput": 5432.1410507351275,
    "itl": 28.11570943162705,
    "ttft": 5677.335113414495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 42197,
    "finished_requests": 42131,
    "scheduler_time": 13.687676314512482
}
#Debug simulation 
Total elapsed time: 3.117699926253408. Arrivals time: 0.10913714254274964 Scheduler time: 2.6952792927622795 Scheduler overhead time: 0.11828403687104583 Adapter cache time: 0.020539847668260336 Engine time: 0.11801887024194002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [5 5 6]
Adapter prompts. [4320, 135, 135, 4320, 17280, 135, 17280, 135, 4320, 4320, 135, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125955 . Total input tokens: 28075492 . Total output tokens: 24680928
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.116650312207639,
    "estimated_duration": 3599.9909519531698,
    "input_throughput": 2923.029013250944,
    "output_throughput": 2509.102417354493,
    "total_throughput": 5432.131430605436,
    "itl": 28.1212438453485,
    "ttft": 5677.334990535353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 42197,
    "finished_requests": 42131,
    "scheduler_time": 13.692203497072791
}
#Debug simulation 
Total elapsed time: 3.1167305819690228. Arrivals time: 0.10216709924861789 Scheduler time: 2.7028305153362453 Scheduler overhead time: 0.11752698803320527 Adapter cache time: 0.020558133255690336 Engine time: 0.11781609850004315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 17280, 66, 17280, 66, 4320, 4320, 66, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125610 . Total input tokens: 27993000 . Total output tokens: 24614762
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.0531352190300822,
    "estimated_duration": 3600.0212050191626,
    "input_throughput": 2920.0644666602857,
    "output_throughput": 2481.489272214564,
    "total_throughput": 5401.55373887485,
    "itl": 27.949402559875047,
    "ttft": 6464.195247065926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 42071,
    "finished_requests": 41995,
    "scheduler_time": 13.15931371780454
}
#Debug simulation 
Total elapsed time: 3.05325696291402. Arrivals time: 0.10282753687351942 Scheduler time: 2.6374849481508136 Scheduler overhead time: 0.11745662987232208 Adapter cache time: 0.020243856124579906 Engine time: 0.11674023186787963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 17280, 66, 17280, 66, 4320, 4320, 66, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125610 . Total input tokens: 27993000 . Total output tokens: 24614762
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.078924953006208,
    "estimated_duration": 3600.0168337462896,
    "input_throughput": 2920.068012309981,
    "output_throughput": 2481.4922853301246,
    "total_throughput": 5401.560297640106,
    "itl": 27.949528696868295,
    "ttft": 6464.140995978248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 42071,
    "finished_requests": 41995,
    "scheduler_time": 13.15938164555524
}
#Debug simulation 
Total elapsed time: 3.079021655023098. Arrivals time: 0.10588515223935246 Scheduler time: 2.660130798816681 Scheduler overhead time: 0.11894381372258067 Adapter cache time: 0.02024201164022088 Engine time: 0.11742181610316038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 17280, 66, 17280, 66, 4320, 4320, 66, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125610 . Total input tokens: 27993000 . Total output tokens: 24614762
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.100311460904777,
    "estimated_duration": 3600.0212380507724,
    "input_throughput": 2920.0644398675468,
    "output_throughput": 2481.489249445925,
    "total_throughput": 5401.553689313471,
    "itl": 27.94949446260546,
    "ttft": 6464.192493880244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 42071,
    "finished_requests": 41995,
    "scheduler_time": 13.159413170033554
}
#Debug simulation 
Total elapsed time: 3.1004422907717526. Arrivals time: 0.10777697758749127 Scheduler time: 2.679440720472485 Scheduler overhead time: 0.11861885292455554 Adapter cache time: 0.020314662717282772 Engine time: 0.11787171382457018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 17280, 66, 17280, 66, 4320, 4320, 66, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125610 . Total input tokens: 27993000 . Total output tokens: 24614762
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.1132330279797316,
    "estimated_duration": 3600.0211352974234,
    "input_throughput": 2920.0645232132797,
    "output_throughput": 2481.4893202736566,
    "total_throughput": 5401.553843486936,
    "itl": 27.949400881312382,
    "ttft": 6464.119723897339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 42071,
    "finished_requests": 41995,
    "scheduler_time": 13.159285404148248
}
#Debug simulation 
Total elapsed time: 3.113327450118959. Arrivals time: 0.10698012914508581 Scheduler time: 2.696836808696389 Scheduler overhead time: 0.11711661610752344 Adapter cache time: 0.02024677349254489 Engine time: 0.11575398873537779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 17280, 66, 17280, 66, 4320, 4320, 66, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125610 . Total input tokens: 27993000 . Total output tokens: 24614762
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.0696143019013107,
    "estimated_duration": 3600.0168861489833,
    "input_throughput": 2920.0679698047834,
    "output_throughput": 2481.4922492089386,
    "total_throughput": 5401.560219013722,
    "itl": 27.949518636200757,
    "ttft": 6464.164623606801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 42071,
    "finished_requests": 41995,
    "scheduler_time": 13.15937760074721
}
#Debug simulation 
Total elapsed time: 3.0697287870571017. Arrivals time: 0.10137310717254877 Scheduler time: 2.656778497621417 Scheduler overhead time: 0.11817730031907558 Adapter cache time: 0.020157809369266033 Engine time: 0.11702698562294245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 17280, 66, 17280, 66, 4320, 4320, 66, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125610 . Total input tokens: 27993000 . Total output tokens: 24614762
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.0781739242374897,
    "estimated_duration": 3600.016800933057,
    "input_throughput": 2920.0680389256545,
    "output_throughput": 2481.4923079482924,
    "total_throughput": 5401.560346873947,
    "itl": 27.949449875925662,
    "ttft": 6464.191666275906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 42071,
    "finished_requests": 41995,
    "scheduler_time": 13.159295870600337
}
#Debug simulation 
Total elapsed time: 3.078256512992084. Arrivals time: 0.10200243163853884 Scheduler time: 2.66165409097448 Scheduler overhead time: 0.11893988773226738 Adapter cache time: 0.020469760987907648 Engine time: 0.11872822837904096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [5 5 6]
Adapter prompts. [4320, 66, 66, 4320, 17280, 66, 17280, 66, 4320, 4320, 66, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125610 . Total input tokens: 27993000 . Total output tokens: 24614762
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.1170375188812613,
    "estimated_duration": 3600.0167479470706,
    "input_throughput": 2920.068081903978,
    "output_throughput": 2481.492344471544,
    "total_throughput": 5401.560426375522,
    "itl": 27.949527041482803,
    "ttft": 6464.177379086138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 42071,
    "finished_requests": 41995,
    "scheduler_time": 13.15938164555522
}
#Debug simulation 
Total elapsed time: 3.117140067741275. Arrivals time: 0.10281214863061905 Scheduler time: 2.699782784562558 Scheduler overhead time: 0.11914021614938974 Adapter cache time: 0.020323694683611393 Engine time: 0.11844959342852235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 17280, 33, 17280, 33, 4320, 4320, 33, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125445 . Total input tokens: 27959342 . Total output tokens: 24581679
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.0675054928287864,
    "estimated_duration": 3600.012250853344,
    "input_throughput": 2913.4248077944308,
    "output_throughput": 2485.7545964958304,
    "total_throughput": 5399.179404290261,
    "itl": 27.914114834106627,
    "ttft": 6559.657163727154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 42007,
    "finished_requests": 41930,
    "scheduler_time": 13.185433130389853
}
#Debug simulation 
Total elapsed time: 3.067600555717945. Arrivals time: 0.10405020695179701 Scheduler time: 2.6524461191147566 Scheduler overhead time: 0.11985298618674278 Adapter cache time: 0.01995951682329178 Engine time: 0.11470675747841597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 17280, 33, 17280, 33, 4320, 4320, 33, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125445 . Total input tokens: 27959342 . Total output tokens: 24581679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.102989188861102,
    "estimated_duration": 3600.0268500872853,
    "input_throughput": 2913.4129929463447,
    "output_throughput": 2485.7445159841045,
    "total_throughput": 5399.15750893045,
    "itl": 27.914095371927537,
    "ttft": 6645.141272289951,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 42007,
    "finished_requests": 41930,
    "scheduler_time": 13.185530246757821
}
#Debug simulation 
Total elapsed time: 3.1030827779322863. Arrivals time: 0.1072093597613275 Scheduler time: 2.6794426161795855 Scheduler overhead time: 0.11904136976227164 Adapter cache time: 0.01994743663817644 Engine time: 0.12085424317047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 17280, 33, 17280, 33, 4320, 4320, 33, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125445 . Total input tokens: 27959342 . Total output tokens: 24581679
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.11518973717466,
    "estimated_duration": 3600.0012115627337,
    "input_throughput": 2913.43374172007,
    "output_throughput": 2485.7622189842027,
    "total_throughput": 5399.195960704273,
    "itl": 27.914142578078888,
    "ttft": 6559.627710991108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 42007,
    "finished_requests": 41930,
    "scheduler_time": 13.185445305788907
}
#Debug simulation 
Total elapsed time: 3.1152921528555453. Arrivals time: 0.10917276097461581 Scheduler time: 2.6935780574567616 Scheduler overhead time: 0.1184387169778347 Adapter cache time: 0.020027571357786655 Engine time: 0.11749051418155432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 17280, 33, 17280, 33, 4320, 4320, 33, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125445 . Total input tokens: 27959342 . Total output tokens: 24581679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 3.118681730236858,
    "estimated_duration": 3600.0175129367517,
    "input_throughput": 2913.420549291719,
    "output_throughput": 2485.7509631112785,
    "total_throughput": 5399.171512402998,
    "itl": 27.91400399425067,
    "ttft": 6645.111261970627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 42007,
    "finished_requests": 41930,
    "scheduler_time": 13.185481668086409
}
#Debug simulation 
Total elapsed time: 3.118776144925505. Arrivals time: 0.10855143377557397 Scheduler time: 2.6966236336156726 Scheduler overhead time: 0.11811688868328929 Adapter cache time: 0.020065914373844862 Engine time: 0.11895509576424956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 17280, 33, 17280, 33, 4320, 4320, 33, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125445 . Total input tokens: 27959342 . Total output tokens: 24581679
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 3.0835056779906154,
    "estimated_duration": 3600.0018849074086,
    "input_throughput": 2913.433196791162,
    "output_throughput": 2485.761754047015,
    "total_throughput": 5399.194950838177,
    "itl": 27.91414702597305,
    "ttft": 6559.631409556552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 42007,
    "finished_requests": 41930,
    "scheduler_time": 13.185450184582912
}
#Debug simulation 
Total elapsed time: 3.0836003529839218. Arrivals time: 0.1080780690535903 Scheduler time: 2.664997228886932 Scheduler overhead time: 0.11855187453329563 Adapter cache time: 0.019992589484900236 Engine time: 0.11555933905765414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 17280, 33, 17280, 33, 4320, 4320, 33, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125445 . Total input tokens: 27959342 . Total output tokens: 24581679
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 3.097864287905395,
    "estimated_duration": 3600.0102559436705,
    "input_throughput": 2913.426422239646,
    "output_throughput": 2485.7559739518756,
    "total_throughput": 5399.182396191522,
    "itl": 27.91404754747303,
    "ttft": 6559.638287840015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 42007,
    "finished_requests": 41930,
    "scheduler_time": 13.18540010403641
}
#Debug simulation 
Total elapsed time: 3.0979524641297758. Arrivals time: 0.10673989402130246 Scheduler time: 2.6730828881263733 Scheduler overhead time: 0.11931060627102852 Adapter cache time: 0.01998205343261361 Engine time: 0.12228342285379767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [5 5 6]
Adapter prompts. [4320, 33, 33, 4320, 17280, 33, 17280, 33, 4320, 4320, 33, 4320, 17280, 17280, 17280, 17280]
Prompts retrieved: 125445 . Total input tokens: 27959342 . Total output tokens: 24581679
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 3.091121079865843,
    "estimated_duration": 3600.0003169744705,
    "input_throughput": 2913.4344656987923,
    "output_throughput": 2485.7628366879558,
    "total_throughput": 5399.197302386749,
    "itl": 27.914105666730087,
    "ttft": 6559.541067206108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 42007,
    "finished_requests": 41930,
    "scheduler_time": 13.185453395404974
}
#Debug simulation 
Total elapsed time: 3.0912111629731953. Arrivals time: 0.10574163310229778 Scheduler time: 2.675201393663883 Scheduler overhead time: 0.11822052206844091 Adapter cache time: 0.02010134467855096 Engine time: 0.115654899738729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 17280, 540, 17280, 540, 1080, 1080, 540, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 111780 . Total input tokens: 24940113 . Total output tokens: 21899921
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.8919426118955016,
    "estimated_duration": 3600.006986926223,
    "input_throughput": 2568.267793250669,
    "output_throughput": 2275.185306513367,
    "total_throughput": 4843.453099764036,
    "itl": 27.037074009150057,
    "ttft": 5611.244048407524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 37510,
    "finished_requests": 37451,
    "scheduler_time": 9.274889917533544
}
#Debug simulation 
Total elapsed time: 2.892048533074558. Arrivals time: 0.09862859966233373 Scheduler time: 2.4682823889888823 Scheduler overhead time: 0.12190046440809965 Adapter cache time: 0.023483301978558302 Engine time: 0.12161592626944184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 17280, 540, 17280, 540, 1080, 1080, 540, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 111780 . Total input tokens: 24940113 . Total output tokens: 21899921
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.8943701777607203,
    "estimated_duration": 3600.023126887881,
    "input_throughput": 2568.2562789513854,
    "output_throughput": 2275.1751061890027,
    "total_throughput": 4843.431385140388,
    "itl": 27.037072696192446,
    "ttft": 5707.074592879735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 37510,
    "finished_requests": 37451,
    "scheduler_time": 9.274990203748567
}
#Debug simulation 
Total elapsed time: 2.8944703727029264. Arrivals time: 0.0961679071187973 Scheduler time: 2.4772336571477354 Scheduler overhead time: 0.12171847512945533 Adapter cache time: 0.02323576994240284 Engine time: 0.11817873222753406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 17280, 540, 17280, 540, 1080, 1080, 540, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 111780 . Total input tokens: 24940113 . Total output tokens: 21899921
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.8666209750808775,
    "estimated_duration": 3600.026353606005,
    "input_throughput": 2568.2539770129356,
    "output_throughput": 2275.173066940389,
    "total_throughput": 4843.427043953325,
    "itl": 27.037110585922157,
    "ttft": 5707.035104140835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 37510,
    "finished_requests": 37451,
    "scheduler_time": 9.275006966722716
}
#Debug simulation 
Total elapsed time: 2.8667439590208232. Arrivals time: 0.0987268858589232 Scheduler time: 2.443777262698859 Scheduler overhead time: 0.1221468336880207 Adapter cache time: 0.02354243490844965 Engine time: 0.12067640526220202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 17280, 540, 17280, 540, 1080, 1080, 540, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 111780 . Total input tokens: 24940113 . Total output tokens: 21899921
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.8736052652820945,
    "estimated_duration": 3600.0183706334847,
    "input_throughput": 2568.259672067464,
    "output_throughput": 2275.178112093553,
    "total_throughput": 4843.437784161017,
    "itl": 27.037038222223334,
    "ttft": 5611.3115266425475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 37510,
    "finished_requests": 37451,
    "scheduler_time": 9.274983782104442
}
#Debug simulation 
Total elapsed time: 2.8737090323120356. Arrivals time: 0.09935726691037416 Scheduler time: 2.45146809797734 Scheduler overhead time: 0.12128171697258949 Adapter cache time: 0.02332204580307007 Engine time: 0.12047915207222104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 17280, 540, 17280, 540, 1080, 1080, 540, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 111780 . Total input tokens: 24940113 . Total output tokens: 21899921
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.8858182816766202,
    "estimated_duration": 3600.025509645418,
    "input_throughput": 2568.254579093429,
    "output_throughput": 2275.1736003133865,
    "total_throughput": 4843.428179406816,
    "itl": 27.037134856653363,
    "ttft": 5707.057387888153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 37510,
    "finished_requests": 37451,
    "scheduler_time": 9.274996500270621
}
#Debug simulation 
Total elapsed time: 2.8859431920573115. Arrivals time: 0.09917074907571077 Scheduler time: 2.4611327284947038 Scheduler overhead time: 0.12219696259126067 Adapter cache time: 0.023239501286298037 Engine time: 0.12191689852625132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 17280, 540, 17280, 540, 1080, 1080, 540, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 111780 . Total input tokens: 24940113 . Total output tokens: 21899921
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.876934095751494,
    "estimated_duration": 3600.0038046592435,
    "input_throughput": 2568.270063502101,
    "output_throughput": 2275.1873176909835,
    "total_throughput": 4843.457381193084,
    "itl": 27.036992631317155,
    "ttft": 5611.268166959835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 37510,
    "finished_requests": 37451,
    "scheduler_time": 9.274902760821673
}
#Debug simulation 
Total elapsed time: 2.8770265080966055. Arrivals time: 0.09860094171017408 Scheduler time: 2.4580408739857376 Scheduler overhead time: 0.12189565366134048 Adapter cache time: 0.023173402529209852 Engine time: 0.1176029946655035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 17280, 540, 17280, 540, 1080, 1080, 540, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 111780 . Total input tokens: 24940113 . Total output tokens: 21899921
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.86628924170509,
    "estimated_duration": 3600.025327274632,
    "input_throughput": 2568.2547091965707,
    "output_throughput": 2275.173715569575,
    "total_throughput": 4843.428424766145,
    "itl": 27.037157179755404,
    "ttft": 5707.045913817808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 37510,
    "finished_requests": 37451,
    "scheduler_time": 9.275011011530758
}
#Debug simulation 
Total elapsed time: 2.8664147607050836. Arrivals time: 0.09935058513656259 Scheduler time: 2.4435376124456525 Scheduler overhead time: 0.12216358492150903 Adapter cache time: 0.023370379116386175 Engine time: 0.12016404746100307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 17280, 270, 17280, 270, 1080, 1080, 270, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 110430 . Total input tokens: 24642320 . Total output tokens: 21630195
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.852967130020261,
    "estimated_duration": 3600.000861057466,
    "input_throughput": 2536.294671140149,
    "output_throughput": 2231.6289106775735,
    "total_throughput": 4767.923581817723,
    "itl": 26.581548503486314,
    "ttft": 5780.962563115838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 37022,
    "finished_requests": 36963,
    "scheduler_time": 8.272924043280062
}
#Debug simulation 
Total elapsed time: 2.853063483722508. Arrivals time: 0.09921094123274088 Scheduler time: 2.421085038688034 Scheduler overhead time: 0.12360297562554479 Adapter cache time: 0.023049032781273127 Engine time: 0.12704451894387603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 17280, 270, 17280, 270, 1080, 1080, 270, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 110430 . Total input tokens: 24642320 . Total output tokens: 21630195
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.82346595171839,
    "estimated_duration": 3600.020020029309,
    "input_throughput": 2536.2811732157156,
    "output_throughput": 2231.617034155992,
    "total_throughput": 4767.898207371707,
    "itl": 26.581699388632288,
    "ttft": 5781.0587201416565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 37022,
    "finished_requests": 36963,
    "scheduler_time": 8.27304693037126
}
#Debug simulation 
Total elapsed time: 2.8235706789419055. Arrivals time: 0.0924764471128583 Scheduler time: 2.4072107253596187 Scheduler overhead time: 0.12265002075582743 Adapter cache time: 0.023160521872341633 Engine time: 0.11941206362098455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 17280, 270, 17280, 270, 1080, 1080, 270, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 110430 . Total input tokens: 24642320 . Total output tokens: 21630195
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.841932258103043,
    "estimated_duration": 3600.021501721042,
    "input_throughput": 2536.280129336715,
    "output_throughput": 2231.616115670224,
    "total_throughput": 4767.896245006939,
    "itl": 26.581711853289896,
    "ttft": 5781.026393147533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 37022,
    "finished_requests": 36963,
    "scheduler_time": 8.273082374535626
}
#Debug simulation 
Total elapsed time: 2.8420305103063583. Arrivals time: 0.09977123327553272 Scheduler time: 2.414903680793941 Scheduler overhead time: 0.12358787236735225 Adapter cache time: 0.023153716698288918 Engine time: 0.12198317563161254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 17280, 270, 17280, 270, 1080, 1080, 270, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 110430 . Total input tokens: 24642320 . Total output tokens: 21630195
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.8455400289967656,
    "estimated_duration": 3600.006024345959,
    "input_throughput": 2536.2910334737117,
    "output_throughput": 2231.625709976298,
    "total_throughput": 4767.91674345001,
    "itl": 26.58177300629114,
    "ttft": 5781.103288028664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 37022,
    "finished_requests": 36963,
    "scheduler_time": 8.272937720554179
}
#Debug simulation 
Total elapsed time: 2.845636162906885. Arrivals time: 0.09469984471797943 Scheduler time: 2.419987401459366 Scheduler overhead time: 0.12553856940940022 Adapter cache time: 0.023224188946187496 Engine time: 0.12321819458156824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 17280, 270, 17280, 270, 1080, 1080, 270, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 110430 . Total input tokens: 24642320 . Total output tokens: 21630195
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.8346339240670204,
    "estimated_duration": 3600.0213214454166,
    "input_throughput": 2536.2802563441537,
    "output_throughput": 2231.6162274212265,
    "total_throughput": 4767.89648376538,
    "itl": 26.58169251725227,
    "ttft": 5781.029728301278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 37022,
    "finished_requests": 36963,
    "scheduler_time": 8.27310272369785
}
#Debug simulation 
Total elapsed time: 2.8347299839369953. Arrivals time: 0.09925509430468082 Scheduler time: 2.4079556055366993 Scheduler overhead time: 0.12361450074240565 Adapter cache time: 0.023224327713251114 Engine time: 0.12185957049950957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 17280, 270, 17280, 270, 1080, 1080, 270, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 110430 . Total input tokens: 24642320 . Total output tokens: 21630195
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.8559687519446015,
    "estimated_duration": 3599.9987734278143,
    "input_throughput": 2536.2961419306394,
    "output_throughput": 2231.630204793205,
    "total_throughput": 4767.926346723844,
    "itl": 26.58151053221179,
    "ttft": 5780.944053488799,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 37022,
    "finished_requests": 36963,
    "scheduler_time": 8.272920123593988
}
#Debug simulation 
Total elapsed time: 2.856062571052462. Arrivals time: 0.09999914281070232 Scheduler time: 2.42935360269621 Scheduler overhead time: 0.12407236360013485 Adapter cache time: 0.02314700558781624 Engine time: 0.12051019724458456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 17280, 270, 17280, 270, 1080, 1080, 270, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 110430 . Total input tokens: 24642320 . Total output tokens: 21630195
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.8335868031717837,
    "estimated_duration": 3600.0213516299145,
    "input_throughput": 2536.2802350786283,
    "output_throughput": 2231.6162087101666,
    "total_throughput": 4767.896443788794,
    "itl": 26.58168434448598,
    "ttft": 5781.007125638028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 37022,
    "finished_requests": 36963,
    "scheduler_time": 8.273119028052019
}
#Debug simulation 
Total elapsed time: 2.8336818059906363. Arrivals time: 0.09365061298012733 Scheduler time: 2.4121789834462106 Scheduler overhead time: 0.12339042313396931 Adapter cache time: 0.023159825708717108 Engine time: 0.12263488955795765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 17280, 135, 17280, 135, 1080, 1080, 135, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109755 . Total input tokens: 24502496 . Total output tokens: 21504462
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.8078368208371103,
    "estimated_duration": 3599.9598477692434,
    "input_throughput": 2513.0599736011013,
    "output_throughput": 2224.8264810406276,
    "total_throughput": 4737.886454641729,
    "itl": 26.48117423033559,
    "ttft": 6988.901986855162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 36802,
    "finished_requests": 36731,
    "scheduler_time": 8.100047652098915
}
#Debug simulation 
Total elapsed time: 2.807958618737757. Arrivals time: 0.096126654651016 Scheduler time: 2.3822666979394853 Scheduler overhead time: 0.125501976814121 Adapter cache time: 0.022624599281698465 Engine time: 0.12218098528683186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 17280, 135, 17280, 135, 1080, 1080, 135, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109755 . Total input tokens: 24502496 . Total output tokens: 21504462
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.8198093022219837,
    "estimated_duration": 3599.949448650171,
    "input_throughput": 2513.0672330391226,
    "output_throughput": 2224.832907862955,
    "total_throughput": 4737.9001409020775,
    "itl": 26.481310775921198,
    "ttft": 6988.9698344907765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 36802,
    "finished_requests": 36731,
    "scheduler_time": 8.100163533804105
}
#Debug simulation 
Total elapsed time: 2.819908380974084. Arrivals time: 0.09743415098637342 Scheduler time: 2.3924604589119554 Scheduler overhead time: 0.12353218253701925 Adapter cache time: 0.022571295965462923 Engine time: 0.12506091967225075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 17280, 135, 17280, 135, 1080, 1080, 135, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109755 . Total input tokens: 24502496 . Total output tokens: 21504462
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.846663744188845,
    "estimated_duration": 3599.952713127879,
    "input_throughput": 2513.064954161422,
    "output_throughput": 2224.8308903593897,
    "total_throughput": 4737.895844520812,
    "itl": 26.481241284568213,
    "ttft": 6988.908873263418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 36802,
    "finished_requests": 36731,
    "scheduler_time": 8.100145811721926
}
#Debug simulation 
Total elapsed time: 2.8467896790243685. Arrivals time: 0.09873927012085915 Scheduler time: 2.4176624356769025 Scheduler overhead time: 0.1241696304641664 Adapter cache time: 0.022641871590167284 Engine time: 0.12435199692845345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 17280, 135, 17280, 135, 1080, 1080, 135, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109755 . Total input tokens: 24502496 . Total output tokens: 21504462
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.804852973204106,
    "estimated_duration": 3599.962928873459,
    "input_throughput": 2513.057822745709,
    "output_throughput": 2224.8245768759502,
    "total_throughput": 4737.882399621659,
    "itl": 26.481169235616303,
    "ttft": 6988.954736268011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 36802,
    "finished_requests": 36731,
    "scheduler_time": 8.100039437360888
}
#Debug simulation 
Total elapsed time: 2.8049514880403876. Arrivals time: 0.09816504269838333 Scheduler time: 2.3796794931404293 Scheduler overhead time: 0.12354105897247791 Adapter cache time: 0.022632059641182423 Engine time: 0.12207686295732856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 17280, 135, 17280, 135, 1080, 1080, 135, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109755 . Total input tokens: 24502496 . Total output tokens: 21504462
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.798104284796864,
    "estimated_duration": 3599.951387631761,
    "input_throughput": 2513.0658794677615,
    "output_throughput": 2224.831709538426,
    "total_throughput": 4737.897589006187,
    "itl": 26.481284656960973,
    "ttft": 6988.894316710875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 36802,
    "finished_requests": 36731,
    "scheduler_time": 8.100161031846085
}
#Debug simulation 
Total elapsed time: 2.7982034371234477. Arrivals time: 0.09550978383049369 Scheduler time: 2.377007791772485 Scheduler overhead time: 0.12255356134846807 Adapter cache time: 0.022509933449327946 Engine time: 0.12199695408344269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 17280, 135, 17280, 135, 1080, 1080, 135, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109755 . Total input tokens: 24502496 . Total output tokens: 21504462
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.829803475178778,
    "estimated_duration": 3599.9566084395137,
    "input_throughput": 2513.062234914437,
    "output_throughput": 2224.8284829943586,
    "total_throughput": 4737.8907179087955,
    "itl": 26.48112236906104,
    "ttft": 6988.826670474788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 36802,
    "finished_requests": 36731,
    "scheduler_time": 8.100008288248494
}
#Debug simulation 
Total elapsed time: 2.829930738080293. Arrivals time: 0.0976318079046905 Scheduler time: 2.4053225768730044 Scheduler overhead time: 0.12376228207722306 Adapter cache time: 0.02245854353532195 Engine time: 0.12192353652790189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 17280, 135, 17280, 135, 1080, 1080, 135, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109755 . Total input tokens: 24502496 . Total output tokens: 21504462
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.8455318990163505,
    "estimated_duration": 3599.9493074965367,
    "input_throughput": 2513.0673315762247,
    "output_throughput": 2224.8329950984194,
    "total_throughput": 4737.900326674644,
    "itl": 26.481293451798724,
    "ttft": 6988.962480390606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 36802,
    "finished_requests": 36731,
    "scheduler_time": 8.100134386161814
}
#Debug simulation 
Total elapsed time: 2.8456239197403193. Arrivals time: 0.09719415009021759 Scheduler time: 2.4187175505794585 Scheduler overhead time: 0.12428157450631261 Adapter cache time: 0.022676837164908648 Engine time: 0.1237485883757472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 17280, 66, 17280, 66, 1080, 1080, 66, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109410 . Total input tokens: 24425743 . Total output tokens: 21442049
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7940034582279623,
    "estimated_duration": 3600.003848162659,
    "input_throughput": 2526.1286886238636,
    "output_throughput": 2190.678769419934,
    "total_throughput": 4716.807458043798,
    "itl": 26.28842996950198,
    "ttft": 7304.514425270356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 36691,
    "finished_requests": 36615,
    "scheduler_time": 7.495961238193946
}
#Debug simulation 
Total elapsed time: 2.794096915051341. Arrivals time: 0.09748289408162236 Scheduler time: 2.3681643456220627 Scheduler overhead time: 0.12482800940051675 Adapter cache time: 0.022366411052644253 Engine time: 0.12224046979099512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 17280, 66, 17280, 66, 1080, 1080, 66, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109410 . Total input tokens: 24425743 . Total output tokens: 21442049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7857074169442058,
    "estimated_duration": 3600.0230514922687,
    "input_throughput": 2526.1152136874116,
    "output_throughput": 2190.6670838485147,
    "total_throughput": 4716.782297535927,
    "itl": 26.28855932568026,
    "ttft": 7402.399941903226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 36691,
    "finished_requests": 36615,
    "scheduler_time": 7.49630463053658
}
#Debug simulation 
Total elapsed time: 2.7858098591677845. Arrivals time: 0.09693993115797639 Scheduler time: 2.361167044378817 Scheduler overhead time: 0.12353992648422718 Adapter cache time: 0.022203385829925537 Engine time: 0.12278111698105931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 17280, 66, 17280, 66, 1080, 1080, 66, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109410 . Total input tokens: 24425743 . Total output tokens: 21442049
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.807754775043577,
    "estimated_duration": 3600.02627168776,
    "input_throughput": 2526.1129540970064,
    "output_throughput": 2190.665124313852,
    "total_throughput": 4716.778078410858,
    "itl": 26.288515229275134,
    "ttft": 7500.344436745628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 36691,
    "finished_requests": 36615,
    "scheduler_time": 7.496279777946336
}
#Debug simulation 
Total elapsed time: 2.807838992215693. Arrivals time: 0.09238765109330416 Scheduler time: 2.3864192003384233 Scheduler overhead time: 0.12388603622093797 Adapter cache time: 0.02243498107418418 Engine time: 0.12357446271926165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 17280, 66, 17280, 66, 1080, 1080, 66, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109410 . Total input tokens: 24425743 . Total output tokens: 21442049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.7934585162438452,
    "estimated_duration": 3600.011090476661,
    "input_throughput": 2526.1236066903048,
    "output_throughput": 2190.674362326976,
    "total_throughput": 4716.797969017281,
    "itl": 26.288405794865856,
    "ttft": 7402.427685981165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 36691,
    "finished_requests": 36615,
    "scheduler_time": 7.496068404673028
}
#Debug simulation 
Total elapsed time: 2.7935528652742505. Arrivals time: 0.09618197660893202 Scheduler time: 2.369925493374467 Scheduler overhead time: 0.12423076014965773 Adapter cache time: 0.02254032762721181 Engine time: 0.12155458377674222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 17280, 66, 17280, 66, 1080, 1080, 66, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109410 . Total input tokens: 24425743 . Total output tokens: 21442049
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.777595997788012,
    "estimated_duration": 3600.025350126199,
    "input_throughput": 2526.1136007503965,
    "output_throughput": 2190.6656850967843,
    "total_throughput": 4716.77928584718,
    "itl": 26.2885474920449,
    "ttft": 7500.339377238177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 36691,
    "finished_requests": 36615,
    "scheduler_time": 7.496278109974325
}
#Debug simulation 
Total elapsed time: 2.7776803919114172. Arrivals time: 0.09115856373682618 Scheduler time: 2.3580338857136667 Scheduler overhead time: 0.12438666820526123 Adapter cache time: 0.022272633854299784 Engine time: 0.1228154394775629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 17280, 66, 17280, 66, 1080, 1080, 66, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109410 . Total input tokens: 24425743 . Total output tokens: 21442049
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7965024039149284,
    "estimated_duration": 3600.0290674536573,
    "input_throughput": 2526.110992329388,
    "output_throughput": 2190.663423053464,
    "total_throughput": 4716.774415382853,
    "itl": 26.28838408371077,
    "ttft": 7500.333345919637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 36691,
    "finished_requests": 36615,
    "scheduler_time": 7.4962142680065735
}
#Debug simulation 
Total elapsed time: 2.796598316170275. Arrivals time: 0.09718282520771027 Scheduler time: 2.36813331162557 Scheduler overhead time: 0.12503938330337405 Adapter cache time: 0.022317275870591402 Engine time: 0.12491782195866108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 17280, 66, 17280, 66, 1080, 1080, 66, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109410 . Total input tokens: 24425743 . Total output tokens: 21442049
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.824891149997711,
    "estimated_duration": 3600.024711386887,
    "input_throughput": 2526.11404894956,
    "output_throughput": 2190.666073778642,
    "total_throughput": 4716.780122728202,
    "itl": 26.288624607377695,
    "ttft": 7500.315731484231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 36691,
    "finished_requests": 36615,
    "scheduler_time": 7.496323186604769
}
#Debug simulation 
Total elapsed time: 2.824991124216467. Arrivals time: 0.09725299337878823 Scheduler time: 2.400647164322436 Scheduler overhead time: 0.12437576241791248 Adapter cache time: 0.022392816841602325 Engine time: 0.12110189627856016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 17280, 33, 17280, 33, 1080, 1080, 33, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109245 . Total input tokens: 24392024 . Total output tokens: 21407631
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7838910720311105,
    "estimated_duration": 3599.915790441873,
    "input_throughput": 2523.2562450815108,
    "output_throughput": 2209.6544650072533,
    "total_throughput": 4732.910710088764,
    "itl": 26.324122887664785,
    "ttft": 6432.330378112701,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 36626,
    "finished_requests": 36561,
    "scheduler_time": 7.737918453558609
}
#Debug simulation 
Total elapsed time: 2.7839866150170565. Arrivals time: 0.09730300819501281 Scheduler time: 2.3609566278755665 Scheduler overhead time: 0.12357890838757157 Adapter cache time: 0.021684602834284306 Engine time: 0.12145695695653558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 17280, 33, 17280, 33, 1080, 1080, 33, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109245 . Total input tokens: 24392024 . Total output tokens: 21407631
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7859685476869345,
    "estimated_duration": 3599.905173357418,
    "input_throughput": 2523.263686839937,
    "output_throughput": 2209.660981870043,
    "total_throughput": 4732.92466870998,
    "itl": 26.324213108931108,
    "ttft": 6432.303137268937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 36626,
    "finished_requests": 36561,
    "scheduler_time": 7.737946433716962
}
#Debug simulation 
Total elapsed time: 2.786063483916223. Arrivals time: 0.09610879654064775 Scheduler time: 2.362438512034714 Scheduler overhead time: 0.12415904086083174 Adapter cache time: 0.02182862628251314 Engine time: 0.12254492286592722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 17280, 33, 17280, 33, 1080, 1080, 33, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109245 . Total input tokens: 24392024 . Total output tokens: 21407631
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7905026357620955,
    "estimated_duration": 3599.910627586241,
    "input_throughput": 2523.259863840159,
    "output_throughput": 2209.6576340100924,
    "total_throughput": 4732.9174978502515,
    "itl": 26.324213406202727,
    "ttft": 6432.358727885284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 36626,
    "finished_requests": 36561,
    "scheduler_time": 7.737955107075022
}
#Debug simulation 
Total elapsed time: 2.7905983608216047. Arrivals time: 0.09673012001439929 Scheduler time: 2.3653531963936985 Scheduler overhead time: 0.12445417232811451 Adapter cache time: 0.02194547513499856 Engine time: 0.12307959329336882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 17280, 33, 17280, 33, 1080, 1080, 33, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109245 . Total input tokens: 24392024 . Total output tokens: 21407631
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.779949420131743,
    "estimated_duration": 3599.9215979289315,
    "input_throughput": 2523.2521744989745,
    "output_throughput": 2209.6509003352567,
    "total_throughput": 4732.903074834231,
    "itl": 26.324193820552534,
    "ttft": 6432.4703805128165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 36626,
    "finished_requests": 36561,
    "scheduler_time": 7.737981377393241
}
#Debug simulation 
Total elapsed time: 2.780045982915908. Arrivals time: 0.09395917318761349 Scheduler time: 2.3619833043776453 Scheduler overhead time: 0.1236421843059361 Adapter cache time: 0.021562652196735144 Engine time: 0.1198575971648097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 17280, 33, 17280, 33, 1080, 1080, 33, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109245 . Total input tokens: 24392024 . Total output tokens: 21407631
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.778708684258163,
    "estimated_duration": 3599.9060503849246,
    "input_throughput": 2523.2630721095443,
    "output_throughput": 2209.660443541144,
    "total_throughput": 4732.923515650688,
    "itl": 26.324166723995955,
    "ttft": 6432.2926900489865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 36626,
    "finished_requests": 36561,
    "scheduler_time": 7.737891474376408
}
#Debug simulation 
Total elapsed time: 2.7787912632338703. Arrivals time: 0.0902639334090054 Scheduler time: 2.3601326891221106 Scheduler overhead time: 0.12416433403268456 Adapter cache time: 0.022103342227637768 Engine time: 0.12305352371186018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 17280, 33, 17280, 33, 1080, 1080, 33, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109245 . Total input tokens: 24392024 . Total output tokens: 21407631
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7784723211079836,
    "estimated_duration": 3599.913427752914,
    "input_throughput": 2523.257901140689,
    "output_throughput": 2209.6559152438526,
    "total_throughput": 4732.913816384542,
    "itl": 26.324114480543805,
    "ttft": 6432.316048404935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 36626,
    "finished_requests": 36561,
    "scheduler_time": 7.737802947219444
}
#Debug simulation 
Total elapsed time: 2.778574281837791. Arrivals time: 0.09623458376154304 Scheduler time: 2.355009213089943 Scheduler overhead time: 0.12411087425425649 Adapter cache time: 0.021892350167036057 Engine time: 0.12232896685600281 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 17280, 33, 17280, 33, 1080, 1080, 33, 1080, 17280, 17280, 17280, 17280]
Prompts retrieved: 109245 . Total input tokens: 24392024 . Total output tokens: 21407631
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7861905451864004,
    "estimated_duration": 3599.9052098104075,
    "input_throughput": 2523.263661289124,
    "output_throughput": 2209.660959494802,
    "total_throughput": 4732.924620783925,
    "itl": 26.324210766695675,
    "ttft": 6432.297490819984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 36626,
    "finished_requests": 36561,
    "scheduler_time": 7.737899563992482
}
#Debug simulation 
Total elapsed time: 2.7862839298322797. Arrivals time: 0.0957122202962637 Scheduler time: 2.364838570356369 Scheduler overhead time: 0.12403119914233685 Adapter cache time: 0.02177661331370473 Engine time: 0.12069890182465315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 17280, 270, 17280, 270, 540, 540, 270, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107730 . Total input tokens: 24066359 . Total output tokens: 21114097
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7213443592190742,
    "estimated_duration": 3599.9265199364672,
    "input_throughput": 2479.757836878722,
    "output_throughput": 2169.4942818271284,
    "total_throughput": 4649.252118705851,
    "itl": 26.09633817109869,
    "ttft": 5125.352240684887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 36126,
    "finished_requests": 36075,
    "scheduler_time": 6.970331719477916
}
#Debug simulation 
Total elapsed time: 2.7214289191178977. Arrivals time: 0.08904508873820305 Scheduler time: 2.304666005540639 Scheduler overhead time: 0.12446248950436711 Adapter cache time: 0.02235974045470357 Engine time: 0.12175960931926966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 17280, 270, 17280, 270, 540, 540, 270, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107730 . Total input tokens: 24066359 . Total output tokens: 21114097
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7351337103173137,
    "estimated_duration": 3599.9391387145884,
    "input_throughput": 2479.749144644567,
    "output_throughput": 2169.486677152182,
    "total_throughput": 4649.235821796749,
    "itl": 26.096501663922123,
    "ttft": 5125.27892013809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 36126,
    "finished_requests": 36075,
    "scheduler_time": 6.97027855323136
}
#Debug simulation 
Total elapsed time: 2.735220070928335. Arrivals time: 0.09024790627881885 Scheduler time: 2.3127329805865884 Scheduler overhead time: 0.12407677248120308 Adapter cache time: 0.022360600996762514 Engine time: 0.12666066456586123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 17280, 270, 17280, 270, 540, 540, 270, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107730 . Total input tokens: 24066359 . Total output tokens: 21114097
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7729000290855765,
    "estimated_duration": 3599.941745548355,
    "input_throughput": 2479.7473489783424,
    "output_throughput": 2169.485106157003,
    "total_throughput": 4649.232455135346,
    "itl": 26.096405028450295,
    "ttft": 5125.274173428636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 36126,
    "finished_requests": 36075,
    "scheduler_time": 6.970276885259354
}
#Debug simulation 
Total elapsed time: 2.7729947078041732. Arrivals time: 0.0960172638297081 Scheduler time: 2.346597989089787 Scheduler overhead time: 0.12529789237305522 Adapter cache time: 0.022417694330215454 Engine time: 0.12319372734054923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 17280, 270, 17280, 270, 540, 540, 270, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107730 . Total input tokens: 24066359 . Total output tokens: 21114097
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.754967308137566,
    "estimated_duration": 3599.9316605503914,
    "input_throughput": 2479.754295845484,
    "output_throughput": 2169.4911838426206,
    "total_throughput": 4649.245479688105,
    "itl": 26.09628420564184,
    "ttft": 5125.355256801275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 36126,
    "finished_requests": 36075,
    "scheduler_time": 6.970342561296059
}
#Debug simulation 
Total elapsed time: 2.755068270023912. Arrivals time: 0.09618563344702125 Scheduler time: 2.332562875468284 Scheduler overhead time: 0.12374197132885456 Adapter cache time: 0.02235243935137987 Engine time: 0.12072325963526964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 17280, 270, 17280, 270, 540, 540, 270, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107730 . Total input tokens: 24066359 . Total output tokens: 21114097
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.727532443124801,
    "estimated_duration": 3599.9406277439816,
    "input_throughput": 2479.7481189556056,
    "output_throughput": 2169.485779795874,
    "total_throughput": 4649.2338987514795,
    "itl": 26.096490644449503,
    "ttft": 5125.290080969478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 36126,
    "finished_requests": 36075,
    "scheduler_time": 6.9702768852593575
}
#Debug simulation 
Total elapsed time: 2.727615090087056. Arrivals time: 0.09002563916146755 Scheduler time: 2.3080012290738523 Scheduler overhead time: 0.1250905697233975 Adapter cache time: 0.02240319876000285 Engine time: 0.1225655754096806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 17280, 270, 17280, 270, 540, 540, 270, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107730 . Total input tokens: 24066359 . Total output tokens: 21114097
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7417303542606533,
    "estimated_duration": 3599.9464613748364,
    "input_throughput": 2479.7441005805285,
    "output_throughput": 2169.482264193817,
    "total_throughput": 4649.226364774346,
    "itl": 26.09621180547916,
    "ttft": 5125.340115177941,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 36126,
    "finished_requests": 36075,
    "scheduler_time": 6.970210625480718
}
#Debug simulation 
Total elapsed time: 2.7418244052678347. Arrivals time: 0.09398081013932824 Scheduler time: 2.3175556380301714 Scheduler overhead time: 0.1251866645179689 Adapter cache time: 0.02239958057180047 Engine time: 0.1231710771098733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 17280, 270, 17280, 270, 540, 540, 270, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107730 . Total input tokens: 24066359 . Total output tokens: 21114097
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7377597647719085,
    "estimated_duration": 3599.9395293989787,
    "input_throughput": 2479.7488755291347,
    "output_throughput": 2169.486441708066,
    "total_throughput": 4649.2353172372,
    "itl": 26.096491566105936,
    "ttft": 5125.272333009156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 36126,
    "finished_requests": 36075,
    "scheduler_time": 6.970271297601292
}
#Debug simulation 
Total elapsed time: 2.737846712116152. Arrivals time: 0.08969871373847127 Scheduler time: 2.3123943149112165 Scheduler overhead time: 0.12546748854219913 Adapter cache time: 0.022478498052805662 Engine time: 0.12835770240053535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 17280, 135, 17280, 135, 540, 540, 135, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107055 . Total input tokens: 23907703 . Total output tokens: 20989989
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7499832352623343,
    "estimated_duration": 3599.6922221112304,
    "input_throughput": 2465.567179739222,
    "output_throughput": 2166.231588384683,
    "total_throughput": 4631.798768123906,
    "itl": 25.997732601817418,
    "ttft": 3951.8938476130893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35918,
    "finished_requests": 35879,
    "scheduler_time": 6.79445838204848
}
#Debug simulation 
Total elapsed time: 2.7500783219002187. Arrivals time: 0.0954408710822463 Scheduler time: 2.3224001051858068 Scheduler overhead time: 0.12505558039993048 Adapter cache time: 0.022085048258304596 Engine time: 0.12549800146371126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 17280, 135, 17280, 135, 540, 540, 135, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107055 . Total input tokens: 23907703 . Total output tokens: 20989989
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7477222089655697,
    "estimated_duration": 3599.704537835477,
    "input_throughput": 2465.5587442564824,
    "output_throughput": 2166.224177023385,
    "total_throughput": 4631.782921279867,
    "itl": 25.997837670180978,
    "ttft": 3951.786100077718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 35918,
    "finished_requests": 35879,
    "scheduler_time": 6.794551871253418
}
#Debug simulation 
Total elapsed time: 2.74783929111436. Arrivals time: 0.0948135694488883 Scheduler time: 2.321915198583156 Scheduler overhead time: 0.1260595377534628 Adapter cache time: 0.022116809152066708 Engine time: 0.12318699061870575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 17280, 135, 17280, 135, 540, 540, 135, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107055 . Total input tokens: 23907703 . Total output tokens: 20989989
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7163405437022448,
    "estimated_duration": 3599.684024446392,
    "input_throughput": 2465.5727946468746,
    "output_throughput": 2166.2365216067114,
    "total_throughput": 4631.809316253586,
    "itl": 25.997887391446728,
    "ttft": 3951.790796519612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 35918,
    "finished_requests": 35879,
    "scheduler_time": 6.794517719695064
}
#Debug simulation 
Total elapsed time: 2.7164324917830527. Arrivals time: 0.09301606612280011 Scheduler time: 2.2946662604808807 Scheduler overhead time: 0.12446147855371237 Adapter cache time: 0.021891719661653042 Engine time: 0.12272981787100434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 17280, 135, 17280, 135, 540, 540, 135, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107055 . Total input tokens: 23907703 . Total output tokens: 20989989
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.71702411910519,
    "estimated_duration": 3599.6964248706236,
    "input_throughput": 2465.564301111582,
    "output_throughput": 2166.229059240811,
    "total_throughput": 4631.7933603523925,
    "itl": 25.99778347547432,
    "ttft": 3951.9291700592275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 35918,
    "finished_requests": 35879,
    "scheduler_time": 6.794229370840131
}
#Debug simulation 
Total elapsed time: 2.717146993149072. Arrivals time: 0.09370464459061623 Scheduler time: 2.296753062400967 Scheduler overhead time: 0.12512385612353683 Adapter cache time: 0.02194112678989768 Engine time: 0.12031300039961934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 17280, 135, 17280, 135, 540, 540, 135, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107055 . Total input tokens: 23907703 . Total output tokens: 20989989
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.725733040366322,
    "estimated_duration": 3599.682492167223,
    "input_throughput": 2465.5738441688372,
    "output_throughput": 2166.2374437100093,
    "total_throughput": 4631.811287878846,
    "itl": 25.997938563979883,
    "ttft": 3951.7867658811656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 35918,
    "finished_requests": 35879,
    "scheduler_time": 6.794583729229741
}
#Debug simulation 
Total elapsed time: 2.7258292511105537. Arrivals time: 0.0947317904792726 Scheduler time: 2.303344593849033 Scheduler overhead time: 0.12526508746668696 Adapter cache time: 0.0218634232878685 Engine time: 0.12108225701376796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 17280, 135, 17280, 135, 540, 540, 135, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107055 . Total input tokens: 23907703 . Total output tokens: 20989989
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.71602521603927,
    "estimated_duration": 3599.6868126697836,
    "input_throughput": 2465.5708848785816,
    "output_throughput": 2166.2348436964776,
    "total_throughput": 4631.805728575059,
    "itl": 25.997619312510164,
    "ttft": 3951.869613081966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35918,
    "finished_requests": 35879,
    "scheduler_time": 6.794407092149947
}
#Debug simulation 
Total elapsed time: 2.7161219059489667. Arrivals time: 0.09249935112893581 Scheduler time: 2.293825297616422 Scheduler overhead time: 0.12537157582119107 Adapter cache time: 0.02194075519219041 Engine time: 0.12307762214913964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 17280, 135, 17280, 135, 540, 540, 135, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 107055 . Total input tokens: 23907703 . Total output tokens: 20989989
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.716961438767612,
    "estimated_duration": 3599.680395566967,
    "input_throughput": 2465.575280219315,
    "output_throughput": 2166.238705414794,
    "total_throughput": 4631.8139856341095,
    "itl": 25.997919752188626,
    "ttft": 3951.707841842268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 35918,
    "finished_requests": 35879,
    "scheduler_time": 6.794584938581764
}
#Debug simulation 
Total elapsed time: 2.71708919480443. Arrivals time: 0.09385720128193498 Scheduler time: 2.2942090411670506 Scheduler overhead time: 0.12540399376302958 Adapter cache time: 0.02195547567680478 Engine time: 0.12219804571941495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 17280, 66, 17280, 66, 540, 540, 66, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106710 . Total input tokens: 23824785 . Total output tokens: 20921182
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7004548450931907,
    "estimated_duration": 3599.9734402128397,
    "input_throughput": 2465.7745806800135,
    "output_throughput": 2135.1226967789967,
    "total_throughput": 4600.89727745901,
    "itl": 25.806911288839657,
    "ttft": 6680.8412420580635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35793,
    "finished_requests": 35727,
    "scheduler_time": 6.2622327741980826
}
#Debug simulation 
Total elapsed time: 2.700551184825599. Arrivals time: 0.0947641534730792 Scheduler time: 2.2757708095014095 Scheduler overhead time: 0.12514141155406833 Adapter cache time: 0.021840546745806932 Engine time: 0.12344038812443614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 17280, 66, 17280, 66, 540, 540, 66, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106710 . Total input tokens: 23824785 . Total output tokens: 20921182
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.7210797858424485,
    "estimated_duration": 3599.9527345984116,
    "input_throughput": 2465.7887629155866,
    "output_throughput": 2135.134977225596,
    "total_throughput": 4600.923740141182,
    "itl": 25.806861733867454,
    "ttft": 6680.846342674011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 35793,
    "finished_requests": 35727,
    "scheduler_time": 6.262029491434041
}
#Debug simulation 
Total elapsed time: 2.721183195710182. Arrivals time: 0.09360165288671851 Scheduler time: 2.2935699494555593 Scheduler overhead time: 0.1259499080479145 Adapter cache time: 0.022005315870046616 Engine time: 0.12624782510101795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 17280, 66, 17280, 66, 540, 540, 66, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106710 . Total input tokens: 23824785 . Total output tokens: 20921182
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.724740935023874,
    "estimated_duration": 3599.9614380468283,
    "input_throughput": 2465.7828015002565,
    "output_throughput": 2135.1298152155414,
    "total_throughput": 4600.912616715798,
    "itl": 25.806970865516735,
    "ttft": 6680.81692228857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 35793,
    "finished_requests": 35727,
    "scheduler_time": 6.262191992619684
}
#Debug simulation 
Total elapsed time: 2.7248376491479576. Arrivals time: 0.09360915841534734 Scheduler time: 2.299479237291962 Scheduler overhead time: 0.12732064444571733 Adapter cache time: 0.021935903932899237 Engine time: 0.12226586043834686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 17280, 66, 17280, 66, 540, 540, 66, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106710 . Total input tokens: 23824785 . Total output tokens: 20921182
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.70499958191067,
    "estimated_duration": 3599.94893675797,
    "input_throughput": 2465.7913642503413,
    "output_throughput": 2135.1372297303137,
    "total_throughput": 4600.928593980655,
    "itl": 25.806821807603832,
    "ttft": 6680.8961288316505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 35793,
    "finished_requests": 35727,
    "scheduler_time": 6.2622344421701355
}
#Debug simulation 
Total elapsed time: 2.7050977908074856. Arrivals time: 0.09509077295660973 Scheduler time: 2.2792914863675833 Scheduler overhead time: 0.1260744808241725 Adapter cache time: 0.02199128596112132 Engine time: 0.1225983016192913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 17280, 66, 17280, 66, 540, 540, 66, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106710 . Total input tokens: 23824785 . Total output tokens: 20921182
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.7080166987143457,
    "estimated_duration": 3599.956501016474,
    "input_throughput": 2465.786183109044,
    "output_throughput": 2135.1327433622305,
    "total_throughput": 4600.918926471274,
    "itl": 25.80695388881752,
    "ttft": 6680.841636103775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 35793,
    "finished_requests": 35727,
    "scheduler_time": 6.262119769816954
}
#Debug simulation 
Total elapsed time: 2.708112074062228. Arrivals time: 0.09281211579218507 Scheduler time: 2.2836421066895127 Scheduler overhead time: 0.12514546420425177 Adapter cache time: 0.02215162245556712 Engine time: 0.12449370650574565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 17280, 66, 17280, 66, 540, 540, 66, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106710 . Total input tokens: 23824785 . Total output tokens: 20921182
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.704888226930052,
    "estimated_duration": 3599.968425194559,
    "input_throughput": 2465.7780156836407,
    "output_throughput": 2135.1256711604606,
    "total_throughput": 4600.903686844101,
    "itl": 25.80684961616484,
    "ttft": 6680.770129265686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35793,
    "finished_requests": 35727,
    "scheduler_time": 6.262269427714444
}
#Debug simulation 
Total elapsed time: 2.7049900800921023. Arrivals time: 0.0954371034167707 Scheduler time: 2.2766382810659707 Scheduler overhead time: 0.12613468943163753 Adapter cache time: 0.02189129637554288 Engine time: 0.12491065170615911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 17280, 66, 17280, 66, 540, 540, 66, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106710 . Total input tokens: 23824785 . Total output tokens: 20921182
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6982425400055945,
    "estimated_duration": 3599.955581446057,
    "input_throughput": 2465.7868129679346,
    "output_throughput": 2135.1332887592116,
    "total_throughput": 4600.920101727146,
    "itl": 25.80691707713782,
    "ttft": 6680.87617388869,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 35793,
    "finished_requests": 35727,
    "scheduler_time": 6.262136783035118
}
#Debug simulation 
Total elapsed time: 2.6983412401750684. Arrivals time: 0.09502519341185689 Scheduler time: 2.2743693594820797 Scheduler overhead time: 0.1252265525981784 Adapter cache time: 0.021827601827681065 Engine time: 0.12192292278632522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 17280, 33, 17280, 33, 540, 540, 33, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106545 . Total input tokens: 23790461 . Total output tokens: 20890011
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6881495867855847,
    "estimated_duration": 3599.900064695652,
    "input_throughput": 2457.203211486329,
    "output_throughput": 2121.6283404371984,
    "total_throughput": 4578.8315519235275,
    "itl": 25.76445894666884,
    "ttft": 5784.176091063685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35739,
    "finished_requests": 35682,
    "scheduler_time": 6.046734486085872
}
#Debug simulation 
Total elapsed time: 2.688242399133742. Arrivals time: 0.09565738448873162 Scheduler time: 2.2601781473495066 Scheduler overhead time: 0.12593843368813396 Adapter cache time: 0.021711532026529312 Engine time: 0.12456953944638371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 17280, 33, 17280, 33, 540, 540, 33, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106545 . Total input tokens: 23790461 . Total output tokens: 20890011
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.678498738911003,
    "estimated_duration": 3599.912836670497,
    "input_throughput": 2457.194493681474,
    "output_throughput": 2121.6208132038946,
    "total_throughput": 4578.815306885368,
    "itl": 25.76451257037123,
    "ttft": 5784.268521932363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 35739,
    "finished_requests": 35682,
    "scheduler_time": 6.046790863154446
}
#Debug simulation 
Total elapsed time: 2.6785886082798243. Arrivals time: 0.08930240431800485 Scheduler time: 2.2582407505251467 Scheduler overhead time: 0.12582979630678892 Adapter cache time: 0.021785999182611704 Engine time: 0.12356689432635903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 17280, 33, 17280, 33, 540, 540, 33, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106545 . Total input tokens: 23790461 . Total output tokens: 20890011
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.691875227726996,
    "estimated_duration": 3599.8909135293793,
    "input_throughput": 2457.209457863148,
    "output_throughput": 2121.6337337599903,
    "total_throughput": 4578.843191623138,
    "itl": 25.764509155758194,
    "ttft": 5784.195560233148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 35739,
    "finished_requests": 35682,
    "scheduler_time": 6.046832020098878
}
#Debug simulation 
Total elapsed time: 2.6919583468697965. Arrivals time: 0.0899824881926179 Scheduler time: 2.2685999567620456 Scheduler overhead time: 0.12565329484641552 Adapter cache time: 0.02174153085798025 Engine time: 0.12606156338006258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 17280, 33, 17280, 33, 540, 540, 33, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106545 . Total input tokens: 23790461 . Total output tokens: 20890011
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.7106794654391706,
    "estimated_duration": 3599.905699852145,
    "input_throughput": 2457.1993650731765,
    "output_throughput": 2121.625019320282,
    "total_throughput": 4578.824384393459,
    "itl": 25.76445945420194,
    "ttft": 5784.209014131551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 35739,
    "finished_requests": 35682,
    "scheduler_time": 6.046706964547618
}
#Debug simulation 
Total elapsed time: 2.7108013150282204. Arrivals time: 0.09349358966574073 Scheduler time: 2.286767586134374 Scheduler overhead time: 0.12553720362484455 Adapter cache time: 0.021705213002860546 Engine time: 0.1233945619314909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 17280, 33, 17280, 33, 540, 540, 33, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106545 . Total input tokens: 23790461 . Total output tokens: 20890011
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.6978158466517925,
    "estimated_duration": 3599.889040402438,
    "input_throughput": 2457.210736420677,
    "output_throughput": 2121.634837707713,
    "total_throughput": 4578.845574128391,
    "itl": 25.764556505546516,
    "ttft": 5784.169744172268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 35739,
    "finished_requests": 35682,
    "scheduler_time": 6.046776351894318
}
#Debug simulation 
Total elapsed time: 2.6979102166369557. Arrivals time: 0.09593417961150408 Scheduler time: 2.269669658038765 Scheduler overhead time: 0.12689778907224536 Adapter cache time: 0.021974040661007166 Engine time: 0.12340571684762836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 17280, 33, 17280, 33, 540, 540, 33, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106545 . Total input tokens: 23790461 . Total output tokens: 20890011
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.7200813298113644,
    "estimated_duration": 3599.8978975178643,
    "input_throughput": 2457.204690749456,
    "output_throughput": 2121.629617680594,
    "total_throughput": 4578.8343084300495,
    "itl": 25.76451187657999,
    "ttft": 5784.193970737002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35739,
    "finished_requests": 35682,
    "scheduler_time": 6.046835939784906
}
#Debug simulation 
Total elapsed time: 2.720177737995982. Arrivals time: 0.09415821684524417 Scheduler time: 2.291509645525366 Scheduler overhead time: 0.12767024198547006 Adapter cache time: 0.02184756100177765 Engine time: 0.12465377291664481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 17280, 33, 17280, 33, 540, 540, 33, 540, 17280, 17280, 17280, 17280]
Prompts retrieved: 106545 . Total input tokens: 23790461 . Total output tokens: 20890011
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6951101450249553,
    "estimated_duration": 3599.914205806527,
    "input_throughput": 2457.1935591498927,
    "output_throughput": 2121.620006299249,
    "total_throughput": 4578.813565449142,
    "itl": 25.764499845364327,
    "ttft": 5784.210941398153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 35739,
    "finished_requests": 35682,
    "scheduler_time": 6.046838566864941
}
#Debug simulation 
Total elapsed time: 2.695210070349276. Arrivals time: 0.09528017742559314 Scheduler time: 2.2679270785301924 Scheduler overhead time: 0.12615552311763167 Adapter cache time: 0.021909328643232584 Engine time: 0.12387834955006838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 17280, 135, 17280, 135, 270, 270, 135, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105705 . Total input tokens: 23597650 . Total output tokens: 20732574
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.663766608107835,
    "estimated_duration": 3599.965165334711,
    "input_throughput": 2423.8923431884646,
    "output_throughput": 2114.352959104898,
    "total_throughput": 4538.245302293362,
    "itl": 25.589585356535704,
    "ttft": 6847.014147853057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35446,
    "finished_requests": 35379,
    "scheduler_time": 5.836466385979216
}
#Debug simulation 
Total elapsed time: 2.663869188167155. Arrivals time: 0.08966572117060423 Scheduler time: 2.2419252106919885 Scheduler overhead time: 0.12657742109149694 Adapter cache time: 0.021489142440259457 Engine time: 0.12401173682883382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 17280, 135, 17280, 135, 270, 270, 135, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105705 . Total input tokens: 23597650 . Total output tokens: 20732574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6857363106682897,
    "estimated_duration": 3599.9797750580387,
    "input_throughput": 2423.8825063563922,
    "output_throughput": 2114.344378470095,
    "total_throughput": 4538.226884826487,
    "itl": 25.58958800410647,
    "ttft": 6846.918717263329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 35446,
    "finished_requests": 35379,
    "scheduler_time": 5.836489237099438
}
#Debug simulation 
Total elapsed time: 2.6858314089477062. Arrivals time: 0.09520215168595314 Scheduler time: 2.251448582392186 Scheduler overhead time: 0.12899905443191528 Adapter cache time: 0.021573865320533514 Engine time: 0.12839400954544544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 17280, 135, 17280, 135, 270, 270, 135, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105705 . Total input tokens: 23597650 . Total output tokens: 20732574
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6897143819369376,
    "estimated_duration": 3599.9554828460964,
    "input_throughput": 2423.898862521864,
    "output_throughput": 2114.358645897013,
    "total_throughput": 4538.257508418877,
    "itl": 25.58942953390955,
    "ttft": 6847.040714724024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 35446,
    "finished_requests": 35379,
    "scheduler_time": 5.836371228802241
}
#Debug simulation 
Total elapsed time: 2.689797454047948. Arrivals time: 0.08974653063341975 Scheduler time: 2.267140795942396 Scheduler overhead time: 0.12659432226791978 Adapter cache time: 0.021623312029987574 Engine time: 0.12437971215695143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 17280, 135, 17280, 135, 270, 270, 135, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105705 . Total input tokens: 23597650 . Total output tokens: 20732574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.681414703838527,
    "estimated_duration": 3599.9709791401137,
    "input_throughput": 2423.8884287017972,
    "output_throughput": 2114.3495445116337,
    "total_throughput": 4538.23797321343,
    "itl": 25.589559853475308,
    "ttft": 6846.992815889788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 35446,
    "finished_requests": 35379,
    "scheduler_time": 5.836433193528842
}
#Debug simulation 
Total elapsed time: 2.6815128778107464. Arrivals time: 0.095729049295187 Scheduler time: 2.246494682505727 Scheduler overhead time: 0.12619355786591768 Adapter cache time: 0.02348392130807042 Engine time: 0.12559026619419456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 17280, 135, 17280, 135, 270, 270, 135, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105705 . Total input tokens: 23597650 . Total output tokens: 20732574
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.6787162413820624,
    "estimated_duration": 3599.9540858502955,
    "input_throughput": 2423.8998031384526,
    "output_throughput": 2114.35946639363,
    "total_throughput": 4538.259269532083,
    "itl": 25.589449485270848,
    "ttft": 6847.044688752812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 35446,
    "finished_requests": 35379,
    "scheduler_time": 5.836347919061979
}
#Debug simulation 
Total elapsed time: 2.6788304313085973. Arrivals time: 0.09267570869997144 Scheduler time: 2.2534223282709718 Scheduler overhead time: 0.12663325993344188 Adapter cache time: 0.02156829135492444 Engine time: 0.12416139803826809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 17280, 135, 17280, 135, 270, 270, 135, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105705 . Total input tokens: 23597650 . Total output tokens: 20732574
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.686245085205883,
    "estimated_duration": 3599.9601816922514,
    "input_throughput": 2423.895698729134,
    "output_throughput": 2114.35588613149,
    "total_throughput": 4538.2515848606245,
    "itl": 25.58951433032698,
    "ttft": 6847.072380679097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35446,
    "finished_requests": 35379,
    "scheduler_time": 5.836314976855689
}
#Debug simulation 
Total elapsed time: 2.6863451208919287. Arrivals time: 0.08868104126304388 Scheduler time: 2.2617558231577277 Scheduler overhead time: 0.12841262854635715 Adapter cache time: 0.02154267206788063 Engine time: 0.1247600894421339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 17280, 135, 17280, 135, 270, 270, 135, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105705 . Total input tokens: 23597650 . Total output tokens: 20732574
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.689109389204532,
    "estimated_duration": 3599.9525078508022,
    "input_throughput": 2423.9008656282085,
    "output_throughput": 2114.3603931997923,
    "total_throughput": 4538.261258828001,
    "itl": 25.589587555142963,
    "ttft": 6846.934156827416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 35446,
    "finished_requests": 35379,
    "scheduler_time": 5.836404296130575
}
#Debug simulation 
Total elapsed time: 2.689235409256071. Arrivals time: 0.09548461902886629 Scheduler time: 2.256887760013342 Scheduler overhead time: 0.12688241945579648 Adapter cache time: 0.021904234774410725 Engine time: 0.1269818372093141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 17280, 66, 17280, 66, 270, 270, 66, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105360 . Total input tokens: 23522503 . Total output tokens: 20666820
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6983892507851124,
    "estimated_duration": 3599.895082380857,
    "input_throughput": 2424.7164987450406,
    "output_throughput": 2123.2138229275647,
    "total_throughput": 4547.930321672606,
    "itl": 25.56275372086857,
    "ttft": 7582.596038508392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 35330,
    "finished_requests": 35256,
    "scheduler_time": 5.910633824077617
}
#Debug simulation 
Total elapsed time: 2.698482546955347. Arrivals time: 0.09206110006198287 Scheduler time: 2.275719763711095 Scheduler overhead time: 0.12710885610431433 Adapter cache time: 0.021376135759055614 Engine time: 0.12199613312259316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 17280, 66, 17280, 66, 270, 270, 66, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105360 . Total input tokens: 23522503 . Total output tokens: 20666820
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6810130649246275,
    "estimated_duration": 3599.8842231630265,
    "input_throughput": 2424.7238130148903,
    "output_throughput": 2123.2202277006004,
    "total_throughput": 4547.944040715491,
    "itl": 25.56275708566984,
    "ttft": 7582.511652432391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 35330,
    "finished_requests": 35256,
    "scheduler_time": 5.910612057187413
}
#Debug simulation 
Total elapsed time: 2.6811132207512856. Arrivals time: 0.09176106657832861 Scheduler time: 2.2573944544419646 Scheduler overhead time: 0.12706587370485067 Adapter cache time: 0.021459252573549747 Engine time: 0.12337703024968505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 17280, 66, 17280, 66, 270, 270, 66, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105360 . Total input tokens: 23522503 . Total output tokens: 20666820
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.6913433237932622,
    "estimated_duration": 3599.8875940242433,
    "input_throughput": 2424.7215425530358,
    "output_throughput": 2123.218239560545,
    "total_throughput": 4547.939782113581,
    "itl": 25.562707635068705,
    "ttft": 7582.533835625253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 35330,
    "finished_requests": 35256,
    "scheduler_time": 5.910736487128669
}
#Debug simulation 
Total elapsed time: 2.691471111960709. Arrivals time: 0.09395911404863 Scheduler time: 2.2577461558394134 Scheduler overhead time: 0.13349512219429016 Adapter cache time: 0.02159661427140236 Engine time: 0.12446891656145453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 17280, 66, 17280, 66, 270, 270, 66, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105360 . Total input tokens: 23522503 . Total output tokens: 20666820
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 2.70952542219311,
    "estimated_duration": 3599.8748467103405,
    "input_throughput": 2424.730128597814,
    "output_throughput": 2123.225757968972,
    "total_throughput": 4547.955886566786,
    "itl": 25.5627244075356,
    "ttft": 7582.558355221828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 35330,
    "finished_requests": 35256,
    "scheduler_time": 5.910765634770966
}
#Debug simulation 
Total elapsed time: 2.709623604081571. Arrivals time: 0.09480051044374704 Scheduler time: 2.277668733149767 Scheduler overhead time: 0.12835758877918124 Adapter cache time: 0.021343699656426907 Engine time: 0.12666047643870115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 17280, 66, 17280, 66, 270, 270, 66, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105360 . Total input tokens: 23522503 . Total output tokens: 20666820
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 2.688765650149435,
    "estimated_duration": 3599.8869098738355,
    "input_throughput": 2424.7220033659096,
    "output_throughput": 2123.2186430733946,
    "total_throughput": 4547.940646439304,
    "itl": 25.56272874651966,
    "ttft": 7582.521738079808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 35330,
    "finished_requests": 35256,
    "scheduler_time": 5.910662387977919
}
#Debug simulation 
Total elapsed time: 2.688894100021571. Arrivals time: 0.08897689683362842 Scheduler time: 2.269189504906535 Scheduler overhead time: 0.12648183852434158 Adapter cache time: 0.021533775608986616 Engine time: 0.12258027773350477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 17280, 66, 17280, 66, 270, 270, 66, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105360 . Total input tokens: 23522503 . Total output tokens: 20666820
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 2.6791449561715126,
    "estimated_duration": 3599.890892872334,
    "input_throughput": 2424.71932060013,
    "output_throughput": 2123.216293897567,
    "total_throughput": 4547.935614497696,
    "itl": 25.56262576228674,
    "ttft": 7582.5456750639105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 35330,
    "finished_requests": 35256,
    "scheduler_time": 5.910776935209096
}
#Debug simulation 
Total elapsed time: 2.6792167918756604. Arrivals time: 0.08826336078345776 Scheduler time: 2.258008036762476 Scheduler overhead time: 0.12621903978288174 Adapter cache time: 0.021446989849209785 Engine time: 0.125154344830662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 17280, 66, 17280, 66, 270, 270, 66, 270, 17280, 17280, 17280, 17280]
Prompts retrieved: 105360 . Total input tokens: 23522503 . Total output tokens: 20666820
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 2.706563937012106,
    "estimated_duration": 3599.8844615733688,
    "input_throughput": 2424.7236524321715,
    "output_throughput": 2123.220087085626,
    "total_throughput": 4547.943739517797,
    "itl": 25.56276271572649,
    "ttft": 7582.525844252136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 35330,
    "finished_requests": 35256,
    "scheduler_time": 5.910613600037426
}
#Debug simulation 
Total elapsed time: 2.7066531907767057. Arrivals time: 0.09195920033380389 Scheduler time: 2.2774259112775326 Scheduler overhead time: 0.128937897272408 Adapter cache time: 0.02139652892947197 Engine time: 0.12643043883144855 
