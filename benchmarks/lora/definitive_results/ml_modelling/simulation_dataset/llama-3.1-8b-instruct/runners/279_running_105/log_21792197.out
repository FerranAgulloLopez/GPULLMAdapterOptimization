INFO 05-31 19:31:06 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:06 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 56.36600304627791,
    "estimated_duration": 3600.092141566882,
    "input_throughput": 5345.094026294802,
    "output_throughput": 4667.168044395413,
    "total_throughput": 10012.262070690214,
    "itl": 111.5771427378161,
    "ttft": 2187330.2023610966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 711,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.1858787623094456,
    "arrivals": 2257312,
    "finished_requests": 77704,
    "scheduler_time": 197.90092584000192
}
#Debug simulation 
Total elapsed time: 56.36624060804024. Arrivals time: 0.4162175697274506 Scheduler time: 55.779639730695635 Scheduler overhead time: 0.062790188472718 Adapter cache time: 0.020214689429849386 Engine time: 0.06207237718626857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.2411028817296,
    "estimated_duration": 3600.0569295316145,
    "input_throughput": 5036.014250574309,
    "output_throughput": 4405.033395420014,
    "total_throughput": 9441.047645994324,
    "itl": 99.42882213314253,
    "ttft": 2214463.240446651,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.769836819339517,
    "arrivals": 2257312,
    "finished_requests": 73366,
    "scheduler_time": 209.76165930494247
}
#Debug simulation 
Total elapsed time: 32.24131023697555. Arrivals time: 0.36480523692443967 Scheduler time: 31.699002933688462 Scheduler overhead time: 0.06331630563363433 Adapter cache time: 0.02604147046804428 Engine time: 0.061725189443677664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 49.57713552610949,
    "estimated_duration": 3600.0680316659036,
    "input_throughput": 5345.003714025854,
    "output_throughput": 4671.808658076169,
    "total_throughput": 10016.812372102022,
    "itl": 111.78921002460038,
    "ttft": 2187569.4692454617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 940,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.44233995364978,
    "arrivals": 2257312,
    "finished_requests": 77849,
    "scheduler_time": 197.59845799761047
}
#Debug simulation 
Total elapsed time: 49.577327363193035. Arrivals time: 0.42521003913134336 Scheduler time: 48.9808047497645 Scheduler overhead time: 0.06276789866387844 Adapter cache time: 0.023031076416373253 Engine time: 0.06061429996043444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 32.439612093847245,
    "estimated_duration": 3600.088080034368,
    "input_throughput": 5036.2015031107,
    "output_throughput": 4405.082222279574,
    "total_throughput": 9441.283725390274,
    "itl": 99.42641772955214,
    "ttft": 2214526.7728631236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.688232651753331,
    "arrivals": 2257312,
    "finished_requests": 73369,
    "scheduler_time": 209.76789345822476
}
#Debug simulation 
Total elapsed time: 32.439728665631264. Arrivals time: 0.37936626467853785 Scheduler time: 31.883486656006426 Scheduler overhead time: 0.06321562081575394 Adapter cache time: 0.026099631562829018 Engine time: 0.06139708962291479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 55.84991627372801,
    "estimated_duration": 3600.0925381859474,
    "input_throughput": 5335.101472052203,
    "output_throughput": 4662.088216337148,
    "total_throughput": 9997.189688389351,
    "itl": 111.42079385227312,
    "ttft": 2186157.3859046656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 848,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.413564957603786,
    "arrivals": 2257312,
    "finished_requests": 77654,
    "scheduler_time": 198.1167553546186
}
#Debug simulation 
Total elapsed time: 55.85008465498686. Arrivals time: 0.419433169066906 Scheduler time: 55.25918730488047 Scheduler overhead time: 0.0640598819591105 Adapter cache time: 0.02189057320356369 Engine time: 0.060601570177823305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 17280, 1080, 34560, 34560, 1080, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 1080, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 17280, 1080, 17280, 1080, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 17280, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 17280, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 17280, 34560, 1080, 1080, 17280, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 17280, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 1080, 1080, 1080, 17280, 1080, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 17280, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 6773760 . Total input tokens: 1510742719 . Total output tokens: 1330273881
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.331769339740276,
    "estimated_duration": 3600.007881999749,
    "input_throughput": 5036.313695493533,
    "output_throughput": 4405.180355102652,
    "total_throughput": 9441.494050596184,
    "itl": 99.42429300002635,
    "ttft": 2214495.4228591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.608285421580058,
    "arrivals": 2257312,
    "finished_requests": 73369,
    "scheduler_time": 209.76764265377884
}
#Debug simulation 
Total elapsed time: 32.33188969781622. Arrivals time: 0.3697534631937742 Scheduler time: 31.78471359796822 Scheduler overhead time: 0.0632073082961142 Adapter cache time: 0.026449041441082954 Engine time: 0.061526925303041935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 57.7794593363069,
    "estimated_duration": 3600.041265898075,
    "input_throughput": 5494.405907890507,
    "output_throughput": 4809.43653730057,
    "total_throughput": 10303.842445191076,
    "itl": 119.85745259377488,
    "ttft": 2175210.402201911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 756,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.998981228154246,
    "arrivals": 2233996,
    "finished_requests": 80296,
    "scheduler_time": 191.290464085069
}
#Debug simulation 
Total elapsed time: 57.77964681200683. Arrivals time: 0.8986821537837386 Scheduler time: 56.71641119197011 Scheduler overhead time: 0.06161116901785135 Adapter cache time: 0.020448559895157814 Engine time: 0.05864708125591278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 46.976808824110776,
    "estimated_duration": 3600.0301454231144,
    "input_throughput": 5349.306039696129,
    "output_throughput": 4684.051332580747,
    "total_throughput": 10033.357372276876,
    "itl": 112.5388612957499,
    "ttft": 2188457.530545312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.647296339874165,
    "arrivals": 2233996,
    "finished_requests": 78201,
    "scheduler_time": 196.93263229301687
}
#Debug simulation 
Total elapsed time: 46.9769315761514. Arrivals time: 0.396777821239084 Scheduler time: 46.41251976881176 Scheduler overhead time: 0.0614441386424005 Adapter cache time: 0.02218218380585313 Engine time: 0.05979029042646289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.17515473207459,
    "estimated_duration": 3600.0270383793245,
    "input_throughput": 5044.446835092497,
    "output_throughput": 4420.521798959663,
    "total_throughput": 9464.96863405216,
    "itl": 100.09332548415165,
    "ttft": 2212924.891873471,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1097,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.240864576068713,
    "arrivals": 2233996,
    "finished_requests": 73737,
    "scheduler_time": 209.01734540284218
}
#Debug simulation 
Total elapsed time: 31.17530093435198. Arrivals time: 0.5105624874122441 Scheduler time: 30.49137140158564 Scheduler overhead time: 0.06230305228382349 Adapter cache time: 0.024715681094676256 Engine time: 0.06042767409235239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 47.121402475982904,
    "estimated_duration": 3600.1065766461243,
    "input_throughput": 5349.426354466792,
    "output_throughput": 4684.2171588459705,
    "total_throughput": 10033.643513312762,
    "itl": 112.52673331784833,
    "ttft": 2188345.2188510047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.233606489826914,
    "arrivals": 2233996,
    "finished_requests": 78205,
    "scheduler_time": 196.95855145370874
}
#Debug simulation 
Total elapsed time: 47.121591503731906. Arrivals time: 0.4245510180480778 Scheduler time: 46.52925093937665 Scheduler overhead time: 0.06108933687210083 Adapter cache time: 0.022288567386567593 Engine time: 0.05948542011901736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 30.996337031945586,
    "estimated_duration": 3600.067030815903,
    "input_throughput": 5044.606626639419,
    "output_throughput": 4420.6174117800265,
    "total_throughput": 9465.224038419445,
    "itl": 100.09183627604449,
    "ttft": 2212893.52866529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1097,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.166716626840651,
    "arrivals": 2233996,
    "finished_requests": 73740,
    "scheduler_time": 209.02368988647152
}
#Debug simulation 
Total elapsed time: 30.996516844723374. Arrivals time: 0.3819914972409606 Scheduler time: 30.44071009941399 Scheduler overhead time: 0.06219161348417401 Adapter cache time: 0.02480030618607998 Engine time: 0.060743982903659344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 48.161357953213155,
    "estimated_duration": 3600.0774150716634,
    "input_throughput": 5340.6237653412445,
    "output_throughput": 4671.817036374814,
    "total_throughput": 10012.440801716059,
    "itl": 111.7504836836603,
    "ttft": 2189900.711246573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.356109669138651,
    "arrivals": 2233996,
    "finished_requests": 78030,
    "scheduler_time": 197.66203806090536
}
#Debug simulation 
Total elapsed time: 48.16152345808223. Arrivals time: 0.42220449121668935 Scheduler time: 47.57185920327902 Scheduler overhead time: 0.06148593779653311 Adapter cache time: 0.021420419681817293 Engine time: 0.059722923673689365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 17280, 540, 34560, 34560, 540, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 540, 540, 17280, 540, 34560, 540, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 34560, 34560, 540, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 540, 34560, 17280, 17280, 540, 540, 540, 17280, 540, 17280, 540, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 34560, 17280, 17280, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 540, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 17280, 540, 34560, 17280, 17280, 540, 34560, 34560, 34560, 540, 17280, 34560, 540, 34560, 17280, 17280, 34560, 34560, 540, 17280, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 17280, 17280, 17280, 540, 540, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 540, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 34560, 540, 17280, 17280, 540, 540, 17280, 540, 34560, 34560, 540, 34560, 34560, 540, 17280, 34560, 540, 540, 17280, 17280, 540, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 540, 540, 540, 540, 540, 540, 540, 540, 17280, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 17280, 540, 34560, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 540, 34560, 34560, 17280, 540, 34560, 17280, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 17280, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 34560, 540, 540, 540, 17280, 540, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 540, 17280, 17280, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 17280, 17280, 17280, 540, 17280, 540, 540, 34560, 540, 17280, 540, 540, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 6704640 . Total input tokens: 1495324409 . Total output tokens: 1316648793
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.08971676696092,
    "estimated_duration": 3600.1025330276166,
    "input_throughput": 5044.689097987055,
    "output_throughput": 4420.687703751552,
    "total_throughput": 9465.376801738606,
    "itl": 100.09002888419349,
    "ttft": 2212930.3245541155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1097,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.08863345125691,
    "arrivals": 2233996,
    "finished_requests": 73742,
    "scheduler_time": 209.02990449996523
}
#Debug simulation 
Total elapsed time: 31.08981656609103. Arrivals time: 0.38663712749257684 Scheduler time: 30.528813499491662 Scheduler overhead time: 0.062447423581033945 Adapter cache time: 0.025286344811320305 Engine time: 0.06071887258440256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 60.0204012459144,
    "estimated_duration": 3600.117182019912,
    "input_throughput": 5532.111315561573,
    "output_throughput": 4806.887422006223,
    "total_throughput": 10338.998737567796,
    "itl": 119.47120167830873,
    "ttft": 2167214.2912869477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5228877778538354,
    "arrivals": 2222461,
    "finished_requests": 80492,
    "scheduler_time": 191.44530262787518
}
#Debug simulation 
Total elapsed time: 60.02057926310226. Arrivals time: 0.41783872339874506 Scheduler time: 59.44064710568637 Scheduler overhead time: 0.06031300965696573 Adapter cache time: 0.018674210645258427 Engine time: 0.058925531804561615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 54.46982025820762,
    "estimated_duration": 3600.07800985045,
    "input_throughput": 5383.63889531525,
    "output_throughput": 4681.3438358520725,
    "total_throughput": 10064.982731167322,
    "itl": 112.16524084368115,
    "ttft": 2180470.0938290716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.932457851902589,
    "arrivals": 2222461,
    "finished_requests": 78383,
    "scheduler_time": 197.24681294772046
}
#Debug simulation 
Total elapsed time: 54.470008586067706. Arrivals time: 0.5852718413807452 Scheduler time: 53.717020622454584 Scheduler overhead time: 0.062757205683738 Adapter cache time: 0.018769167829304934 Engine time: 0.061013853177428246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 28.353237750008702,
    "estimated_duration": 3600.104454800854,
    "input_throughput": 5072.641982831077,
    "output_throughput": 4408.739301670618,
    "total_throughput": 9481.381284501696,
    "itl": 99.38171778757727,
    "ttft": 2205384.905503322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.870301118078654,
    "arrivals": 2222461,
    "finished_requests": 73737,
    "scheduler_time": 209.69385631150288
}
#Debug simulation 
Total elapsed time: 28.353378803934902. Arrivals time: 0.3705355669371784 Scheduler time: 27.81183276651427 Scheduler overhead time: 0.06162274023517966 Adapter cache time: 0.023101413156837225 Engine time: 0.06053199153393507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 50.37256029294804,
    "estimated_duration": 3600.110425392957,
    "input_throughput": 5389.423575217748,
    "output_throughput": 4682.044162064878,
    "total_throughput": 10071.467737282626,
    "itl": 112.03143540746758,
    "ttft": 2180676.0734045794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 680,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.663133505396538,
    "arrivals": 2222461,
    "finished_requests": 78456,
    "scheduler_time": 197.31522722393035
}
#Debug simulation 
Total elapsed time: 50.3727161437273. Arrivals time: 0.426904727704823 Scheduler time: 49.779213428497314 Scheduler overhead time: 0.06210885336622596 Adapter cache time: 0.019473408814519644 Engine time: 0.059930033050477505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 28.33543570106849,
    "estimated_duration": 3600.0323452882817,
    "input_throughput": 5072.743589068398,
    "output_throughput": 4408.827609777772,
    "total_throughput": 9481.57119884617,
    "itl": 99.37987912090267,
    "ttft": 2205359.6736361296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.7984314577933525,
    "arrivals": 2222461,
    "finished_requests": 73737,
    "scheduler_time": 209.69361645921617
}
#Debug simulation 
Total elapsed time: 28.335554395336658. Arrivals time: 0.5308112720958889 Scheduler time: 27.631873805075884 Scheduler overhead time: 0.06251549627631903 Adapter cache time: 0.0235772212035954 Engine time: 0.06096159527078271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 52.10283158393577,
    "estimated_duration": 3600.0179509030368,
    "input_throughput": 5385.08120359152,
    "output_throughput": 4675.128910337291,
    "total_throughput": 10060.21011392881,
    "itl": 111.66647994385848,
    "ttft": 2182521.7466965592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3729858442907945,
    "arrivals": 2222461,
    "finished_requests": 78324,
    "scheduler_time": 197.62345232210805
}
#Debug simulation 
Total elapsed time: 52.103019087109715. Arrivals time: 0.4146711095236242 Scheduler time: 51.52160579478368 Scheduler overhead time: 0.061919067054986954 Adapter cache time: 0.019366838037967682 Engine time: 0.06031237682327628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 17280, 270, 34560, 34560, 270, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 270, 270, 17280, 270, 34560, 270, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 34560, 34560, 270, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 270, 34560, 17280, 17280, 270, 270, 270, 17280, 270, 17280, 270, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 34560, 17280, 17280, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 270, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 17280, 270, 34560, 17280, 17280, 270, 34560, 34560, 34560, 270, 17280, 34560, 270, 34560, 17280, 17280, 34560, 34560, 270, 17280, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 17280, 17280, 17280, 270, 270, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 270, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 34560, 270, 17280, 17280, 270, 270, 17280, 270, 34560, 34560, 270, 34560, 34560, 270, 17280, 34560, 270, 270, 17280, 17280, 270, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 270, 270, 270, 270, 270, 270, 270, 270, 17280, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 17280, 270, 34560, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 270, 34560, 34560, 17280, 270, 34560, 17280, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 17280, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 34560, 270, 270, 270, 17280, 270, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 270, 17280, 17280, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 17280, 17280, 17280, 270, 17280, 270, 270, 34560, 270, 17280, 270, 270, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 6670080 . Total input tokens: 1487578716 . Total output tokens: 1309898733
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 28.21304118121043,
    "estimated_duration": 3600.066993529047,
    "input_throughput": 5072.694767298823,
    "output_throughput": 4408.785177756147,
    "total_throughput": 9481.47994505497,
    "itl": 99.3779972528163,
    "ttft": 2205333.5544622946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.725940445978207,
    "arrivals": 2222461,
    "finished_requests": 73737,
    "scheduler_time": 209.6997618635879
}
#Debug simulation 
Total elapsed time: 28.21320550935343. Arrivals time: 0.35157550871372223 Scheduler time: 27.689962558913976 Scheduler overhead time: 0.06201020674780011 Adapter cache time: 0.0233682319521904 Engine time: 0.06030213600024581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 59.543327062856406,
    "estimated_duration": 3600.0914942016548,
    "input_throughput": 5563.242498769157,
    "output_throughput": 4802.548220745732,
    "total_throughput": 10365.790719514887,
    "itl": 118.90982077156862,
    "ttft": 2175079.5834052674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8947089198185716,
    "arrivals": 2216625,
    "finished_requests": 80669,
    "scheduler_time": 191.77230689784116
}
#Debug simulation 
Total elapsed time: 59.54351694462821. Arrivals time: 0.4265916580334306 Scheduler time: 58.9539725119248 Scheduler overhead time: 0.060713605023920536 Adapter cache time: 0.017911552917212248 Engine time: 0.059903078712522984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 47.64839755510911,
    "estimated_duration": 3600.052549854613,
    "input_throughput": 5406.322749590421,
    "output_throughput": 4676.142019282427,
    "total_throughput": 10082.46476887285,
    "itl": 111.73459354284951,
    "ttft": 2188052.604656697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.79006618631073,
    "arrivals": 2216625,
    "finished_requests": 78507,
    "scheduler_time": 197.503357665211
}
#Debug simulation 
Total elapsed time: 47.648570235818624. Arrivals time: 0.5764205292798579 Scheduler time: 46.907120851799846 Scheduler overhead time: 0.0614955173805356 Adapter cache time: 0.01902096252888441 Engine time: 0.05982441268861294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.784539475105703,
    "estimated_duration": 3600.003482726343,
    "input_throughput": 5105.467283070722,
    "output_throughput": 4411.594065451426,
    "total_throughput": 9517.061348522147,
    "itl": 99.39046603673222,
    "ttft": 2211526.0103265364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.0015239319205484,
    "arrivals": 2216625,
    "finished_requests": 74018,
    "scheduler_time": 209.66388238645678
}
#Debug simulation 
Total elapsed time: 27.784698413219303. Arrivals time: 0.3920330498367548 Scheduler time: 27.221962550655007 Scheduler overhead time: 0.06196491979062557 Adapter cache time: 0.022366989869624376 Engine time: 0.060258771292865276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 47.560447896365076,
    "estimated_duration": 3600.1181111553847,
    "input_throughput": 5406.522341499898,
    "output_throughput": 4676.450460842492,
    "total_throughput": 10082.97280234239,
    "itl": 111.72697646223234,
    "ttft": 2187914.2896601083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.484657572181888,
    "arrivals": 2216625,
    "finished_requests": 78513,
    "scheduler_time": 197.5224562725772
}
#Debug simulation 
Total elapsed time: 47.560610049404204. Arrivals time: 0.42360866349190474 Scheduler time: 46.97260140441358 Scheduler overhead time: 0.061077488120645285 Adapter cache time: 0.019047708250582218 Engine time: 0.059490191750228405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 27.879495366942137,
    "estimated_duration": 3600.0490833358567,
    "input_throughput": 5105.53594535124,
    "output_throughput": 4411.538740822872,
    "total_throughput": 9517.074686174112,
    "itl": 99.38814202810396,
    "ttft": 2211556.12650626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.935039318227229,
    "arrivals": 2216625,
    "finished_requests": 74019,
    "scheduler_time": 209.67015563989457
}
#Debug simulation 
Total elapsed time: 27.879585972055793. Arrivals time: 0.5488107246346772 Scheduler time: 27.15864935424179 Scheduler overhead time: 0.06243338715285063 Adapter cache time: 0.022314269095659256 Engine time: 0.06112522119656205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 47.68049803003669,
    "estimated_duration": 3600.05433315885,
    "input_throughput": 5406.978394939992,
    "output_throughput": 4676.94607965159,
    "total_throughput": 10083.92447459158,
    "itl": 111.71864272289629,
    "ttft": 2187792.43137233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.175084295133109,
    "arrivals": 2216625,
    "finished_requests": 78518,
    "scheduler_time": 197.53437922868605
}
#Debug simulation 
Total elapsed time: 47.680682154837996. Arrivals time: 0.39252522541210055 Scheduler time: 47.12387892091647 Scheduler overhead time: 0.06093654269352555 Adapter cache time: 0.019046198576688766 Engine time: 0.05938665708526969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 17280, 135, 34560, 34560, 135, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 135, 135, 17280, 135, 34560, 135, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 34560, 34560, 135, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 135, 34560, 17280, 17280, 135, 135, 135, 17280, 135, 17280, 135, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 34560, 17280, 17280, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 135, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 17280, 135, 34560, 17280, 17280, 135, 34560, 34560, 34560, 135, 17280, 34560, 135, 34560, 17280, 17280, 34560, 34560, 135, 17280, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 17280, 17280, 17280, 135, 135, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 135, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 34560, 135, 17280, 17280, 135, 135, 17280, 135, 34560, 34560, 135, 34560, 34560, 135, 17280, 34560, 135, 135, 17280, 17280, 135, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 135, 135, 135, 135, 135, 135, 135, 135, 17280, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 17280, 135, 34560, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 135, 34560, 34560, 17280, 135, 34560, 17280, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 17280, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 34560, 135, 135, 135, 17280, 135, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 135, 17280, 17280, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 17280, 17280, 17280, 135, 17280, 135, 135, 34560, 135, 17280, 135, 135, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 6652800 . Total input tokens: 1483707749 . Total output tokens: 1306543596
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.670169767923653,
    "estimated_duration": 3600.0965509513553,
    "input_throughput": 5105.468628374116,
    "output_throughput": 4411.480574264908,
    "total_throughput": 9516.949202639023,
    "itl": 99.38647711800628,
    "ttft": 2211601.560211508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.870625876300058,
    "arrivals": 2216625,
    "finished_requests": 74019,
    "scheduler_time": 209.67636643700854
}
#Debug simulation 
Total elapsed time: 27.670335677918047. Arrivals time: 0.36507526226341724 Scheduler time: 27.134196882136166 Scheduler overhead time: 0.06232480891048908 Adapter cache time: 0.02211065636947751 Engine time: 0.06044046860188246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.699087098706514,
    "estimated_duration": 3600.057198708115,
    "input_throughput": 5527.16995917189,
    "output_throughput": 4816.8773557883615,
    "total_throughput": 10344.047314960251,
    "itl": 119.13654002201343,
    "ttft": 2175367.806900688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5905381043488647,
    "arrivals": 2213766,
    "finished_requests": 80484,
    "scheduler_time": 191.93238210771005
}
#Debug simulation 
Total elapsed time: 56.699273651931435. Arrivals time: 0.4397680633701384 Scheduler time: 56.10005750833079 Scheduler overhead time: 0.05980032682418823 Adapter cache time: 0.017376936972141266 Engine time: 0.058333645574748516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 45.618884024210274,
    "estimated_duration": 3600.022918071844,
    "input_throughput": 5385.031551516672,
    "output_throughput": 4698.577310463232,
    "total_throughput": 10083.608861979905,
    "itl": 111.77648716763923,
    "ttft": 2185311.3536946094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.687640120456933,
    "arrivals": 2213766,
    "finished_requests": 78513,
    "scheduler_time": 197.8938914663209
}
#Debug simulation 
Total elapsed time: 45.61908217240125. Arrivals time: 0.5996922235935926 Scheduler time: 44.853085859678686 Scheduler overhead time: 0.06241678353399038 Adapter cache time: 0.018718145787715912 Engine time: 0.0600423407740891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 25.741291138343513,
    "estimated_duration": 3600.025127027563,
    "input_throughput": 5052.96389834357,
    "output_throughput": 4422.022191035145,
    "total_throughput": 9474.986089378715,
    "itl": 99.72474387726223,
    "ttft": 2211247.5133391274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 970,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.283555513396882,
    "arrivals": 2213766,
    "finished_requests": 73712,
    "scheduler_time": 209.51704926838715
}
#Debug simulation 
Total elapsed time: 25.741442528087646. Arrivals time: 0.3902983092702925 Scheduler time: 25.179167414084077 Scheduler overhead time: 0.06203579483553767 Adapter cache time: 0.023129513021558523 Engine time: 0.06049679173156619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 45.45255877729505,
    "estimated_duration": 3600.1242361654104,
    "input_throughput": 5384.953053911574,
    "output_throughput": 4698.532853415343,
    "total_throughput": 10083.485907326918,
    "itl": 111.76859463168945,
    "ttft": 2185259.822301642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4002783789811595,
    "arrivals": 2213766,
    "finished_requests": 78514,
    "scheduler_time": 197.9136948589082
}
#Debug simulation 
Total elapsed time: 45.45269210729748. Arrivals time: 0.4192404206842184 Scheduler time: 44.868049913086 Scheduler overhead time: 0.06208864878863096 Adapter cache time: 0.01858595060184598 Engine time: 0.05992716457694769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 25.734268087893724,
    "estimated_duration": 3600.0679016042336,
    "input_throughput": 5052.914694162836,
    "output_throughput": 4422.042426729232,
    "total_throughput": 9474.957120892068,
    "itl": 99.72249917971094,
    "ttft": 2211318.7734487364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 970,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.218727837116481,
    "arrivals": 2213766,
    "finished_requests": 73714,
    "scheduler_time": 209.52323254475715
}
#Debug simulation 
Total elapsed time: 25.734352299943566. Arrivals time: 0.5252369698137045 Scheduler time: 25.038913858588785 Scheduler overhead time: 0.06146569503471255 Adapter cache time: 0.02269013039767742 Engine time: 0.06028546066954732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 47.76231295987964,
    "estimated_duration": 3600.0898203299816,
    "input_throughput": 5379.328007495357,
    "output_throughput": 4699.248308879509,
    "total_throughput": 10078.576316374865,
    "itl": 111.65034061528749,
    "ttft": 2184637.19477228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.894191773748007,
    "arrivals": 2213766,
    "finished_requests": 78446,
    "scheduler_time": 198.19391662435112
}
#Debug simulation 
Total elapsed time: 47.76246863603592. Arrivals time: 0.3865899331867695 Scheduler time: 47.21035850280896 Scheduler overhead time: 0.06186865922063589 Adapter cache time: 0.01844859914854169 Engine time: 0.0602398500777781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 17280, 66, 34560, 34560, 66, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 66, 66, 17280, 66, 34560, 66, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 34560, 34560, 66, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 66, 34560, 17280, 17280, 66, 66, 66, 17280, 66, 17280, 66, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 34560, 17280, 17280, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 66, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 17280, 66, 34560, 17280, 17280, 66, 34560, 34560, 34560, 66, 17280, 34560, 66, 34560, 17280, 17280, 34560, 34560, 66, 17280, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 17280, 17280, 17280, 66, 66, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 66, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 34560, 66, 17280, 17280, 66, 66, 17280, 66, 34560, 34560, 66, 34560, 34560, 66, 17280, 34560, 66, 66, 17280, 17280, 66, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 66, 66, 66, 66, 66, 66, 66, 66, 17280, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 17280, 66, 34560, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 66, 34560, 34560, 17280, 66, 34560, 17280, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 17280, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 34560, 66, 66, 66, 17280, 66, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 66, 17280, 17280, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 17280, 17280, 17280, 66, 17280, 66, 66, 34560, 66, 17280, 66, 66, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 6643968 . Total input tokens: 1481710116 . Total output tokens: 1304810010
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 25.54025624273345,
    "estimated_duration": 3600.105761833925,
    "input_throughput": 5052.920164971299,
    "output_throughput": 4422.165917672952,
    "total_throughput": 9475.086082644251,
    "itl": 99.72025666537763,
    "ttft": 2211387.307188998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 970,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.149343582950558,
    "arrivals": 2213766,
    "finished_requests": 73717,
    "scheduler_time": 209.5293414709478
}
#Debug simulation 
Total elapsed time: 25.54037928255275. Arrivals time: 0.3514116364531219 Scheduler time: 25.018180168233812 Scheduler overhead time: 0.06183336488902569 Adapter cache time: 0.022888158913701773 Engine time: 0.0602544816210866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 55.886181473731995,
    "estimated_duration": 3600.000342485777,
    "input_throughput": 5520.309197047949,
    "output_throughput": 4827.535096288303,
    "total_throughput": 10347.844293336253,
    "itl": 118.83269806578588,
    "ttft": 2164060.710283837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4979643779015626,
    "arrivals": 2212338,
    "finished_requests": 80971,
    "scheduler_time": 192.27841149023294
}
#Debug simulation 
Total elapsed time: 55.886371727101505. Arrivals time: 0.4517221786081791 Scheduler time: 55.27421785378829 Scheduler overhead time: 0.060536089818924665 Adapter cache time: 0.017266327049583197 Engine time: 0.058563658967614174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 47.54627093859017,
    "estimated_duration": 3600.013249539897,
    "input_throughput": 5379.585478602104,
    "output_throughput": 4712.019602196746,
    "total_throughput": 10091.605080798849,
    "itl": 111.70871631775263,
    "ttft": 2177108.711866609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.328185665202333,
    "arrivals": 2212338,
    "finished_requests": 79055,
    "scheduler_time": 198.33199510029505
}
#Debug simulation 
Total elapsed time: 47.546445657964796. Arrivals time: 0.5856163669377565 Scheduler time: 46.795896749943495 Scheduler overhead time: 0.06249446701258421 Adapter cache time: 0.017947109881788492 Engine time: 0.059918809682130814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 24.553585597779602,
    "estimated_duration": 3600.0505729846377,
    "input_throughput": 5066.685489609047,
    "output_throughput": 4432.0285719686435,
    "total_throughput": 9498.71406157769,
    "itl": 100.14226767260747,
    "ttft": 2205151.3351692595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1051,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.913794167139589,
    "arrivals": 2212338,
    "finished_requests": 74520,
    "scheduler_time": 208.87786814917257
}
#Debug simulation 
Total elapsed time: 24.553773424588144. Arrivals time: 0.3663446418941021 Scheduler time: 24.01853062119335 Scheduler overhead time: 0.06068352377042174 Adapter cache time: 0.023478101007640362 Engine time: 0.059154403395950794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 48.849806597922,
    "estimated_duration": 3600.1110343131472,
    "input_throughput": 5388.6893529385925,
    "output_throughput": 4718.2797525134565,
    "total_throughput": 10106.969105452048,
    "itl": 111.42105697715998,
    "ttft": 2179756.54116534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8690152422571504,
    "arrivals": 2212338,
    "finished_requests": 79198,
    "scheduler_time": 198.66315227021013
}
#Debug simulation 
Total elapsed time: 48.849966986104846. Arrivals time: 0.426454137545079 Scheduler time: 48.25765574676916 Scheduler overhead time: 0.062255936209112406 Adapter cache time: 0.01796165481209755 Engine time: 0.06053402787074447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 24.771965999156237,
    "estimated_duration": 3600.0878466024546,
    "input_throughput": 5066.806360632201,
    "output_throughput": 4432.01490626346,
    "total_throughput": 9498.82126689566,
    "itl": 100.14058209687444,
    "ttft": 2205122.5664760903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1051,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.8429600927373615,
    "arrivals": 2212338,
    "finished_requests": 74522,
    "scheduler_time": 208.8839900269803
}
#Debug simulation 
Total elapsed time: 24.77205134090036. Arrivals time: 0.5267011695541441 Scheduler time: 24.076077362988144 Scheduler overhead time: 0.06098409229889512 Adapter cache time: 0.023232548963278532 Engine time: 0.05922034103423357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 48.89307989226654,
    "estimated_duration": 3600.0776176885847,
    "input_throughput": 5389.100752904447,
    "output_throughput": 4718.733817441123,
    "total_throughput": 10107.83457034557,
    "itl": 111.41383702069903,
    "ttft": 2179707.296067438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.594147489541194,
    "arrivals": 2212338,
    "finished_requests": 79203,
    "scheduler_time": 198.67498235769168
}
#Debug simulation 
Total elapsed time: 48.89324407000095. Arrivals time: 0.3992751110345125 Scheduler time: 48.329465058166534 Scheduler overhead time: 0.0618259864859283 Adapter cache time: 0.017922102473676205 Engine time: 0.05996954347938299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_384_slots_64_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 17280, 33, 34560, 34560, 33, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 33, 33, 17280, 33, 34560, 33, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 34560, 34560, 33, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 33, 34560, 17280, 17280, 33, 33, 33, 17280, 33, 17280, 33, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 34560, 17280, 17280, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 34560, 34560, 33, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 17280, 33, 34560, 17280, 17280, 33, 34560, 34560, 34560, 33, 17280, 34560, 33, 34560, 17280, 17280, 34560, 34560, 33, 17280, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 17280, 17280, 17280, 33, 33, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 33, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 34560, 33, 17280, 17280, 33, 33, 17280, 33, 34560, 34560, 33, 34560, 34560, 33, 17280, 34560, 33, 33, 17280, 17280, 33, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 33, 33, 33, 33, 33, 33, 33, 33, 17280, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 17280, 33, 34560, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 33, 34560, 34560, 17280, 33, 34560, 17280, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 17280, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 33, 17280, 17280, 33, 34560, 33, 33, 33, 17280, 33, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 33, 17280, 17280, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 17280, 17280, 17280, 33, 17280, 33, 33, 34560, 33, 17280, 33, 33, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 6639744 . Total input tokens: 1480772512 . Total output tokens: 1303978802
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 24.642274442128837,
    "estimated_duration": 3600.0103107901577,
    "input_throughput": 5066.915487804905,
    "output_throughput": 4432.110361511141,
    "total_throughput": 9499.025849316045,
    "itl": 100.13851122259005,
    "ttft": 2205093.949813957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1051,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.765705385860079,
    "arrivals": 2212338,
    "finished_requests": 74522,
    "scheduler_time": 208.88370892156058
}
#Debug simulation 
Total elapsed time: 24.642398963216692. Arrivals time: 0.35817092284560204 Scheduler time: 24.113518701400608 Scheduler overhead time: 0.061673525255173445 Adapter cache time: 0.023424302227795124 Engine time: 0.0595361921004951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 55.814356555230916,
    "estimated_duration": 3600.1247260308824,
    "input_throughput": 5481.746189876509,
    "output_throughput": 4805.085744646394,
    "total_throughput": 10286.831934522903,
    "itl": 119.49331232643385,
    "ttft": 2165633.2693887684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 838,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.541198768774159,
    "arrivals": 2026382,
    "finished_requests": 80144,
    "scheduler_time": 191.3902504608842
}
#Debug simulation 
Total elapsed time: 55.814541360829026. Arrivals time: 0.46314550191164017 Scheduler time: 55.18450419371948 Scheduler overhead time: 0.060645009856671095 Adapter cache time: 0.023033461067825556 Engine time: 0.059143461752682924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 53.846312280744314,
    "estimated_duration": 3600.071094246512,
    "input_throughput": 5337.825975356742,
    "output_throughput": 4674.420465444203,
    "total_throughput": 10012.246440800944,
    "itl": 111.98158702710626,
    "ttft": 2178194.3222057046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 912,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.673115354441117,
    "arrivals": 2026382,
    "finished_requests": 77999,
    "scheduler_time": 197.3041661986263
}
#Debug simulation 
Total elapsed time: 53.8464952101931. Arrivals time: 0.5872363010421395 Scheduler time: 53.08663020841777 Scheduler overhead time: 0.06291981553658843 Adapter cache time: 0.023859709035605192 Engine time: 0.06079802429303527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.98431300604716,
    "estimated_duration": 3600.0576376764925,
    "input_throughput": 5039.144320952731,
    "output_throughput": 4415.747357383984,
    "total_throughput": 9454.891678336715,
    "itl": 99.9246490779637,
    "ttft": 2205067.4531838926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.544508876609552,
    "arrivals": 2026382,
    "finished_requests": 73702,
    "scheduler_time": 209.03929592871987
}
#Debug simulation 
Total elapsed time: 31.984472112264484. Arrivals time: 0.8376581971533597 Scheduler time: 30.96728114085272 Scheduler overhead time: 0.06320481281727552 Adapter cache time: 0.028226390946656466 Engine time: 0.06204758584499359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 53.73100017523393,
    "estimated_duration": 3600.0310301086115,
    "input_throughput": 5338.017044653148,
    "output_throughput": 4674.718317495244,
    "total_throughput": 10012.735362148393,
    "itl": 111.96911244087276,
    "ttft": 2178076.58833313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 913,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.258868320467872,
    "arrivals": 2026382,
    "finished_requests": 78003,
    "scheduler_time": 197.32307676017402
}
#Debug simulation 
Total elapsed time: 53.731176149100065. Arrivals time: 0.4263940183445811 Scheduler time: 53.132287053857 Scheduler overhead time: 0.06310385605320334 Adapter cache time: 0.023842854890972376 Engine time: 0.06038536876440048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 31.579212380107492,
    "estimated_duration": 3600.0887192692817,
    "input_throughput": 5039.101092953666,
    "output_throughput": 4415.80839797379,
    "total_throughput": 9454.909490927455,
    "itl": 99.92245299079876,
    "ttft": 2205033.616542406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.463265577880605,
    "arrivals": 2026382,
    "finished_requests": 73703,
    "scheduler_time": 209.0455254315535
}
#Debug simulation 
Total elapsed time: 31.579411894083023. Arrivals time: 0.376172729767859 Scheduler time: 31.02362780785188 Scheduler overhead time: 0.063731306232512 Adapter cache time: 0.028333721216768026 Engine time: 0.061213438399136066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 44.01469366531819,
    "estimated_duration": 3600.104358533053,
    "input_throughput": 5349.622144800162,
    "output_throughput": 4681.678729687651,
    "total_throughput": 10031.300874487812,
    "itl": 112.30459555456696,
    "ttft": 2180470.5770584173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.830795406410436,
    "arrivals": 2026382,
    "finished_requests": 78152,
    "scheduler_time": 196.9784915106036
}
#Debug simulation 
Total elapsed time: 44.01489182235673. Arrivals time: 0.42757847905158997 Scheduler time: 43.41676283814013 Scheduler overhead time: 0.06101131206378341 Adapter cache time: 0.02591485157608986 Engine time: 0.05894583836197853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [4320, 4320, 34560, 8640, 4320, 34560, 34560, 4320, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 4320, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 34560, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 8640, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 8640, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 34560, 4320, 8640, 34560, 4320, 4320, 8640, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 8640, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 4320, 4320, 4320, 8640, 4320, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 8640, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 8640, 8640, 8640, 4320, 8640, 4320, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 4320]
Prompts retrieved: 6082560 . Total input tokens: 1356545991 . Total output tokens: 1194658886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.735712652094662,
    "estimated_duration": 3600.1110265277302,
    "input_throughput": 5039.212364930174,
    "output_throughput": 4416.1993568665675,
    "total_throughput": 9455.411721796741,
    "itl": 99.91926262975619,
    "ttft": 2205076.394900286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.373376723229832,
    "arrivals": 2026382,
    "finished_requests": 73708,
    "scheduler_time": 209.05176786542586
}
#Debug simulation 
Total elapsed time: 31.735873288009316. Arrivals time: 0.39932256238535047 Scheduler time: 31.156409945804626 Scheduler overhead time: 0.06346791051328182 Adapter cache time: 0.028635306283831596 Engine time: 0.06172603741288185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 59.13541080476716,
    "estimated_duration": 3600.0727391620876,
    "input_throughput": 5508.168983445418,
    "output_throughput": 4814.669106945397,
    "total_throughput": 10322.838090390816,
    "itl": 119.88298556898918,
    "ttft": 2158823.4569455157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 661,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.370802370118982,
    "arrivals": 1888011,
    "finished_requests": 80334,
    "scheduler_time": 190.98364517468346
}
#Debug simulation 
Total elapsed time: 59.13560426002368. Arrivals time: 0.4407823230139911 Scheduler time: 58.53024654602632 Scheduler overhead time: 0.06078480836004019 Adapter cache time: 0.020262703765183687 Engine time: 0.05915991356596351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 52.0723943519406,
    "estimated_duration": 3600.0267913096227,
    "input_throughput": 5351.620173079711,
    "output_throughput": 4673.092444925948,
    "total_throughput": 10024.712618005658,
    "itl": 111.70715286231614,
    "ttft": 2172836.750347029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.703184404284699,
    "arrivals": 1888011,
    "finished_requests": 78052,
    "scheduler_time": 197.5594882885951
}
#Debug simulation 
Total elapsed time: 52.07258741976693. Arrivals time: 0.4138570209033787 Scheduler time: 51.49131527962163 Scheduler overhead time: 0.0625893590040505 Adapter cache time: 0.019267786294221878 Engine time: 0.060190601740032434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 39.8718393528834,
    "estimated_duration": 3600.108157841847,
    "input_throughput": 5076.1636036388245,
    "output_throughput": 4417.257288606884,
    "total_throughput": 9493.420892245707,
    "itl": 99.89423986139153,
    "ttft": 2197698.3328133603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.8940532824723295,
    "arrivals": 1888011,
    "finished_requests": 73840,
    "scheduler_time": 209.19324206926564
}
#Debug simulation 
Total elapsed time: 39.872004673816264. Arrivals time: 0.37785763619467616 Scheduler time: 39.317158141639084 Scheduler overhead time: 0.06450981739908457 Adapter cache time: 0.023546772077679634 Engine time: 0.06236830307170749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 56.16945624584332,
    "estimated_duration": 3600.0429458255912,
    "input_throughput": 5364.920999732899,
    "output_throughput": 4684.847445932963,
    "total_throughput": 10049.76844566586,
    "itl": 112.44035041434246,
    "ttft": 2168006.3543671747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.399721195055165,
    "arrivals": 1888011,
    "finished_requests": 78123,
    "scheduler_time": 197.00788243016345
}
#Debug simulation 
Total elapsed time: 56.16965494398028. Arrivals time: 0.9005288402549922 Scheduler time: 55.09906618250534 Scheduler overhead time: 0.06305983476340771 Adapter cache time: 0.019852017983794212 Engine time: 0.06178895849734545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 39.826306726783514,
    "estimated_duration": 3600.0420154821595,
    "input_throughput": 5076.256866283388,
    "output_throughput": 4417.338445387599,
    "total_throughput": 9493.595311670986,
    "itl": 99.89253028492773,
    "ttft": 2197670.6592178615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.828190020308854,
    "arrivals": 1888011,
    "finished_requests": 73840,
    "scheduler_time": 209.1929629717418
}
#Debug simulation 
Total elapsed time: 39.826468128710985. Arrivals time: 0.3781158970668912 Scheduler time: 39.27145128697157 Scheduler overhead time: 0.06422215327620506 Adapter cache time: 0.02363939117640257 Engine time: 0.062496100552380085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.05171074671671,
    "estimated_duration": 3600.102067322405,
    "input_throughput": 5365.481766567409,
    "output_throughput": 4685.336327851791,
    "total_throughput": 10050.8180944192,
    "itl": 112.43194876739001,
    "ttft": 2168092.9474241263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.098477243846263,
    "arrivals": 1888011,
    "finished_requests": 78131,
    "scheduler_time": 197.0267213610601
}
#Debug simulation 
Total elapsed time: 56.05190644506365. Arrivals time: 0.42964116064831614 Scheduler time: 55.45143899973482 Scheduler overhead time: 0.06392732635140419 Adapter cache time: 0.020235737785696983 Engine time: 0.061710698530077934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 8640, 1080, 34560, 34560, 1080, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 1080, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 8640, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 8640, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 8640, 34560, 1080, 1080, 8640, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 8640, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 1080, 1080, 1080, 8640, 1080, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 8640, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5667840 . Total input tokens: 1264219555 . Total output tokens: 1113108685
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 40.34958763234317,
    "estimated_duration": 3600.089072974904,
    "input_throughput": 5076.29939969749,
    "output_throughput": 4417.399313639387,
    "total_throughput": 9493.698713336878,
    "itl": 99.8910218911981,
    "ttft": 2197656.3062071246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.764605047088141,
    "arrivals": 1888011,
    "finished_requests": 73841,
    "scheduler_time": 209.1992105625141
}
#Debug simulation 
Total elapsed time: 40.349742722231895. Arrivals time: 0.8660209747031331 Scheduler time: 39.30732965609059 Scheduler overhead time: 0.06393571384251118 Adapter cache time: 0.023675781209021807 Engine time: 0.0626770961098373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 57.69724304927513,
    "estimated_duration": 3600.08946959161,
    "input_throughput": 5540.936459632728,
    "output_throughput": 4810.743217991374,
    "total_throughput": 10351.6796776241,
    "itl": 119.34004263211551,
    "ttft": 2161151.3600370707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 632,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.179042508192428,
    "arrivals": 1864865,
    "finished_requests": 80615,
    "scheduler_time": 191.20288522477162
}
#Debug simulation 
Total elapsed time: 57.697444163262844. Arrivals time: 0.569741306360811 Scheduler time: 56.96064664097503 Scheduler overhead time: 0.06254730513319373 Adapter cache time: 0.019528468139469624 Engine time: 0.06064248736947775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 47.26703598210588,
    "estimated_duration": 3600.087632328191,
    "input_throughput": 5371.716739987693,
    "output_throughput": 4671.6540589106435,
    "total_throughput": 10043.370798898335,
    "itl": 111.5837034747639,
    "ttft": 2174309.0923715374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 664,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.83724674403668,
    "arrivals": 1864865,
    "finished_requests": 78276,
    "scheduler_time": 197.56286256987138
}
#Debug simulation 
Total elapsed time: 47.26720942975953. Arrivals time: 0.40361858531832695 Scheduler time: 46.69766100263223 Scheduler overhead time: 0.06182541744783521 Adapter cache time: 0.01939509017392993 Engine time: 0.05983481137081981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.98481067409739,
    "estimated_duration": 3600.014392115561,
    "input_throughput": 5072.653887160849,
    "output_throughput": 4416.627898716914,
    "total_throughput": 9489.281785877763,
    "itl": 99.79075608680434,
    "ttft": 2196765.704566497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.437009004624551,
    "arrivals": 1864865,
    "finished_requests": 73964,
    "scheduler_time": 208.928397951109
}
#Debug simulation 
Total elapsed time: 31.98491876712069. Arrivals time: 0.3461934463120997 Scheduler time: 31.464789119083434 Scheduler overhead time: 0.06239268369972706 Adapter cache time: 0.02524762786924839 Engine time: 0.06045777862891555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 45.826071007177234,
    "estimated_duration": 3600.0868326498166,
    "input_throughput": 5381.514919110985,
    "output_throughput": 4675.218066229678,
    "total_throughput": 10056.732985340663,
    "itl": 111.69767137711877,
    "ttft": 2174779.0365461935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 700,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.786647261288009,
    "arrivals": 1864865,
    "finished_requests": 78401,
    "scheduler_time": 197.42342085590883
}
#Debug simulation 
Total elapsed time: 45.82624781411141. Arrivals time: 0.3801399525254965 Scheduler time: 45.281815715134144 Scheduler overhead time: 0.06102977693080902 Adapter cache time: 0.019941559992730618 Engine time: 0.05883198417723179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 31.77552510984242,
    "estimated_duration": 3600.0465420648106,
    "input_throughput": 5072.634141425841,
    "output_throughput": 4416.677344088001,
    "total_throughput": 9489.311485513843,
    "itl": 99.78893575182943,
    "ttft": 2196749.3884142376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.36244682104326,
    "arrivals": 1864865,
    "finished_requests": 73965,
    "scheduler_time": 208.93443699503305
}
#Debug simulation 
Total elapsed time: 31.77570902975276. Arrivals time: 0.35799934761598706 Scheduler time: 31.242577417287976 Scheduler overhead time: 0.06254039239138365 Adapter cache time: 0.02583843469619751 Engine time: 0.06074842903763056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 47.40610359190032,
    "estimated_duration": 3600.1181366887427,
    "input_throughput": 5375.07000195228,
    "output_throughput": 4672.500279524675,
    "total_throughput": 10047.570281476956,
    "itl": 111.56571733793513,
    "ttft": 2174373.3355400106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.31553055582566,
    "arrivals": 1864865,
    "finished_requests": 78326,
    "scheduler_time": 197.55850376778216
}
#Debug simulation 
Total elapsed time: 47.406309816055. Arrivals time: 0.8971785991452634 Scheduler time: 46.34410036448389 Scheduler overhead time: 0.061231321189552546 Adapter cache time: 0.019680072087794542 Engine time: 0.0593894487246871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 8640, 540, 34560, 34560, 540, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 540, 540, 8640, 540, 34560, 540, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 34560, 34560, 540, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 540, 34560, 8640, 8640, 540, 540, 540, 8640, 540, 8640, 540, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 34560, 8640, 8640, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 540, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 8640, 540, 34560, 8640, 8640, 540, 34560, 34560, 34560, 540, 8640, 34560, 540, 34560, 8640, 8640, 34560, 34560, 540, 8640, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 8640, 8640, 8640, 540, 540, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 540, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 34560, 540, 8640, 8640, 540, 540, 8640, 540, 34560, 34560, 540, 34560, 34560, 540, 8640, 34560, 540, 540, 8640, 8640, 540, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 540, 540, 540, 540, 540, 540, 540, 540, 8640, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 8640, 540, 34560, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 540, 34560, 34560, 8640, 540, 34560, 8640, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 8640, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 34560, 540, 540, 540, 8640, 540, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 540, 8640, 8640, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 8640, 8640, 8640, 540, 8640, 540, 540, 34560, 540, 8640, 540, 540, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5598720 . Total input tokens: 1248854355 . Total output tokens: 1099522211
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.989072627853602,
    "estimated_duration": 3600.0748783201902,
    "input_throughput": 5072.5942146295,
    "output_throughput": 4416.64258033964,
    "total_throughput": 9489.23679496914,
    "itl": 99.78683183118339,
    "ttft": 2196691.8970570485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.284156528282908,
    "arrivals": 1864865,
    "finished_requests": 73965,
    "scheduler_time": 208.94053131834477
}
#Debug simulation 
Total elapsed time: 31.989241214003414. Arrivals time: 0.5204990492202342 Scheduler time: 31.292994163930416 Scheduler overhead time: 0.06293106311932206 Adapter cache time: 0.025969502981752157 Engine time: 0.060708776116371155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.213528466876596,
    "estimated_duration": 3600.045217367908,
    "input_throughput": 5527.919456120052,
    "output_throughput": 4804.3413223141315,
    "total_throughput": 10332.260778434184,
    "itl": 119.11156396639352,
    "ttft": 2158651.057821677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 707,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.674973185588689,
    "arrivals": 1853406,
    "finished_requests": 80479,
    "scheduler_time": 191.50765220492124
}
#Debug simulation 
Total elapsed time: 56.213713753037155. Arrivals time: 0.9026257675141096 Scheduler time: 55.14970199530944 Scheduler overhead time: 0.059897169936448336 Adapter cache time: 0.01918637566268444 Engine time: 0.058490476571023464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 49.82122836029157,
    "estimated_duration": 3600.1238985664595,
    "input_throughput": 5375.841650257228,
    "output_throughput": 4670.513147254563,
    "total_throughput": 10046.35479751179,
    "itl": 111.84592042925301,
    "ttft": 2172187.026378915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 732,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.339376195687808,
    "arrivals": 1853406,
    "finished_requests": 78231,
    "scheduler_time": 197.5938843424424
}
#Debug simulation 
Total elapsed time: 49.82141707511619. Arrivals time: 0.39606948336586356 Scheduler time: 49.26037978567183 Scheduler overhead time: 0.06073988089337945 Adapter cache time: 0.020118327345699072 Engine time: 0.0596258407458663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.299315511714667,
    "estimated_duration": 3600.020629487971,
    "input_throughput": 5079.034783920284,
    "output_throughput": 4420.138004115615,
    "total_throughput": 9499.1727880359,
    "itl": 99.84874957242931,
    "ttft": 2197985.7229776024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.35607714575717,
    "arrivals": 1853406,
    "finished_requests": 73994,
    "scheduler_time": 209.07959890295845
}
#Debug simulation 
Total elapsed time: 31.299437943845987. Arrivals time: 0.3592658741399646 Scheduler time: 30.76775938551873 Scheduler overhead time: 0.06266788858920336 Adapter cache time: 0.023171634413301945 Engine time: 0.0602442785166204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 49.70121330814436,
    "estimated_duration": 3600.059015203758,
    "input_throughput": 5376.0310367869315,
    "output_throughput": 4670.725376719499,
    "total_throughput": 10046.756413506431,
    "itl": 111.83640670144547,
    "ttft": 2172108.1047109384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 732,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.022861813772463,
    "arrivals": 1853406,
    "finished_requests": 78233,
    "scheduler_time": 197.60611636839798
}
#Debug simulation 
Total elapsed time: 49.701381182298064. Arrivals time: 0.38098150165751576 Scheduler time: 49.15324289072305 Scheduler overhead time: 0.06219663517549634 Adapter cache time: 0.02035545464605093 Engine time: 0.05976733285933733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 31.4016414671205,
    "estimated_duration": 3600.0663357645503,
    "input_throughput": 5078.987244860547,
    "output_throughput": 4420.1521627296825,
    "total_throughput": 9499.13940759023,
    "itl": 99.8471376973241,
    "ttft": 2198084.5745249595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.292492172536457,
    "arrivals": 1853406,
    "finished_requests": 73995,
    "scheduler_time": 209.08577066268523
}
#Debug simulation 
Total elapsed time: 31.401811860036105. Arrivals time: 0.3463195748627186 Scheduler time: 30.883108660113066 Scheduler overhead time: 0.06251194048672915 Adapter cache time: 0.023472853936254978 Engine time: 0.06031726719811559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 50.02491504885256,
    "estimated_duration": 3600.0866590748365,
    "input_throughput": 5376.160029707125,
    "output_throughput": 4670.955060932171,
    "total_throughput": 10047.115090639298,
    "itl": 111.82685967536325,
    "ttft": 2171956.9993408117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 732,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.673030128497608,
    "arrivals": 1853406,
    "finished_requests": 78237,
    "scheduler_time": 197.62534369161787
}
#Debug simulation 
Total elapsed time: 50.02509292727336. Arrivals time: 0.5415447596460581 Scheduler time: 49.317661909386516 Scheduler overhead time: 0.061435017734766006 Adapter cache time: 0.020289696753025055 Engine time: 0.05933963833376765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 8640, 270, 34560, 34560, 270, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 270, 270, 8640, 270, 34560, 270, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 34560, 34560, 270, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 270, 34560, 8640, 8640, 270, 270, 270, 8640, 270, 8640, 270, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 34560, 8640, 8640, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 270, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 8640, 270, 34560, 8640, 8640, 270, 34560, 34560, 34560, 270, 8640, 34560, 270, 34560, 8640, 8640, 34560, 34560, 270, 8640, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 8640, 8640, 8640, 270, 270, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 270, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 34560, 270, 8640, 8640, 270, 270, 8640, 270, 34560, 34560, 270, 34560, 34560, 270, 8640, 34560, 270, 270, 8640, 8640, 270, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 270, 270, 270, 270, 270, 270, 270, 270, 8640, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 8640, 270, 34560, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 270, 34560, 34560, 8640, 270, 34560, 8640, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 8640, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 34560, 270, 270, 270, 8640, 270, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 270, 8640, 8640, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 8640, 8640, 8640, 270, 8640, 270, 270, 34560, 270, 8640, 270, 270, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5564160 . Total input tokens: 1241213153 . Total output tokens: 1092778456
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 31.87440386088565,
    "estimated_duration": 3600.106107593006,
    "input_throughput": 5079.093908214097,
    "output_throughput": 4420.390545277526,
    "total_throughput": 9499.484453491623,
    "itl": 99.84557503737018,
    "ttft": 2198050.695705157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.223107918370532,
    "arrivals": 1853406,
    "finished_requests": 73998,
    "scheduler_time": 209.09194896469123
}
#Debug simulation 
Total elapsed time: 31.87458900688216. Arrivals time: 0.8528542825952172 Scheduler time: 30.849740975070745 Scheduler overhead time: 0.06234153360128403 Adapter cache time: 0.02324390085414052 Engine time: 0.060526704881340265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 52.46311276499182,
    "estimated_duration": 3600.0861487887787,
    "input_throughput": 5543.721504196413,
    "output_throughput": 4802.2770249030345,
    "total_throughput": 10345.998529099448,
    "itl": 119.19160380988632,
    "ttft": 2156617.5804426693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 680,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.496438141726035,
    "arrivals": 1847695,
    "finished_requests": 80796,
    "scheduler_time": 191.464253503065
}
#Debug simulation 
Total elapsed time: 52.46328629786149. Arrivals time: 0.5462142284959555 Scheduler time: 51.75867307605222 Scheduler overhead time: 0.05895087029784918 Adapter cache time: 0.01886415435001254 Engine time: 0.05728406831622124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 49.63357648579404,
    "estimated_duration": 3600.0862595760323,
    "input_throughput": 5398.546478797097,
    "output_throughput": 4663.753807381627,
    "total_throughput": 10062.300286178724,
    "itl": 111.38294635174341,
    "ttft": 2165070.9768194226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.728257144503304,
    "arrivals": 1847695,
    "finished_requests": 78316,
    "scheduler_time": 197.824251403297
}
#Debug simulation 
Total elapsed time: 49.63375696679577. Arrivals time: 0.39734632009640336 Scheduler time: 49.067995010875165 Scheduler overhead time: 0.06203192984685302 Adapter cache time: 0.021181177347898483 Engine time: 0.0602322556078434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 30.408905375748873,
    "estimated_duration": 3600.054610969434,
    "input_throughput": 5097.918777142634,
    "output_throughput": 4418.941299258826,
    "total_throughput": 9516.86007640146,
    "itl": 99.74690777860454,
    "ttft": 2195832.0760141364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1034,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.788462564549431,
    "arrivals": 1847695,
    "finished_requests": 74222,
    "scheduler_time": 209.02093479414845
}
#Debug simulation 
Total elapsed time: 30.409036035183817. Arrivals time: 0.3689360418356955 Scheduler time: 29.86513333907351 Scheduler overhead time: 0.06321152253076434 Adapter cache time: 0.024288843385875225 Engine time: 0.061297915410250425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 47.039934845175594,
    "estimated_duration": 3600.0433464554344,
    "input_throughput": 5396.106416087143,
    "output_throughput": 4678.66699899151,
    "total_throughput": 10074.773415078653,
    "itl": 111.87525344084143,
    "ttft": 2167238.8784581088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 663,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5379481977270855,
    "arrivals": 1847695,
    "finished_requests": 78594,
    "scheduler_time": 197.26803420146524
}
#Debug simulation 
Total elapsed time: 47.04012771695852. Arrivals time: 0.3991190097294748 Scheduler time: 46.476792162284255 Scheduler overhead time: 0.0611900226213038 Adapter cache time: 0.019092226400971413 Engine time: 0.05969555163756013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 30.248078786302358,
    "estimated_duration": 3600.094479039276,
    "input_throughput": 5097.862321907073,
    "output_throughput": 4418.892363137463,
    "total_throughput": 9516.754685044536,
    "itl": 99.74489823506256,
    "ttft": 2195805.0804443564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1034,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.714521732497984,
    "arrivals": 1847695,
    "finished_requests": 74222,
    "scheduler_time": 209.02723121278058
}
#Debug simulation 
Total elapsed time: 30.248190869111568. Arrivals time: 0.34933982929214835 Scheduler time: 29.726273592095822 Scheduler overhead time: 0.06239786744117737 Adapter cache time: 0.023696559946984053 Engine time: 0.06058572232723236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 46.87813013698906,
    "estimated_duration": 3600.097115165469,
    "input_throughput": 5396.432201276064,
    "output_throughput": 4678.991555266912,
    "total_throughput": 10075.423756542976,
    "itl": 111.86717164718931,
    "ttft": 2167068.247150324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 663,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.232539583598244,
    "arrivals": 1847695,
    "finished_requests": 78599,
    "scheduler_time": 197.28625184633333
}
#Debug simulation 
Total elapsed time: 46.87831523967907. Arrivals time: 0.3789448873139918 Scheduler time: 46.336029927711934 Scheduler overhead time: 0.060441064182668924 Adapter cache time: 0.018855230882763863 Engine time: 0.059922977816313505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [128 128 128]
Adapter prompts. [135, 135, 34560, 8640, 135, 34560, 34560, 135, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 135, 135, 8640, 135, 34560, 135, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 34560, 34560, 135, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 135, 34560, 8640, 8640, 135, 135, 135, 8640, 135, 8640, 135, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 34560, 8640, 8640, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 135, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 8640, 135, 34560, 8640, 8640, 135, 34560, 34560, 34560, 135, 8640, 34560, 135, 34560, 8640, 8640, 34560, 34560, 135, 8640, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 8640, 8640, 8640, 135, 135, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 135, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 34560, 135, 8640, 8640, 135, 135, 8640, 135, 34560, 34560, 135, 34560, 34560, 135, 8640, 34560, 135, 135, 8640, 8640, 135, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 135, 135, 135, 135, 135, 135, 135, 135, 8640, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 135, 135, 8640, 135, 34560, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 135, 34560, 34560, 8640, 135, 34560, 8640, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 34560, 135, 8640, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 34560, 135, 135, 135, 8640, 135, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 135, 8640, 8640, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 8640, 8640, 8640, 135, 8640, 135, 135, 34560, 135, 8640, 135, 135, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 34560, 135, 135, 135]
Prompts retrieved: 5546880 . Total input tokens: 1237372107 . Total output tokens: 1089400593
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 30.467686163261533,
    "estimated_duration": 3600.0220873872368,
    "input_throughput": 5097.964833132392,
    "output_throughput": 4418.981221180716,
    "total_throughput": 9516.946054313108,
    "itl": 99.7429550962546,
    "ttft": 2195779.1843671156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1034,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.642444955036067,
    "arrivals": 1847695,
    "finished_requests": 74222,
    "scheduler_time": 209.0269163382033
}
#Debug simulation 
Total elapsed time: 30.467873070389032. Arrivals time: 0.4879221199080348 Scheduler time: 29.80746258981526 Scheduler overhead time: 0.06204653391614556 Adapter cache time: 0.02401391603052616 Engine time: 0.06049767229706049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 48.59405464679003,
    "estimated_duration": 3600.0215144651047,
    "input_throughput": 5551.812654366772,
    "output_throughput": 4819.787306903937,
    "total_throughput": 10371.59996127071,
    "itl": 119.32564735335595,
    "ttft": 2164217.0870600506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.007119873361724,
    "arrivals": 1844823,
    "finished_requests": 80572,
    "scheduler_time": 191.3902245367549
}
#Debug simulation 
Total elapsed time: 48.59423693874851. Arrivals time: 0.5543079385533929 Scheduler time: 47.87955321185291 Scheduler overhead time: 0.0600347314029932 Adapter cache time: 0.018069314304739237 Engine time: 0.05851248372346163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 46.28048597369343,
    "estimated_duration": 3600.090637954013,
    "input_throughput": 5383.676954037728,
    "output_throughput": 4678.394433305694,
    "total_throughput": 10062.071387343422,
    "itl": 111.85101458634301,
    "ttft": 2175731.7418805566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.698745888243436,
    "arrivals": 1844823,
    "finished_requests": 78183,
    "scheduler_time": 197.27497317874398
}
#Debug simulation 
Total elapsed time: 46.280687214806676. Arrivals time: 0.8837047526612878 Scheduler time: 45.23235339485109 Scheduler overhead time: 0.06145541416481137 Adapter cache time: 0.019216561689972878 Engine time: 0.05942496610805392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.883310641162097,
    "estimated_duration": 3600.0554540080293,
    "input_throughput": 5087.6919075255855,
    "output_throughput": 4416.324749192194,
    "total_throughput": 9504.01665671778,
    "itl": 99.75875737894823,
    "ttft": 2199934.9427693137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 957,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.185401785117591,
    "arrivals": 1844823,
    "finished_requests": 73807,
    "scheduler_time": 209.1014364618805
}
#Debug simulation 
Total elapsed time: 27.883442123886198. Arrivals time: 0.35906142415478826 Scheduler time: 27.353058309759945 Scheduler overhead time: 0.06190425390377641 Adapter cache time: 0.02326094964519143 Engine time: 0.06031481269747019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 45.76548727508634,
    "estimated_duration": 3600.052379389245,
    "input_throughput": 5383.956942117681,
    "output_throughput": 4678.521094978466,
    "total_throughput": 10062.478037096147,
    "itl": 111.84217902715763,
    "ttft": 2175720.8900509872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.414160588714288,
    "arrivals": 1844823,
    "finished_requests": 78185,
    "scheduler_time": 197.28756929855598
}
#Debug simulation 
Total elapsed time: 45.76566827436909. Arrivals time: 0.3804126768372953 Scheduler time: 45.21982514439151 Scheduler overhead time: 0.061578980181366205 Adapter cache time: 0.01932585285976529 Engine time: 0.05964335473254323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 27.99340320099145,
    "estimated_duration": 3600.102502946672,
    "input_throughput": 5087.625973151718,
    "output_throughput": 4416.479804946132,
    "total_throughput": 9504.105778097852,
    "itl": 99.75734500258166,
    "ttft": 2199905.150879167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 957,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.12015987448396,
    "arrivals": 1844823,
    "finished_requests": 73809,
    "scheduler_time": 209.10777363193003
}
#Debug simulation 
Total elapsed time: 27.99358766619116. Arrivals time: 0.3407705929130316 Scheduler time: 27.481734809465706 Scheduler overhead time: 0.062163660768419504 Adapter cache time: 0.022948276717215776 Engine time: 0.0600806069560349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 45.725373969879,
    "estimated_duration": 3600.1018350260733,
    "input_throughput": 5384.315746684875,
    "output_throughput": 4678.9501441639095,
    "total_throughput": 10063.265890848785,
    "itl": 111.83422970727965,
    "ttft": 2175753.0496878196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.092093322905693,
    "arrivals": 1844823,
    "finished_requests": 78192,
    "scheduler_time": 197.30622892745217
}
#Debug simulation 
Total elapsed time: 45.725566952023655. Arrivals time: 0.4063045014627278 Scheduler time: 45.153935274574906 Scheduler overhead time: 0.06160452915355563 Adapter cache time: 0.019118154887109995 Engine time: 0.059963323175907135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [128 128 128]
Adapter prompts. [66, 66, 34560, 8640, 66, 34560, 34560, 66, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 66, 66, 8640, 66, 34560, 66, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 34560, 34560, 66, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 66, 34560, 8640, 8640, 66, 66, 66, 8640, 66, 8640, 66, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 34560, 8640, 8640, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 66, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 8640, 66, 34560, 8640, 8640, 66, 34560, 34560, 34560, 66, 8640, 34560, 66, 34560, 8640, 8640, 34560, 34560, 66, 8640, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 8640, 8640, 8640, 66, 66, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 66, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 34560, 66, 8640, 8640, 66, 66, 8640, 66, 34560, 34560, 66, 34560, 34560, 66, 8640, 34560, 66, 66, 8640, 8640, 66, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 66, 66, 66, 66, 66, 66, 66, 66, 8640, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 66, 66, 8640, 66, 34560, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 66, 34560, 34560, 8640, 66, 34560, 8640, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 34560, 66, 8640, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 34560, 66, 66, 66, 8640, 66, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 66, 8640, 8640, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 8640, 8640, 8640, 66, 8640, 66, 66, 34560, 66, 8640, 66, 66, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 34560, 66, 66, 66]
Prompts retrieved: 5538048 . Total input tokens: 1235433857 . Total output tokens: 1087643923
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 27.942336870823056,
    "estimated_duration": 3600.0350844798395,
    "input_throughput": 5087.721249985105,
    "output_throughput": 4416.562513111541,
    "total_throughput": 9504.283763096646,
    "itl": 99.75563181638522,
    "ttft": 2199877.2484698277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 957,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.053053909260797,
    "arrivals": 1844823,
    "finished_requests": 73809,
    "scheduler_time": 209.10746113032056
}
#Debug simulation 
Total elapsed time: 27.94246799359098. Arrivals time: 0.3600674094632268 Scheduler time: 27.408882890362293 Scheduler overhead time: 0.06429165741428733 Adapter cache time: 0.02297627506777644 Engine time: 0.06012136256322265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 53.03206719504669,
    "estimated_duration": 3600.024319907485,
    "input_throughput": 5510.754716376424,
    "output_throughput": 4811.052498791201,
    "total_throughput": 10321.807215167626,
    "itl": 119.81873102347294,
    "ttft": 2162243.8644911516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 630,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.165817690128527,
    "arrivals": 1843393,
    "finished_requests": 80443,
    "scheduler_time": 191.0696218564364
}
#Debug simulation 
Total elapsed time: 53.03224758896977. Arrivals time: 0.4038431332446635 Scheduler time: 52.46939507778734 Scheduler overhead time: 0.05940198479220271 Adapter cache time: 0.018321453128010035 Engine time: 0.05783904390409589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 50.26245579728857,
    "estimated_duration": 3600.0516634571536,
    "input_throughput": 5370.157933076588,
    "output_throughput": 4687.331621178784,
    "total_throughput": 10057.489554255373,
    "itl": 111.72944215869799,
    "ttft": 2176140.657684096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.354278532890609,
    "arrivals": 1843393,
    "finished_requests": 78339,
    "scheduler_time": 197.7443578605981
}
#Debug simulation 
Total elapsed time: 50.262630285229534. Arrivals time: 0.3927489388734102 Scheduler time: 49.70627348590642 Scheduler overhead time: 0.06156766368076205 Adapter cache time: 0.01780171925202012 Engine time: 0.059660814236849546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 40.40311373444274,
    "estimated_duration": 3600.025770146469,
    "input_throughput": 5050.038294381522,
    "output_throughput": 4421.078907819166,
    "total_throughput": 9471.117202200689,
    "itl": 99.56375307639979,
    "ttft": 2195545.234197057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 804,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.033494072281777,
    "arrivals": 1843393,
    "finished_requests": 73812,
    "scheduler_time": 209.60347709825214
}
#Debug simulation 
Total elapsed time: 40.403240769170225. Arrivals time: 0.36955658067017794 Scheduler time: 39.860004047863185 Scheduler overhead time: 0.06402549194172025 Adapter cache time: 0.0210113194771111 Engine time: 0.06223873142153025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 50.06456159008667,
    "estimated_duration": 3600.0399087226433,
    "input_throughput": 5370.434075787776,
    "output_throughput": 4687.873586931453,
    "total_throughput": 10058.30766271923,
    "itl": 111.72250437913443,
    "ttft": 2176189.1129885097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.09745765282772,
    "arrivals": 1843393,
    "finished_requests": 78343,
    "scheduler_time": 197.75654364794966
}
#Debug simulation 
Total elapsed time: 50.064708614721894. Arrivals time: 0.3921862794086337 Scheduler time: 49.50698111904785 Scheduler overhead time: 0.06235950440168381 Adapter cache time: 0.01826708670705557 Engine time: 0.060249592643231153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 40.08811164973304,
    "estimated_duration": 3600.082519165738,
    "input_throughput": 5050.135351956636,
    "output_throughput": 4421.263655836558,
    "total_throughput": 9471.399007793196,
    "itl": 99.56280434295121,
    "ttft": 2195661.300749955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 804,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.981507660951489,
    "arrivals": 1843393,
    "finished_requests": 73815,
    "scheduler_time": 209.60965987660882
}
#Debug simulation 
Total elapsed time: 40.0882458887063. Arrivals time: 0.35614656936377287 Scheduler time: 39.55890859244391 Scheduler overhead time: 0.06369206169620156 Adapter cache time: 0.0211258539929986 Engine time: 0.06214355630800128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 50.73050017794594,
    "estimated_duration": 3600.0330750449066,
    "input_throughput": 5362.164068384067,
    "output_throughput": 4689.360805327995,
    "total_throughput": 10051.524873712062,
    "itl": 112.21520746054668,
    "ttft": 2172838.34121808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.970798825034853,
    "arrivals": 1843393,
    "finished_requests": 78387,
    "scheduler_time": 197.24227726875512
}
#Debug simulation 
Total elapsed time: 50.73069304926321. Arrivals time: 0.5243489961139858 Scheduler time: 50.042153374291956 Scheduler overhead time: 0.06111819576472044 Adapter cache time: 0.0182958971709013 Engine time: 0.06006581895053387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_384_slots_64_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [128 128 128]
Adapter prompts. [33, 33, 34560, 8640, 33, 34560, 34560, 33, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 33, 33, 8640, 33, 34560, 33, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 34560, 34560, 33, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 33, 34560, 8640, 8640, 33, 33, 33, 8640, 33, 8640, 33, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 34560, 8640, 8640, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 34560, 34560, 33, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 8640, 33, 34560, 8640, 8640, 33, 34560, 34560, 34560, 33, 8640, 34560, 33, 34560, 8640, 8640, 34560, 34560, 33, 8640, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 8640, 8640, 8640, 33, 33, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 33, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 34560, 33, 8640, 8640, 33, 33, 8640, 33, 34560, 34560, 33, 34560, 34560, 33, 8640, 34560, 33, 33, 8640, 8640, 33, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 33, 33, 33, 33, 33, 33, 33, 33, 8640, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 33, 33, 8640, 33, 34560, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 33, 34560, 34560, 8640, 33, 34560, 8640, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 34560, 33, 33, 34560, 33, 8640, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 33, 8640, 8640, 33, 34560, 33, 33, 33, 8640, 33, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 33, 8640, 8640, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 8640, 8640, 8640, 33, 8640, 33, 33, 34560, 33, 8640, 33, 33, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 34560, 33, 33, 33]
Prompts retrieved: 5533824 . Total input tokens: 1234516864 . Total output tokens: 1086797679
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 40.39830555114895,
    "estimated_duration": 3600.0233950603774,
    "input_throughput": 5050.218291621708,
    "output_throughput": 4421.336267380854,
    "total_throughput": 9471.554559002563,
    "itl": 99.56124568451078,
    "ttft": 2195639.4922599792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 804,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.922686382792914,
    "arrivals": 1843393,
    "finished_requests": 73815,
    "scheduler_time": 209.60935704940664
}
#Debug simulation 
Total elapsed time: 40.398480750154704. Arrivals time: 0.352756941691041 Scheduler time: 39.87355424417183 Scheduler overhead time: 0.06384474644437432 Adapter cache time: 0.021014372818171978 Engine time: 0.061273842584341764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 56.97224207269028,
    "estimated_duration": 3600.0117141604055,
    "input_throughput": 5514.918443711313,
    "output_throughput": 4790.860799746698,
    "total_throughput": 10305.779243458011,
    "itl": 118.51345663725408,
    "ttft": 2159381.11548986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7690731482115187,
    "arrivals": 1703822,
    "finished_requests": 80142,
    "scheduler_time": 192.24128063486543
}
#Debug simulation 
Total elapsed time: 56.972430374938995. Arrivals time: 0.4217478451319039 Scheduler time: 56.38804739154875 Scheduler overhead time: 0.06174752442166209 Adapter cache time: 0.01796872913837433 Engine time: 0.05906782764941454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 54.063034364022315,
    "estimated_duration": 3600.0609923429656,
    "input_throughput": 5363.900511983427,
    "output_throughput": 4663.305437243172,
    "total_throughput": 10027.2059492266,
    "itl": 111.40796290821292,
    "ttft": 2172593.2954508704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.569669770775369,
    "arrivals": 1703822,
    "finished_requests": 77959,
    "scheduler_time": 197.8703794108211
}
#Debug simulation 
Total elapsed time: 54.063224700279534. Arrivals time: 0.3900223602540791 Scheduler time: 53.50408452702686 Scheduler overhead time: 0.06327031413093209 Adapter cache time: 0.019638186786323786 Engine time: 0.061067798640578985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 36.72419862309471,
    "estimated_duration": 3600.017138692702,
    "input_throughput": 5074.70445172218,
    "output_throughput": 4413.462877504442,
    "total_throughput": 9488.167329226622,
    "itl": 99.56325753020481,
    "ttft": 2197528.7606589994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 949,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.117375585339052,
    "arrivals": 1703822,
    "finished_requests": 73750,
    "scheduler_time": 209.26949188068588
}
#Debug simulation 
Total elapsed time: 36.72437045816332. Arrivals time: 0.3414315143600106 Scheduler time: 36.20573780545965 Scheduler overhead time: 0.06480242218822241 Adapter cache time: 0.023353546857833862 Engine time: 0.06236069509759545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 54.27060275711119,
    "estimated_duration": 3600.0142563822105,
    "input_throughput": 5364.297090147331,
    "output_throughput": 4663.602920526191,
    "total_throughput": 10027.900010673522,
    "itl": 111.40038502097087,
    "ttft": 2172489.2266224003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.287860913192847,
    "arrivals": 1703822,
    "finished_requests": 77963,
    "scheduler_time": 197.8822081925678
}
#Debug simulation 
Total elapsed time: 54.27077584993094. Arrivals time: 0.4051074432209134 Scheduler time: 53.69645890686661 Scheduler overhead time: 0.06339583871886134 Adapter cache time: 0.01957628969103098 Engine time: 0.06114565534517169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_384_slots_64_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 36.78865890391171,
    "estimated_duration": 3600.0614773499683,
    "input_throughput": 5074.87139176539,
    "output_throughput": 4413.452409067133,
    "total_throughput": 9488.323800832522,
    "itl": 99.55999964224416,
    "ttft": 2197627.987162268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 949,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.052133674705421,
    "arrivals": 1703822,
    "finished_requests": 73755,
    "scheduler_time": 209.2756695395969
}
#Debug simulation 
Total elapsed time: 36.788852935191244. Arrivals time: 0.3659967668354511 Scheduler time: 36.244760585483164 Scheduler overhead time: 0.06494050147011876 Adapter cache time: 0.024226362816989422 Engine time: 0.06225695367902517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_384_slots_64_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_384_slots_64_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 54.32233283901587,
    "estimated_duration": 3600.075106047097,
    "input_throughput": 5364.60141277601,
    "output_throughput": 4663.808811043274,
    "total_throughput": 10028.410223819285,
    "itl": 111.39200118798297,
    "ttft": 2172399.5165470853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.996334508797135,
    "arrivals": 1703822,
    "finished_requests": 77971,
    "scheduler_time": 197.90071005543876
}
#Debug simulation 
Total elapsed time: 54.32250672020018. Arrivals time: 0.4010730921290815 Scheduler time: 53.75396979600191 Scheduler overhead time: 0.06239253794774413 Adapter cache time: 0.019188234582543373 Engine time: 0.060686520766466856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_384_slots_64_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_384_slots_64_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [128 128 128]
Adapter prompts. [1080, 1080, 34560, 4320, 1080, 34560, 34560, 1080, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 1080, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 34560, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 4320, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 4320, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 34560, 1080, 4320, 34560, 1080, 1080, 4320, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 4320, 1080, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 4320, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 1080, 1080, 1080, 4320, 1080, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 4320, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 4320, 4320, 4320, 1080, 4320, 1080, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 1080]
Prompts retrieved: 5114880 . Total input tokens: 1140924615 . Total output tokens: 1004347701
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 36.7936310400255,
    "estimated_duration": 3600.1053047576333,
    "input_throughput": 5074.809610667754,
    "output_throughput": 4413.398680033794,
    "total_throughput": 9488.208290701548,
    "itl": 99.55828363398734,
    "ttft": 2197621.162395295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 949,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.9870988812484045,
    "arrivals": 1703822,
    "finished_requests": 73755,
    "scheduler_time": 209.28183737901898
}
#Debug simulation 
Total elapsed time: 36.793787078000605. Arrivals time: 0.3763656415976584 Scheduler time: 36.239793739281595 Scheduler overhead time: 0.06467824010178447 Adapter cache time: 0.024089145939797163 Engine time: 0.06238494114950299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 59.45935539295897,
    "estimated_duration": 3600.0320708697845,
    "input_throughput": 5517.40584777653,
    "output_throughput": 4797.636982113631,
    "total_throughput": 10315.04282989016,
    "itl": 118.99432328550914,
    "ttft": 2154753.359468646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.921158555946372,
    "arrivals": 1680953,
    "finished_requests": 80116,
    "scheduler_time": 191.74696161015555
}
#Debug simulation 
Total elapsed time: 59.459533888846636. Arrivals time: 0.40953246550634503 Scheduler time: 58.88562183780596 Scheduler overhead time: 0.06206305790692568 Adapter cache time: 0.01869261870160699 Engine time: 0.05935374181717634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 55.34353522723541,
    "estimated_duration": 3600.099570615721,
    "input_throughput": 5282.4497286744445,
    "output_throughput": 4594.726527847376,
    "total_throughput": 9877.17625652182,
    "itl": 107.77697306747092,
    "ttft": 2174201.5858698743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.872689065034506,
    "arrivals": 1680953,
    "finished_requests": 76787,
    "scheduler_time": 201.24637472854232
}
#Debug simulation 
Total elapsed time: 55.34372362913564. Arrivals time: 0.40268227038905025 Scheduler time: 54.76665186043829 Scheduler overhead time: 0.06615754822269082 Adapter cache time: 0.01838982291519642 Engine time: 0.06384860025718808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.17835611710325,
    "estimated_duration": 3600.071661034652,
    "input_throughput": 5079.038064132579,
    "output_throughput": 4416.4995858528155,
    "total_throughput": 9495.537649985396,
    "itl": 99.82661122315064,
    "ttft": 2194531.934243627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.591992079988134,
    "arrivals": 1680953,
    "finished_requests": 73749,
    "scheduler_time": 209.06551008373458
}
#Debug simulation 
Total elapsed time: 32.17851824499667. Arrivals time: 0.4900214821100235 Scheduler time: 31.511465740855783 Scheduler overhead time: 0.0635369224473834 Adapter cache time: 0.02448245184496045 Engine time: 0.06251682108268142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128 256]
---Simulation End---
#Simulation results
{
    "duration": 53.08433519722894,
    "estimated_duration": 3600.0681505456378,
    "input_throughput": 5375.411017446148,
    "output_throughput": 4680.440284844656,
    "total_throughput": 10055.851302290805,
    "itl": 112.12220844595129,
    "ttft": 2166171.4340288164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 702,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7952504402492115,
    "arrivals": 1680953,
    "finished_requests": 78266,
    "scheduler_time": 197.072712897848
}
#Debug simulation 
Total elapsed time: 53.084508057218045. Arrivals time: 0.8376352437771857 Scheduler time: 52.07808998785913 Scheduler overhead time: 0.06276373844593763 Adapter cache time: 0.020228054840117693 Engine time: 0.06053841486573219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_384_slots_64_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [128 128 128]
---Simulation End---
#Simulation results
{
    "duration": 31.737581307068467,
    "estimated_duration": 3600.1122477534345,
    "input_throughput": 5079.047746750233,
    "output_throughput": 4416.5909021092775,
    "total_throughput": 9495.63864885951,
    "itl": 99.82497554104178,
    "ttft": 2194557.4415425826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.520536654056063,
    "arrivals": 1680953,
    "finished_requests": 73751,
    "scheduler_time": 209.07174025867948
}
#Debug simulation 
Total elapsed time: 31.737716434057802. Arrivals time: 0.3377143372781575 Scheduler time: 31.224251989275217 Scheduler overhead time: 0.06356585584580898 Adapter cache time: 0.024174427147954702 Engine time: 0.06167352432385087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_384_slots_64_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_384_slots_64_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 52.58927094610408,
    "estimated_duration": 3600.0039896480453,
    "input_throughput": 5375.710153557915,
    "output_throughput": 4680.813145889717,
    "total_throughput": 10056.523299447632,
    "itl": 112.11285188388388,
    "ttft": 2166266.69549569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 702,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.481512500280493,
    "arrivals": 1680953,
    "finished_requests": 78272,
    "scheduler_time": 197.08473316991848
}
#Debug simulation 
Total elapsed time: 52.58944097580388. Arrivals time: 0.3820274365134537 Scheduler time: 52.0383460088633 Scheduler overhead time: 0.06331547955051064 Adapter cache time: 0.02004843018949032 Engine time: 0.060876000206917524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_384_slots_64_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_384_slots_64_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [128 128 128]
Adapter prompts. [540, 540, 34560, 4320, 540, 34560, 34560, 540, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 540, 540, 4320, 540, 34560, 540, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 34560, 34560, 540, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 540, 34560, 4320, 4320, 540, 540, 540, 4320, 540, 4320, 540, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 34560, 4320, 4320, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 540, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 4320, 540, 34560, 4320, 4320, 540, 34560, 34560, 34560, 540, 4320, 34560, 540, 34560, 4320, 4320, 34560, 34560, 540, 4320, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 4320, 4320, 4320, 540, 540, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 540, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 34560, 540, 4320, 4320, 540, 540, 4320, 540, 34560, 34560, 540, 34560, 34560, 540, 4320, 34560, 540, 540, 4320, 4320, 540, 540, 34560, 34560, 34560, 34560, 4320, 540, 4320, 4320, 540, 540, 540, 540, 540, 540, 540, 540, 4320, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 540, 540, 4320, 540, 34560, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 540, 34560, 34560, 4320, 540, 34560, 4320, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 34560, 540, 4320, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 34560, 540, 540, 540, 4320, 540, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 540, 4320, 4320, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 4320, 4320, 4320, 540, 4320, 540, 540, 34560, 540, 4320, 540, 540, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 34560, 540, 540, 540]
Prompts retrieved: 5045760 . Total input tokens: 1125450398 . Total output tokens: 990769994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [256 128]
---Simulation End---
#Simulation results
{
    "duration": 32.16339046228677,
    "estimated_duration": 3600.0442340377185,
    "input_throughput": 5079.1437024905235,
    "output_throughput": 4416.674342405708,
    "total_throughput": 9495.818044896232,
    "itl": 99.82321453160884,
    "ttft": 2194531.354678112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.452809337303056,
    "arrivals": 1680953,
    "finished_requests": 73751,
    "scheduler_time": 209.07145385971614
}
#Debug simulation 
Total elapsed time: 32.163554694037884. Arrivals time: 0.35272179916501045 Scheduler time: 31.634636173956096 Scheduler overhead time: 0.06343699200078845 Adapter cache time: 0.024070485960692167 Engine time: 0.06221425859257579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_384_slots_64_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 384,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_384_slots_64_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [128 128 128]
Adapter prompts. [270, 270, 34560, 4320, 270, 34560, 34560, 270, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 270, 270, 4320, 270, 34560, 270, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 34560, 34560, 270, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 270, 34560, 4320, 4320, 270, 270, 270, 4320, 270, 4320, 270, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 34560, 4320, 4320, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 34560, 34560, 270, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 4320, 270, 34560, 4320, 4320, 270, 34560, 34560, 34560, 270, 4320, 34560, 270, 34560, 4320, 4320, 34560, 34560, 270, 4320, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 4320, 4320, 4320, 270, 270, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 270, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 34560, 270, 4320, 4320, 270, 270, 4320, 270, 34560, 34560, 270, 34560, 34560, 270, 4320, 34560, 270, 270, 4320, 4320, 270, 270, 34560, 34560, 34560, 34560, 4320, 270, 4320, 4320, 270, 270, 270, 270, 270, 270, 270, 270, 4320, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 270, 270, 4320, 270, 34560, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 270, 34560, 34560, 4320, 270, 34560, 4320, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 34560, 270, 4320, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 34560, 270, 270, 270, 4320, 270, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 270, 4320, 4320, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 4320, 4320, 4320, 270, 4320, 270, 270, 34560, 270, 4320, 270, 270, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 34560, 270, 270, 270]
Prompts retrieved: 5011200 . Total input tokens: 1117859278 . Total output tokens: 983942946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [384]
---Simulation End---
#Simulation results
{
    "duration": 54.531663418747485,
    "estimated_duration": 3600.0220688733707,
    "input_throughput": 5511.738433928458,
    "output_throughput": 4808.6888549040505,
    "total_throughput": 10320.427288832509,
    "itl": 119.63934087504909,
    "ttft": 2157348.54044504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 612,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.046794327553425,
    "arrivals": 1669342,
    "finished_requests": 80184,
    "scheduler_time": 191.10198829606384
}
#Debug simulation 
Total elapsed time: 54.53182721603662. Arrivals time: 0.38985710637643933 Scheduler time: 53.9807070735842 Scheduler overhead time: 0.059949102345854044 Adapter cache time: 0.018712453078478575 Engine time: 0.05885390006005764 
