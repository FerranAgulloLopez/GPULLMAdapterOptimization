INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 4320, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29970 . Total input tokens: 6626288 . Total output tokens: 5912170
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.13936848891899,
    "estimated_duration": 3599.275373963983,
    "input_throughput": 686.8179683838598,
    "output_throughput": 598.6352185181709,
    "total_throughput": 1285.4531869020307,
    "itl": 21.85197187437579,
    "ttft": 2896.9642279494888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 10066,
    "finished_requests": 10058,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1395015777088702. Arrivals time: 0.03618518030270934 Scheduler time: 0.7344635725021362 Scheduler overhead time: 0.13563380111008883 Adapter cache time: 0.028510676696896553 Engine time: 0.13756428007036448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 4320, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29970 . Total input tokens: 6626288 . Total output tokens: 5912170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1357124391943216,
    "estimated_duration": 3599.287562717102,
    "input_throughput": 686.8156425195024,
    "output_throughput": 598.6331912789576,
    "total_throughput": 1285.4488337984599,
    "itl": 21.852075739313275,
    "ttft": 2897.008257197173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 10066,
    "finished_requests": 10058,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1358628454618156. Arrivals time: 0.0376948700286448 Scheduler time: 0.7333580534905195 Scheduler overhead time: 0.1371960649266839 Adapter cache time: 0.0280117173679173 Engine time: 0.13222229573875666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 4320, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29970 . Total input tokens: 6626288 . Total output tokens: 5912170
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1262129927054048,
    "estimated_duration": 3599.271043252333,
    "input_throughput": 686.8187947763546,
    "output_throughput": 598.6359388074972,
    "total_throughput": 1285.4547335838517,
    "itl": 21.85203258090432,
    "ttft": 2897.118913829771,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 10066,
    "finished_requests": 10058,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1263816189020872. Arrivals time: 0.03737802430987358 Scheduler time: 0.7262350814417005 Scheduler overhead time: 0.13389480859041214 Adapter cache time: 0.028212380595505238 Engine time: 0.13377046585083008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 4320, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29970 . Total input tokens: 6626288 . Total output tokens: 5912170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1266367267817259,
    "estimated_duration": 3599.2851742638773,
    "input_throughput": 686.816098284177,
    "output_throughput": 598.6335885265518,
    "total_throughput": 1285.4496868107287,
    "itl": 21.851679101844493,
    "ttft": 2897.106822372438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 10066,
    "finished_requests": 10058,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.12674989271909. Arrivals time: 0.03717813454568386 Scheduler time: 0.7236488740891218 Scheduler overhead time: 0.13311530696228147 Adapter cache time: 0.028173555620014668 Engine time: 0.13816183991730213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 4320, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29970 . Total input tokens: 6626288 . Total output tokens: 5912170
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.132376052904874,
    "estimated_duration": 3599.266802099611,
    "input_throughput": 686.8196040810161,
    "output_throughput": 598.6366442029515,
    "total_throughput": 1285.4562482839676,
    "itl": 21.852092106833418,
    "ttft": 2896.9484259477426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 10066,
    "finished_requests": 10058,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1325072930194438. Arrivals time: 0.03577060531824827 Scheduler time: 0.7298064953647554 Scheduler overhead time: 0.1331509118899703 Adapter cache time: 0.028208035044372082 Engine time: 0.1394851915538311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 4320, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29970 . Total input tokens: 6626288 . Total output tokens: 5912170
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1428025178611279,
    "estimated_duration": 3599.275318291301,
    "input_throughput": 686.8179790073867,
    "output_throughput": 598.6352277777092,
    "total_throughput": 1285.453206785096,
    "itl": 21.851971830527294,
    "ttft": 2896.9425330049125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 10066,
    "finished_requests": 10058,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1428904980421066. Arrivals time: 0.03633866040036082 Scheduler time: 0.7392232865095139 Scheduler overhead time: 0.13587809121236205 Adapter cache time: 0.02844685036689043 Engine time: 0.13541864277794957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 4320, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29970 . Total input tokens: 6626288 . Total output tokens: 5912170
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1556573919951916,
    "estimated_duration": 3599.2874940939055,
    "input_throughput": 686.8156556141731,
    "output_throughput": 598.6332026923618,
    "total_throughput": 1285.448858306535,
    "itl": 21.852055350911407,
    "ttft": 2897.0150658485727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 10066,
    "finished_requests": 10058,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.155794233083725. Arrivals time: 0.03848609421402216 Scheduler time: 0.7459681024774909 Scheduler overhead time: 0.13573010312393308 Adapter cache time: 0.028665507212281227 Engine time: 0.1390348058193922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 4320, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29295 . Total input tokens: 6486947 . Total output tokens: 5784298
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.111391020938754,
    "estimated_duration": 3599.895072418127,
    "input_throughput": 664.3718641481026,
    "output_throughput": 588.382982667663,
    "total_throughput": 1252.7548468157656,
    "itl": 21.75422586958936,
    "ttft": 2954.2867531554016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9868,
    "finished_requests": 9860,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.111483525019139. Arrivals time: 0.0368147911503911 Scheduler time: 0.7125649126246572 Scheduler overhead time: 0.135050549171865 Adapter cache time: 0.027287440840154886 Engine time: 0.1326478528790176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 4320, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29295 . Total input tokens: 6486947 . Total output tokens: 5784298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1422124430537224,
    "estimated_duration": 3599.9117949101537,
    "input_throughput": 664.3687779743757,
    "output_throughput": 588.3802494813248,
    "total_throughput": 1252.7490274557003,
    "itl": 21.754395732038255,
    "ttft": 2954.2485495888905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 9868,
    "finished_requests": 9860,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.142308150883764. Arrivals time: 0.036921161226928234 Scheduler time: 0.737380157224834 Scheduler overhead time: 0.13607585011050105 Adapter cache time: 0.027268363162875175 Engine time: 0.13726156810298562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 4320, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29295 . Total input tokens: 6486947 . Total output tokens: 5784298
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.133110902737826,
    "estimated_duration": 3599.9096320361314,
    "input_throughput": 664.3691771360541,
    "output_throughput": 588.3806029880755,
    "total_throughput": 1252.7497801241295,
    "itl": 21.75441212193675,
    "ttft": 2954.31298174792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9868,
    "finished_requests": 9860,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1332174157723784. Arrivals time: 0.0373767320998013 Scheduler time: 0.7249677889049053 Scheduler overhead time: 0.1380794388242066 Adapter cache time: 0.027998645789921284 Engine time: 0.13617254607379436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 4320, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29295 . Total input tokens: 6486947 . Total output tokens: 5784298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1691568437963724,
    "estimated_duration": 3599.9023396228067,
    "input_throughput": 664.3705229655192,
    "output_throughput": 588.3817948855617,
    "total_throughput": 1252.7523178510808,
    "itl": 21.754223479689962,
    "ttft": 2954.3637817926665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 9868,
    "finished_requests": 9860,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.169232518877834. Arrivals time: 0.037695173639804125 Scheduler time: 0.753624068107456 Scheduler overhead time: 0.13822117680683732 Adapter cache time: 0.028410619124770164 Engine time: 0.14203061489388347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 4320, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29295 . Total input tokens: 6486947 . Total output tokens: 5784298
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.2065767808817327,
    "estimated_duration": 3599.9096831120887,
    "input_throughput": 664.3691677099033,
    "output_throughput": 588.3805946400598,
    "total_throughput": 1252.749762349963,
    "itl": 21.7544122748691,
    "ttft": 2954.3008013888216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9868,
    "finished_requests": 9860,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2067046626470983. Arrivals time: 0.039119998924434185 Scheduler time: 0.7738799108192325 Scheduler overhead time: 0.15181781724095345 Adapter cache time: 0.029060293454676867 Engine time: 0.1403265488334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 4320, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29295 . Total input tokens: 6486947 . Total output tokens: 5784298
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1356809027493,
    "estimated_duration": 3599.8948115865514,
    "input_throughput": 664.371912285387,
    "output_throughput": 588.3830252991476,
    "total_throughput": 1252.7549375845344,
    "itl": 21.754369940643205,
    "ttft": 2954.1347782872836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9868,
    "finished_requests": 9860,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1358008841052651. Arrivals time: 0.03787096869200468 Scheduler time: 0.7342383461073041 Scheduler overhead time: 0.13370826421305537 Adapter cache time: 0.027568459045141935 Engine time: 0.13512705406174064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 4320, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 29295 . Total input tokens: 6486947 . Total output tokens: 5784298
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1847266368567944,
    "estimated_duration": 3599.9099551416416,
    "input_throughput": 664.3691175064121,
    "output_throughput": 588.3805501786949,
    "total_throughput": 1252.749667685107,
    "itl": 21.754380541257685,
    "ttft": 2954.243836571129,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9868,
    "finished_requests": 9860,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1848216359503567. Arrivals time: 0.03850264381617308 Scheduler time: 0.7622123858891428 Scheduler overhead time: 0.1419656379148364 Adapter cache time: 0.02904618624597788 Engine time: 0.14248603815212846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 4320, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28950 . Total input tokens: 6408746 . Total output tokens: 5719576
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1531918211840093,
    "estimated_duration": 3599.752747337258,
    "input_throughput": 670.9830284278983,
    "output_throughput": 575.5786981571491,
    "total_throughput": 1246.5617265850474,
    "itl": 21.486581038276743,
    "ttft": 3351.5704881064853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9771,
    "finished_requests": 9762,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.153309979941696. Arrivals time: 0.037434556521475315 Scheduler time: 0.7387160612270236 Scheduler overhead time: 0.14025577018037438 Adapter cache time: 0.028054007794708014 Engine time: 0.13870005309581757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 4320, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28950 . Total input tokens: 6408746 . Total output tokens: 5719576
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.123306828085333,
    "estimated_duration": 3599.7669073682814,
    "input_throughput": 670.9803890513099,
    "output_throughput": 575.5764340627142,
    "total_throughput": 1246.556823114024,
    "itl": 21.486565940988775,
    "ttft": 3351.590301722328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 9771,
    "finished_requests": 9762,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1234038951806724. Arrivals time: 0.037759213242679834 Scheduler time: 0.721247112378478 Scheduler overhead time: 0.13533694949001074 Adapter cache time: 0.02688058465719223 Engine time: 0.13495017774403095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 4320, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28950 . Total input tokens: 6408746 . Total output tokens: 5719576
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.127757573965937,
    "estimated_duration": 3599.7694292417937,
    "input_throughput": 670.9799189857393,
    "output_throughput": 575.576030833843,
    "total_throughput": 1246.5559498195823,
    "itl": 21.48659497958987,
    "ttft": 3351.5347364480467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9771,
    "finished_requests": 9762,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.127877851948142. Arrivals time: 0.03767918422818184 Scheduler time: 0.7223853562027216 Scheduler overhead time: 0.13598399003967643 Adapter cache time: 0.027169360779225826 Engine time: 0.13640719559043646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 4320, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28950 . Total input tokens: 6408746 . Total output tokens: 5719576
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.140842174179852,
    "estimated_duration": 3599.759449257045,
    "input_throughput": 670.9817792126386,
    "output_throughput": 575.5776265626938,
    "total_throughput": 1246.5594057753324,
    "itl": 21.486604635368423,
    "ttft": 3351.607849226377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 9771,
    "finished_requests": 9762,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1409573182463646. Arrivals time: 0.03717093775048852 Scheduler time: 0.728347301017493 Scheduler overhead time: 0.14146104268729687 Adapter cache time: 0.027126289438456297 Engine time: 0.13763166405260563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 4320, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28950 . Total input tokens: 6408746 . Total output tokens: 5719576
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.159348193090409,
    "estimated_duration": 3599.7694549563535,
    "input_throughput": 670.9799141926676,
    "output_throughput": 575.5760267222785,
    "total_throughput": 1246.5559409149462,
    "itl": 21.486584370959104,
    "ttft": 3351.508276416735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9771,
    "finished_requests": 9762,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1594471842981875. Arrivals time: 0.03777669416740537 Scheduler time: 0.74262789869681 Scheduler overhead time: 0.14031372871249914 Adapter cache time: 0.02795580541715026 Engine time: 0.14062723703682423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 4320, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28950 . Total input tokens: 6408746 . Total output tokens: 5719576
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1106855771504343,
    "estimated_duration": 3599.7520761878836,
    "input_throughput": 670.9831535281357,
    "output_throughput": 575.578805469896,
    "total_throughput": 1246.5619589980317,
    "itl": 21.486524381683985,
    "ttft": 3351.574546781283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9771,
    "finished_requests": 9762,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1108143371529877. Arrivals time: 0.03720091329887509 Scheduler time: 0.7072229338809848 Scheduler overhead time: 0.13521666219457984 Adapter cache time: 0.02694059582427144 Engine time: 0.13657815754413605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 4320, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28950 . Total input tokens: 6408746 . Total output tokens: 5719576
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.124247259926051,
    "estimated_duration": 3599.768424175242,
    "input_throughput": 670.9801063254217,
    "output_throughput": 575.5761915364628,
    "total_throughput": 1246.5562978618846,
    "itl": 21.486543257693782,
    "ttft": 3351.6139717140413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9771,
    "finished_requests": 9762,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1243893187493086. Arrivals time: 0.03780561778694391 Scheduler time: 0.7211511167697608 Scheduler overhead time: 0.13485220028087497 Adapter cache time: 0.02687022415921092 Engine time: 0.13621305907145143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 4320, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28785 . Total input tokens: 6370538 . Total output tokens: 5684723
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1210149312391877,
    "estimated_duration": 3599.629616923347,
    "input_throughput": 657.2773456681953,
    "output_throughput": 586.7581459131485,
    "total_throughput": 1244.0354915813436,
    "itl": 21.689473381838344,
    "ttft": 3368.4492957876223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9722,
    "finished_requests": 9713,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1210986692458391. Arrivals time: 0.03655542014166713 Scheduler time: 0.7191023058257997 Scheduler overhead time: 0.1347939963452518 Adapter cache time: 0.026836846955120564 Engine time: 0.13632325688377023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 4320, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28785 . Total input tokens: 6370538 . Total output tokens: 5684723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1756133302114904,
    "estimated_duration": 3599.6457853829997,
    "input_throughput": 657.2743933881994,
    "output_throughput": 586.7555103828842,
    "total_throughput": 1244.0299037710836,
    "itl": 21.689601156684084,
    "ttft": 3368.263760869291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 9722,
    "finished_requests": 9713,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1756865670904517. Arrivals time: 0.03796840738505125 Scheduler time: 0.7578572183847427 Scheduler overhead time: 0.14075802359730005 Adapter cache time: 0.02804137347266078 Engine time: 0.14032255206257105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 4320, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28785 . Total input tokens: 6370538 . Total output tokens: 5684723
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1203575492836535,
    "estimated_duration": 3599.625838599269,
    "input_throughput": 657.2780355751279,
    "output_throughput": 586.7587618000573,
    "total_throughput": 1244.0367973751852,
    "itl": 21.689555965310685,
    "ttft": 3368.318610316898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9722,
    "finished_requests": 9713,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1204850599169731. Arrivals time: 0.03709515603259206 Scheduler time: 0.7181044491007924 Scheduler overhead time: 0.13579013152047992 Adapter cache time: 0.026898771058768034 Engine time: 0.13515318604186177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 4320, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28785 . Total input tokens: 6370538 . Total output tokens: 5684723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1190839209593832,
    "estimated_duration": 3599.6340908381803,
    "input_throughput": 657.2765287510331,
    "output_throughput": 586.7574166429209,
    "total_throughput": 1244.033945393954,
    "itl": 21.68952603803416,
    "ttft": 3368.377166181529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 9722,
    "finished_requests": 9713,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.119423408061266. Arrivals time: 0.03431409131735563 Scheduler time: 0.7225440987385809 Scheduler overhead time: 0.13450436340644956 Adapter cache time: 0.026605801656842232 Engine time: 0.13396330457180738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 4320, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28785 . Total input tokens: 6370538 . Total output tokens: 5684723
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.1196158179081976,
    "estimated_duration": 3599.623707052363,
    "input_throughput": 657.2784247877449,
    "output_throughput": 586.7591092541039,
    "total_throughput": 1244.0375340418489,
    "itl": 21.68952575719875,
    "ttft": 3368.2687079108105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9722,
    "finished_requests": 9713,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.119718193076551. Arrivals time: 0.035268436186015606 Scheduler time: 0.7195965643040836 Scheduler overhead time: 0.13570131082087755 Adapter cache time: 0.027147652581334114 Engine time: 0.13429520186036825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 4320, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28785 . Total input tokens: 6370538 . Total output tokens: 5684723
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.142451258841902,
    "estimated_duration": 3599.626284140902,
    "input_throughput": 657.2779542209244,
    "output_throughput": 586.7586891743356,
    "total_throughput": 1244.03664339526,
    "itl": 21.689461140129325,
    "ttft": 3368.3813470197056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9722,
    "finished_requests": 9713,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1425523580983281. Arrivals time: 0.03752585966140032 Scheduler time: 0.7341698971576989 Scheduler overhead time: 0.13868674216791987 Adapter cache time: 0.027397082187235355 Engine time: 0.1355415009893477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 4320, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 4320]
Prompts retrieved: 28785 . Total input tokens: 6370538 . Total output tokens: 5684723
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.116706449072808,
    "estimated_duration": 3599.645900784203,
    "input_throughput": 657.2743723166113,
    "output_throughput": 586.755491572064,
    "total_throughput": 1244.0298638886752,
    "itl": 21.68956489998563,
    "ttft": 3368.1901343193417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9722,
    "finished_requests": 9713,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1168258851394057. Arrivals time: 0.036339669954031706 Scheduler time: 0.7157652908936143 Scheduler overhead time: 0.13545219041407108 Adapter cache time: 0.026828237809240818 Engine time: 0.13483775546774268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 4320, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27945 . Total input tokens: 6181302 . Total output tokens: 5521798
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1369666359387338,
    "estimated_duration": 3599.285680253052,
    "input_throughput": 637.1928220598364,
    "output_throughput": 564.1925038462715,
    "total_throughput": 1201.385325906108,
    "itl": 21.59268191877214,
    "ttft": 6519.974960715539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9438,
    "finished_requests": 9421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1372240758500993. Arrivals time: 0.03676363546401262 Scheduler time: 0.7214279579930007 Scheduler overhead time: 0.14102259231731296 Adapter cache time: 0.027710809372365475 Engine time: 0.13941110111773014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 4320, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27945 . Total input tokens: 6181302 . Total output tokens: 5521798
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1122000208124518,
    "estimated_duration": 3599.2795049032493,
    "input_throughput": 637.1939153032376,
    "output_throughput": 564.1934718416891,
    "total_throughput": 1201.3873871449266,
    "itl": 21.592569378237226,
    "ttft": 6519.791373526582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 9438,
    "finished_requests": 9421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1122793019749224. Arrivals time: 0.03733045095577836 Scheduler time: 0.7033245507627726 Scheduler overhead time: 0.13780685560777783 Adapter cache time: 0.027294442988932133 Engine time: 0.13717977842316031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 4320, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27945 . Total input tokens: 6181302 . Total output tokens: 5521798
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1017643231898546,
    "estimated_duration": 3599.281604938027,
    "input_throughput": 637.1935435264418,
    "output_throughput": 564.19314265769,
    "total_throughput": 1201.3866861841318,
    "itl": 21.5926497901473,
    "ttft": 6519.905685230615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9438,
    "finished_requests": 9421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1018805061466992. Arrivals time: 0.03659657575190067 Scheduler time: 0.7005987991578877 Scheduler overhead time: 0.13527212524786592 Adapter cache time: 0.02656870847567916 Engine time: 0.13500103540718555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 4320, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27945 . Total input tokens: 6181302 . Total output tokens: 5521798
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.0928968479856849,
    "estimated_duration": 3599.2903036564526,
    "input_throughput": 637.1920035652967,
    "output_throughput": 564.1917791229731,
    "total_throughput": 1201.3837826882698,
    "itl": 21.59262889796046,
    "ttft": 6519.976451807798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 9438,
    "finished_requests": 9421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0930014932528138. Arrivals time: 0.03526868810877204 Scheduler time: 0.6922272113151848 Scheduler overhead time: 0.13571608997881413 Adapter cache time: 0.02693835273385048 Engine time: 0.13540875539183617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 4320, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27945 . Total input tokens: 6181302 . Total output tokens: 5521798
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.193118552211672,
    "estimated_duration": 3599.2815731667533,
    "input_throughput": 637.1935491510227,
    "output_throughput": 564.193147637888,
    "total_throughput": 1201.3866967889107,
    "itl": 21.5925810219943,
    "ttft": 6519.872026373204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9438,
    "finished_requests": 9421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1934323790483177. Arrivals time: 0.039205287117511034 Scheduler time: 0.7566809784621 Scheduler overhead time: 0.14872788870707154 Adapter cache time: 0.02899901196360588 Engine time: 0.1461651842109859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 4320, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27945 . Total input tokens: 6181302 . Total output tokens: 5521798
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1250601038336754,
    "estimated_duration": 3599.28431109412,
    "input_throughput": 637.1930644464245,
    "output_throughput": 564.1927184637175,
    "total_throughput": 1201.3857829101419,
    "itl": 21.592580116859317,
    "ttft": 6519.964111560693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9438,
    "finished_requests": 9421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1251999577507377. Arrivals time: 0.03617854043841362 Scheduler time: 0.7112533887848258 Scheduler overhead time: 0.14296072954311967 Adapter cache time: 0.0268624946475029 Engine time: 0.13833673112094402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 4320, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27945 . Total input tokens: 6181302 . Total output tokens: 5521798
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.095045386813581,
    "estimated_duration": 3599.280331309073,
    "input_throughput": 637.1937690015567,
    "output_throughput": 564.1933423011343,
    "total_throughput": 1201.387111302691,
    "itl": 21.592563398718134,
    "ttft": 6519.840028459092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9438,
    "finished_requests": 9421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0951544549316168. Arrivals time: 0.03434595139697194 Scheduler time: 0.6952571147121489 Scheduler overhead time: 0.13616578886285424 Adapter cache time: 0.026623842772096395 Engine time: 0.1348177078180015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 4320, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27600 . Total input tokens: 6103483 . Total output tokens: 5456429
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.0868577887304127,
    "estimated_duration": 3599.5639026937456,
    "input_throughput": 634.0498631770452,
    "output_throughput": 552.8044657051054,
    "total_throughput": 1186.8543288821504,
    "itl": 21.34123581949035,
    "ttft": 3903.471289997249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9307,
    "finished_requests": 9297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0869640139862895. Arrivals time: 0.03630764642730355 Scheduler time: 0.6834333171136677 Scheduler overhead time: 0.13646672666072845 Adapter cache time: 0.02671902161091566 Engine time: 0.13641529669985175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 4320, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27600 . Total input tokens: 6103483 . Total output tokens: 5456429
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1242578621022403,
    "estimated_duration": 3599.5574856170606,
    "input_throughput": 634.0509935233753,
    "output_throughput": 552.8054512119802,
    "total_throughput": 1186.8564447353556,
    "itl": 21.34120504613475,
    "ttft": 3903.4685570251395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 9307,
    "finished_requests": 9297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.124344787094742. Arrivals time: 0.03613729355856776 Scheduler time: 0.7121134274639189 Scheduler overhead time: 0.13940963288769126 Adapter cache time: 0.027419881895184517 Engine time: 0.13859787117689848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 4320, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27600 . Total input tokens: 6103483 . Total output tokens: 5456429
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1192992120049894,
    "estimated_duration": 3599.5596644888237,
    "input_throughput": 634.0506097220399,
    "output_throughput": 552.8051165898873,
    "total_throughput": 1186.8557263119271,
    "itl": 21.341281447130772,
    "ttft": 3903.554673604393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9307,
    "finished_requests": 9297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1193946623243392. Arrivals time: 0.03652950096875429 Scheduler time: 0.7037161984480917 Scheduler overhead time: 0.13976945308968425 Adapter cache time: 0.027435279451310635 Engine time: 0.14201707625761628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 4320, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27600 . Total input tokens: 6103483 . Total output tokens: 5456429
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.0921717011369765,
    "estimated_duration": 3599.546968091997,
    "input_throughput": 634.0528461585194,
    "output_throughput": 552.8070664555761,
    "total_throughput": 1186.8599126140955,
    "itl": 21.341186983253976,
    "ttft": 3903.3634400447436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 9307,
    "finished_requests": 9297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.092277086339891. Arrivals time: 0.03708399459719658 Scheduler time: 0.6868371181190014 Scheduler overhead time: 0.13999736215919256 Adapter cache time: 0.026365033350884914 Engine time: 0.1338868886232376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 4320, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27600 . Total input tokens: 6103483 . Total output tokens: 5456429
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.0923699396662414,
    "estimated_duration": 3599.5579173673073,
    "input_throughput": 634.0509174719048,
    "output_throughput": 552.8053849055349,
    "total_throughput": 1186.8563023774398,
    "itl": 21.341281852028363,
    "ttft": 3903.4829856433635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9307,
    "finished_requests": 9297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0924950926564634. Arrivals time: 0.035685496404767036 Scheduler time: 0.6913429168052971 Scheduler overhead time: 0.13495223643258214 Adapter cache time: 0.02630619565024972 Engine time: 0.13603903632611036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 4320, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27600 . Total input tokens: 6103483 . Total output tokens: 5456429
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.179944363888353,
    "estimated_duration": 3599.563809824351,
    "input_throughput": 634.0498795356458,
    "output_throughput": 552.8044799675602,
    "total_throughput": 1186.8543595032058,
    "itl": 21.341161653758924,
    "ttft": 3903.4906087832524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9307,
    "finished_requests": 9297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1800801381468773. Arrivals time: 0.038730855099856853 Scheduler time: 0.7447884869761765 Scheduler overhead time: 0.1471545621752739 Adapter cache time: 0.02881207223981619 Engine time: 0.14676355198025703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 4320, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27600 . Total input tokens: 6103483 . Total output tokens: 5456429
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1053728321567178,
    "estimated_duration": 3599.557739346204,
    "input_throughput": 634.0509488297693,
    "output_throughput": 552.8054122452893,
    "total_throughput": 1186.8563610750587,
    "itl": 21.341200262472316,
    "ttft": 3903.4948807843107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9307,
    "finished_requests": 9297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1055199541151524. Arrivals time: 0.03393949335440993 Scheduler time: 0.6911700386554003 Scheduler overhead time: 0.14221506798639894 Adapter cache time: 0.027017551474273205 Engine time: 0.14202150562778115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 4320, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27435 . Total input tokens: 6067194 . Total output tokens: 5422916
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1533112619072199,
    "estimated_duration": 3599.2106462831143,
    "input_throughput": 624.2188137335473,
    "output_throughput": 559.1976124205105,
    "total_throughput": 1183.416426154058,
    "itl": 21.505023891034718,
    "ttft": 3924.5746582066704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9256,
    "finished_requests": 9246,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.153549499809742. Arrivals time: 0.03656479390338063 Scheduler time: 0.726558143272996 Scheduler overhead time: 0.1494648978114128 Adapter cache time: 0.02721084328368306 Engine time: 0.14178891479969025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 4320, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27435 . Total input tokens: 6067194 . Total output tokens: 5422916
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.0969389490783215,
    "estimated_duration": 3599.216377688126,
    "input_throughput": 624.2178197252795,
    "output_throughput": 559.1967219522356,
    "total_throughput": 1183.4145416775152,
    "itl": 21.504946383220727,
    "ttft": 3924.7558427550603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 9256,
    "finished_requests": 9246,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.097084403038025. Arrivals time: 0.03640482574701309 Scheduler time: 0.6952962470240891 Scheduler overhead time: 0.1355454782024026 Adapter cache time: 0.02632054639980197 Engine time: 0.13552960334345698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 4320, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27435 . Total input tokens: 6067194 . Total output tokens: 5422916
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.0872744536027312,
    "estimated_duration": 3599.222259265018,
    "input_throughput": 624.2167996757133,
    "output_throughput": 559.1958081552316,
    "total_throughput": 1183.4126078309448,
    "itl": 21.50496605722098,
    "ttft": 3924.822508070603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9256,
    "finished_requests": 9246,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0873715756461024. Arrivals time: 0.03534958744421601 Scheduler time: 0.6876236731186509 Scheduler overhead time: 0.13555117370560765 Adapter cache time: 0.026217691134661436 Engine time: 0.13458006270229816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 4320, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27435 . Total input tokens: 6067194 . Total output tokens: 5422916
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1202909760177135,
    "estimated_duration": 3599.2131694906384,
    "input_throughput": 624.2183761285672,
    "output_throughput": 559.1972203982665,
    "total_throughput": 1183.4155965268337,
    "itl": 21.504901456254228,
    "ttft": 3924.7416661981642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 9256,
    "finished_requests": 9246,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.120394746772945. Arrivals time: 0.036891304422169924 Scheduler time: 0.7072858824394643 Scheduler overhead time: 0.14014794491231441 Adapter cache time: 0.02687806962057948 Engine time: 0.13923661736771464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 4320, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27435 . Total input tokens: 6067194 . Total output tokens: 5422916
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.1123332139104605,
    "estimated_duration": 3599.2197972263466,
    "input_throughput": 624.2172266698917,
    "output_throughput": 559.1961906719386,
    "total_throughput": 1183.4134173418302,
    "itl": 21.504938322247042,
    "ttft": 3924.774149667638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9256,
    "finished_requests": 9246,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.112427792046219. Arrivals time: 0.035279329400509596 Scheduler time: 0.7074092756956816 Scheduler overhead time: 0.13695490825921297 Adapter cache time: 0.026507369708269835 Engine time: 0.1379182292148471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 4320, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27435 . Total input tokens: 6067194 . Total output tokens: 5422916
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1399709940887988,
    "estimated_duration": 3599.2048577930263,
    "input_throughput": 624.2198176453998,
    "output_throughput": 559.1985117607716,
    "total_throughput": 1183.4183294061713,
    "itl": 21.504907898148012,
    "ttft": 3924.7986348493014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9256,
    "finished_requests": 9246,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1401652712374926. Arrivals time: 0.03701532119885087 Scheduler time: 0.720720840152353 Scheduler overhead time: 0.1415809034369886 Adapter cache time: 0.027381462510675192 Engine time: 0.14272827887907624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 4320, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 4320]
Prompts retrieved: 27435 . Total input tokens: 6067194 . Total output tokens: 5422916
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.0969294221140444,
    "estimated_duration": 3599.2171319830263,
    "input_throughput": 624.2176889067429,
    "output_throughput": 559.1966047602964,
    "total_throughput": 1183.4142936670394,
    "itl": 21.504968577228492,
    "ttft": 3924.7602214872154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9256,
    "finished_requests": 9246,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0970503906719387. Arrivals time: 0.03616803139448166 Scheduler time: 0.6971524870023131 Scheduler overhead time: 0.13541666278615594 Adapter cache time: 0.026080209761857986 Engine time: 0.13438659440726042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 4320, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26925 . Total input tokens: 5948922 . Total output tokens: 5323573
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.0820283722132444,
    "estimated_duration": 3599.938773304645,
    "input_throughput": 617.4299453319906,
    "output_throughput": 549.5271238138331,
    "total_throughput": 1166.9570691458237,
    "itl": 21.46902701844565,
    "ttft": 5574.886404514558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9098,
    "finished_requests": 9084,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0821317778900266. Arrivals time: 0.03348314017057419 Scheduler time: 0.6820290926843882 Scheduler overhead time: 0.13588752830401063 Adapter cache time: 0.02574782120063901 Engine time: 0.1370180957019329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 4320, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26925 . Total input tokens: 5948922 . Total output tokens: 5323573
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.0829002470709383,
    "estimated_duration": 3599.9297607511676,
    "input_throughput": 617.4314910900388,
    "output_throughput": 549.5284995747284,
    "total_throughput": 1166.9599906647672,
    "itl": 21.469129476340445,
    "ttft": 5574.905121766656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 9098,
    "finished_requests": 9084,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.083038942888379. Arrivals time: 0.035612212028354406 Scheduler time: 0.6824776758439839 Scheduler overhead time: 0.13547881739214063 Adapter cache time: 0.02556533459573984 Engine time: 0.13630221085622907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 4320, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26925 . Total input tokens: 5948922 . Total output tokens: 5323573
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1033381456509233,
    "estimated_duration": 3599.9386133553944,
    "input_throughput": 617.4299727650853,
    "output_throughput": 549.5271482299304,
    "total_throughput": 1166.9571209950157,
    "itl": 21.469201516249743,
    "ttft": 5574.934176783763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9098,
    "finished_requests": 9084,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1034342418424785. Arrivals time: 0.03270922461524606 Scheduler time: 0.6884647351689637 Scheduler overhead time: 0.14994260482490063 Adapter cache time: 0.026097925379872322 Engine time: 0.1359938527457416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 4320, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26925 . Total input tokens: 5948922 . Total output tokens: 5323573
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.0835571587085724,
    "estimated_duration": 3599.9163022303223,
    "input_throughput": 617.4337993977592,
    "output_throughput": 549.5305540227059,
    "total_throughput": 1166.9643534204652,
    "itl": 21.46899200792504,
    "ttft": 5574.855222170373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 9098,
    "finished_requests": 9084,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.083680720999837. Arrivals time: 0.03526836447417736 Scheduler time: 0.6832949132658541 Scheduler overhead time: 0.13608220824971795 Adapter cache time: 0.02565863961353898 Engine time: 0.13558074412867427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 4320, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26925 . Total input tokens: 5948922 . Total output tokens: 5323573
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.0876947441138327,
    "estimated_duration": 3599.9359334771243,
    "input_throughput": 617.4304323947003,
    "output_throughput": 549.5275573110615,
    "total_throughput": 1166.9579897057617,
    "itl": 21.469154357193517,
    "ttft": 5574.900123714951,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9098,
    "finished_requests": 9084,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0878610517829657. Arrivals time: 0.03498503053560853 Scheduler time: 0.6853974768891931 Scheduler overhead time: 0.1366676758043468 Adapter cache time: 0.02590829087421298 Engine time: 0.1369311618618667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 4320, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26925 . Total input tokens: 5948922 . Total output tokens: 5323573
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.0861324621364474,
    "estimated_duration": 3599.938976895179,
    "input_throughput": 617.4299104139285,
    "output_throughput": 549.5270927359394,
    "total_throughput": 1166.957003149868,
    "itl": 21.469039453523074,
    "ttft": 5574.892403550697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9098,
    "finished_requests": 9084,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0862393048591912. Arrivals time: 0.03527197754010558 Scheduler time: 0.6865000356920063 Scheduler overhead time: 0.13633173517882824 Adapter cache time: 0.025561204180121422 Engine time: 0.1349027673713863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 4320, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26925 . Total input tokens: 5948922 . Total output tokens: 5323573
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.108542937785387,
    "estimated_duration": 3599.9300838474082,
    "input_throughput": 617.431435675131,
    "output_throughput": 549.5284502541615,
    "total_throughput": 1166.9598859292926,
    "itl": 21.46912486908207,
    "ttft": 5574.949189877694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9098,
    "finished_requests": 9084,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.108661999925971. Arrivals time: 0.03578193346038461 Scheduler time: 0.6979027665220201 Scheduler overhead time: 0.1404555100016296 Adapter cache time: 0.026483130641281605 Engine time: 0.13881730288267136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 4320, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26760 . Total input tokens: 5911513 . Total output tokens: 5293624
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.073980123270303,
    "estimated_duration": 3599.838385887082,
    "input_throughput": 615.703196201632,
    "output_throughput": 537.2721752127757,
    "total_throughput": 1152.9753714144076,
    "itl": 21.191294486046193,
    "ttft": 6416.02158775572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 9027,
    "finished_requests": 9011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0741581842303276. Arrivals time: 0.03556978050619364 Scheduler time: 0.6699954285286367 Scheduler overhead time: 0.13766460167244077 Adapter cache time: 0.02618582732975483 Engine time: 0.13630325766280293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 4320, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26760 . Total input tokens: 5911513 . Total output tokens: 5293624
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1082229879684746,
    "estimated_duration": 3599.8447541988094,
    "input_throughput": 615.702106990804,
    "output_throughput": 537.2712247504842,
    "total_throughput": 1152.9733317412881,
    "itl": 21.19130002096075,
    "ttft": 6416.1206414025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 9027,
    "finished_requests": 9011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1083156107924879. Arrivals time: 0.03563466900959611 Scheduler time: 0.6924439147114754 Scheduler overhead time: 0.14066287642344832 Adapter cache time: 0.026626757346093655 Engine time: 0.14238432003185153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 4320, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26760 . Total input tokens: 5911513 . Total output tokens: 5293624
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.0806433660909534,
    "estimated_duration": 3599.8408268212474,
    "input_throughput": 615.7027787134596,
    "output_throughput": 537.27181090611,
    "total_throughput": 1152.9745896195695,
    "itl": 21.209426139275223,
    "ttft": 6416.055335477745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 9027,
    "finished_requests": 9011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0807702760212123. Arrivals time: 0.03541418071836233 Scheduler time: 0.6776978368870914 Scheduler overhead time: 0.1375970495864749 Adapter cache time: 0.025663151405751705 Engine time: 0.13540724059566855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 4320, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26760 . Total input tokens: 5911513 . Total output tokens: 5293624
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.084405721630901,
    "estimated_duration": 3599.8388781362078,
    "input_throughput": 615.7031120091526,
    "output_throughput": 537.2721017451103,
    "total_throughput": 1152.9752137542628,
    "itl": 21.191253629484972,
    "ttft": 6415.990107120244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 9027,
    "finished_requests": 9011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.084514572750777. Arrivals time: 0.035300908144563437 Scheduler time: 0.6758386217989028 Scheduler overhead time: 0.13804011698812246 Adapter cache time: 0.026561304461210966 Engine time: 0.13943057972937822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 4320, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26760 . Total input tokens: 5911513 . Total output tokens: 5293624
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.0666841063648462,
    "estimated_duration": 3599.8297695779243,
    "input_throughput": 615.7046699071757,
    "output_throughput": 537.2734611911302,
    "total_throughput": 1152.9781310983058,
    "itl": 21.191409770656975,
    "ttft": 6415.977104314094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 9027,
    "finished_requests": 9011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.066779050976038. Arrivals time: 0.03489590948447585 Scheduler time: 0.6676128446124494 Scheduler overhead time: 0.13541593356058002 Adapter cache time: 0.02581430645659566 Engine time: 0.13491622218862176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 4320, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26760 . Total input tokens: 5911513 . Total output tokens: 5293624
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1136143165640533,
    "estimated_duration": 3599.836007041838,
    "input_throughput": 615.703603070894,
    "output_throughput": 537.2725302532153,
    "total_throughput": 1152.9761333241095,
    "itl": 21.191328688571367,
    "ttft": 6415.996972502379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 9027,
    "finished_requests": 9011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1137054115533829. Arrivals time: 0.03558178339153528 Scheduler time: 0.6957733565941453 Scheduler overhead time: 0.14118150621652603 Adapter cache time: 0.02696194965392351 Engine time: 0.1428128224797547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 4320, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 4320]
Prompts retrieved: 26760 . Total input tokens: 5911513 . Total output tokens: 5293624
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1285458011552691,
    "estimated_duration": 3599.8461300957533,
    "input_throughput": 615.7018716633437,
    "output_throughput": 537.271019400086,
    "total_throughput": 1152.9728910634296,
    "itl": 21.191343914457807,
    "ttft": 6416.080118819309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 9027,
    "finished_requests": 9011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1286409017629921. Arrivals time: 0.03837358299642801 Scheduler time: 0.7012169337831438 Scheduler overhead time: 0.14339699828997254 Adapter cache time: 0.031562339048832655 Engine time: 0.14200513251125813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 4320, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 4320]
Prompts retrieved: 26415 . Total input tokens: 5835453 . Total output tokens: 5217556
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.0889924857765436,
    "estimated_duration": 3599.893974826634,
    "input_throughput": 606.9867655214015,
    "output_throughput": 536.1694020705022,
    "total_throughput": 1143.1561675919038,
    "itl": 21.340555019094797,
    "ttft": 8508.4111162062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 8922,
    "finished_requests": 8901,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0891188350506127. Arrivals time: 0.03482306469231844 Scheduler time: 0.6792229642160237 Scheduler overhead time: 0.13871458591893315 Adapter cache time: 0.02628119708970189 Engine time: 0.14059363771229982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 4320, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 4320]
Prompts retrieved: 26415 . Total input tokens: 5835453 . Total output tokens: 5217556
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.0635102312080562,
    "estimated_duration": 3599.884752394238,
    "input_throughput": 606.9883205418522,
    "output_throughput": 536.1707756661597,
    "total_throughput": 1143.159096208012,
    "itl": 21.340670035476762,
    "ttft": 8508.457734932512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 8922,
    "finished_requests": 8901,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0636227950453758. Arrivals time: 0.035058573354035616 Scheduler time: 0.6629260648041964 Scheduler overhead time: 0.13649081718176603 Adapter cache time: 0.025687035638839006 Engine time: 0.13525628158822656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 4320, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 4320]
Prompts retrieved: 26415 . Total input tokens: 5835453 . Total output tokens: 5217556
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.0676461160182953,
    "estimated_duration": 3599.8920985421614,
    "input_throughput": 606.9870818863958,
    "output_throughput": 536.1696815250792,
    "total_throughput": 1143.1567634114751,
    "itl": 21.340641375473385,
    "ttft": 8508.515625598518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 8922,
    "finished_requests": 8901,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.067751853261143. Arrivals time: 0.03382800426334143 Scheduler time: 0.6661209296435118 Scheduler overhead time: 0.13684673653915524 Adapter cache time: 0.02573505463078618 Engine time: 0.13691962882876396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 4320, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 4320]
Prompts retrieved: 26415 . Total input tokens: 5835453 . Total output tokens: 5217556
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.083400263916701,
    "estimated_duration": 3599.8940944776227,
    "input_throughput": 606.9867453467617,
    "output_throughput": 536.1693842496448,
    "total_throughput": 1143.1561295964066,
    "itl": 21.340521395035623,
    "ttft": 8508.4887922552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 8922,
    "finished_requests": 8901,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0835364991798997. Arrivals time: 0.034918643068522215 Scheduler time: 0.6807661261409521 Scheduler overhead time: 0.13744319276884198 Adapter cache time: 0.025754923466593027 Engine time: 0.13606343232095242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 4320, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 4320]
Prompts retrieved: 26415 . Total input tokens: 5835453 . Total output tokens: 5217556
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.0661595021374524,
    "estimated_duration": 3599.8874550403834,
    "input_throughput": 606.9878648402045,
    "output_throughput": 536.1703731313866,
    "total_throughput": 1143.158237971591,
    "itl": 21.340672149537298,
    "ttft": 8508.56424297317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 8922,
    "finished_requests": 8901,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0662610670551658. Arrivals time: 0.033511125948280096 Scheduler time: 0.6657950500957668 Scheduler overhead time: 0.13648179592564702 Adapter cache time: 0.025906129740178585 Engine time: 0.13662029011175036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 4320, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 4320]
Prompts retrieved: 26415 . Total input tokens: 5835453 . Total output tokens: 5217556
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.064670184161514,
    "estimated_duration": 3599.892977916321,
    "input_throughput": 606.9869336128893,
    "output_throughput": 536.1695505506959,
    "total_throughput": 1143.1564841635852,
    "itl": 21.340611410871244,
    "ttft": 8508.601089141031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 8922,
    "finished_requests": 8901,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0647662421688437. Arrivals time: 0.03456077538430691 Scheduler time: 0.6641359119676054 Scheduler overhead time: 0.136556806974113 Adapter cache time: 0.025958179961889982 Engine time: 0.1349441851489246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 4320, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 4320]
Prompts retrieved: 26415 . Total input tokens: 5835453 . Total output tokens: 5217556
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.072158362250775,
    "estimated_duration": 3599.886600097378,
    "input_throughput": 606.9880089947535,
    "output_throughput": 536.1705004673727,
    "total_throughput": 1143.1585094621262,
    "itl": 21.34067947803583,
    "ttft": 8508.500940884423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 8922,
    "finished_requests": 8901,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.072256678249687. Arrivals time: 0.032159303314983845 Scheduler time: 0.6729956213384867 Scheduler overhead time: 0.13693221099674702 Adapter cache time: 0.025356848258525133 Engine time: 0.13660726882517338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 1080, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 10530 . Total input tokens: 2309149 . Total output tokens: 2121671
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.7195392050780356,
    "estimated_duration": 3599.4368438788247,
    "input_throughput": 219.6440816414045,
    "output_throughput": 204.86371395958975,
    "total_throughput": 424.50779560099426,
    "itl": 20.502824580332465,
    "ttft": 7462.731212102983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 3392,
    "finished_requests": 3385,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7196689792908728. Arrivals time: 0.021628931164741516 Scheduler time: 0.3279951340518892 Scheduler overhead time: 0.13400521129369736 Adapter cache time: 0.027671249583363533 Engine time: 0.13961613969877362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 1080, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 10530 . Total input tokens: 2309149 . Total output tokens: 2121671
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.7240652143955231,
    "estimated_duration": 3599.4368438788247,
    "input_throughput": 219.6440816414045,
    "output_throughput": 204.86371395958975,
    "total_throughput": 424.50779560099426,
    "itl": 20.50422545014771,
    "ttft": 7462.805713355216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 3392,
    "finished_requests": 3385,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.724133538082242. Arrivals time: 0.02114375028759241 Scheduler time: 0.3330707889981568 Scheduler overhead time: 0.13360785134136677 Adapter cache time: 0.02782166702672839 Engine time: 0.1399152553640306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 1080, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 10530 . Total input tokens: 2309149 . Total output tokens: 2121671
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.7395677543245256,
    "estimated_duration": 3599.4368438788247,
    "input_throughput": 219.6440816414045,
    "output_throughput": 204.86371395958975,
    "total_throughput": 424.50779560099426,
    "itl": 20.504295238945634,
    "ttft": 7462.765334255336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 3392,
    "finished_requests": 3385,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7396537950262427. Arrivals time: 0.02200475288555026 Scheduler time: 0.3393501038663089 Scheduler overhead time: 0.13754631951451302 Adapter cache time: 0.028427723329514265 Engine time: 0.141210469417274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 1080, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 10530 . Total input tokens: 2309149 . Total output tokens: 2121671
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7143983012065291,
    "estimated_duration": 3599.4368438788247,
    "input_throughput": 219.6440816414045,
    "output_throughput": 204.86371395958975,
    "total_throughput": 424.50779560099426,
    "itl": 20.502848803685584,
    "ttft": 7462.758778218266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900566,
    "arrivals": 3392,
    "finished_requests": 3385,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7144976803101599. Arrivals time: 0.021505686454474926 Scheduler time: 0.3276005322113633 Scheduler overhead time: 0.13336809119209647 Adapter cache time: 0.027617940213531256 Engine time: 0.1361688687466085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 1080, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 10530 . Total input tokens: 2309149 . Total output tokens: 2121671
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.7148724310100079,
    "estimated_duration": 3599.4368438788247,
    "input_throughput": 219.6440816414045,
    "output_throughput": 204.86371395958975,
    "total_throughput": 424.50779560099426,
    "itl": 20.50426079402577,
    "ttft": 7462.763548628145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 3392,
    "finished_requests": 3385,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7149451649747789. Arrivals time: 0.020957420114427805 Scheduler time: 0.326600918546319 Scheduler overhead time: 0.133684151340276 Adapter cache time: 0.02772185392677784 Engine time: 0.13760777516290545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 1080, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 10530 . Total input tokens: 2309149 . Total output tokens: 2121671
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.7912347461096942,
    "estimated_duration": 3599.4368438788247,
    "input_throughput": 219.6440816414045,
    "output_throughput": 204.86371395958975,
    "total_throughput": 424.50779560099426,
    "itl": 20.502798767275994,
    "ttft": 7462.756178268781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 3392,
    "finished_requests": 3385,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.791332851164043. Arrivals time: 0.02345203049480915 Scheduler time: 0.36251241248100996 Scheduler overhead time: 0.14820510428398848 Adapter cache time: 0.030519239138811827 Engine time: 0.15069015230983496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 1080, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 10530 . Total input tokens: 2309149 . Total output tokens: 2121671
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.7181713101454079,
    "estimated_duration": 3599.4368438788247,
    "input_throughput": 219.6440816414045,
    "output_throughput": 204.86371395958975,
    "total_throughput": 424.50779560099426,
    "itl": 20.50424766452477,
    "ttft": 7462.811749956138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 3392,
    "finished_requests": 3385,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.718256208114326. Arrivals time: 0.02168596675619483 Scheduler time: 0.32858472177758813 Scheduler overhead time: 0.13573800027370453 Adapter cache time: 0.027509473264217377 Engine time: 0.13621579622849822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 1080, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9855 . Total input tokens: 2162126 . Total output tokens: 1984484
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.7220630887895823,
    "estimated_duration": 3599.5122623836114,
    "input_throughput": 209.86621101264438,
    "output_throughput": 192.90780233102547,
    "total_throughput": 402.7740133436699,
    "itl": 20.35393751874097,
    "ttft": 6776.823213933545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 3203,
    "finished_requests": 3197,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7221796992234886. Arrivals time: 0.02166802017018199 Scheduler time: 0.32543606124818325 Scheduler overhead time: 0.1367640495300293 Adapter cache time: 0.027962460182607174 Engine time: 0.14049076242372394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 1080, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9855 . Total input tokens: 2162126 . Total output tokens: 1984484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.701396987773478,
    "estimated_duration": 3599.5122623836114,
    "input_throughput": 209.86621101264438,
    "output_throughput": 192.90780233102547,
    "total_throughput": 402.7740133436699,
    "itl": 20.354001009748966,
    "ttft": 6776.796287901714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 3203,
    "finished_requests": 3197,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7014848128892481. Arrivals time: 0.02129733469337225 Scheduler time: 0.3154275855049491 Scheduler overhead time: 0.1329981661401689 Adapter cache time: 0.027377639431506395 Engine time: 0.13639802299439907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 1080, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9855 . Total input tokens: 2162126 . Total output tokens: 1984484
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.7079735239967704,
    "estimated_duration": 3599.5122623836114,
    "input_throughput": 209.86621101264438,
    "output_throughput": 192.90780233102547,
    "total_throughput": 402.7740133436699,
    "itl": 20.354014299798852,
    "ttft": 6776.806858590217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 3203,
    "finished_requests": 3197,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7080492679961026. Arrivals time: 0.02103762375190854 Scheduler time: 0.32031122548505664 Scheduler overhead time: 0.13413346884772182 Adapter cache time: 0.02752302773296833 Engine time: 0.13643006281927228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 1080, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9855 . Total input tokens: 2162126 . Total output tokens: 1984484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6983035691082478,
    "estimated_duration": 3599.5122623836114,
    "input_throughput": 209.86621101264438,
    "output_throughput": 192.90780233102547,
    "total_throughput": 402.7740133436699,
    "itl": 20.35396545879223,
    "ttft": 6776.7678975934805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 3203,
    "finished_requests": 3197,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6984653710387647. Arrivals time: 0.021160052623599768 Scheduler time: 0.3148815487511456 Scheduler overhead time: 0.13373334240168333 Adapter cache time: 0.026870096568018198 Engine time: 0.13387376116588712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 1080, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9855 . Total input tokens: 2162126 . Total output tokens: 1984484
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6985265086404979,
    "estimated_duration": 3599.5122623836114,
    "input_throughput": 209.86621101264438,
    "output_throughput": 192.90780233102547,
    "total_throughput": 402.7740133436699,
    "itl": 20.354009087542593,
    "ttft": 6776.798710989763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 3203,
    "finished_requests": 3197,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6985810506157577. Arrivals time: 0.020156673155725002 Scheduler time: 0.3142929901368916 Scheduler overhead time: 0.1321719866245985 Adapter cache time: 0.02721549104899168 Engine time: 0.13730423664674163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 1080, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9855 . Total input tokens: 2162126 . Total output tokens: 1984484
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.7004024698399007,
    "estimated_duration": 3599.5122623836114,
    "input_throughput": 209.86621101264438,
    "output_throughput": 192.90780233102547,
    "total_throughput": 402.7740133436699,
    "itl": 20.353922766186688,
    "ttft": 6776.820473281781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 3203,
    "finished_requests": 3197,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7005705828778446. Arrivals time: 0.021450893487781286 Scheduler time: 0.3147103888913989 Scheduler overhead time: 0.13271881639957428 Adapter cache time: 0.027087752241641283 Engine time: 0.1365776751190424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 1080, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9855 . Total input tokens: 2162126 . Total output tokens: 1984484
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6924927029758692,
    "estimated_duration": 3599.5122623836114,
    "input_throughput": 209.86621101264438,
    "output_throughput": 192.90780233102547,
    "total_throughput": 402.7740133436699,
    "itl": 20.354003189578645,
    "ttft": 6776.791531940809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 3203,
    "finished_requests": 3197,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6926243412308395. Arrivals time: 0.020457282196730375 Scheduler time: 0.31227986281737685 Scheduler overhead time: 0.13174708792939782 Adapter cache time: 0.026600034907460213 Engine time: 0.13377223070710897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 1080, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9510 . Total input tokens: 2082665 . Total output tokens: 1917552
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6954284929670393,
    "estimated_duration": 3596.4133852063737,
    "input_throughput": 202.07354443440803,
    "output_throughput": 191.2875207365861,
    "total_throughput": 393.36106517099415,
    "itl": 20.31511502271955,
    "ttft": 8196.545560069206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 3087,
    "finished_requests": 3080,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.695532119832933. Arrivals time: 0.021036935970187187 Scheduler time: 0.312546590808779 Scheduler overhead time: 0.13220314495265484 Adapter cache time: 0.026202085427939892 Engine time: 0.13605265272781253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 1080, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9510 . Total input tokens: 2082665 . Total output tokens: 1917552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6927718631923199,
    "estimated_duration": 3596.4133852063737,
    "input_throughput": 202.07354443440803,
    "output_throughput": 191.2875207365861,
    "total_throughput": 393.36106517099415,
    "itl": 20.315174693699547,
    "ttft": 8196.571328607512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 3087,
    "finished_requests": 3080,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.692898188252002. Arrivals time: 0.02026898553594947 Scheduler time: 0.31071751471608877 Scheduler overhead time: 0.1324237948283553 Adapter cache time: 0.026481150183826685 Engine time: 0.13502764143049717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 1080, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9510 . Total input tokens: 2082665 . Total output tokens: 1917552
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6961339619010687,
    "estimated_duration": 3596.4133852063737,
    "input_throughput": 202.07354443440803,
    "output_throughput": 191.2875207365861,
    "total_throughput": 393.36106517099415,
    "itl": 20.315208617358785,
    "ttft": 8196.54650885504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 3087,
    "finished_requests": 3080,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6962279467843473. Arrivals time: 0.02093785023316741 Scheduler time: 0.3142242510803044 Scheduler overhead time: 0.13251946261152625 Adapter cache time: 0.02637588605284691 Engine time: 0.13418003637343645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 1080, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9510 . Total input tokens: 2082665 . Total output tokens: 1917552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6938833929598331,
    "estimated_duration": 3596.4133852063737,
    "input_throughput": 202.07354443440803,
    "output_throughput": 191.2875207365861,
    "total_throughput": 393.36106517099415,
    "itl": 20.315134767047354,
    "ttft": 8196.601531821143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 3087,
    "finished_requests": 3080,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6939643719233572. Arrivals time: 0.019880674313753843 Scheduler time: 0.3125230628065765 Scheduler overhead time: 0.13197121815755963 Adapter cache time: 0.026364183519035578 Engine time: 0.13586917705833912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 1080, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9510 . Total input tokens: 2082665 . Total output tokens: 1917552
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6929081473499537,
    "estimated_duration": 3596.4133852063737,
    "input_throughput": 202.07354443440803,
    "output_throughput": 191.2875207365861,
    "total_throughput": 393.36106517099415,
    "itl": 20.315198444825384,
    "ttft": 8196.555217029767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 3087,
    "finished_requests": 3080,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6929540643468499. Arrivals time: 0.020063295494765043 Scheduler time: 0.3121614516712725 Scheduler overhead time: 0.1314146565273404 Adapter cache time: 0.026152899488806725 Engine time: 0.135859708301723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 1080, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9510 . Total input tokens: 2082665 . Total output tokens: 1917552
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6927926288917661,
    "estimated_duration": 3596.4133852063737,
    "input_throughput": 202.07354443440803,
    "output_throughput": 191.2875207365861,
    "total_throughput": 393.36106517099415,
    "itl": 20.315099736715137,
    "ttft": 8196.539525435772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 3087,
    "finished_requests": 3080,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6928425938822329. Arrivals time: 0.020003024023026228 Scheduler time: 0.312459010630846 Scheduler overhead time: 0.1324678622186184 Adapter cache time: 0.02617257507517934 Engine time: 0.13402271550148726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 1080, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9510 . Total input tokens: 2082665 . Total output tokens: 1917552
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6930747237056494,
    "estimated_duration": 3596.4133852063737,
    "input_throughput": 202.07354443440803,
    "output_throughput": 191.2875207365861,
    "total_throughput": 393.36106517099415,
    "itl": 20.315180142543706,
    "ttft": 8196.569541631769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 3087,
    "finished_requests": 3080,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6931556477211416. Arrivals time: 0.019723345525562763 Scheduler time: 0.31220790510997176 Scheduler overhead time: 0.13163715694099665 Adapter cache time: 0.02632195968180895 Engine time: 0.13530348660424352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 1080, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9345 . Total input tokens: 2046080 . Total output tokens: 1885756
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6813589348457754,
    "estimated_duration": 3599.720001892809,
    "input_throughput": 198.74601347432906,
    "output_throughput": 179.73092342176716,
    "total_throughput": 378.4769368960962,
    "itl": 20.310449351470417,
    "ttft": 5989.462473012483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 3022,
    "finished_requests": 3017,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6814076779410243. Arrivals time: 0.019852486439049244 Scheduler time: 0.29932681657373905 Scheduler overhead time: 0.13094061380252242 Adapter cache time: 0.026023030746728182 Engine time: 0.13765738671645522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 1080, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9345 . Total input tokens: 2046080 . Total output tokens: 1885756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.677012586966157,
    "estimated_duration": 3599.720001892809,
    "input_throughput": 198.74601347432906,
    "output_throughput": 179.73092342176716,
    "total_throughput": 378.4769368960962,
    "itl": 20.310485767202916,
    "ttft": 5989.45331505086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 3022,
    "finished_requests": 3017,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6770602967590094. Arrivals time: 0.019501171074807644 Scheduler time: 0.29894987447187304 Scheduler overhead time: 0.13081151340156794 Adapter cache time: 0.025915142614394426 Engine time: 0.13428983138874173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 1080, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9345 . Total input tokens: 2046080 . Total output tokens: 1885756
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6891898601315916,
    "estimated_duration": 3599.720001892809,
    "input_throughput": 198.74601347432906,
    "output_throughput": 179.73092342176716,
    "total_throughput": 378.4769368960962,
    "itl": 20.310500760530818,
    "ttft": 5989.457224654671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 3022,
    "finished_requests": 3017,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6892438600771129. Arrivals time: 0.01949659874662757 Scheduler time: 0.3009899281896651 Scheduler overhead time: 0.1322371419519186 Adapter cache time: 0.02614592667669058 Engine time: 0.14275521878153086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 1080, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9345 . Total input tokens: 2046080 . Total output tokens: 1885756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6783700259402394,
    "estimated_duration": 3599.720001892809,
    "input_throughput": 198.74601347432906,
    "output_throughput": 179.73092342176716,
    "total_throughput": 378.4769368960962,
    "itl": 20.310463875957254,
    "ttft": 5989.450241243504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 3022,
    "finished_requests": 3017,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6784177888184786. Arrivals time: 0.01965714432299137 Scheduler time: 0.2992375041358173 Scheduler overhead time: 0.1311930981464684 Adapter cache time: 0.025937324855476618 Engine time: 0.13517669634893537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 1080, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9345 . Total input tokens: 2046080 . Total output tokens: 1885756
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6788562620058656,
    "estimated_duration": 3599.720001892809,
    "input_throughput": 198.74601347432906,
    "output_throughput": 179.73092342176716,
    "total_throughput": 378.4769368960962,
    "itl": 20.310496984511293,
    "ttft": 5989.454609853397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 3022,
    "finished_requests": 3017,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6789080719463527. Arrivals time: 0.019975808449089527 Scheduler time: 0.29830023320391774 Scheduler overhead time: 0.13199266651645303 Adapter cache time: 0.025932359974831343 Engine time: 0.13523503253236413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 1080, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9345 . Total input tokens: 2046080 . Total output tokens: 1885756
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6767295459285378,
    "estimated_duration": 3599.720001892809,
    "input_throughput": 198.74601347432906,
    "output_throughput": 179.73092342176716,
    "total_throughput": 378.4769368960962,
    "itl": 20.310440151340135,
    "ttft": 5989.484975965446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 3022,
    "finished_requests": 3017,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.67679677112028. Arrivals time: 0.019952410832047462 Scheduler time: 0.29860154166817665 Scheduler overhead time: 0.13145718723535538 Adapter cache time: 0.025685144122689962 Engine time: 0.13350881868973374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 1080, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 1080]
Prompts retrieved: 9345 . Total input tokens: 2046080 . Total output tokens: 1885756
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6926515609957278,
    "estimated_duration": 3599.720001892809,
    "input_throughput": 198.74601347432906,
    "output_throughput": 179.73092342176716,
    "total_throughput": 378.4769368960962,
    "itl": 20.31049084405793,
    "ttft": 5989.452812483747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 3022,
    "finished_requests": 3017,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6927036019042134. Arrivals time: 0.020048076286911964 Scheduler time: 0.3029178539291024 Scheduler overhead time: 0.13219174975529313 Adapter cache time: 0.02629942586645484 Engine time: 0.14320527808740735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 1080, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8505 . Total input tokens: 1864775 . Total output tokens: 1728866
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6636351966299117,
    "estimated_duration": 3597.782962438285,
    "input_throughput": 184.12144560022583,
    "output_throughput": 164.95825517995806,
    "total_throughput": 349.0797007801839,
    "itl": 20.21061353677189,
    "ttft": 9216.785344875005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 2744,
    "finished_requests": 2737,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.663700541947037. Arrivals time: 0.01933348271995783 Scheduler time: 0.28475367091596127 Scheduler overhead time: 0.13252532621845603 Adapter cache time: 0.025624072179198265 Engine time: 0.13431442342698574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 1080, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8505 . Total input tokens: 1864775 . Total output tokens: 1728866
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6611585943028331,
    "estimated_duration": 3597.782962438285,
    "input_throughput": 184.12144560022583,
    "output_throughput": 164.95825517995806,
    "total_throughput": 349.0797007801839,
    "itl": 20.2106985024137,
    "ttft": 9216.796514551128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 2744,
    "finished_requests": 2737,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6612869231030345. Arrivals time: 0.019906114786863327 Scheduler time: 0.283688488882035 Scheduler overhead time: 0.131484049372375 Adapter cache time: 0.0258666118606925 Engine time: 0.13335752533748746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 1080, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8505 . Total input tokens: 1864775 . Total output tokens: 1728866
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6652475660666823,
    "estimated_duration": 3597.782962438285,
    "input_throughput": 184.12144560022583,
    "output_throughput": 164.95825517995806,
    "total_throughput": 349.0797007801839,
    "itl": 20.210652570400246,
    "ttft": 9216.795893882656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 2744,
    "finished_requests": 2737,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6653901860117912. Arrivals time: 0.02015673415735364 Scheduler time: 0.2839575670659542 Scheduler overhead time: 0.13285606680437922 Adapter cache time: 0.025768693070858717 Engine time: 0.13496498856693506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 1080, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8505 . Total input tokens: 1864775 . Total output tokens: 1728866
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6655285856686532,
    "estimated_duration": 3597.782962438285,
    "input_throughput": 184.12144560022583,
    "output_throughput": 164.95825517995806,
    "total_throughput": 349.0797007801839,
    "itl": 20.21067504659062,
    "ttft": 9216.793993691443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 2744,
    "finished_requests": 2737,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6656517889350653. Arrivals time: 0.02003360167145729 Scheduler time: 0.28491168143227696 Scheduler overhead time: 0.13169951364398003 Adapter cache time: 0.025649569928646088 Engine time: 0.1357734389603138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 1080, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8505 . Total input tokens: 1864775 . Total output tokens: 1728866
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6896160431206226,
    "estimated_duration": 3597.782962438285,
    "input_throughput": 184.12144560022583,
    "output_throughput": 164.95825517995806,
    "total_throughput": 349.0797007801839,
    "itl": 20.21064633640292,
    "ttft": 9216.80733949484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 2744,
    "finished_requests": 2737,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6897415197454393. Arrivals time: 0.020788541063666344 Scheduler time: 0.3032074528746307 Scheduler overhead time: 0.13555632159113884 Adapter cache time: 0.025692669674754143 Engine time: 0.13656123401597142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 1080, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8505 . Total input tokens: 1864775 . Total output tokens: 1728866
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6706235059536994,
    "estimated_duration": 3597.782962438285,
    "input_throughput": 184.12144560022583,
    "output_throughput": 164.95825517995806,
    "total_throughput": 349.0797007801839,
    "itl": 20.21060081966382,
    "ttft": 9216.769013121433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 2744,
    "finished_requests": 2737,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6707634348422289. Arrivals time: 0.02033212687820196 Scheduler time: 0.2885498786345124 Scheduler overhead time: 0.13129095872864127 Adapter cache time: 0.026268605142831802 Engine time: 0.13655860582366586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 1080, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8505 . Total input tokens: 1864775 . Total output tokens: 1728866
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6647252282127738,
    "estimated_duration": 3597.782962438285,
    "input_throughput": 184.12144560022583,
    "output_throughput": 164.95825517995806,
    "total_throughput": 349.0797007801839,
    "itl": 20.210701452901382,
    "ttft": 9216.807128769726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 2744,
    "finished_requests": 2737,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6648166459053755. Arrivals time: 0.019881834741681814 Scheduler time: 0.2835607985034585 Scheduler overhead time: 0.13156382599845529 Adapter cache time: 0.025768577586859465 Engine time: 0.13667497923597693 
