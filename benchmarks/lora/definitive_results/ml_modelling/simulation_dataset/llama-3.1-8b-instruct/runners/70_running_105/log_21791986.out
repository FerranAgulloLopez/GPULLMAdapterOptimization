INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.8796342280693352,
    "estimated_duration": 3600.0190560737574,
    "input_throughput": 3803.404589456004,
    "output_throughput": 3288.4903706347072,
    "total_throughput": 7091.894960090712,
    "itl": 55.23307267062965,
    "ttft": 12984.011183857425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.124101849056462,
    "arrivals": 55037,
    "finished_requests": 54840,
    "scheduler_time": 36.331536654556814
}
#Debug simulation 
Total elapsed time: 3.8797688111662865. Arrivals time: 0.12875578692182899 Scheduler time: 3.5173917007632554 Scheduler overhead time: 0.07128852000460029 Adapter cache time: 0.05758710764348507 Engine time: 0.07101117726415396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.8653721450828016,
    "estimated_duration": 3600.0579494431795,
    "input_throughput": 3803.3634992230586,
    "output_throughput": 3288.4548432980305,
    "total_throughput": 7091.818342521089,
    "itl": 55.337647433549606,
    "ttft": 12984.400212956198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.95087741847993,
    "arrivals": 55037,
    "finished_requests": 54840,
    "scheduler_time": 36.354086639528106
}
#Debug simulation 
Total elapsed time: 3.865454776212573. Arrivals time: 0.1285825064405799 Scheduler time: 3.5032109110616148 Scheduler overhead time: 0.07120679318904877 Adapter cache time: 0.05769685236737132 Engine time: 0.07125333277508616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.8461243738420308,
    "estimated_duration": 3600.032747432829,
    "input_throughput": 3803.390124649269,
    "output_throughput": 3288.4778641088988,
    "total_throughput": 7091.867988758168,
    "itl": 55.36508210668351,
    "ttft": 12984.429553214628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4099,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.781556868405485,
    "arrivals": 55037,
    "finished_requests": 54840,
    "scheduler_time": 36.36013250575529
}
#Debug simulation 
Total elapsed time: 3.846207122784108. Arrivals time: 0.12841092003509402 Scheduler time: 3.484895625151694 Scheduler overhead time: 0.0707395737990737 Adapter cache time: 0.057817361783236265 Engine time: 0.07083925185725093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.8478451520204544,
    "estimated_duration": 3600.055290613853,
    "input_throughput": 3803.366308206142,
    "output_throughput": 3288.457271994112,
    "total_throughput": 7091.823580200255,
    "itl": 55.26760579052849,
    "ttft": 12984.104841750697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.079546068772554,
    "arrivals": 55037,
    "finished_requests": 54840,
    "scheduler_time": 36.33962755653274
}
#Debug simulation 
Total elapsed time: 3.847916205879301. Arrivals time: 0.12832451658323407 Scheduler time: 3.4847452831454575 Scheduler overhead time: 0.07220551837235689 Adapter cache time: 0.057539223693311214 Engine time: 0.07149547897279263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.8276160690002143,
    "estimated_duration": 3600.0320528955035,
    "input_throughput": 3803.3908584195156,
    "output_throughput": 3288.4784985395336,
    "total_throughput": 7091.869356959049,
    "itl": 55.35544294559895,
    "ttft": 12984.499508018578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4097,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.48294876173735,
    "arrivals": 55037,
    "finished_requests": 54840,
    "scheduler_time": 36.357827527363526
}
#Debug simulation 
Total elapsed time: 3.8276894772425294. Arrivals time: 0.12770385295152664 Scheduler time: 3.466538416687399 Scheduler overhead time: 0.0710478825494647 Adapter cache time: 0.057572809047997 Engine time: 0.07063106494024396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.861426714807749,
    "estimated_duration": 3600.0466098350753,
    "input_throughput": 3803.37547924894,
    "output_throughput": 3288.4652014386975,
    "total_throughput": 7091.840680687637,
    "itl": 55.19606335628313,
    "ttft": 12983.863105786437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.18045977728214,
    "arrivals": 55037,
    "finished_requests": 54840,
    "scheduler_time": 36.32460835974193
}
#Debug simulation 
Total elapsed time: 3.861498183105141. Arrivals time: 0.1283993129618466 Scheduler time: 3.4999179136939347 Scheduler overhead time: 0.0711176279000938 Adapter cache time: 0.05779550829902291 Engine time: 0.07067036023363471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 270, 4320, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 270, 540, 540, 540, 270, 270, 4320, 540, 540, 270, 4320, 4320, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 4320, 270, 270, 540, 270, 4320, 270, 270, 4320, 270, 540, 540, 270, 540, 4320, 4320, 4320, 540, 4320, 4320, 270, 270, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 4320, 270, 270, 4320, 270, 270, 540]
Prompts retrieved: 164160 . Total input tokens: 36601301 . Total output tokens: 32243492
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.9033567872829735,
    "estimated_duration": 3600.0000556398654,
    "input_throughput": 3803.424663438323,
    "output_throughput": 3288.5077269521867,
    "total_throughput": 7091.93239039051,
    "itl": 55.34572582413353,
    "ttft": 12984.41878436496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4099,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.212554115263178,
    "arrivals": 55037,
    "finished_requests": 54840,
    "scheduler_time": 36.35527999509192
}
#Debug simulation 
Total elapsed time: 3.903429120313376. Arrivals time: 0.12952607683837414 Scheduler time: 3.5386586314998567 Scheduler overhead time: 0.0713497893884778 Adapter cache time: 0.05819708248600364 Engine time: 0.07218270469456911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.7932509728707373,
    "estimated_duration": 3600.0168980261938,
    "input_throughput": 3684.1254848752933,
    "output_throughput": 3198.8702626129198,
    "total_throughput": 6882.995747488213,
    "itl": 50.2411975221635,
    "ttft": 10248.14640667978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.641907297242494,
    "arrivals": 53479,
    "finished_requests": 53327,
    "scheduler_time": 33.809782851288055
}
#Debug simulation 
Total elapsed time: 3.7933403439819813. Arrivals time: 0.12884706258773804 Scheduler time: 3.4235052606090903 Scheduler overhead time: 0.07621993869543076 Adapter cache time: 0.05191203625872731 Engine time: 0.07688053091987967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.781321105081588,
    "estimated_duration": 3600.0286123096594,
    "input_throughput": 3684.1134969455015,
    "output_throughput": 3198.859853675364,
    "total_throughput": 6882.973350620865,
    "itl": 50.29560579452856,
    "ttft": 10315.583665498316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.49334300200411,
    "arrivals": 53479,
    "finished_requests": 53327,
    "scheduler_time": 33.823942020962086
}
#Debug simulation 
Total elapsed time: 3.7814231528900564. Arrivals time: 0.12709886534139514 Scheduler time: 3.414780890569091 Scheduler overhead time: 0.07603365182876587 Adapter cache time: 0.051866126246750355 Engine time: 0.0756486370228231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.787784659769386,
    "estimated_duration": 3600.047312098802,
    "input_throughput": 3684.0943604898944,
    "output_throughput": 3198.8432377813006,
    "total_throughput": 6882.9375982711945,
    "itl": 50.312343228520476,
    "ttft": 10315.564639660348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.01215068799919,
    "arrivals": 53479,
    "finished_requests": 53327,
    "scheduler_time": 33.82804285896897
}
#Debug simulation 
Total elapsed time: 3.787860923912376. Arrivals time: 0.12771538319066167 Scheduler time: 3.42172264540568 Scheduler overhead time: 0.07573580835014582 Adapter cache time: 0.05161474784836173 Engine time: 0.07524622790515423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.795471165329218,
    "estimated_duration": 3600.037265677775,
    "input_throughput": 3684.1049192592195,
    "output_throughput": 3198.910775117173,
    "total_throughput": 6883.015694376392,
    "itl": 50.25732876739128,
    "ttft": 10248.11142064593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.26976314059034,
    "arrivals": 53479,
    "finished_requests": 53328,
    "scheduler_time": 33.81467007019336
}
#Debug simulation 
Total elapsed time: 3.7955452082678676. Arrivals time: 0.12755523389205337 Scheduler time: 3.427087939810008 Scheduler overhead time: 0.07633322570472956 Adapter cache time: 0.05182218737900257 Engine time: 0.07672859029844403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.803140335716307,
    "estimated_duration": 3600.045715948908,
    "input_throughput": 3684.095993904381,
    "output_throughput": 3198.8446560503166,
    "total_throughput": 6882.940649954698,
    "itl": 50.30624232672856,
    "ttft": 10315.517573927518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.83672094289221,
    "arrivals": 53479,
    "finished_requests": 53327,
    "scheduler_time": 33.82664120053126
}
#Debug simulation 
Total elapsed time: 3.8032138566486537. Arrivals time: 0.1268054274842143 Scheduler time: 3.4357671248726547 Scheduler overhead time: 0.07647258369252086 Adapter cache time: 0.05184038542211056 Engine time: 0.07621540175750852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.811874284874648,
    "estimated_duration": 3600.0364119750107,
    "input_throughput": 3684.105792897759,
    "output_throughput": 3198.911533698104,
    "total_throughput": 6883.017326595863,
    "itl": 50.22332811859598,
    "ttft": 10248.231352617886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2669,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.038684990383917,
    "arrivals": 53479,
    "finished_requests": 53328,
    "scheduler_time": 33.805388361836336
}
#Debug simulation 
Total elapsed time: 3.811974043957889. Arrivals time: 0.129602640401572 Scheduler time: 3.4404902197420597 Scheduler overhead time: 0.07645668042823672 Adapter cache time: 0.05225702794268727 Engine time: 0.07696555135771632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 135, 4320, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 135, 540, 540, 540, 135, 135, 4320, 540, 540, 135, 4320, 4320, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 4320, 135, 135, 540, 135, 4320, 135, 135, 4320, 135, 540, 540, 135, 540, 4320, 4320, 4320, 540, 4320, 4320, 135, 135, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 4320, 135, 135, 4320, 135, 135, 540]
Prompts retrieved: 159840 . Total input tokens: 35649766 . Total output tokens: 31399264
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.7964674062095582,
    "estimated_duration": 3600.01314286388,
    "input_throughput": 3684.1293277749246,
    "output_throughput": 3198.873599344365,
    "total_throughput": 6883.00292711929,
    "itl": 50.30014757292532,
    "ttft": 10248.381164559389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.64477668566635,
    "arrivals": 53479,
    "finished_requests": 53327,
    "scheduler_time": 33.824863035940226
}
#Debug simulation 
Total elapsed time: 3.7965743532404304. Arrivals time: 0.12824421701952815 Scheduler time: 3.427577913273126 Scheduler overhead time: 0.07616650266572833 Adapter cache time: 0.05208289157599211 Engine time: 0.07662064069882035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.7229816326871514,
    "estimated_duration": 3600.028393389722,
    "input_throughput": 3643.8298720328785,
    "output_throughput": 3139.6374597361164,
    "total_throughput": 6783.467331768995,
    "itl": 48.163803212735615,
    "ttft": 13438.423567073965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1844,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.19328225491601,
    "arrivals": 52821,
    "finished_requests": 52624,
    "scheduler_time": 32.42225517097706
}
#Debug simulation 
Total elapsed time: 3.723059414885938. Arrivals time: 0.12578138569369912 Scheduler time: 3.3562255413271487 Scheduler overhead time: 0.07848345721140504 Adapter cache time: 0.04731694655492902 Engine time: 0.07818830385804176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.7244611149653792,
    "estimated_duration": 3600.0274638403143,
    "input_throughput": 3643.8308128923395,
    "output_throughput": 3139.6382704099715,
    "total_throughput": 6783.4690833023105,
    "itl": 48.19389070030051,
    "ttft": 13438.4167062033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1847,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.445861377422522,
    "arrivals": 52821,
    "finished_requests": 52624,
    "scheduler_time": 32.42998882224995
}
#Debug simulation 
Total elapsed time: 3.7245330652222037. Arrivals time: 0.12576008727774024 Scheduler time: 3.3572333827614784 Scheduler overhead time: 0.07815725263208151 Adapter cache time: 0.04778805188834667 Engine time: 0.07860824884846807 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.7428107103332877,
    "estimated_duration": 3600.048782840029,
    "input_throughput": 3643.809234621392,
    "output_throughput": 3139.61967789875,
    "total_throughput": 6783.428912520142,
    "itl": 48.20898907546569,
    "ttft": 13506.457146701516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1845,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.78108718595445,
    "arrivals": 52821,
    "finished_requests": 52624,
    "scheduler_time": 32.4343235219963
}
#Debug simulation 
Total elapsed time: 3.7428843802772462. Arrivals time: 0.12707425002008677 Scheduler time: 3.373227753210813 Scheduler overhead time: 0.07853364665061235 Adapter cache time: 0.047587116714566946 Engine time: 0.0791655876673758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.7251248648390174,
    "estimated_duration": 3600.0419408802763,
    "input_throughput": 3643.960324748969,
    "output_throughput": 3139.8109204350267,
    "total_throughput": 6783.771245183995,
    "itl": 48.170493379558344,
    "ttft": 13438.108071181254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.623203524174334,
    "arrivals": 52821,
    "finished_requests": 52625,
    "scheduler_time": 32.42396359455736
}
#Debug simulation 
Total elapsed time: 3.725195409730077. Arrivals time: 0.1256894045509398 Scheduler time: 3.358075321651995 Scheduler overhead time: 0.07820950774475932 Adapter cache time: 0.04791402816772461 Engine time: 0.07786747114732862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.741057924926281,
    "estimated_duration": 3600.030219481815,
    "input_throughput": 3643.8280237236945,
    "output_throughput": 3139.6358671752796,
    "total_throughput": 6783.463890898974,
    "itl": 48.198023454276225,
    "ttft": 13438.310815985946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1845,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.662719898894208,
    "arrivals": 52821,
    "finished_requests": 52624,
    "scheduler_time": 32.43154729735626
}
#Debug simulation 
Total elapsed time: 3.741129456087947. Arrivals time: 0.12491712672635913 Scheduler time: 3.373259801417589 Scheduler overhead time: 0.07843045052140951 Adapter cache time: 0.047802687622606754 Engine time: 0.07943387236446142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.757052489090711,
    "estimated_duration": 3600.058354235889,
    "input_throughput": 3643.9437112358632,
    "output_throughput": 3139.796605435623,
    "total_throughput": 6783.740316671487,
    "itl": 48.14691664196806,
    "ttft": 13438.029209931945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1842,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.759182372531383,
    "arrivals": 52821,
    "finished_requests": 52625,
    "scheduler_time": 32.41764564626491
}
#Debug simulation 
Total elapsed time: 3.7571447119116783. Arrivals time: 0.12733303289860487 Scheduler time: 3.3858635663054883 Scheduler overhead time: 0.07902157725766301 Adapter cache time: 0.04780660895630717 Engine time: 0.07944826036691666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 66, 4320, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 66, 540, 540, 540, 66, 66, 4320, 540, 540, 66, 4320, 4320, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 4320, 66, 66, 540, 66, 4320, 66, 66, 4320, 66, 540, 540, 66, 540, 4320, 4320, 4320, 540, 4320, 4320, 66, 66, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 4320, 66, 66, 4320, 66, 66, 540]
Prompts retrieved: 157632 . Total input tokens: 35157081 . Total output tokens: 30969386
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.746440581046045,
    "estimated_duration": 3600.0282205062263,
    "input_throughput": 3643.830047019853,
    "output_throughput": 3139.6376105103514,
    "total_throughput": 6783.467657530204,
    "itl": 48.19573870161568,
    "ttft": 13438.574957756955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1844,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.528250478803892,
    "arrivals": 52821,
    "finished_requests": 52624,
    "scheduler_time": 32.430540265276015
}
#Debug simulation 
Total elapsed time: 3.7465204489417374. Arrivals time: 0.12604855746030807 Scheduler time: 3.3786190054379404 Scheduler overhead time: 0.07805467303842306 Adapter cache time: 0.04768526600673795 Engine time: 0.07864669198170304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.740776088088751,
    "estimated_duration": 3600.041876378283,
    "input_throughput": 3616.3407113189924,
    "output_throughput": 3137.9301652359372,
    "total_throughput": 6754.27087655493,
    "itl": 47.23591507473576,
    "ttft": 10369.58036889475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.399285706752151,
    "arrivals": 52471,
    "finished_requests": 52321,
    "scheduler_time": 32.09677991039793
}
#Debug simulation 
Total elapsed time: 3.7408484080806375. Arrivals time: 0.12474999204277992 Scheduler time: 3.3753243680112064 Scheduler overhead time: 0.07904036249965429 Adapter cache time: 0.04458818770945072 Engine time: 0.07961016753688455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.7368785236030817,
    "estimated_duration": 3600.0089252962,
    "input_throughput": 3616.3738118868223,
    "output_throughput": 3137.958886885409,
    "total_throughput": 6754.332698772231,
    "itl": 47.25284657484062,
    "ttft": 10301.254454514303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.10897829287686,
    "arrivals": 52471,
    "finished_requests": 52321,
    "scheduler_time": 32.10171544501894
}
#Debug simulation 
Total elapsed time: 3.7369786738418043. Arrivals time: 0.12566040363162756 Scheduler time: 3.3688415959477425 Scheduler overhead time: 0.07977660978212953 Adapter cache time: 0.04469011165201664 Engine time: 0.0802614321000874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.791694703977555,
    "estimated_duration": 3600.025850255587,
    "input_throughput": 3616.356810070046,
    "output_throughput": 3137.9441342616974,
    "total_throughput": 6754.300944331743,
    "itl": 47.258872453314325,
    "ttft": 10369.752177090475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.31306015242356,
    "arrivals": 52471,
    "finished_requests": 52321,
    "scheduler_time": 32.10345701261998
}
#Debug simulation 
Total elapsed time: 3.791785975918174. Arrivals time: 0.1272855824790895 Scheduler time: 3.4215474417433143 Scheduler overhead time: 0.07958698319271207 Adapter cache time: 0.04467831505462527 Engine time: 0.0809093196876347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.7362421778962016,
    "estimated_duration": 3600.021471349027,
    "input_throughput": 3616.361208846188,
    "output_throughput": 3137.9479511178647,
    "total_throughput": 6754.309159964053,
    "itl": 47.24106911151904,
    "ttft": 10369.695118774469,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.623658136143336,
    "arrivals": 52471,
    "finished_requests": 52321,
    "scheduler_time": 32.09822044580261
}
#Debug simulation 
Total elapsed time: 3.7363149132579565. Arrivals time: 0.12522039143368602 Scheduler time: 3.3676492744125426 Scheduler overhead time: 0.08043132442981005 Adapter cache time: 0.044731419533491135 Engine time: 0.08049667626619339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.7327495152130723,
    "estimated_duration": 3600.021705558975,
    "input_throughput": 3616.3609735732257,
    "output_throughput": 3137.947746969477,
    "total_throughput": 6754.308720542703,
    "itl": 47.25615303475271,
    "ttft": 10369.678980703542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.243313532886093,
    "arrivals": 52471,
    "finished_requests": 52321,
    "scheduler_time": 32.10284484904407
}
#Debug simulation 
Total elapsed time: 3.732825192157179. Arrivals time: 0.12583933677524328 Scheduler time: 3.3654170515947044 Scheduler overhead time: 0.07930591236799955 Adapter cache time: 0.04462656797841191 Engine time: 0.07986426493152976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.725102046970278,
    "estimated_duration": 3600.0040080344056,
    "input_throughput": 3616.378751508206,
    "output_throughput": 3137.9631730376773,
    "total_throughput": 6754.341924545883,
    "itl": 47.22907130138704,
    "ttft": 10301.261642313682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.1436075324983905,
    "arrivals": 52471,
    "finished_requests": 52321,
    "scheduler_time": 32.09457412071694
}
#Debug simulation 
Total elapsed time: 3.72520216088742. Arrivals time: 0.12554831011220813 Scheduler time: 3.35639930004254 Scheduler overhead time: 0.08034511329606175 Adapter cache time: 0.044818149879574776 Engine time: 0.07984013855457306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 4320, 540, 540, 540, 540, 4320, 540, 4320, 540, 33, 4320, 33, 33, 4320, 540, 4320, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 33, 540, 540, 540, 33, 33, 4320, 540, 540, 33, 4320, 4320, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 4320, 33, 33, 540, 33, 4320, 33, 33, 4320, 33, 540, 540, 33, 540, 4320, 4320, 4320, 540, 4320, 4320, 33, 33, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 4320, 33, 33, 4320, 33, 33, 540]
Prompts retrieved: 156576 . Total input tokens: 34912786 . Total output tokens: 30776050
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.718257891945541,
    "estimated_duration": 3600.021681669429,
    "input_throughput": 3616.3609975711984,
    "output_throughput": 3137.9477677927257,
    "total_throughput": 6754.308765363924,
    "itl": 47.25535549289479,
    "ttft": 10369.672897861034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.169786935187878,
    "arrivals": 52471,
    "finished_requests": 52321,
    "scheduler_time": 32.102263822510864
}
#Debug simulation 
Total elapsed time: 3.7183586098253727. Arrivals time: 0.12403519684448838 Scheduler time: 3.352151465602219 Scheduler overhead time: 0.07882777974009514 Adapter cache time: 0.04474150435999036 Engine time: 0.08082999289035797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.686599767766893,
    "estimated_duration": 3599.911743017063,
    "input_throughput": 3491.5964327129955,
    "output_throughput": 3061.6725594396767,
    "total_throughput": 6553.268992152673,
    "itl": 44.856491155295515,
    "ttft": 10808.565771106194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2038,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.47608960711434,
    "arrivals": 50642,
    "finished_requests": 50491,
    "scheduler_time": 30.27046089787204
}
#Debug simulation 
Total elapsed time: 3.686684861779213. Arrivals time: 0.12441697809845209 Scheduler time: 3.3101288308389485 Scheduler overhead time: 0.08329589664936066 Adapter cache time: 0.04468912165611982 Engine time: 0.08475317899137735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.6547660348005593,
    "estimated_duration": 3599.9254114194405,
    "input_throughput": 3491.723178520998,
    "output_throughput": 3061.668434861861,
    "total_throughput": 6553.391613382859,
    "itl": 44.890442803780935,
    "ttft": 10737.564737833722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.946168097606565,
    "arrivals": 50642,
    "finished_requests": 50492,
    "scheduler_time": 30.281708132692547
}
#Debug simulation 
Total elapsed time: 3.6548430919647217. Arrivals time: 0.12132359482347965 Scheduler time: 3.282768963370472 Scheduler overhead time: 0.08325465582311153 Adapter cache time: 0.04465806530788541 Engine time: 0.08336856821551919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.659242618829012,
    "estimated_duration": 3599.9425827653376,
    "input_throughput": 3491.751801870164,
    "output_throughput": 3061.7152208948078,
    "total_throughput": 6553.467022764971,
    "itl": 44.90066657913585,
    "ttft": 10666.555814774005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.360742982406766,
    "arrivals": 50642,
    "finished_requests": 50493,
    "scheduler_time": 30.285023617504994
}
#Debug simulation 
Total elapsed time: 3.659315972123295. Arrivals time: 0.12459336314350367 Scheduler time: 3.285210377071053 Scheduler overhead time: 0.08274666871875525 Adapter cache time: 0.044807431288063526 Engine time: 0.0823987596668303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.6694753929041326,
    "estimated_duration": 3599.9250603469004,
    "input_throughput": 3491.76879776178,
    "output_throughput": 3061.7301236092635,
    "total_throughput": 6553.498921371043,
    "itl": 44.8672179460431,
    "ttft": 10666.364657549464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.9660840904482,
    "arrivals": 50642,
    "finished_requests": 50493,
    "scheduler_time": 30.274376605553787
}
#Debug simulation 
Total elapsed time: 3.669562974013388. Arrivals time: 0.12317004194483161 Scheduler time: 3.2935282993130386 Scheduler overhead time: 0.08345575490966439 Adapter cache time: 0.04488722560927272 Engine time: 0.08496350096538663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.652750973124057,
    "estimated_duration": 3599.9453224445915,
    "input_throughput": 3491.749144529812,
    "output_throughput": 3061.7128908267314,
    "total_throughput": 6553.462035356543,
    "itl": 44.90286073789243,
    "ttft": 10666.501808917375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2039,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.230463285287508,
    "arrivals": 50642,
    "finished_requests": 50493,
    "scheduler_time": 30.285652105601198
}
#Debug simulation 
Total elapsed time: 3.6528320661745965. Arrivals time: 0.12156233470886946 Scheduler time: 3.2800824819132686 Scheduler overhead time: 0.083401741925627 Adapter cache time: 0.044534694869071245 Engine time: 0.08359125256538391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.654393816832453,
    "estimated_duration": 3599.9127500321065,
    "input_throughput": 3491.5954559976203,
    "output_throughput": 3061.671702988274,
    "total_throughput": 6553.267158985895,
    "itl": 44.84438175480306,
    "ttft": 10808.614853919944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2038,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.010430876883374,
    "arrivals": 50642,
    "finished_requests": 50491,
    "scheduler_time": 30.26701043997376
}
#Debug simulation 
Total elapsed time: 3.6544676437042654. Arrivals time: 0.12155221961438656 Scheduler time: 3.2829266525804996 Scheduler overhead time: 0.08303075563162565 Adapter cache time: 0.04473317414522171 Engine time: 0.08261266071349382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 135, 4320, 135, 135, 4320, 270, 4320, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 135, 270, 270, 270, 135, 135, 4320, 270, 270, 135, 4320, 4320, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 4320, 135, 135, 270, 135, 4320, 135, 135, 4320, 135, 270, 270, 135, 270, 4320, 4320, 4320, 270, 4320, 4320, 135, 135, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 4320, 135, 135, 4320, 135, 135, 270]
Prompts retrieved: 151200 . Total input tokens: 33684514 . Total output tokens: 29744448
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.6648907880298793,
    "estimated_duration": 3599.950551390358,
    "input_throughput": 3491.744072747117,
    "output_throughput": 3061.708443673797,
    "total_throughput": 6553.452516420914,
    "itl": 44.8974029857213,
    "ttft": 10666.436580412399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.070986052322805,
    "arrivals": 50642,
    "finished_requests": 50493,
    "scheduler_time": 30.284540742497907
}
#Debug simulation 
Total elapsed time: 3.664964696392417. Arrivals time: 0.12265539215877652 Scheduler time: 3.2897950764745474 Scheduler overhead time: 0.08355819992721081 Adapter cache time: 0.044731998816132545 Engine time: 0.08473212225362659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.566455132327974,
    "estimated_duration": 3599.9643342437766,
    "input_throughput": 3443.93773073432,
    "output_throughput": 2991.1776896150827,
    "total_throughput": 6435.115420349403,
    "itl": 42.98555797891876,
    "ttft": 9156.043154867055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.197860963442592,
    "arrivals": 49929,
    "finished_requests": 49803,
    "scheduler_time": 28.62562540578127
}
#Debug simulation 
Total elapsed time: 3.5665419241413474. Arrivals time: 0.12199913756921887 Scheduler time: 3.192889824975282 Scheduler overhead time: 0.08488351106643677 Adapter cache time: 0.04124528914690018 Engine time: 0.08517868397757411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.6514142607338727,
    "estimated_duration": 3599.928517777581,
    "input_throughput": 3443.971995214491,
    "output_throughput": 2991.2074494878348,
    "total_throughput": 6435.179444702326,
    "itl": 43.00640157663516,
    "ttft": 9156.19152701567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.156366286734047,
    "arrivals": 49929,
    "finished_requests": 49803,
    "scheduler_time": 28.632640555477522
}
#Debug simulation 
Total elapsed time: 3.6515225060284138. Arrivals time: 0.12932849442586303 Scheduler time: 3.2667670748196542 Scheduler overhead time: 0.08631817251443863 Adapter cache time: 0.04168976517394185 Engine time: 0.08641234505921602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.6018308540806174,
    "estimated_duration": 3599.9363967373056,
    "input_throughput": 3443.9644576044743,
    "output_throughput": 2991.200902815776,
    "total_throughput": 6435.16536042025,
    "itl": 43.0120119035922,
    "ttft": 9156.151856734468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.443881549867703,
    "arrivals": 49929,
    "finished_requests": 49803,
    "scheduler_time": 28.63484783957923
}
#Debug simulation 
Total elapsed time: 3.6019457108341157. Arrivals time: 0.12416038382798433 Scheduler time: 3.2239819345995784 Scheduler overhead time: 0.0853280802257359 Adapter cache time: 0.041503284592181444 Engine time: 0.08614528644829988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.635924723930657,
    "estimated_duration": 3599.928983811719,
    "input_throughput": 3443.971549369996,
    "output_throughput": 2991.207062256645,
    "total_throughput": 6435.17861162664,
    "itl": 42.99260459467217,
    "ttft": 9156.179090688831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.520834934078211,
    "arrivals": 49929,
    "finished_requests": 49803,
    "scheduler_time": 28.627787411437943
}
#Debug simulation 
Total elapsed time: 3.636013280134648. Arrivals time: 0.12245562113821507 Scheduler time: 3.2595296096988022 Scheduler overhead time: 0.08582163881510496 Adapter cache time: 0.04149831971153617 Engine time: 0.0859924373216927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.615909636951983,
    "estimated_duration": 3599.932040690602,
    "input_throughput": 3443.968624924816,
    "output_throughput": 2991.2045222760003,
    "total_throughput": 6435.173147200816,
    "itl": 43.01050492993264,
    "ttft": 9156.248779535461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.340583444232964,
    "arrivals": 49929,
    "finished_requests": 49803,
    "scheduler_time": 28.634038265965756
}
#Debug simulation 
Total elapsed time: 3.615990766789764. Arrivals time: 0.1231365124695003 Scheduler time: 3.24008607538417 Scheduler overhead time: 0.08552399976179004 Adapter cache time: 0.041439580731093884 Engine time: 0.08508794428780675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.5831147199496627,
    "estimated_duration": 3599.9515838758207,
    "input_throughput": 3443.9499285298352,
    "output_throughput": 2991.1882838176093,
    "total_throughput": 6435.138212347444,
    "itl": 42.97848291690122,
    "ttft": 9156.04714603274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.880034028333688,
    "arrivals": 49929,
    "finished_requests": 49803,
    "scheduler_time": 28.623151853046693
}
#Debug simulation 
Total elapsed time: 3.583199884276837. Arrivals time: 0.12251091795042157 Scheduler time: 3.207877187989652 Scheduler overhead time: 0.08538079168647528 Adapter cache time: 0.04148884769529104 Engine time: 0.08535124827176332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 66, 4320, 66, 66, 4320, 270, 4320, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 66, 270, 270, 270, 66, 66, 4320, 270, 270, 66, 4320, 4320, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 4320, 66, 66, 270, 66, 4320, 66, 66, 4320, 66, 270, 270, 66, 270, 4320, 4320, 4320, 270, 4320, 4320, 66, 66, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 4320, 66, 66, 4320, 66, 66, 270]
Prompts retrieved: 148992 . Total input tokens: 33188119 . Total output tokens: 29304622
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.6129175759851933,
    "estimated_duration": 3599.9286944355854,
    "input_throughput": 3443.9718262096935,
    "output_throughput": 2991.207302701389,
    "total_throughput": 6435.179128911082,
    "itl": 43.00844759236055,
    "ttft": 9156.125333031117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.238061938322994,
    "arrivals": 49929,
    "finished_requests": 49803,
    "scheduler_time": 28.63323653982128
}
#Debug simulation 
Total elapsed time: 3.6130021889694035. Arrivals time: 0.1214943933300674 Scheduler time: 3.2386923381127417 Scheduler overhead time: 0.08562838239595294 Adapter cache time: 0.04144265269860625 Engine time: 0.085130182094872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.5735981538891792,
    "estimated_duration": 3600.012232876868,
    "input_throughput": 3409.028416048652,
    "output_throughput": 2984.2295817463837,
    "total_throughput": 6393.257997795035,
    "itl": 42.317132282621756,
    "ttft": 11391.139107651969,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.924718492627267,
    "arrivals": 49606,
    "finished_requests": 49450,
    "scheduler_time": 28.26183148539555
}
#Debug simulation 
Total elapsed time: 3.5736853620037436. Arrivals time: 0.12120704166591167 Scheduler time: 3.2015820993110538 Scheduler overhead time: 0.08581948420032859 Adapter cache time: 0.03822704963386059 Engine time: 0.0858541214838624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.5938556687906384,
    "estimated_duration": 3600.0121661421385,
    "input_throughput": 3409.0284792430466,
    "output_throughput": 2984.2296370661284,
    "total_throughput": 6393.2581163091745,
    "itl": 42.32909515336059,
    "ttft": 11391.14316065282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.512667338512847,
    "arrivals": 49606,
    "finished_requests": 49450,
    "scheduler_time": 28.2662647549552
}
#Debug simulation 
Total elapsed time: 3.5939482348039746. Arrivals time: 0.12068881141021848 Scheduler time: 3.2196774245239794 Scheduler overhead time: 0.08736545592546463 Adapter cache time: 0.03852626355364919 Engine time: 0.08641389943659306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.5852410942316055,
    "estimated_duration": 3600.0462800313467,
    "input_throughput": 3408.9961754305946,
    "output_throughput": 2984.2013586298826,
    "total_throughput": 6393.197534060478,
    "itl": 42.333531820757315,
    "ttft": 11391.219104578287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.680799664561652,
    "arrivals": 49606,
    "finished_requests": 49450,
    "scheduler_time": 28.267822666485706
}
#Debug simulation 
Total elapsed time: 3.5853366409428418. Arrivals time: 0.12223727675154805 Scheduler time: 3.2088829493150115 Scheduler overhead time: 0.08688547927886248 Adapter cache time: 0.03827860625460744 Engine time: 0.08762792870402336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.5985520109534264,
    "estimated_duration": 3600.0436978362673,
    "input_throughput": 3408.998620593456,
    "output_throughput": 2984.203499101141,
    "total_throughput": 6393.202119694597,
    "itl": 42.328077522444666,
    "ttft": 11391.210917509494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.101753930412222,
    "arrivals": 49606,
    "finished_requests": 49450,
    "scheduler_time": 28.26657475918326
}
#Debug simulation 
Total elapsed time: 3.598643009085208. Arrivals time: 0.12266402132809162 Scheduler time: 3.223216490354389 Scheduler overhead time: 0.08635632693767548 Adapter cache time: 0.038141120225191116 Engine time: 0.08697390556335449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.570803804323077,
    "estimated_duration": 3600.0137919851645,
    "input_throughput": 3409.026939653062,
    "output_throughput": 2984.228289324363,
    "total_throughput": 6393.255228977425,
    "itl": 42.34079677267958,
    "ttft": 11391.269138877125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.619492980283701,
    "arrivals": 49606,
    "finished_requests": 49450,
    "scheduler_time": 28.27041655592411
}
#Debug simulation 
Total elapsed time: 3.5708884899504483. Arrivals time: 0.12230287119746208 Scheduler time: 3.1937156366184354 Scheduler overhead time: 0.08656078297644854 Adapter cache time: 0.03820583038032055 Engine time: 0.08917433954775333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.5911335861310363,
    "estimated_duration": 3600.0008435651343,
    "input_throughput": 3409.0392011814965,
    "output_throughput": 2984.239022944447,
    "total_throughput": 6393.278224125943,
    "itl": 42.32140777429708,
    "ttft": 11318.814519376065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.7199931627511695,
    "arrivals": 49606,
    "finished_requests": 49450,
    "scheduler_time": 28.26329540723304
}
#Debug simulation 
Total elapsed time: 3.591248845215887. Arrivals time: 0.12214920530095696 Scheduler time: 3.2168671479448676 Scheduler overhead time: 0.08665892062708735 Adapter cache time: 0.03810128150507808 Engine time: 0.08632900845259428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 4320, 270, 270, 270, 270, 4320, 270, 4320, 270, 33, 4320, 33, 33, 4320, 270, 4320, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 33, 270, 270, 270, 33, 33, 4320, 270, 270, 33, 4320, 4320, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 4320, 33, 33, 270, 33, 4320, 33, 33, 4320, 33, 270, 270, 33, 270, 4320, 4320, 4320, 270, 4320, 4320, 33, 33, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 4320, 33, 33, 4320, 33, 33, 270]
Prompts retrieved: 147936 . Total input tokens: 32950508 . Total output tokens: 29089465
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.5709979590028524,
    "estimated_duration": 3600.0027901715475,
    "input_throughput": 3409.0373578336,
    "output_throughput": 2984.237409296025,
    "total_throughput": 6393.274767129625,
    "itl": 42.33877429248435,
    "ttft": 11318.767221800477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.562535756714658,
    "arrivals": 49606,
    "finished_requests": 49450,
    "scheduler_time": 28.269880361285615
}
#Debug simulation 
Total elapsed time: 3.571111781988293. Arrivals time: 0.12165118334814906 Scheduler time: 3.1980260526761413 Scheduler overhead time: 0.08618004573509097 Adapter cache time: 0.03790550213307142 Engine time: 0.08616644283756614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.5307161933742464,
    "estimated_duration": 3599.9824842925655,
    "input_throughput": 3349.7535203618445,
    "output_throughput": 2931.8320425311954,
    "total_throughput": 6281.58556289304,
    "itl": 40.55770617835085,
    "ttft": 11193.650374457398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.830618530004438,
    "arrivals": 48532,
    "finished_requests": 48382,
    "scheduler_time": 26.85931979168249
}
#Debug simulation 
Total elapsed time: 3.5308181210421026. Arrivals time: 0.12124792439863086 Scheduler time: 3.1516890358179808 Scheduler overhead time: 0.08961963513866067 Adapter cache time: 0.034982149954885244 Engine time: 0.09080417454242706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.5442271353676915,
    "estimated_duration": 3599.982930292763,
    "input_throughput": 3349.7531053624516,
    "output_throughput": 2931.8316793078984,
    "total_throughput": 6281.58478467035,
    "itl": 40.57245002403931,
    "ttft": 11193.84422626746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.559403908061807,
    "arrivals": 48532,
    "finished_requests": 48382,
    "scheduler_time": 26.864729402830296
}
#Debug simulation 
Total elapsed time: 3.5443165032193065. Arrivals time: 0.12056493293493986 Scheduler time: 3.162396939471364 Scheduler overhead time: 0.08950078021734953 Adapter cache time: 0.03519831784069538 Engine time: 0.09395657666027546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.550752514973283,
    "estimated_duration": 3599.981090738753,
    "input_throughput": 3349.7548170524856,
    "output_throughput": 2931.8331774443013,
    "total_throughput": 6281.587994496786,
    "itl": 40.57635236703036,
    "ttft": 11193.703484090844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.765007233624366,
    "arrivals": 48532,
    "finished_requests": 48382,
    "scheduler_time": 26.866203082255737
}
#Debug simulation 
Total elapsed time: 3.5508725000545382. Arrivals time: 0.12530690152198076 Scheduler time: 3.1680745212361217 Scheduler overhead time: 0.08940740255638957 Adapter cache time: 0.03514007478952408 Engine time: 0.09034916199743748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.536422125995159,
    "estimated_duration": 3599.9601431007122,
    "input_throughput": 3349.774308782573,
    "output_throughput": 2931.850237349899,
    "total_throughput": 6281.624546132472,
    "itl": 40.562527718491765,
    "ttft": 11193.793066256332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.059644357669155,
    "arrivals": 48532,
    "finished_requests": 48382,
    "scheduler_time": 26.860780551127462
}
#Debug simulation 
Total elapsed time: 3.536545990034938. Arrivals time: 0.12329798610880971 Scheduler time: 3.1586887696757913 Scheduler overhead time: 0.0888212863355875 Adapter cache time: 0.03503138944506645 Engine time: 0.08823569910600781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.5384664991870522,
    "estimated_duration": 3599.9494942780593,
    "input_throughput": 3349.7842175750707,
    "output_throughput": 2931.85890990302,
    "total_throughput": 6281.643127478091,
    "itl": 40.57518159388961,
    "ttft": 11193.803627562398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.690445050043074,
    "arrivals": 48532,
    "finished_requests": 48382,
    "scheduler_time": 26.865468692023136
}
#Debug simulation 
Total elapsed time: 3.5385623071342707. Arrivals time: 0.12567290384322405 Scheduler time: 3.1570805069059134 Scheduler overhead time: 0.08913287660107017 Adapter cache time: 0.034916804637759924 Engine time: 0.08931436110287905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.531479707919061,
    "estimated_duration": 3599.972804013895,
    "input_throughput": 3349.7625278042115,
    "output_throughput": 2931.839926188304,
    "total_throughput": 6281.602453992516,
    "itl": 40.55322095859625,
    "ttft": 11193.756187664452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.594590331609328,
    "arrivals": 48532,
    "finished_requests": 48382,
    "scheduler_time": 26.857426140918545
}
#Debug simulation 
Total elapsed time: 3.5315744020044804. Arrivals time: 0.12481725960969925 Scheduler time: 3.1503342003561556 Scheduler overhead time: 0.08886374859139323 Adapter cache time: 0.035019994247704744 Engine time: 0.08999842824414372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 66, 4320, 66, 66, 4320, 135, 4320, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 66, 135, 135, 135, 66, 66, 4320, 135, 135, 66, 4320, 4320, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 4320, 66, 66, 135, 66, 4320, 66, 66, 4320, 66, 135, 135, 66, 135, 4320, 4320, 4320, 135, 4320, 4320, 66, 66, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 4320, 66, 66, 4320, 66, 66, 135]
Prompts retrieved: 144672 . Total input tokens: 32218919 . Total output tokens: 28446828
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.548112777993083,
    "estimated_duration": 3599.957146910455,
    "input_throughput": 3349.777096749412,
    "output_throughput": 2931.8526774848115,
    "total_throughput": 6281.629774234223,
    "itl": 40.57341413990936,
    "ttft": 11193.707819158335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.621060795877149,
    "arrivals": 48532,
    "finished_requests": 48382,
    "scheduler_time": 26.86498172377867
}
#Debug simulation 
Total elapsed time: 3.548218049108982. Arrivals time: 0.13126132683828473 Scheduler time: 3.160033385269344 Scheduler overhead time: 0.09092164086177945 Adapter cache time: 0.034921085461974144 Engine time: 0.08838740782812238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.4744918029755354,
    "estimated_duration": 3600.030195787284,
    "input_throughput": 3317.7674492777346,
    "output_throughput": 2864.3484746507647,
    "total_throughput": 6182.115923928499,
    "itl": 39.488175118386984,
    "ttft": 10011.577511120544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.820446184291592,
    "arrivals": 48139,
    "finished_requests": 48006,
    "scheduler_time": 25.428208338755226
}
#Debug simulation 
Total elapsed time: 3.474585538264364. Arrivals time: 0.12524302024394274 Scheduler time: 3.088867269922048 Scheduler overhead time: 0.09071644395589828 Adapter cache time: 0.03386897873133421 Engine time: 0.09267657855525613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.4600575170479715,
    "estimated_duration": 3600.0151387975493,
    "input_throughput": 3317.781325772277,
    "output_throughput": 2864.360454729713,
    "total_throughput": 6182.14178050199,
    "itl": 39.49026573009428,
    "ttft": 9936.948498350255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.332718421625913,
    "arrivals": 48139,
    "finished_requests": 48006,
    "scheduler_time": 25.429068972189434
}
#Debug simulation 
Total elapsed time: 3.4601524961180985. Arrivals time: 0.12230308214202523 Scheduler time: 3.0768865295685828 Scheduler overhead time: 0.09054736560210586 Adapter cache time: 0.03406035806983709 Engine time: 0.09280944056808949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.45171290403232,
    "estimated_duration": 3600.032931432463,
    "input_throughput": 3317.764928124539,
    "output_throughput": 2864.3462980481486,
    "total_throughput": 6182.111226172688,
    "itl": 39.49333820269951,
    "ttft": 10011.542212129012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.477343954700073,
    "arrivals": 48139,
    "finished_requests": 48006,
    "scheduler_time": 25.430218309678224
}
#Debug simulation 
Total elapsed time: 3.451835098210722. Arrivals time: 0.12458639731630683 Scheduler time: 3.066925653722137 Scheduler overhead time: 0.09081337507814169 Adapter cache time: 0.034119296818971634 Engine time: 0.09177735866978765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.4636366311460733,
    "estimated_duration": 3600.0184310194763,
    "input_throughput": 3317.77829165658,
    "output_throughput": 2864.357835268042,
    "total_throughput": 6182.136126924622,
    "itl": 39.483673183436615,
    "ttft": 10011.555799828282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.980110294404431,
    "arrivals": 48139,
    "finished_requests": 48006,
    "scheduler_time": 25.426419019013924
}
#Debug simulation 
Total elapsed time: 3.463730996940285. Arrivals time: 0.12295799935236573 Scheduler time: 3.083023078739643 Scheduler overhead time: 0.09042766876518726 Adapter cache time: 0.0339074432849884 Engine time: 0.09014779655262828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.4552441742271185,
    "estimated_duration": 3600.0388269088194,
    "input_throughput": 3317.7594949040576,
    "output_throughput": 2864.3416073526623,
    "total_throughput": 6182.10110225672,
    "itl": 39.492645788899594,
    "ttft": 10011.526195987002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.424736191839939,
    "arrivals": 48139,
    "finished_requests": 48006,
    "scheduler_time": 25.429891611071803
}
#Debug simulation 
Total elapsed time: 3.455341503955424. Arrivals time: 0.1234730058349669 Scheduler time: 3.0711544756777585 Scheduler overhead time: 0.09055043710395694 Adapter cache time: 0.03693940071389079 Engine time: 0.0897713708691299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.4920497070997953,
    "estimated_duration": 3600.034783870806,
    "input_throughput": 3317.7632209313215,
    "output_throughput": 2864.344824166581,
    "total_throughput": 6182.1080450979025,
    "itl": 39.47800632190809,
    "ttft": 10011.451291804233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.653878365675896,
    "arrivals": 48139,
    "finished_requests": 48006,
    "scheduler_time": 25.42416398277254
}
#Debug simulation 
Total elapsed time: 3.492149367928505. Arrivals time: 0.12595969252288342 Scheduler time: 3.105857398826629 Scheduler overhead time: 0.09047839464619756 Adapter cache time: 0.033973085694015026 Engine time: 0.09252547100186348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 4320, 135, 135, 135, 135, 4320, 135, 4320, 135, 33, 4320, 33, 33, 4320, 135, 4320, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 33, 135, 135, 135, 33, 33, 4320, 135, 135, 33, 4320, 4320, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 4320, 33, 33, 135, 33, 4320, 33, 33, 4320, 33, 135, 135, 33, 135, 4320, 4320, 4320, 135, 4320, 4320, 33, 33, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 4320, 33, 33, 4320, 33, 33, 135]
Prompts retrieved: 143616 . Total input tokens: 31983104 . Total output tokens: 28242155
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.5008476953953505,
    "estimated_duration": 3600.0298060339,
    "input_throughput": 3317.7678084722857,
    "output_throughput": 2864.3487847563943,
    "total_throughput": 6182.11659322868,
    "itl": 39.49161941781094,
    "ttft": 10011.594261658945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.376063655335485,
    "arrivals": 48139,
    "finished_requests": 48006,
    "scheduler_time": 25.429415943265578
}
#Debug simulation 
Total elapsed time: 3.500943790189922. Arrivals time: 0.1241712705232203 Scheduler time: 3.1167832515202463 Scheduler overhead time: 0.0910016312263906 Adapter cache time: 0.03403737163171172 Engine time: 0.09172277338802814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.4510726551525295,
    "estimated_duration": 3600.027333000236,
    "input_throughput": 3264.682990671957,
    "output_throughput": 2867.229897223484,
    "total_throughput": 6131.912887895441,
    "itl": 38.63537795356019,
    "ttft": 7952.430284959892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3987782424223103,
    "arrivals": 47462,
    "finished_requests": 47357,
    "scheduler_time": 25.108005567283428
}
#Debug simulation 
Total elapsed time: 3.451160999946296. Arrivals time: 0.12171511584892869 Scheduler time: 3.074245751835406 Scheduler overhead time: 0.09132541343569756 Adapter cache time: 0.0294176721945405 Engine time: 0.09061795612797141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.4737625615671277,
    "estimated_duration": 3600.024161796627,
    "input_throughput": 3264.6858664789006,
    "output_throughput": 2867.232422920365,
    "total_throughput": 6131.918289399266,
    "itl": 38.642130586506674,
    "ttft": 7952.490722846007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7810949138458883,
    "arrivals": 47462,
    "finished_requests": 47357,
    "scheduler_time": 25.11083334672434
}
#Debug simulation 
Total elapsed time: 3.4738504788838327. Arrivals time: 0.1249015536159277 Scheduler time: 3.089603452011943 Scheduler overhead time: 0.0929416548460722 Adapter cache time: 0.02960211969912052 Engine time: 0.09248700505122542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.457850409206003,
    "estimated_duration": 3600.0313382469544,
    "input_throughput": 3264.67935852001,
    "output_throughput": 2867.2267072614927,
    "total_throughput": 6131.906065781503,
    "itl": 38.64353275083968,
    "ttft": 7952.407228160369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8879455292970277,
    "arrivals": 47462,
    "finished_requests": 47357,
    "scheduler_time": 25.11166319280734
}
#Debug simulation 
Total elapsed time: 3.457941316999495. Arrivals time: 0.12194363167509437 Scheduler time: 3.0758109646849334 Scheduler overhead time: 0.09193671308457851 Adapter cache time: 0.029532444663345814 Engine time: 0.09482085844501853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 3.470203955192119,
    "estimated_duration": 3600.0137012858377,
    "input_throughput": 3264.6953526321668,
    "output_throughput": 2867.2407541985726,
    "total_throughput": 6131.93610683074,
    "itl": 38.63739217674155,
    "ttft": 7952.37125203824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5187211498897466,
    "arrivals": 47462,
    "finished_requests": 47357,
    "scheduler_time": 25.108819234134195
}
#Debug simulation 
Total elapsed time: 3.4703093101270497. Arrivals time: 0.12284331442788243 Scheduler time: 3.0887234527617693 Scheduler overhead time: 0.09224658831954002 Adapter cache time: 0.02983425697311759 Engine time: 0.0925725526176393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 3.497706988826394,
    "estimated_duration": 3600.0191914149673,
    "input_throughput": 3264.690373881193,
    "output_throughput": 2867.2363815768867,
    "total_throughput": 6131.92675545808,
    "itl": 38.642773183344566,
    "ttft": 7952.357611923944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.848800382916849,
    "arrivals": 47462,
    "finished_requests": 47357,
    "scheduler_time": 25.111340490579288
}
#Debug simulation 
Total elapsed time: 3.4978228518739343. Arrivals time: 0.12272383039817214 Scheduler time: 3.1178792039863765 Scheduler overhead time: 0.09209600789472461 Adapter cache time: 0.02977173775434494 Engine time: 0.09146496374160051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 3.4789328668266535,
    "estimated_duration": 3600.012296254715,
    "input_throughput": 3264.696626794086,
    "output_throughput": 2867.2418732398883,
    "total_throughput": 6131.938500033974,
    "itl": 38.63333986292491,
    "ttft": 7952.438597757457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2813353634532394,
    "arrivals": 47462,
    "finished_requests": 47357,
    "scheduler_time": 25.10709877298883
}
#Debug simulation 
Total elapsed time: 3.4790513799525797. Arrivals time: 0.12236114777624607 Scheduler time: 3.0992334955371916 Scheduler overhead time: 0.09203629195690155 Adapter cache time: 0.029622592497617006 Engine time: 0.09190069185569882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.4-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 4320, 66, 66, 66, 66, 4320, 66, 4320, 66, 33, 4320, 33, 33, 4320, 66, 4320, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 33, 66, 66, 66, 33, 33, 4320, 66, 66, 33, 4320, 4320, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 4320, 33, 33, 66, 33, 4320, 33, 33, 4320, 33, 66, 66, 33, 66, 4320, 4320, 4320, 66, 4320, 4320, 33, 33, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 33, 33, 66]
Prompts retrieved: 141408 . Total input tokens: 31487596 . Total output tokens: 27831016
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 3.47178937215358,
    "estimated_duration": 3600.0108599388022,
    "input_throughput": 3264.6979293278555,
    "output_throughput": 2867.2430171989727,
    "total_throughput": 6131.940946526828,
    "itl": 38.642492935392696,
    "ttft": 7952.528972483921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8133833457157347,
    "arrivals": 47462,
    "finished_requests": 47357,
    "scheduler_time": 25.111110722077655
}
#Debug simulation 
Total elapsed time: 3.47188009833917. Arrivals time: 0.12362880585715175 Scheduler time: 3.0891925366595387 Scheduler overhead time: 0.0914922165684402 Adapter cache time: 0.02973854076117277 Engine time: 0.09393137041479349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.8119932264089584,
    "estimated_duration": 3599.9657333820933,
    "input_throughput": 1391.3160210262445,
    "output_throughput": 1210.7651357850784,
    "total_throughput": 2602.081156811323,
    "itl": 29.269190272695766,
    "ttft": 5721.921450821603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.68666408110084,
    "arrivals": 20310,
    "finished_requests": 20278,
    "scheduler_time": 0.14881171029919096
}
#Debug simulation 
Total elapsed time: 1.8120886851102114. Arrivals time: 0.06210842449218035 Scheduler time: 1.3775269924663007 Scheduler overhead time: 0.11016586748883128 Adapter cache time: 0.09722230536863208 Engine time: 0.11078608362004161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8155482891015708,
    "estimated_duration": 3599.98185093761,
    "input_throughput": 1391.309791935616,
    "output_throughput": 1210.7597150426689,
    "total_throughput": 2602.0695069782846,
    "itl": 29.306769192660344,
    "ttft": 5722.058283718546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4793,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.99879355355532,
    "arrivals": 20310,
    "finished_requests": 20278,
    "scheduler_time": 0.1515710205973968
}
#Debug simulation 
Total elapsed time: 1.8156464761123061. Arrivals time: 0.061825788114219904 Scheduler time: 1.3836226710118353 Scheduler overhead time: 0.11006889585405588 Adapter cache time: 0.09714167844504118 Engine time: 0.10926582152023911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8415395277552307,
    "estimated_duration": 3599.980951056973,
    "input_throughput": 1391.3101397187734,
    "output_throughput": 1210.7600176940546,
    "total_throughput": 2602.070157412828,
    "itl": 29.32192773863513,
    "ttft": 5722.276291761913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.95137904830345,
    "arrivals": 20310,
    "finished_requests": 20278,
    "scheduler_time": 0.15247643247066747
}
#Debug simulation 
Total elapsed time: 1.8416631016880274. Arrivals time: 0.062192680314183235 Scheduler time: 1.4064715201966465 Scheduler overhead time: 0.11066845990717411 Adapter cache time: 0.09799953689798713 Engine time: 0.11004946380853653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.8103018533438444,
    "estimated_duration": 3599.967046117824,
    "input_throughput": 1391.3155136798632,
    "output_throughput": 1210.764694276966,
    "total_throughput": 2602.0802079568293,
    "itl": 29.277428583126266,
    "ttft": 5721.948301893574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4794,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 32.82289410445013,
    "arrivals": 20310,
    "finished_requests": 20278,
    "scheduler_time": 0.14987545342807163
}
#Debug simulation 
Total elapsed time: 1.8103947550989687. Arrivals time: 0.06246733758598566 Scheduler time: 1.3761458257213235 Scheduler overhead time: 0.10966112371534109 Adapter cache time: 0.09764395747333765 Engine time: 0.11053705122321844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.8329958217218518,
    "estimated_duration": 3599.988648614916,
    "input_throughput": 1391.3071647953882,
    "output_throughput": 1210.7574288260605,
    "total_throughput": 2602.0645936214487,
    "itl": 29.317034495736078,
    "ttft": 5722.197562140325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4793,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.610311920429396,
    "arrivals": 20310,
    "finished_requests": 20278,
    "scheduler_time": 0.15215826849540118
}
#Debug simulation 
Total elapsed time: 1.8330929558724165. Arrivals time: 0.061406283639371395 Scheduler time: 1.3985450295731425 Scheduler overhead time: 0.11032616905868053 Adapter cache time: 0.09765167208388448 Engine time: 0.11095233261585236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.8173088738694787,
    "estimated_duration": 3599.985488796061,
    "input_throughput": 1391.3083859888143,
    "output_throughput": 1210.7584915453865,
    "total_throughput": 2602.066877534201,
    "itl": 29.254722615336696,
    "ttft": 5721.880389499129,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.59174914721697,
    "arrivals": 20310,
    "finished_requests": 20278,
    "scheduler_time": 0.1480356490292544
}
#Debug simulation 
Total elapsed time: 1.8174102101475. Arrivals time: 0.06061922386288643 Scheduler time: 1.3861750517971814 Scheduler overhead time: 0.1106209084391594 Adapter cache time: 0.09718559589236975 Engine time: 0.10915287071838975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 270, 1080, 270, 270, 1080, 540, 1080, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 270, 540, 540, 540, 270, 270, 1080, 540, 540, 270, 1080, 1080, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 1080, 270, 270, 540, 270, 1080, 270, 270, 1080, 270, 540, 540, 270, 540, 1080, 1080, 1080, 540, 1080, 1080, 270, 270, 1080, 540, 1080, 270, 540, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 270, 270, 540]
Prompts retrieved: 60480 . Total input tokens: 13439753 . Total output tokens: 11895104
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.815297453198582,
    "estimated_duration": 3599.9900810207437,
    "input_throughput": 1391.306611205949,
    "output_throughput": 1210.7569470758451,
    "total_throughput": 2602.063558281794,
    "itl": 29.311267710006135,
    "ttft": 5722.210415914844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4794,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.286534407884695,
    "arrivals": 20310,
    "finished_requests": 20278,
    "scheduler_time": 0.15189548109472406
}
#Debug simulation 
Total elapsed time: 1.8153946110978723. Arrivals time: 0.061602259520441294 Scheduler time: 1.382457916624844 Scheduler overhead time: 0.11035935953259468 Adapter cache time: 0.09742669947445393 Engine time: 0.10968078626319766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.741127135232091,
    "estimated_duration": 3599.6887314958303,
    "input_throughput": 1275.3747177724047,
    "output_throughput": 1127.6533341573736,
    "total_throughput": 2403.0280519297785,
    "itl": 28.153910704568183,
    "ttft": 6524.772740889378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.92013594091309,
    "arrivals": 18897,
    "finished_requests": 18863,
    "scheduler_time": 0.07859957083839754
}
#Debug simulation 
Total elapsed time: 1.7412252994254231. Arrivals time: 0.05800373246893287 Scheduler time: 1.3097637798637152 Scheduler overhead time: 0.11412900546565652 Adapter cache time: 0.08853412978351116 Engine time: 0.11509758327156305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7068130560219288,
    "estimated_duration": 3599.6893797667994,
    "input_throughput": 1275.374488089141,
    "output_throughput": 1127.6531310773735,
    "total_throughput": 2403.027619166515,
    "itl": 28.179475737258397,
    "ttft": 6524.928209010962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.207623696391803,
    "arrivals": 18897,
    "finished_requests": 18863,
    "scheduler_time": 0.07957228485234144
}
#Debug simulation 
Total elapsed time: 1.7069068029522896. Arrivals time: 0.05902265477925539 Scheduler time: 1.2761762323789299 Scheduler overhead time: 0.11316244164481759 Adapter cache time: 0.08871922269463539 Engine time: 0.11396097298711538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7122331350110471,
    "estimated_duration": 3599.708510166745,
    "input_throughput": 1275.3677102003292,
    "output_throughput": 1127.6471382434158,
    "total_throughput": 2403.014848443745,
    "itl": 28.18672971158915,
    "ttft": 6525.045514076213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.849003150589514,
    "arrivals": 18897,
    "finished_requests": 18863,
    "scheduler_time": 0.07980892705676355
}
#Debug simulation 
Total elapsed time: 1.7123308181762695. Arrivals time: 0.05697670532390475 Scheduler time: 1.285309120081365 Scheduler overhead time: 0.11239684699103236 Adapter cache time: 0.08886575791984797 Engine time: 0.11298342933878303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.7192763490602374,
    "estimated_duration": 3599.693376041547,
    "input_throughput": 1275.373072205529,
    "output_throughput": 1127.6518791897095,
    "total_throughput": 2403.0249513952385,
    "itl": 28.161979984607708,
    "ttft": 6524.878826397709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.663648120947816,
    "arrivals": 18897,
    "finished_requests": 18863,
    "scheduler_time": 0.07885689570302336
}
#Debug simulation 
Total elapsed time: 1.719372371211648. Arrivals time: 0.05815375689417124 Scheduler time: 1.2920629754662514 Scheduler overhead time: 0.11308117024600506 Adapter cache time: 0.08874116512015462 Engine time: 0.11166155058890581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7572807827964425,
    "estimated_duration": 3599.7038094554646,
    "input_throughput": 1275.3693756527384,
    "output_throughput": 1127.6486107933542,
    "total_throughput": 2403.017986446093,
    "itl": 28.183684426261916,
    "ttft": 6524.947071194276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.612891065763037,
    "arrivals": 18897,
    "finished_requests": 18863,
    "scheduler_time": 0.07962845354492151
}
#Debug simulation 
Total elapsed time: 1.7573747839778662. Arrivals time: 0.05838913517072797 Scheduler time: 1.3296619961038232 Scheduler overhead time: 0.11305340193212032 Adapter cache time: 0.08858363609761 Engine time: 0.1122037973254919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.7357872547581792,
    "estimated_duration": 3599.700927576434,
    "input_throughput": 1275.370396698746,
    "output_throughput": 1127.6495135758216,
    "total_throughput": 2403.019910274568,
    "itl": 28.14666020297085,
    "ttft": 6524.746984852449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.175465759874175,
    "arrivals": 18897,
    "finished_requests": 18863,
    "scheduler_time": 0.07820968454840535
}
#Debug simulation 
Total elapsed time: 1.7358813341706991. Arrivals time: 0.05868747504428029 Scheduler time: 1.3069806462153792 Scheduler overhead time: 0.11338533973321319 Adapter cache time: 0.08837748505175114 Engine time: 0.11280399654060602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 135, 1080, 135, 135, 1080, 540, 1080, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 135, 540, 540, 540, 135, 135, 1080, 540, 540, 135, 1080, 1080, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 1080, 135, 135, 540, 135, 1080, 135, 135, 1080, 135, 540, 540, 135, 540, 1080, 1080, 1080, 540, 1080, 1080, 135, 135, 1080, 540, 1080, 135, 540, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 135, 135, 540]
Prompts retrieved: 56160 . Total input tokens: 12463566 . Total output tokens: 11044236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7164714289829135,
    "estimated_duration": 3599.6935420295176,
    "input_throughput": 1275.3730133959148,
    "output_throughput": 1127.65182719177,
    "total_throughput": 2403.0248405876846,
    "itl": 28.18367209229723,
    "ttft": 6525.120415634276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.40183566976403,
    "arrivals": 18897,
    "finished_requests": 18863,
    "scheduler_time": 0.07967982669743705
}
#Debug simulation 
Total elapsed time: 1.7165665309876204. Arrivals time: 0.05884142033755779 Scheduler time: 1.2862175609916449 Scheduler overhead time: 0.11237603472545743 Adapter cache time: 0.08841722086071968 Engine time: 0.11548795271664858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.684314819984138,
    "estimated_duration": 3599.680510021005,
    "input_throughput": 1245.6316574533544,
    "output_throughput": 1080.763414744632,
    "total_throughput": 2326.3950721979863,
    "itl": 27.53997969391946,
    "ttft": 4393.344890955936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1971,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.03305820197368,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.017342060351739557
}
#Debug simulation 
Total elapsed time: 1.684415069874376. Arrivals time: 0.057137325406074524 Scheduler time: 1.2582152667455375 Scheduler overhead time: 0.11489376239478588 Adapter cache time: 0.08321367017924786 Engine time: 0.11472739651799202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.7045006668195128,
    "estimated_duration": 3599.67673034939,
    "input_throughput": 1245.6329653704177,
    "output_throughput": 1080.7645495495346,
    "total_throughput": 2326.3975149199523,
    "itl": 27.356257809359942,
    "ttft": 4393.133906850336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.450394133846082,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.01614538043148045
}
#Debug simulation 
Total elapsed time: 1.7045945096760988. Arrivals time: 0.05694789765402675 Scheduler time: 1.2760210204869509 Scheduler overhead time: 0.11572245508432388 Adapter cache time: 0.08355202525854111 Engine time: 0.11574145872145891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6609063036739826,
    "estimated_duration": 3599.6706374276105,
    "input_throughput": 1245.6350737700432,
    "output_throughput": 1080.7663788874174,
    "total_throughput": 2326.4014526574606,
    "itl": 27.36001867038103,
    "ttft": 4393.03693843376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1986,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.836082018306227,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.01622473374229126
}
#Debug simulation 
Total elapsed time: 1.66099813580513. Arrivals time: 0.056331361178308725 Scheduler time: 1.2342548766173422 Scheduler overhead time: 0.11483699316158891 Adapter cache time: 0.08349112188443542 Engine time: 0.11541145062074065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.672193001024425,
    "estimated_duration": 3599.661246311182,
    "input_throughput": 1245.6383234936156,
    "output_throughput": 1080.7691984868745,
    "total_throughput": 2326.4075219804904,
    "itl": 27.34933019346237,
    "ttft": 4392.960384158073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.575814920659404,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.01612599037728323
}
#Debug simulation 
Total elapsed time: 1.672290806658566. Arrivals time: 0.05717176338657737 Scheduler time: 1.2434107288718224 Scheduler overhead time: 0.1155145256780088 Adapter cache time: 0.08282446162775159 Engine time: 0.11691352259367704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.662141680251807,
    "estimated_duration": 3599.6584324426076,
    "input_throughput": 1245.6392972144838,
    "output_throughput": 1080.7700433288342,
    "total_throughput": 2326.4093405433177,
    "itl": 27.359031878924903,
    "ttft": 4393.116412585641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.699023812883285,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.016225692850304164
}
#Debug simulation 
Total elapsed time: 1.662239980418235. Arrivals time: 0.055837458465248346 Scheduler time: 1.2366305319592357 Scheduler overhead time: 0.11518946988508105 Adapter cache time: 0.08344436576589942 Engine time: 0.11461652955040336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6807697350159287,
    "estimated_duration": 3599.6559744287683,
    "input_throughput": 1245.6401477953873,
    "output_throughput": 1080.7707813292825,
    "total_throughput": 2326.41092912467,
    "itl": 27.53634220808093,
    "ttft": 4393.397439426286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1969,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.569940331983949,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.0172539085608411
}
#Debug simulation 
Total elapsed time: 1.6808632826432586. Arrivals time: 0.05655794544145465 Scheduler time: 1.2532684626057744 Scheduler overhead time: 0.11476894980296493 Adapter cache time: 0.08323324518278241 Engine time: 0.11654619127511978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 66, 1080, 66, 66, 1080, 540, 1080, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 66, 540, 540, 540, 66, 66, 1080, 540, 540, 66, 1080, 1080, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 1080, 66, 66, 540, 66, 1080, 66, 66, 1080, 66, 540, 540, 66, 540, 1080, 1080, 1080, 540, 1080, 1080, 66, 66, 1080, 540, 1080, 66, 540, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 66, 66, 540]
Prompts retrieved: 53952 . Total input tokens: 11974144 . Total output tokens: 10619251
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6695240042172372,
    "estimated_duration": 3599.668457802906,
    "input_throughput": 1245.6358280108884,
    "output_throughput": 1080.767033299102,
    "total_throughput": 2326.4028613099904,
    "itl": 27.35850953971665,
    "ttft": 4393.04894584633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1986,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.572059487104168,
    "arrivals": 18219,
    "finished_requests": 18197,
    "scheduler_time": 0.016250795684560695
}
#Debug simulation 
Total elapsed time: 1.669620358850807. Arrivals time: 0.05577226262539625 Scheduler time: 1.242592799011618 Scheduler overhead time: 0.11477832635864615 Adapter cache time: 0.08336441311985254 Engine time: 0.11671925894916058 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6588727622292936,
    "estimated_duration": 3599.9997042305918,
    "input_throughput": 1210.6359327969649,
    "output_throughput": 1063.2278651306324,
    "total_throughput": 2273.8637979275973,
    "itl": 27.076258044685122,
    "ttft": 7310.378331183795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.743130976413559,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.0075818928237823
}
#Debug simulation 
Total elapsed time: 1.6589679880999029. Arrivals time: 0.056564660742878914 Scheduler time: 1.2311479379422963 Scheduler overhead time: 0.11705508828163147 Adapter cache time: 0.08007546700537205 Engine time: 0.11688257195055485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6571811973117292,
    "estimated_duration": 3599.9918651986004,
    "input_throughput": 1210.6385689734236,
    "output_throughput": 1063.2301803239886,
    "total_throughput": 2273.8687492974123,
    "itl": 27.084947088615067,
    "ttft": 7310.507460919557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.525897514279924,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.007641981202399782
}
#Debug simulation 
Total elapsed time: 1.657265265006572. Arrivals time: 0.05339599307626486 Scheduler time: 1.2341100652702153 Scheduler overhead time: 0.11607419699430466 Adapter cache time: 0.0806627906858921 Engine time: 0.11618636455386877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6702303988859057,
    "estimated_duration": 3599.995980046265,
    "input_throughput": 1210.6371851959652,
    "output_throughput": 1063.2289650364582,
    "total_throughput": 2273.8661502324235,
    "itl": 27.084784110077557,
    "ttft": 7310.523559817754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.741092041227944,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.007658994420573805
}
#Debug simulation 
Total elapsed time: 1.6703124772757292. Arrivals time: 0.05304378643631935 Scheduler time: 1.2465445026755333 Scheduler overhead time: 0.11625499092042446 Adapter cache time: 0.08058306435123086 Engine time: 0.11681369319558144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.6629530601203442,
    "estimated_duration": 3599.9919009007576,
    "input_throughput": 1210.638556967172,
    "output_throughput": 1063.2301697796286,
    "total_throughput": 2273.8687267468003,
    "itl": 27.078583524249982,
    "ttft": 7310.456892711188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.000876133246335,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.007619130082162571
}
#Debug simulation 
Total elapsed time: 1.6630408498458564. Arrivals time: 0.05396257806569338 Scheduler time: 1.2365450039505959 Scheduler overhead time: 0.11586078209802508 Adapter cache time: 0.08075425401329994 Engine time: 0.11838106624782085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.6454322529025376,
    "estimated_duration": 3599.9738093749256,
    "input_throughput": 1210.644640983303,
    "output_throughput": 1063.2355130007463,
    "total_throughput": 2273.8801539840492,
    "itl": 27.08589602231456,
    "ttft": 7310.403661185489,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.663164113839139,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.007651738790500332
}
#Debug simulation 
Total elapsed time: 1.6455165999941528. Arrivals time: 0.05295416619628668 Scheduler time: 1.2223689081147313 Scheduler overhead time: 0.11581918457522988 Adapter cache time: 0.08020832063630223 Engine time: 0.11716242972761393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.6706673596054316,
    "estimated_duration": 3599.980433898686,
    "input_throughput": 1210.64241320892,
    "output_throughput": 1063.2335564820796,
    "total_throughput": 2273.8759696909997,
    "itl": 27.073334816870965,
    "ttft": 7310.416435667682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.469187500467486,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.007594152369909932
}
#Debug simulation 
Total elapsed time: 1.6707573886960745. Arrivals time: 0.06626033876091242 Scheduler time: 1.2339361254125834 Scheduler overhead time: 0.11612859414890409 Adapter cache time: 0.08063389454036951 Engine time: 0.11666557937860489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_96_slots_64_rate_0.1-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 1080, 540, 540, 540, 540, 1080, 540, 1080, 540, 33, 1080, 33, 33, 1080, 540, 1080, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 33, 540, 540, 540, 33, 33, 1080, 540, 540, 33, 1080, 1080, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 1080, 33, 33, 540, 33, 1080, 33, 33, 1080, 33, 540, 540, 33, 540, 1080, 1080, 1080, 540, 1080, 1080, 33, 33, 1080, 540, 1080, 33, 540, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 33, 33, 540]
Prompts retrieved: 52896 . Total input tokens: 11729594 . Total output tokens: 10409148
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.653061074204743,
    "estimated_duration": 3599.9869421350577,
    "input_throughput": 1210.6402245490406,
    "output_throughput": 1063.2316343153009,
    "total_throughput": 2273.8718588643414,
    "itl": 27.084780429878904,
    "ttft": 7310.476946642964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.5853399244137,
    "arrivals": 17838,
    "finished_requests": 17802,
    "scheduler_time": 0.007683972132826443
}
#Debug simulation 
Total elapsed time: 1.6531386501155794. Arrivals time: 0.05298638017848134 Scheduler time: 1.2306902264244854 Scheduler overhead time: 0.11608832469210029 Adapter cache time: 0.07992956507951021 Engine time: 0.11657113255932927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.541850320994854,
    "estimated_duration": 3600.016260934171,
    "input_throughput": 1097.9900404600644,
    "output_throughput": 956.0984591516375,
    "total_throughput": 2054.088499611702,
    "itl": 26.141984030197502,
    "ttft": 6307.127148949781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.666323232865366,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 8.339860090264041e-07
}
#Debug simulation 
Total elapsed time: 1.5419393070042133. Arrivals time: 0.04956942331045866 Scheduler time: 1.1145067014731467 Scheduler overhead time: 0.119347233325243 Adapter cache time: 0.07744209375232458 Engine time: 0.12224250705912709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5319126630201936,
    "estimated_duration": 3600.022682808744,
    "input_throughput": 1097.988081818427,
    "output_throughput": 956.0967536222769,
    "total_throughput": 2054.084835440704,
    "itl": 26.157350188402482,
    "ttft": 6307.28909628058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.209939023768236,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 8.339860090264041e-07
}
#Debug simulation 
Total elapsed time: 1.531997780315578. Arrivals time: 0.04938923753798008 Scheduler time: 1.106758788228035 Scheduler overhead time: 0.11991509003564715 Adapter cache time: 0.07695921091362834 Engine time: 0.12019110983237624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5352790937758982,
    "estimated_duration": 3600.0128090912344,
    "input_throughput": 1097.9910932588643,
    "output_throughput": 956.0993758988513,
    "total_throughput": 2054.0904691577157,
    "itl": 26.161869783624866,
    "ttft": 6307.234316454757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.636720324628726,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 1.6679720180528082e-06
}
#Debug simulation 
Total elapsed time: 1.535369929857552. Arrivals time: 0.049530393444001675 Scheduler time: 1.1104131862521172 Scheduler overhead time: 0.11967427423223853 Adapter cache time: 0.07772275386378169 Engine time: 0.11934346193447709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 1.5621248274110258,
    "estimated_duration": 3600.000443750531,
    "input_throughput": 1097.9948646567211,
    "output_throughput": 956.1026599247048,
    "total_throughput": 2054.0975245814257,
    "itl": 26.148792310446222,
    "ttft": 6083.802032753165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.167942256736847,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 8.339860090264041e-07
}
#Debug simulation 
Total elapsed time: 1.562235875055194. Arrivals time: 0.049495710991322994 Scheduler time: 1.136220228858292 Scheduler overhead time: 0.1197657622396946 Adapter cache time: 0.07752060331404209 Engine time: 0.11979083577170968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5436430601403117,
    "estimated_duration": 3600.0028622742,
    "input_throughput": 1097.9941270110385,
    "output_throughput": 956.1020176038506,
    "total_throughput": 2054.096144614889,
    "itl": 26.159677969574656,
    "ttft": 6307.355446175568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.476050761071686,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 8.339860090264041e-07
}
#Debug simulation 
Total elapsed time: 1.5437383372336626. Arrivals time: 0.050037486013025045 Scheduler time: 1.116941586136818 Scheduler overhead time: 0.11936041573062539 Adapter cache time: 0.07694905158132315 Engine time: 0.12166025163605809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 1.5526992017403245,
    "estimated_duration": 3600.0175607731376,
    "input_throughput": 1097.9896440147095,
    "output_throughput": 956.0981139383122,
    "total_throughput": 2054.0877579530215,
    "itl": 26.137203633803203,
    "ttft": 6307.057073133278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.153152725245652,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 8.339860090264041e-07
}
#Debug simulation 
Total elapsed time: 1.552796624135226. Arrivals time: 0.04991460731253028 Scheduler time: 1.12555423239246 Scheduler overhead time: 0.11961197992786765 Adapter cache time: 0.0770227643661201 Engine time: 0.12181334476917982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_96_slots_64_rate_0.1-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 1080, 270, 270, 270, 270, 1080, 270, 1080, 270, 135, 1080, 135, 135, 1080, 270, 1080, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 135, 270, 270, 270, 135, 135, 1080, 270, 270, 135, 1080, 1080, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 1080, 135, 135, 270, 135, 1080, 135, 135, 1080, 135, 270, 270, 135, 270, 1080, 1080, 1080, 270, 1080, 1080, 135, 135, 1080, 270, 1080, 135, 270, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 135, 135, 270]
Prompts retrieved: 47520 . Total input tokens: 10535497 . Total output tokens: 9362774
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 1.5345356878824532,
    "estimated_duration": 3600.0233362652534,
    "input_throughput": 1097.9878825176468,
    "output_throughput": 956.0965800768332,
    "total_throughput": 2054.08446259448,
    "itl": 26.159386937082633,
    "ttft": 6307.305441065773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.32635691136092,
    "arrivals": 16094,
    "finished_requests": 16066,
    "scheduler_time": 8.339860090264041e-07
}
#Debug simulation 
Total elapsed time: 1.534622651990503. Arrivals time: 0.049367944709956646 Scheduler time: 1.1124992426484823 Scheduler overhead time: 0.1189102684147656 Adapter cache time: 0.07713526440784335 Engine time: 0.11862995522096753 
