INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 8640, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55890 . Total input tokens: 12403995 . Total output tokens: 10991832
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6791187250055373,
    "estimated_duration": 3599.7781906736336,
    "input_throughput": 1298.094702642098,
    "output_throughput": 1129.5798753753427,
    "total_throughput": 2427.6745780174406,
    "itl": 23.089829692172938,
    "ttft": 3672.756714941294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 18819,
    "finished_requests": 18800,
    "scheduler_time": 0.0006456269186362935
}
#Debug simulation 
Total elapsed time: 1.6792574422433972. Arrivals time: 0.055870684795081615 Scheduler time: 1.2647548373788595 Scheduler overhead time: 0.13273365749046206 Adapter cache time: 0.024693883024156094 Engine time: 0.1362628247588873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 8640, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55890 . Total input tokens: 12403995 . Total output tokens: 10991832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6611443390138447,
    "estimated_duration": 3599.7890217009544,
    "input_throughput": 1298.0907969412071,
    "output_throughput": 1129.5764767010266,
    "total_throughput": 2427.667273642234,
    "itl": 23.08992708594188,
    "ttft": 3672.845455257409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 18819,
    "finished_requests": 18800,
    "scheduler_time": 0.0006496717266775426
}
#Debug simulation 
Total elapsed time: 1.6612703688442707. Arrivals time: 0.053854437079280615 Scheduler time: 1.2572902077808976 Scheduler overhead time: 0.13178772805258632 Adapter cache time: 0.024332602508366108 Engine time: 0.12935685506090522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 8640, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55890 . Total input tokens: 12403995 . Total output tokens: 10991832
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6523749642074108,
    "estimated_duration": 3599.792053553238,
    "input_throughput": 1298.0897036504034,
    "output_throughput": 1129.575525338012,
    "total_throughput": 2427.6652289884155,
    "itl": 23.089950051511373,
    "ttft": 3672.804998454823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 18819,
    "finished_requests": 18800,
    "scheduler_time": 0.0006383712885628211
}
#Debug simulation 
Total elapsed time: 1.652474652044475. Arrivals time: 0.053955381736159325 Scheduler time: 1.2482909383252263 Scheduler overhead time: 0.1313375816680491 Adapter cache time: 0.024230111856013536 Engine time: 0.13026742124930024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 8640, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55890 . Total input tokens: 12403995 . Total output tokens: 10991832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6439122958108783,
    "estimated_duration": 3599.784677778902,
    "input_throughput": 1298.0923633696864,
    "output_throughput": 1129.577839780379,
    "total_throughput": 2427.6702031500654,
    "itl": 23.08991197325959,
    "ttft": 3672.856621350353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900566,
    "arrivals": 18819,
    "finished_requests": 18800,
    "scheduler_time": 0.0006375373025537946
}
#Debug simulation 
Total elapsed time: 1.6440106718800962. Arrivals time: 0.05205019097775221 Scheduler time: 1.2441736892797053 Scheduler overhead time: 0.13107915641739964 Adapter cache time: 0.024060631170868874 Engine time: 0.12845582235604525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 8640, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55890 . Total input tokens: 12403995 . Total output tokens: 10991832
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.6631511319428682,
    "estimated_duration": 3599.7912406971514,
    "input_throughput": 1298.0899967674334,
    "output_throughput": 1129.5757804034533,
    "total_throughput": 2427.6657771708865,
    "itl": 23.089942418050253,
    "ttft": 3672.8440889160206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 18819,
    "finished_requests": 18800,
    "scheduler_time": 0.0006424160966040704
}
#Debug simulation 
Total elapsed time: 1.6632473901845515. Arrivals time: 0.0550063350237906 Scheduler time: 1.2553044785745442 Scheduler overhead time: 0.13076612167060375 Adapter cache time: 0.024357987567782402 Engine time: 0.13325058482587337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 8640, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55890 . Total input tokens: 12403995 . Total output tokens: 10991832
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6503040860407054,
    "estimated_duration": 3599.7935908137274,
    "input_throughput": 1298.0891493125052,
    "output_throughput": 1129.5750429626255,
    "total_throughput": 2427.6641922751305,
    "itl": 23.089832192624762,
    "ttft": 3672.752451944798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 18819,
    "finished_requests": 18800,
    "scheduler_time": 0.0006505057126865692
}
#Debug simulation 
Total elapsed time: 1.6504043359309435. Arrivals time: 0.05342921242117882 Scheduler time: 1.2476335177198052 Scheduler overhead time: 0.1314215613529086 Adapter cache time: 0.02446010196581483 Engine time: 0.1290296195074916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [5 5 6]
Adapter prompts. [540, 270, 270, 540, 8640, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55890 . Total input tokens: 12403995 . Total output tokens: 10991832
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.662487762980163,
    "estimated_duration": 3599.790398310999,
    "input_throughput": 1298.0903005331854,
    "output_throughput": 1129.5760447352309,
    "total_throughput": 2427.6663452684165,
    "itl": 23.089961979432957,
    "ttft": 3672.8462734780105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 18819,
    "finished_requests": 18800,
    "scheduler_time": 0.0006456269186362933
}
#Debug simulation 
Total elapsed time: 1.6625853450968862. Arrivals time: 0.053189277183264494 Scheduler time: 1.2605707636103034 Scheduler overhead time: 0.13028495339676738 Adapter cache time: 0.024152738507837057 Engine time: 0.13029539957642555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 8640, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55215 . Total input tokens: 12258210 . Total output tokens: 10858024
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6456289538182318,
    "estimated_duration": 3599.866235740847,
    "input_throughput": 1275.2196052238194,
    "output_throughput": 1123.7489770692755,
    "total_throughput": 2398.968582293095,
    "itl": 23.010204382347805,
    "ttft": 6225.868768116811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 18617,
    "finished_requests": 18585,
    "scheduler_time": 1.5011748162475273e-05
}
#Debug simulation 
Total elapsed time: 1.645737080834806. Arrivals time: 0.05296369595453143 Scheduler time: 1.2433350654318929 Scheduler overhead time: 0.130544972140342 Adapter cache time: 0.023925072979182005 Engine time: 0.13034259900450706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 8640, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55215 . Total input tokens: 12258210 . Total output tokens: 10858024
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.655684377066791,
    "estimated_duration": 3599.8531751158926,
    "input_throughput": 1275.2242318472368,
    "output_throughput": 1123.7530541422054,
    "total_throughput": 2398.977285989442,
    "itl": 23.010325444163993,
    "ttft": 6225.99105515251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 18617,
    "finished_requests": 18585,
    "scheduler_time": 1.5011748162475273e-05
}
#Debug simulation 
Total elapsed time: 1.6557861836627126. Arrivals time: 0.05385746667161584 Scheduler time: 1.249951892066747 Scheduler overhead time: 0.13111553993076086 Adapter cache time: 0.02369980001822114 Engine time: 0.1326312473975122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 8640, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55215 . Total input tokens: 12258210 . Total output tokens: 10858024
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.63569689495489,
    "estimated_duration": 3599.8569637442074,
    "input_throughput": 1275.2228897520697,
    "output_throughput": 1123.7518714611483,
    "total_throughput": 2398.9747612132182,
    "itl": 23.01035177084195,
    "ttft": 6226.038178456459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 18617,
    "finished_requests": 18585,
    "scheduler_time": 1.5011748162475273e-05
}
#Debug simulation 
Total elapsed time: 1.6357986028306186. Arrivals time: 0.05308617511764169 Scheduler time: 1.2295335452072322 Scheduler overhead time: 0.13117479579523206 Adapter cache time: 0.024138974957168102 Engine time: 0.13374376762658358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 8640, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55215 . Total input tokens: 12258210 . Total output tokens: 10858024
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6401170333847404,
    "estimated_duration": 3599.873332840386,
    "input_throughput": 1275.2170911463409,
    "output_throughput": 1123.7467616140052,
    "total_throughput": 2398.963852760346,
    "itl": 23.01025423122851,
    "ttft": 6225.87997383517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 18617,
    "finished_requests": 18585,
    "scheduler_time": 1.5011748162475273e-05
}
#Debug simulation 
Total elapsed time: 1.6402219701558352. Arrivals time: 0.053646416403353214 Scheduler time: 1.2365295197814703 Scheduler overhead time: 0.13187453662976623 Adapter cache time: 0.023860894609242678 Engine time: 0.12952110636979342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 8640, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55215 . Total input tokens: 12258210 . Total output tokens: 10858024
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.635494484566152,
    "estimated_duration": 3599.8556878537775,
    "input_throughput": 1275.2233417270438,
    "output_throughput": 1123.7522697505194,
    "total_throughput": 2398.9756114775632,
    "itl": 23.010349833603243,
    "ttft": 6226.025340697448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 18617,
    "finished_requests": 18585,
    "scheduler_time": 1.5011748162475273e-05
}
#Debug simulation 
Total elapsed time: 1.6355967298150063. Arrivals time: 0.05226712767034769 Scheduler time: 1.2342030215077102 Scheduler overhead time: 0.13056319719180465 Adapter cache time: 0.02380303619429469 Engine time: 0.1303385654464364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 8640, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55215 . Total input tokens: 12258210 . Total output tokens: 10858024
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6444831192493439,
    "estimated_duration": 3599.8642345234,
    "input_throughput": 1275.2203141371442,
    "output_throughput": 1123.7496017778512,
    "total_throughput": 2398.9699159149955,
    "itl": 23.01036689719581,
    "ttft": 6225.8618348321515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 18617,
    "finished_requests": 18585,
    "scheduler_time": 1.417776215344887e-05
}
#Debug simulation 
Total elapsed time: 1.644596841186285. Arrivals time: 0.05406555300578475 Scheduler time: 1.2384892986156046 Scheduler overhead time: 0.13078955421224236 Adapter cache time: 0.02388606918975711 Engine time: 0.1329406565055251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [5 5 6]
Adapter prompts. [540, 135, 135, 540, 8640, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 55215 . Total input tokens: 12258210 . Total output tokens: 10858024
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6516115157864988,
    "estimated_duration": 3599.8531941437845,
    "input_throughput": 1275.2242251067314,
    "output_throughput": 1123.7530482023378,
    "total_throughput": 2398.9772733090695,
    "itl": 23.010316230815036,
    "ttft": 6225.98602647782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 18617,
    "finished_requests": 18585,
    "scheduler_time": 1.5011748162475273e-05
}
#Debug simulation 
Total elapsed time: 1.6517437756992877. Arrivals time: 0.052750533912330866 Scheduler time: 1.250418686773628 Scheduler overhead time: 0.13067425740882754 Adapter cache time: 0.02400236250832677 Engine time: 0.12944734608754516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 8640, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54870 . Total input tokens: 12181996 . Total output tokens: 10786850
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6319326451048255,
    "estimated_duration": 3599.7660785573294,
    "input_throughput": 1264.1440862245536,
    "output_throughput": 1111.3388794427708,
    "total_throughput": 2375.4829656673246,
    "itl": 22.91564051770408,
    "ttft": 5092.149424686917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 18519,
    "finished_requests": 18493,
    "scheduler_time": 0.00010437289108160828
}
#Debug simulation 
Total elapsed time: 1.6320267152041197. Arrivals time: 0.05346864042803645 Scheduler time: 1.2254571048542857 Scheduler overhead time: 0.1296242312528193 Adapter cache time: 0.027130214497447014 Engine time: 0.13131672330200672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 8640, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54870 . Total input tokens: 12181996 . Total output tokens: 10786850
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6347962142899632,
    "estimated_duration": 3599.7548580541857,
    "input_throughput": 1264.1480265852874,
    "output_throughput": 1111.3423435068203,
    "total_throughput": 2375.4903700921077,
    "itl": 22.915664765501674,
    "ttft": 5092.218656615541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 18519,
    "finished_requests": 18493,
    "scheduler_time": 0.00010520687709063468
}
#Debug simulation 
Total elapsed time: 1.6348949349485338. Arrivals time: 0.05288032861426473 Scheduler time: 1.2299345075152814 Scheduler overhead time: 0.13143786462023854 Adapter cache time: 0.024070726241916418 Engine time: 0.13185803405940533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 8640, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54870 . Total input tokens: 12181996 . Total output tokens: 10786850
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6736279847100377,
    "estimated_duration": 3599.7633582423873,
    "input_throughput": 1264.1450415290292,
    "output_throughput": 1111.3397192734649,
    "total_throughput": 2375.484760802494,
    "itl": 22.915714406719008,
    "ttft": 5092.304719726547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 18519,
    "finished_requests": 18493,
    "scheduler_time": 0.00010032808304035896
}
#Debug simulation 
Total elapsed time: 1.6737245447002351. Arrivals time: 0.054344607051461935 Scheduler time: 1.2666634172201157 Scheduler overhead time: 0.13197612296789885 Adapter cache time: 0.02427378064021468 Engine time: 0.131372791249305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 8640, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54870 . Total input tokens: 12181996 . Total output tokens: 10786850
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6765881278552115,
    "estimated_duration": 3599.771814173968,
    "input_throughput": 1264.142072028591,
    "output_throughput": 1111.3371087155979,
    "total_throughput": 2375.479180744189,
    "itl": 22.91568276992228,
    "ttft": 5092.097781418616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 18519,
    "finished_requests": 18493,
    "scheduler_time": 0.00011413047918215971
}
#Debug simulation 
Total elapsed time: 1.6767107490450144. Arrivals time: 0.05491443397477269 Scheduler time: 1.2666527023538947 Scheduler overhead time: 0.13210642291232944 Adapter cache time: 0.02398016070947051 Engine time: 0.13344876002520323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 8640, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54870 . Total input tokens: 12181996 . Total output tokens: 10786850
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.6565052988007665,
    "estimated_duration": 3599.762710994429,
    "input_throughput": 1264.145268826038,
    "output_throughput": 1111.3399190956259,
    "total_throughput": 2375.485187921664,
    "itl": 22.915721196295628,
    "ttft": 5092.295676274259,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 18519,
    "finished_requests": 18493,
    "scheduler_time": 0.00010032808304035896
}
#Debug simulation 
Total elapsed time: 1.6566027668304741. Arrivals time: 0.05315382219851017 Scheduler time: 1.250384354032576 Scheduler overhead time: 0.13224770780652761 Adapter cache time: 0.02396270027384162 Engine time: 0.13171302387490869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 8640, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54870 . Total input tokens: 12181996 . Total output tokens: 10786850
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6478118388913572,
    "estimated_duration": 3599.763778336292,
    "input_throughput": 1264.1448940027858,
    "output_throughput": 1111.3395895796652,
    "total_throughput": 2375.484483582451,
    "itl": 22.9156339278147,
    "ttft": 5092.206609485595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 18519,
    "finished_requests": 18493,
    "scheduler_time": 9.628327499910965e-05
}
#Debug simulation 
Total elapsed time: 1.6479128780774772. Arrivals time: 0.054346281103789806 Scheduler time: 1.240212813951075 Scheduler overhead time: 0.13143909443169832 Adapter cache time: 0.02400772087275982 Engine time: 0.13309917179867625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [5 5 6]
Adapter prompts. [540, 66, 66, 540, 8640, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54870 . Total input tokens: 12181996 . Total output tokens: 10786850
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.663985373917967,
    "estimated_duration": 3599.757162492364,
    "input_throughput": 1264.1472173220943,
    "output_throughput": 1111.341632064462,
    "total_throughput": 2375.488849386556,
    "itl": 22.915651350852084,
    "ttft": 5092.312671698446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 18519,
    "finished_requests": 18493,
    "scheduler_time": 0.00010437289108160828
}
#Debug simulation 
Total elapsed time: 1.66408482613042. Arrivals time: 0.05484368046745658 Scheduler time: 1.2540049427188933 Scheduler overhead time: 0.13146394677460194 Adapter cache time: 0.02450045058503747 Engine time: 0.13168588420376182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 8640, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54705 . Total input tokens: 12148169 . Total output tokens: 10753418
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6697480538859963,
    "estimated_duration": 3599.025213993942,
    "input_throughput": 1261.7194184535222,
    "output_throughput": 1103.0772956420533,
    "total_throughput": 2364.7967140955757,
    "itl": 22.908454568052306,
    "ttft": 7640.507739647053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 18467,
    "finished_requests": 18428,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6698501599021256. Arrivals time: 0.055076895747333765 Scheduler time: 1.2618383853696287 Scheduler overhead time: 0.13136981846764684 Adapter cache time: 0.02395357098430395 Engine time: 0.1328149251639843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 8640, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54705 . Total input tokens: 12148169 . Total output tokens: 10753418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.659933503717184,
    "estimated_duration": 3599.020421863636,
    "input_throughput": 1261.7210984450628,
    "output_throughput": 1103.0787644000816,
    "total_throughput": 2364.7998628451446,
    "itl": 22.908539312721622,
    "ttft": 7640.587173247016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 18467,
    "finished_requests": 18428,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6600262238644063. Arrivals time: 0.054149032570421696 Scheduler time: 1.2529891678132117 Scheduler overhead time: 0.13260054914280772 Adapter cache time: 0.02376224985346198 Engine time: 0.1311950790695846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 8640, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54705 . Total input tokens: 12148169 . Total output tokens: 10753418
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6695416700094938,
    "estimated_duration": 3599.021344512503,
    "input_throughput": 1261.7207749889255,
    "output_throughput": 1103.0784816136586,
    "total_throughput": 2364.799256602584,
    "itl": 22.90866431217321,
    "ttft": 7640.5453817906155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 18467,
    "finished_requests": 18428,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6696413829922676. Arrivals time: 0.055112081579864025 Scheduler time: 1.259392770472914 Scheduler overhead time: 0.13230367377400398 Adapter cache time: 0.024090271908789873 Engine time: 0.13360816752538085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 8640, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54705 . Total input tokens: 12148169 . Total output tokens: 10753418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6536590731702745,
    "estimated_duration": 3599.029018013388,
    "input_throughput": 1261.7180848701644,
    "output_throughput": 1103.0761297366212,
    "total_throughput": 2364.7942146067853,
    "itl": 22.90846214449066,
    "ttft": 7640.637626938781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 18467,
    "finished_requests": 18428,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.653762994799763. Arrivals time: 0.05299099953845143 Scheduler time: 1.2488401154987514 Scheduler overhead time: 0.13205423997715116 Adapter cache time: 0.023823097348213196 Engine time: 0.13114630291238427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 8640, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54705 . Total input tokens: 12148169 . Total output tokens: 10753418
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.6812726547941566,
    "estimated_duration": 3599.0212217086196,
    "input_throughput": 1261.7208180406892,
    "output_throughput": 1103.0785192523144,
    "total_throughput": 2364.7993372930036,
    "itl": 22.908672272982205,
    "ttft": 7640.550926914676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 18467,
    "finished_requests": 18428,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6813939958810806. Arrivals time: 0.05502447113394737 Scheduler time: 1.2733973297290504 Scheduler overhead time: 0.13122917944565415 Adapter cache time: 0.024065370671451092 Engine time: 0.13268840219825506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 8640, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54705 . Total input tokens: 12148169 . Total output tokens: 10753418
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6661715409718454,
    "estimated_duration": 3599.0224006751964,
    "input_throughput": 1261.7204047265977,
    "output_throughput": 1103.0781579062152,
    "total_throughput": 2364.7985626328127,
    "itl": 22.9083974354182,
    "ttft": 7640.610658554819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 18467,
    "finished_requests": 18428,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6662680660374463. Arrivals time: 0.054475787095725536 Scheduler time: 1.2486137058585882 Scheduler overhead time: 0.13452992402017117 Adapter cache time: 0.02377457031980157 Engine time: 0.1391731179319322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [5 5 6]
Adapter prompts. [540, 33, 33, 540, 8640, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 8640]
Prompts retrieved: 54705 . Total input tokens: 12148169 . Total output tokens: 10753418
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6542427837848663,
    "estimated_duration": 3599.02113583393,
    "input_throughput": 1261.7208481460648,
    "output_throughput": 1103.078545572395,
    "total_throughput": 2364.7993937184597,
    "itl": 22.90867868603335,
    "ttft": 7640.5801053135565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 18467,
    "finished_requests": 18428,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6543356678448617. Arrivals time: 0.0537976142950356 Scheduler time: 1.2437327499501407 Scheduler overhead time: 0.13280833838507533 Adapter cache time: 0.02385749388486147 Engine time: 0.13433753652498126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 8640, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53865 . Total input tokens: 11960736 . Total output tokens: 10596570
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6199201135896146,
    "estimated_duration": 3599.96746011476,
    "input_throughput": 1247.707111179503,
    "output_throughput": 1092.2904286125545,
    "total_throughput": 2339.9975397920575,
    "itl": 22.79694537526904,
    "ttft": 6169.977553324711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 18199,
    "finished_requests": 18168,
    "scheduler_time": 6.671888072211233e-06
}
#Debug simulation 
Total elapsed time: 1.620051879901439. Arrivals time: 0.05349582899361849 Scheduler time: 1.2134405239485204 Scheduler overhead time: 0.13371056504547596 Adapter cache time: 0.023783891461789608 Engine time: 0.1298736804164946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 8640, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53865 . Total input tokens: 11960736 . Total output tokens: 10596570
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.650893003679812,
    "estimated_duration": 3599.955000246112,
    "input_throughput": 1247.7114296409047,
    "output_throughput": 1092.294209158496,
    "total_throughput": 2340.005638799401,
    "itl": 22.7970065798194,
    "ttft": 6169.857041296242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 18199,
    "finished_requests": 18168,
    "scheduler_time": 5.8379020631848285e-06
}
#Debug simulation 
Total elapsed time: 1.6509936871007085. Arrivals time: 0.05382092157378793 Scheduler time: 1.2397874295711517 Scheduler overhead time: 0.13385826023295522 Adapter cache time: 0.02388538420200348 Engine time: 0.13360545877367258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 8640, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53865 . Total input tokens: 11960736 . Total output tokens: 10596570
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.624486851040274,
    "estimated_duration": 3599.961891042338,
    "input_throughput": 1247.70904135862,
    "output_throughput": 1092.292118365026,
    "total_throughput": 2340.0011597236457,
    "itl": 22.797072514109914,
    "ttft": 6169.913456386317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 18199,
    "finished_requests": 18168,
    "scheduler_time": 5.8379020631848285e-06
}
#Debug simulation 
Total elapsed time: 1.6246040230616927. Arrivals time: 0.05620379373431206 Scheduler time: 1.2174435420893133 Scheduler overhead time: 0.13227814622223377 Adapter cache time: 0.02354218205437064 Engine time: 0.12995209684595466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 8640, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53865 . Total input tokens: 11960736 . Total output tokens: 10596570
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6486483723856509,
    "estimated_duration": 3599.947627742429,
    "input_throughput": 1247.6137056517614,
    "output_throughput": 1092.1975557921417,
    "total_throughput": 2339.8112614439033,
    "itl": 22.796827093671734,
    "ttft": 6367.6360082292795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 18199,
    "finished_requests": 18167,
    "scheduler_time": 6.671888072211233e-06
}
#Debug simulation 
Total elapsed time: 1.6488064182922244. Arrivals time: 0.05419615237042308 Scheduler time: 1.2384828557260334 Scheduler overhead time: 0.1318628415465355 Adapter cache time: 0.023526315111666918 Engine time: 0.13522737985476851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 8640, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53865 . Total input tokens: 11960736 . Total output tokens: 10596570
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.623671348206699,
    "estimated_duration": 3599.960278382168,
    "input_throughput": 1247.7096002899746,
    "output_throughput": 1092.2926076748674,
    "total_throughput": 2340.002207964842,
    "itl": 22.797058646061004,
    "ttft": 6169.932887262952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 18199,
    "finished_requests": 18168,
    "scheduler_time": 5.8379020631848285e-06
}
#Debug simulation 
Total elapsed time: 1.6237791762687266. Arrivals time: 0.0522650396451354 Scheduler time: 1.2193917855620384 Scheduler overhead time: 0.13142018765211105 Adapter cache time: 0.023757662624120712 Engine time: 0.1318240617401898 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 8640, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53865 . Total input tokens: 11960736 . Total output tokens: 10596570
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.613657047972083,
    "estimated_duration": 3599.964553723739,
    "input_throughput": 1247.7081185018505,
    "output_throughput": 1092.2913104609856,
    "total_throughput": 2339.999428962836,
    "itl": 22.796954271897715,
    "ttft": 6169.886199087555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 18199,
    "finished_requests": 18168,
    "scheduler_time": 5.8379020631848285e-06
}
#Debug simulation 
Total elapsed time: 1.6137682045809925. Arrivals time: 0.052952765952795744 Scheduler time: 1.206387990154326 Scheduler overhead time: 0.13448267616331577 Adapter cache time: 0.023664483800530434 Engine time: 0.1308999191969633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 8640, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53865 . Total input tokens: 11960736 . Total output tokens: 10596570
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6278387210331857,
    "estimated_duration": 3599.9594705129093,
    "input_throughput": 1247.7098802893017,
    "output_throughput": 1092.2928527969657,
    "total_throughput": 2340.0027330862677,
    "itl": 22.797043968197517,
    "ttft": 6169.938270247051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 18199,
    "finished_requests": 18168,
    "scheduler_time": 5.8379020631848285e-06
}
#Debug simulation 
Total elapsed time: 1.6279695089906454. Arrivals time: 0.05221701646223664 Scheduler time: 1.2225541584193707 Scheduler overhead time: 0.13252165541052818 Adapter cache time: 0.023849457502365112 Engine time: 0.13114682491868734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 8640, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53520 . Total input tokens: 11884777 . Total output tokens: 10531483
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6130126579664648,
    "estimated_duration": 3600.0051811715434,
    "input_throughput": 1225.9601800250005,
    "output_throughput": 1082.3498311555152,
    "total_throughput": 2308.3100111805156,
    "itl": 22.719439298594292,
    "ttft": 5214.8256827392715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 18079,
    "finished_requests": 18053,
    "scheduler_time": 7.505874081237637e-06
}
#Debug simulation 
Total elapsed time: 1.6131553738377988. Arrivals time: 0.05291057052090764 Scheduler time: 1.2057154825888574 Scheduler overhead time: 0.13294307934120297 Adapter cache time: 0.02333770925179124 Engine time: 0.13298311037942767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 8640, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53520 . Total input tokens: 11884777 . Total output tokens: 10531483
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.60300378408283,
    "estimated_duration": 3600.0039212325864,
    "input_throughput": 1225.9606090898083,
    "output_throughput": 1082.3502099591908,
    "total_throughput": 2308.3108190489993,
    "itl": 22.717623280342313,
    "ttft": 5214.786714360505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 18079,
    "finished_requests": 18053,
    "scheduler_time": 7.505874081237637e-06
}
#Debug simulation 
Total elapsed time: 1.6031028148718178. Arrivals time: 0.05230318522080779 Scheduler time: 1.196972705423832 Scheduler overhead time: 0.13291308842599392 Adapter cache time: 0.023486453108489513 Engine time: 0.13205415336415172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 8640, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53520 . Total input tokens: 11884777 . Total output tokens: 10531483
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6035615229047835,
    "estimated_duration": 3600.0054474309495,
    "input_throughput": 1225.9600893519628,
    "output_throughput": 1082.3497511040189,
    "total_throughput": 2308.309840455982,
    "itl": 22.717659201595893,
    "ttft": 5214.87184901452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 18079,
    "finished_requests": 18053,
    "scheduler_time": 7.505874081237637e-06
}
#Debug simulation 
Total elapsed time: 1.6036737086251378. Arrivals time: 0.052726661786437035 Scheduler time: 1.1991044823080301 Scheduler overhead time: 0.13225711742416024 Adapter cache time: 0.023573820013552904 Engine time: 0.13066954677924514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 8640, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53520 . Total input tokens: 11884777 . Total output tokens: 10531483
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6243149568326771,
    "estimated_duration": 3600.0122376468908,
    "input_throughput": 1225.9577769893394,
    "output_throughput": 1082.3477096141435,
    "total_throughput": 2308.305486603483,
    "itl": 22.71944178889504,
    "ttft": 5214.849491522224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 18079,
    "finished_requests": 18053,
    "scheduler_time": 7.505874081237637e-06
}
#Debug simulation 
Total elapsed time: 1.624445921741426. Arrivals time: 0.052521290723234415 Scheduler time: 1.2155644386075437 Scheduler overhead time: 0.13263589469715953 Adapter cache time: 0.023498493246734142 Engine time: 0.13454358978196979 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 8640, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53520 . Total input tokens: 11884777 . Total output tokens: 10531483
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.6179115623235703,
    "estimated_duration": 3600.0052705227463,
    "input_throughput": 1225.9601495969848,
    "output_throughput": 1082.3498042918714,
    "total_throughput": 2308.3099538888564,
    "itl": 22.717630936154233,
    "ttft": 5214.872644666122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 18079,
    "finished_requests": 18053,
    "scheduler_time": 7.505874081237637e-06
}
#Debug simulation 
Total elapsed time: 1.618024638388306. Arrivals time: 0.053420282900333405 Scheduler time: 1.2094022338278592 Scheduler overhead time: 0.13313181372359395 Adapter cache time: 0.023326606024056673 Engine time: 0.13325810711830854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 8640, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53520 . Total input tokens: 11884777 . Total output tokens: 10531483
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.614861885085702,
    "estimated_duration": 3600.0030120264605,
    "input_throughput": 1225.960918714798,
    "output_throughput": 1082.3504833143625,
    "total_throughput": 2308.3114020291605,
    "itl": 22.71947699828482,
    "ttft": 5214.822137550912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 18079,
    "finished_requests": 18053,
    "scheduler_time": 7.505874081237637e-06
}
#Debug simulation 
Total elapsed time: 1.6149634113535285. Arrivals time: 0.05236286111176014 Scheduler time: 1.2090803603641689 Scheduler overhead time: 0.13418591115623713 Adapter cache time: 0.023304881062358618 Engine time: 0.13067439990118146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 8640, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53520 . Total input tokens: 11884777 . Total output tokens: 10531483
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6037319097667933,
    "estimated_duration": 3600.0045827319946,
    "input_throughput": 1225.9603838200346,
    "output_throughput": 1082.350011077771,
    "total_throughput": 2308.3103948978055,
    "itl": 22.71765110560501,
    "ttft": 5214.807085258027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 18079,
    "finished_requests": 18053,
    "scheduler_time": 8.33986009026404e-06
}
#Debug simulation 
Total elapsed time: 1.6038418617099524. Arrivals time: 0.05251347925513983 Scheduler time: 1.1977166323922575 Scheduler overhead time: 0.13324322691187263 Adapter cache time: 0.023323463276028633 Engine time: 0.1317666689865291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 8640, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53355 . Total input tokens: 11845135 . Total output tokens: 10501708
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6166447489522398,
    "estimated_duration": 3599.8631279451706,
    "input_throughput": 1218.9471777235954,
    "output_throughput": 1086.4207501779129,
    "total_throughput": 2305.367927901508,
    "itl": 22.725645332790982,
    "ttft": 6033.657577786521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 18012,
    "finished_requests": 17982,
    "scheduler_time": 8.310648887672244e-05
}
#Debug simulation 
Total elapsed time: 1.6167515171691775. Arrivals time: 0.053182387724518776 Scheduler time: 1.2097329027019441 Scheduler overhead time: 0.132219054736197 Adapter cache time: 0.023242854978889227 Engine time: 0.1329211750999093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 8640, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53355 . Total input tokens: 11845135 . Total output tokens: 10501708
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6157151660881937,
    "estimated_duration": 3599.880472365152,
    "input_throughput": 1218.9413047697717,
    "output_throughput": 1086.4155157436276,
    "total_throughput": 2305.3568205133993,
    "itl": 22.725638982884384,
    "ttft": 6033.441595127478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 18012,
    "finished_requests": 17982,
    "scheduler_time": 8.072965285352594e-05
}
#Debug simulation 
Total elapsed time: 1.6158609711565077. Arrivals time: 0.05272134719416499 Scheduler time: 1.208783505950123 Scheduler overhead time: 0.1330026830546558 Adapter cache time: 0.022951685823500156 Engine time: 0.1326586394570768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 8640, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53355 . Total input tokens: 11845135 . Total output tokens: 10501708
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.610988370142877,
    "estimated_duration": 3599.85804911855,
    "input_throughput": 1218.9488974640105,
    "output_throughput": 1086.4222829446364,
    "total_throughput": 2305.371180408647,
    "itl": 22.725631003205947,
    "ttft": 6033.45863058447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 18012,
    "finished_requests": 17982,
    "scheduler_time": 8.072965285352594e-05
}
#Debug simulation 
Total elapsed time: 1.6110943430103362. Arrivals time: 0.05277356365695596 Scheduler time: 1.2047461103647947 Scheduler overhead time: 0.13254186371341348 Adapter cache time: 0.02348523773252964 Engine time: 0.13202452193945646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 8640, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53355 . Total input tokens: 11845135 . Total output tokens: 10501708
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.636720738839358,
    "estimated_duration": 3599.8665126734163,
    "input_throughput": 1218.9460316241698,
    "output_throughput": 1086.4197286847584,
    "total_throughput": 2305.365760308928,
    "itl": 22.72554979513736,
    "ttft": 6033.603526149013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 18012,
    "finished_requests": 17982,
    "scheduler_time": 8.227250286769604e-05
}
#Debug simulation 
Total elapsed time: 1.6368264239281416. Arrivals time: 0.056302179116755724 Scheduler time: 1.2224694308824837 Scheduler overhead time: 0.13469424191862345 Adapter cache time: 0.02324564289301634 Engine time: 0.13419354986399412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 8640, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53355 . Total input tokens: 11845135 . Total output tokens: 10501708
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.6210311721079051,
    "estimated_duration": 3599.8565908944897,
    "input_throughput": 1218.9493912338498,
    "output_throughput": 1086.42272303081,
    "total_throughput": 2305.3721142646596,
    "itl": 22.725614349364733,
    "ttft": 6033.3913209073935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 18012,
    "finished_requests": 17982,
    "scheduler_time": 8.072965285352594e-05
}
#Debug simulation 
Total elapsed time: 1.6211376483552158. Arrivals time: 0.05341690592467785 Scheduler time: 1.211194726638496 Scheduler overhead time: 0.1336187869310379 Adapter cache time: 0.02317622024565935 Engine time: 0.13395547354593873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 8640, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53355 . Total input tokens: 11845135 . Total output tokens: 10501708
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.617771829944104,
    "estimated_duration": 3599.8601435971063,
    "input_throughput": 1218.9481882524785,
    "output_throughput": 1086.4216508400311,
    "total_throughput": 2305.3698390925097,
    "itl": 22.72565957017415,
    "ttft": 6033.58013639325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 18012,
    "finished_requests": 17982,
    "scheduler_time": 8.310648887672244e-05
}
#Debug simulation 
Total elapsed time: 1.617880375124514. Arrivals time: 0.05301364650949836 Scheduler time: 1.2115520774386823 Scheduler overhead time: 0.13294358830899 Adapter cache time: 0.023247493896633387 Engine time: 0.1318186055868864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 8640, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 8640]
Prompts retrieved: 53355 . Total input tokens: 11845135 . Total output tokens: 10501708
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6143512721173465,
    "estimated_duration": 3599.8561167164476,
    "input_throughput": 1218.9495517955547,
    "output_throughput": 1086.4228661359184,
    "total_throughput": 2305.372417931473,
    "itl": 22.72564434761216,
    "ttft": 6033.426534985812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 18012,
    "finished_requests": 17982,
    "scheduler_time": 8.156363886255234e-05
}
#Debug simulation 
Total elapsed time: 1.6144673968665302. Arrivals time: 0.051474608946591616 Scheduler time: 1.2082416000775993 Scheduler overhead time: 0.13226834451779723 Adapter cache time: 0.023088264744728804 Engine time: 0.13410302950069308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 8640, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52845 . Total input tokens: 11728666 . Total output tokens: 10390801
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.603424611967057,
    "estimated_duration": 3599.9383106552764,
    "input_throughput": 1224.4079258118384,
    "output_throughput": 1074.755361376106,
    "total_throughput": 2299.163287187944,
    "itl": 22.61644436821911,
    "ttft": 6492.97363602999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 17845,
    "finished_requests": 17813,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.603529036976397. Arrivals time: 0.05285985581576824 Scheduler time: 1.1962018702179193 Scheduler overhead time: 0.13242106279358268 Adapter cache time: 0.02310793474316597 Engine time: 0.1336344052106142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 8640, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52845 . Total input tokens: 11728666 . Total output tokens: 10390801
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6101366677321494,
    "estimated_duration": 3599.9486490928207,
    "input_throughput": 1224.40440952144,
    "output_throughput": 1074.7522748623076,
    "total_throughput": 2299.1566843837472,
    "itl": 22.616502422786674,
    "ttft": 6493.020404821227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 17845,
    "finished_requests": 17813,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6102582160383463. Arrivals time: 0.05244914162904024 Scheduler time: 1.202530404087156 Scheduler overhead time: 0.13319448055699468 Adapter cache time: 0.023047212976962328 Engine time: 0.13345144176855683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 8640, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52845 . Total input tokens: 11728666 . Total output tokens: 10390801
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.612454554066062,
    "estimated_duration": 3599.95404802762,
    "input_throughput": 1224.4025732536745,
    "output_throughput": 1074.7506630313285,
    "total_throughput": 2299.153236285003,
    "itl": 22.616544850644516,
    "ttft": 6492.993400822089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 17845,
    "finished_requests": 17813,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6125492001883686. Arrivals time: 0.05098202172666788 Scheduler time: 1.2085976689122617 Scheduler overhead time: 0.13314348459243774 Adapter cache time: 0.02303170831874013 Engine time: 0.13133841240778565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 8640, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52845 . Total input tokens: 11728666 . Total output tokens: 10390801
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6031001098453999,
    "estimated_duration": 3599.9434886220765,
    "input_throughput": 1224.4061646887512,
    "output_throughput": 1074.7538155052896,
    "total_throughput": 2299.159980194041,
    "itl": 22.616546411423503,
    "ttft": 6492.968139257811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 17845,
    "finished_requests": 17813,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6031973878853023. Arrivals time: 0.05082900309935212 Scheduler time: 1.1989528960548341 Scheduler overhead time: 0.1329304249957204 Adapter cache time: 0.023158698808401823 Engine time: 0.13190098386257887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 8640, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52845 . Total input tokens: 11728666 . Total output tokens: 10390801
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.6077253250405192,
    "estimated_duration": 3599.9539091615125,
    "input_throughput": 1224.4026204842846,
    "output_throughput": 1074.750704489204,
    "total_throughput": 2299.153324973489,
    "itl": 22.61652115810293,
    "ttft": 6493.020319097335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 17845,
    "finished_requests": 17813,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.607833443209529. Arrivals time: 0.05349266482517123 Scheduler time: 1.1984284855425358 Scheduler overhead time: 0.13291412265971303 Adapter cache time: 0.023138297721743584 Engine time: 0.13413484022021294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 8640, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52845 . Total input tokens: 11728666 . Total output tokens: 10390801
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6042285179719329,
    "estimated_duration": 3599.9353322248435,
    "input_throughput": 1224.4089388338766,
    "output_throughput": 1074.7562505821002,
    "total_throughput": 2299.165189415977,
    "itl": 22.61648773439841,
    "ttft": 6492.988529576623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 17845,
    "finished_requests": 17813,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6043224539607763. Arrivals time: 0.052952214144170284 Scheduler time: 1.1975588318891823 Scheduler overhead time: 0.13381820265203714 Adapter cache time: 0.023040972650051117 Engine time: 0.13138297852128744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 8640, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52845 . Total input tokens: 11728666 . Total output tokens: 10390801
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.5922860424034297,
    "estimated_duration": 3599.952823745061,
    "input_throughput": 1224.4029896521079,
    "output_throughput": 1074.7510285356996,
    "total_throughput": 2299.154018187807,
    "itl": 22.616470007873826,
    "ttft": 6493.017463420291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 17845,
    "finished_requests": 17813,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5923837162554264. Arrivals time: 0.05214067082852125 Scheduler time: 1.186913295648992 Scheduler overhead time: 0.13254360016435385 Adapter cache time: 0.023398599587380886 Engine time: 0.1317079379223287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 8640, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52680 . Total input tokens: 11692233 . Total output tokens: 10356480
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.611361827235669,
    "estimated_duration": 3599.7492247755376,
    "input_throughput": 1222.7596216162706,
    "output_throughput": 1066.765977354839,
    "total_throughput": 2289.5255989711095,
    "itl": 22.587438518553743,
    "ttft": 4692.388560176471,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 17787,
    "finished_requests": 17764,
    "scheduler_time": 0.001057863840850228
}
#Debug simulation 
Total elapsed time: 1.61145845009014. Arrivals time: 0.05292024789378047 Scheduler time: 1.202460072003305 Scheduler overhead time: 0.13395080575719476 Adapter cache time: 0.022794642951339483 Engine time: 0.1333879018202424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 8640, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52680 . Total input tokens: 11692233 . Total output tokens: 10356480
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6018924647942185,
    "estimated_duration": 3599.7505298832093,
    "input_throughput": 1222.7591782986158,
    "output_throughput": 1066.7655905934648,
    "total_throughput": 2289.524768892081,
    "itl": 22.589061408703895,
    "ttft": 4692.5089267409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 17787,
    "finished_requests": 17764,
    "scheduler_time": 0.0010489402387587033
}
#Debug simulation 
Total elapsed time: 1.6019865428097546. Arrivals time: 0.052149499766528606 Scheduler time: 1.1950298864394426 Scheduler overhead time: 0.13366230810061097 Adapter cache time: 0.022899625822901726 Engine time: 0.13253679824993014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 8640, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52680 . Total input tokens: 11692233 . Total output tokens: 10356480
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.6206925818696618,
    "estimated_duration": 3599.756515384969,
    "input_throughput": 1222.7571451535455,
    "output_throughput": 1066.7638168270191,
    "total_throughput": 2289.5209619805646,
    "itl": 22.589073609905185,
    "ttft": 4692.440195919707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 17787,
    "finished_requests": 17764,
    "scheduler_time": 0.0010376398006439814
}
#Debug simulation 
Total elapsed time: 1.620790485292673. Arrivals time: 0.05304882628843188 Scheduler time: 1.2069774814881384 Scheduler overhead time: 0.13391730142757297 Adapter cache time: 0.023523144889622927 Engine time: 0.13638551998883486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 8640, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52680 . Total input tokens: 11692233 . Total output tokens: 10356480
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.6033392581157386,
    "estimated_duration": 3599.7534450571056,
    "input_throughput": 1222.7581880764542,
    "output_throughput": 1066.7647266989648,
    "total_throughput": 2289.522914775419,
    "itl": 22.587485213729792,
    "ttft": 4692.366582789566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 17787,
    "finished_requests": 17764,
    "scheduler_time": 0.0010481062527496765
}
#Debug simulation 
Total elapsed time: 1.603430070914328. Arrivals time: 0.052482934668660164 Scheduler time: 1.1932444707490504 Scheduler overhead time: 0.13363252021372318 Adapter cache time: 0.023105877451598644 Engine time: 0.13461515260860324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 8640, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52680 . Total input tokens: 11692233 . Total output tokens: 10356480
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.5890374211594462,
    "estimated_duration": 3599.7555838381572,
    "input_throughput": 1222.7574615793399,
    "output_throughput": 1066.7640928847707,
    "total_throughput": 2289.5215544641105,
    "itl": 22.589073197247714,
    "ttft": 4692.410991479887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 17787,
    "finished_requests": 17764,
    "scheduler_time": 0.001036805814634955
}
#Debug simulation 
Total elapsed time: 1.589147747028619. Arrivals time: 0.05096863629296422 Scheduler time: 1.184355495031923 Scheduler overhead time: 0.13350791716948152 Adapter cache time: 0.023018925450742245 Engine time: 0.13136842101812363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 8640, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52680 . Total input tokens: 11692233 . Total output tokens: 10356480
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.6057448098435998,
    "estimated_duration": 3599.7444328369456,
    "input_throughput": 1222.7612493398851,
    "output_throughput": 1066.7673974214995,
    "total_throughput": 2289.5286467613846,
    "itl": 22.587458171291683,
    "ttft": 4692.480147720179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 17787,
    "finished_requests": 17764,
    "scheduler_time": 0.0010465634027355063
}
#Debug simulation 
Total elapsed time: 1.605845304671675. Arrivals time: 0.05251038074493408 Scheduler time: 1.1937852534465492 Scheduler overhead time: 0.13339210068807006 Adapter cache time: 0.022848878987133503 Engine time: 0.13715942669659853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 8640, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 8640]
Prompts retrieved: 52680 . Total input tokens: 11692233 . Total output tokens: 10356480
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.612730236724019,
    "estimated_duration": 3599.752998409467,
    "input_throughput": 1222.7583397929907,
    "output_throughput": 1066.7648590602535,
    "total_throughput": 2289.523198853244,
    "itl": 22.589059513666907,
    "ttft": 4692.492118589846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 17787,
    "finished_requests": 17764,
    "scheduler_time": 0.0010408506226762045
}
#Debug simulation 
Total elapsed time: 1.6128310430794954. Arrivals time: 0.05104628764092922 Scheduler time: 1.2054645479656756 Scheduler overhead time: 0.13399444799870253 Adapter cache time: 0.022768546361476183 Engine time: 0.13377934228628874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 8640, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 8640]
Prompts retrieved: 52335 . Total input tokens: 11614618 . Total output tokens: 10293831
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.5861067650839686,
    "estimated_duration": 3599.9618119888028,
    "input_throughput": 1227.1588507655372,
    "output_throughput": 1051.6867116177552,
    "total_throughput": 2278.8455623832924,
    "itl": 22.515399707779824,
    "ttft": 3293.855732853357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 17687,
    "finished_requests": 17671,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.58620616607368. Arrivals time: 0.051101199351251125 Scheduler time: 1.1788613768294454 Scheduler overhead time: 0.13314449740573764 Adapter cache time: 0.02265094593167305 Engine time: 0.1347654857672751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 8640, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 8640]
Prompts retrieved: 52335 . Total input tokens: 11614618 . Total output tokens: 10293831
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.604092090856284,
    "estimated_duration": 3599.977868192865,
    "input_throughput": 1227.1533775338546,
    "output_throughput": 1051.6820210065712,
    "total_throughput": 2278.8353985404256,
    "itl": 22.51649892085951,
    "ttft": 3293.920720435567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 17687,
    "finished_requests": 17671,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.604192074853927. Arrivals time: 0.0527770584449172 Scheduler time: 1.1939358902163804 Scheduler overhead time: 0.13398023461923003 Adapter cache time: 0.022822906728833914 Engine time: 0.13389100087806582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 8640, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 8640]
Prompts retrieved: 52335 . Total input tokens: 11614618 . Total output tokens: 10293831
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.5801592259667814,
    "estimated_duration": 3599.95810579384,
    "input_throughput": 1227.1601141385593,
    "output_throughput": 1051.6877943403533,
    "total_throughput": 2278.847908478913,
    "itl": 22.516524255580926,
    "ttft": 3293.9378743435473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 17687,
    "finished_requests": 17671,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5802625375799835. Arrivals time: 0.05078445700928569 Scheduler time: 1.1742314542643726 Scheduler overhead time: 0.13410969637334347 Adapter cache time: 0.022364427335560322 Engine time: 0.13308025896549225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 8640, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 8640]
Prompts retrieved: 52335 . Total input tokens: 11614618 . Total output tokens: 10293831
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.5870911767706275,
    "estimated_duration": 3599.9664758456906,
    "input_throughput": 1227.1572609470495,
    "output_throughput": 1051.6853491283136,
    "total_throughput": 2278.842610075363,
    "itl": 22.515391626499618,
    "ttft": 3293.847295629024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 17687,
    "finished_requests": 17671,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.587186650838703. Arrivals time: 0.05184348579496145 Scheduler time: 1.1788058714009821 Scheduler overhead time: 0.13326377281919122 Adapter cache time: 0.022777271457016468 Engine time: 0.13419959926977754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 8640, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 8640]
Prompts retrieved: 52335 . Total input tokens: 11614618 . Total output tokens: 10293831
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.5933091626502573,
    "estimated_duration": 3599.956888719699,
    "input_throughput": 1227.1605290170946,
    "output_throughput": 1051.688149895172,
    "total_throughput": 2278.8486789122667,
    "itl": 22.51651422303711,
    "ttft": 3293.917208154476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 17687,
    "finished_requests": 17671,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5934116076678038. Arrivals time: 0.0535053638741374 Scheduler time: 1.1830763351172209 Scheduler overhead time: 0.13324807165190578 Adapter cache time: 0.022746250964701176 Engine time: 0.13469989178702235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 8640, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 8640]
Prompts retrieved: 52335 . Total input tokens: 11614618 . Total output tokens: 10293831
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.5765183931216598,
    "estimated_duration": 3599.956978530835,
    "input_throughput": 1227.160498402095,
    "output_throughput": 1051.6881236578288,
    "total_throughput": 2278.848622059924,
    "itl": 22.515439789746583,
    "ttft": 3293.8569365941157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 17687,
    "finished_requests": 17671,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5766326170414686. Arrivals time: 0.04987151734530926 Scheduler time: 1.1733872168697417 Scheduler overhead time: 0.13372172228991985 Adapter cache time: 0.02241433970630169 Engine time: 0.13160831294953823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 8640, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 8640]
Prompts retrieved: 52335 . Total input tokens: 11614618 . Total output tokens: 10293831
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.572310226969421,
    "estimated_duration": 3599.9560291115727,
    "input_throughput": 1227.1608220421078,
    "output_throughput": 1051.6884010203726,
    "total_throughput": 2278.8492230624806,
    "itl": 22.51650986090999,
    "ttft": 3293.932801218934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 17687,
    "finished_requests": 17671,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.572407245170325. Arrivals time: 0.050829512532800436 Scheduler time: 1.1654952098615468 Scheduler overhead time: 0.13394698686897755 Adapter cache time: 0.022612777072936296 Engine time: 0.13321651378646493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 4320, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 34020 . Total input tokens: 7527272 . Total output tokens: 6710527
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.2054013540036976,
    "estimated_duration": 3599.807744932845,
    "input_throughput": 780.7950310558695,
    "output_throughput": 679.5676806472443,
    "total_throughput": 1460.3627117031137,
    "itl": 22.21861136335147,
    "ttft": 9142.695580456577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 11465,
    "finished_requests": 11436,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2054959228262305. Arrivals time: 0.038813505321741104 Scheduler time: 0.8054750524461269 Scheduler overhead time: 0.1327614551410079 Adapter cache time: 0.02891126601025462 Engine time: 0.13279737252742052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 4320, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 34020 . Total input tokens: 7527272 . Total output tokens: 6710527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.2128124148584902,
    "estimated_duration": 3599.822180034807,
    "input_throughput": 780.7919001079167,
    "output_throughput": 679.564955615765,
    "total_throughput": 1460.3568557236817,
    "itl": 22.218654155045204,
    "ttft": 9142.595253147309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 11465,
    "finished_requests": 11436,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2129155257716775. Arrivals time: 0.039726948365569115 Scheduler time: 0.8094216114841402 Scheduler overhead time: 0.13365021301433444 Adapter cache time: 0.029062739573419094 Engine time: 0.13518215529620647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 4320, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 34020 . Total input tokens: 7527272 . Total output tokens: 6710527
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.2079529529437423,
    "estimated_duration": 3599.8304666794274,
    "input_throughput": 780.7901027607753,
    "output_throughput": 679.5633912884068,
    "total_throughput": 1460.3534940491822,
    "itl": 22.21892809247135,
    "ttft": 9142.579445530104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 11465,
    "finished_requests": 11436,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2080745068378747. Arrivals time: 0.03838244779035449 Scheduler time: 0.8079867521300912 Scheduler overhead time: 0.13497363729402423 Adapter cache time: 0.02876212215051055 Engine time: 0.13183856569230556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 4320, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 34020 . Total input tokens: 7527272 . Total output tokens: 6710527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.218679185025394,
    "estimated_duration": 3599.810055793305,
    "input_throughput": 780.7945298326558,
    "output_throughput": 679.5672444058707,
    "total_throughput": 1460.3617742385266,
    "itl": 22.218574824024863,
    "ttft": 9142.702520736879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 11465,
    "finished_requests": 11436,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2187715312466025. Arrivals time: 0.03952288068830967 Scheduler time: 0.8152966424822807 Scheduler overhead time: 0.13306368235498667 Adapter cache time: 0.02904990315437317 Engine time: 0.13585846778005362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 4320, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 34020 . Total input tokens: 7527272 . Total output tokens: 6710527
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.222127449233085,
    "estimated_duration": 3599.8304915486074,
    "input_throughput": 780.790097366741,
    "output_throughput": 679.5633865936902,
    "total_throughput": 1460.353483960431,
    "itl": 22.218939608930526,
    "ttft": 9142.5857443256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 11465,
    "finished_requests": 11436,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2222054689191282. Arrivals time: 0.03933420963585377 Scheduler time: 0.821183058898896 Scheduler overhead time: 0.13271982036530972 Adapter cache time: 0.029098618309944868 Engine time: 0.13364612823352218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 4320, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 34020 . Total input tokens: 7527272 . Total output tokens: 6710527
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.2106812479905784,
    "estimated_duration": 3599.8306385863975,
    "input_throughput": 780.7900654747821,
    "output_throughput": 679.5633588364125,
    "total_throughput": 1460.3534243111947,
    "itl": 22.218575765759212,
    "ttft": 9142.548047965274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 11465,
    "finished_requests": 11436,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2107840399257839. Arrivals time: 0.03933564526960254 Scheduler time: 0.8099511419422925 Scheduler overhead time: 0.1320858341641724 Adapter cache time: 0.028784701600670815 Engine time: 0.1349915936589241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [5 5 6]
Adapter prompts. [1080, 540, 540, 1080, 4320, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 34020 . Total input tokens: 7527272 . Total output tokens: 6710527
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.2025224603712559,
    "estimated_duration": 3599.8259946069893,
    "input_throughput": 780.7910727381864,
    "output_throughput": 679.5642355116323,
    "total_throughput": 1460.3553082498188,
    "itl": 22.218986048799163,
    "ttft": 9142.606041066545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 11465,
    "finished_requests": 11436,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2026090142317116. Arrivals time: 0.03899366455152631 Scheduler time: 0.8050844338722527 Scheduler overhead time: 0.13301507523283362 Adapter cache time: 0.02861440321430564 Engine time: 0.13104819832369685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 4320, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 32670 . Total input tokens: 7237990 . Total output tokens: 6449689
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.194172048009932,
    "estimated_duration": 3600.014541342486,
    "input_throughput": 749.5522501400054,
    "output_throughput": 660.4578877932378,
    "total_throughput": 1410.0101379332432,
    "itl": 22.03210404806144,
    "ttft": 6889.180707864575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 11033,
    "finished_requests": 11011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.194271197076887. Arrivals time: 0.038254180457443 Scheduler time: 0.7892810283228755 Scheduler overhead time: 0.13785915216431022 Adapter cache time: 0.028180123306810856 Engine time: 0.13317374046891928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 4320, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 32670 . Total input tokens: 7237990 . Total output tokens: 6449689
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1847718902863562,
    "estimated_duration": 3600.020539925558,
    "input_throughput": 749.5510011883982,
    "output_throughput": 660.4567872963207,
    "total_throughput": 1410.007788484719,
    "itl": 22.032097064606948,
    "ttft": 7214.9804089288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 11033,
    "finished_requests": 11011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.184869288932532. Arrivals time: 0.037121182307600975 Scheduler time: 0.7873164298944175 Scheduler overhead time: 0.1342435679398477 Adapter cache time: 0.027725706808269024 Engine time: 0.13192813890054822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 4320, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 32670 . Total input tokens: 7237990 . Total output tokens: 6449689
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1899088458158076,
    "estimated_duration": 3600.0207856128313,
    "input_throughput": 749.5509500344875,
    "output_throughput": 660.4567422227401,
    "total_throughput": 1410.0076922572277,
    "itl": 22.032188939695736,
    "ttft": 7214.992622024378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 11033,
    "finished_requests": 11011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1900442619808018. Arrivals time: 0.03831569664180279 Scheduler time: 0.7889092941768467 Scheduler overhead time: 0.13335307175293565 Adapter cache time: 0.02792626153677702 Engine time: 0.13520807679742575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 4320, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 32670 . Total input tokens: 7237990 . Total output tokens: 6449689
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1872010831721127,
    "estimated_duration": 3600.0152888094194,
    "input_throughput": 749.5520945113548,
    "output_throughput": 660.4577506631446,
    "total_throughput": 1410.0098451744993,
    "itl": 22.032071136300758,
    "ttft": 6889.172865244831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 11033,
    "finished_requests": 11011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1872767922468483. Arrivals time: 0.03779392223805189 Scheduler time: 0.7874252279289067 Scheduler overhead time: 0.13427171157673 Adapter cache time: 0.02808307856321335 Engine time: 0.1332156341522932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 4320, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 32670 . Total input tokens: 7237990 . Total output tokens: 6449689
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.1881976411677897,
    "estimated_duration": 3600.0207693259104,
    "input_throughput": 749.550953425545,
    "output_throughput": 660.4567452107248,
    "total_throughput": 1410.0076986362697,
    "itl": 22.0321393247333,
    "ttft": 7214.957170794993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 11033,
    "finished_requests": 11011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1882911170832813. Arrivals time: 0.03779218904674053 Scheduler time: 0.789976910687983 Scheduler overhead time: 0.13346614316105843 Adapter cache time: 0.0274873161688447 Engine time: 0.13351760804653168 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 4320, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 32670 . Total input tokens: 7237990 . Total output tokens: 6449689
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.194584638811648,
    "estimated_duration": 3600.010655679222,
    "input_throughput": 749.5530591675116,
    "output_throughput": 660.4586006569477,
    "total_throughput": 1410.0116598244592,
    "itl": 22.032148404161006,
    "ttft": 6889.267895608711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 11033,
    "finished_requests": 11011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1946821138262749. Arrivals time: 0.037973848171532154 Scheduler time: 0.7951338076964021 Scheduler overhead time: 0.1329708443954587 Adapter cache time: 0.0277026928961277 Engine time: 0.1345581472851336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [5 5 6]
Adapter prompts. [1080, 270, 270, 1080, 4320, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 32670 . Total input tokens: 7237990 . Total output tokens: 6449689
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1894325008615851,
    "estimated_duration": 3600.0205098848505,
    "input_throughput": 749.5510074430966,
    "output_throughput": 660.456792807564,
    "total_throughput": 1410.0078002506607,
    "itl": 22.032138401851654,
    "ttft": 7214.9701082679285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 11033,
    "finished_requests": 11011,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1895287451334298. Arrivals time: 0.03844754723832011 Scheduler time: 0.7907981015741825 Scheduler overhead time: 0.13252217462286353 Adapter cache time: 0.027677810285240412 Engine time: 0.13411795068532228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 4320, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31995 . Total input tokens: 7089628 . Total output tokens: 6311301
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1642206870019436,
    "estimated_duration": 3600.020195465847,
    "input_throughput": 739.3436301697133,
    "output_throughput": 645.4947122037737,
    "total_throughput": 1384.838342373487,
    "itl": 22.059750052932348,
    "ttft": 6367.205840791317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 10804,
    "finished_requests": 10785,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1643179808743298. Arrivals time: 0.036392067559063435 Scheduler time: 0.769141782540828 Scheduler overhead time: 0.13343403907492757 Adapter cache time: 0.027343076653778553 Engine time: 0.13169535668566823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 4320, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31995 . Total input tokens: 7089628 . Total output tokens: 6311301
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1765889041125774,
    "estimated_duration": 3600.0043863939873,
    "input_throughput": 739.346876925918,
    "output_throughput": 645.4975468315116,
    "total_throughput": 1384.8444237574295,
    "itl": 22.059927127994293,
    "ttft": 6034.606709174208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 10804,
    "finished_requests": 10785,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1767053860239685. Arrivals time: 0.03782836114987731 Scheduler time: 0.7770621790550649 Scheduler overhead time: 0.1338918381370604 Adapter cache time: 0.027403208892792463 Engine time: 0.13407143577933311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 4320, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31995 . Total input tokens: 7089628 . Total output tokens: 6311301
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1697064749896526,
    "estimated_duration": 3600.0049442588265,
    "input_throughput": 739.3467623550679,
    "output_throughput": 645.4974468037642,
    "total_throughput": 1384.844209158832,
    "itl": 22.05997168599177,
    "ttft": 6034.7555307800085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 10804,
    "finished_requests": 10785,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1697848178446293. Arrivals time: 0.0371879730373621 Scheduler time: 0.7707963078282773 Scheduler overhead time: 0.1337439212948084 Adapter cache time: 0.027221479453146458 Engine time: 0.13425818970426917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 4320, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31995 . Total input tokens: 7089628 . Total output tokens: 6311301
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1746430736966431,
    "estimated_duration": 3600.023307214602,
    "input_throughput": 739.342991103956,
    "output_throughput": 645.4941542581173,
    "total_throughput": 1384.8371453620734,
    "itl": 22.059818468690686,
    "ttft": 6367.1833580015345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 10804,
    "finished_requests": 10785,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1747312936931849. Arrivals time: 0.0372203653678298 Scheduler time: 0.7749344073235989 Scheduler overhead time: 0.1339882966130972 Adapter cache time: 0.0274420203641057 Engine time: 0.13463899213820696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 4320, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31995 . Total input tokens: 7089628 . Total output tokens: 6311301
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.1770051000639796,
    "estimated_duration": 3600.0050308407403,
    "input_throughput": 739.3467445734101,
    "output_throughput": 645.4974312792292,
    "total_throughput": 1384.8441758526392,
    "itl": 22.059973388207066,
    "ttft": 6034.707128261888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 10804,
    "finished_requests": 10785,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1771065769717097. Arrivals time: 0.03755865013226867 Scheduler time: 0.7768894694745541 Scheduler overhead time: 0.13354795472696424 Adapter cache time: 0.027378696482628584 Engine time: 0.13527816301211715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 4320, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31995 . Total input tokens: 7089628 . Total output tokens: 6311301
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1801355839706957,
    "estimated_duration": 3600.0160244573203,
    "input_throughput": 739.3444867793962,
    "output_throughput": 645.4954600793193,
    "total_throughput": 1384.8399468587156,
    "itl": 22.059884694444317,
    "ttft": 6367.3383804832965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 10804,
    "finished_requests": 10785,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.180255256127566. Arrivals time: 0.03696009609848261 Scheduler time: 0.7817643419839442 Scheduler overhead time: 0.13393921637907624 Adapter cache time: 0.027689479291439056 Engine time: 0.13326511485502124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [5 5 6]
Adapter prompts. [1080, 135, 135, 1080, 4320, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31995 . Total input tokens: 7089628 . Total output tokens: 6311301
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1758199678733945,
    "estimated_duration": 3600.005038802323,
    "input_throughput": 739.3467429383095,
    "output_throughput": 645.4974298516809,
    "total_throughput": 1384.8441727899904,
    "itl": 22.059983461575463,
    "ttft": 6034.722247982625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 10804,
    "finished_requests": 10785,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1759169180877507. Arrivals time: 0.03747303457930684 Scheduler time: 0.776223324239254 Scheduler overhead time: 0.1341974181123078 Adapter cache time: 0.02747258311137557 Engine time: 0.13394899619743228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 4320, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31650 . Total input tokens: 7008662 . Total output tokens: 6245794
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1548813651315868,
    "estimated_duration": 3600.0233919390394,
    "input_throughput": 735.287444500256,
    "output_throughput": 632.5686674978588,
    "total_throughput": 1367.8561119981148,
    "itl": 21.956078112904123,
    "ttft": 9132.163694627283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 10686,
    "finished_requests": 10659,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1549802008084953. Arrivals time: 0.03697492042556405 Scheduler time: 0.756446017883718 Scheduler overhead time: 0.13392284186556935 Adapter cache time: 0.02718512574210763 Engine time: 0.13350166333839297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 4320, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31650 . Total input tokens: 7008662 . Total output tokens: 6245794
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1523948861286044,
    "estimated_duration": 3600.0212243145847,
    "input_throughput": 735.2878872273809,
    "output_throughput": 632.5690483765335,
    "total_throughput": 1367.8569356039145,
    "itl": 21.975185478989324,
    "ttft": 9132.24439725562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 10686,
    "finished_requests": 10659,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1525208838284016. Arrivals time: 0.03693639626726508 Scheduler time: 0.7543480605818331 Scheduler overhead time: 0.13247624319046736 Adapter cache time: 0.026400255970656872 Engine time: 0.13643025374040008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 4320, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31650 . Total input tokens: 7008662 . Total output tokens: 6245794
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1620658189058304,
    "estimated_duration": 3600.0212065523388,
    "input_throughput": 735.2878908552385,
    "output_throughput": 632.5690514975838,
    "total_throughput": 1367.8569423528222,
    "itl": 21.975110061088667,
    "ttft": 9132.21409923149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 10686,
    "finished_requests": 10659,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.16217607492581. Arrivals time: 0.037093264516443014 Scheduler time: 0.7624002387747169 Scheduler overhead time: 0.13504832657054067 Adapter cache time: 0.026645035482943058 Engine time: 0.13404136477038264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 4320, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31650 . Total input tokens: 7008662 . Total output tokens: 6245794
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1577823283150792,
    "estimated_duration": 3600.0079819190237,
    "input_throughput": 735.2905919361212,
    "output_throughput": 632.5713752406962,
    "total_throughput": 1367.8619671768174,
    "itl": 21.956249538700487,
    "ttft": 9132.240737734444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 10686,
    "finished_requests": 10659,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.157862490043044. Arrivals time: 0.03714130446314812 Scheduler time: 0.7582741333171725 Scheduler overhead time: 0.13450827775523067 Adapter cache time: 0.02716474002227187 Engine time: 0.13405520655214787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 4320, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31650 . Total input tokens: 7008662 . Total output tokens: 6245794
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.1668545030988753,
    "estimated_duration": 3600.021244102218,
    "input_throughput": 735.2878831858471,
    "output_throughput": 632.5690448995972,
    "total_throughput": 1367.8569280854445,
    "itl": 21.975109775733298,
    "ttft": 9132.20942817052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 10686,
    "finished_requests": 10659,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1669409950263798. Arrivals time: 0.03819081699475646 Scheduler time: 0.7666665026918054 Scheduler overhead time: 0.1344076655805111 Adapter cache time: 0.026641410775482655 Engine time: 0.13453435897827148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 4320, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31650 . Total input tokens: 7008662 . Total output tokens: 6245794
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.154742456972599,
    "estimated_duration": 3600.0229918881946,
    "input_throughput": 735.287526208724,
    "output_throughput": 632.5687377917515,
    "total_throughput": 1367.8562640004754,
    "itl": 21.95605646476514,
    "ttft": 9132.176316820942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 10686,
    "finished_requests": 10659,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1548445499502122. Arrivals time: 0.03700975049287081 Scheduler time: 0.7566523156128824 Scheduler overhead time: 0.13423524564132094 Adapter cache time: 0.02708885306492448 Engine time: 0.13339417800307274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [5 5 6]
Adapter prompts. [1080, 66, 66, 1080, 4320, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31650 . Total input tokens: 7008662 . Total output tokens: 6245794
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1623648600652814,
    "estimated_duration": 3600.0213774435183,
    "input_throughput": 735.2878559514971,
    "output_throughput": 632.5690214698534,
    "total_throughput": 1367.8568774213504,
    "itl": 21.975160790446484,
    "ttft": 9132.205931240233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 10686,
    "finished_requests": 10659,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1624616039916873. Arrivals time: 0.03674390492960811 Scheduler time: 0.7622188413515687 Scheduler overhead time: 0.13404578994959593 Adapter cache time: 0.026724521070718765 Engine time: 0.1353818946518004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 4320, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31485 . Total input tokens: 6969401 . Total output tokens: 6209593
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.158732526935637,
    "estimated_duration": 3599.2533202899713,
    "input_throughput": 721.753310709098,
    "output_throughput": 632.9751749222406,
    "total_throughput": 1354.7284856313386,
    "itl": 21.969619724594786,
    "ttft": 3772.772928205871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 10598,
    "finished_requests": 10587,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1588238100521266. Arrivals time: 0.036642509046941996 Scheduler time: 0.7570451134815812 Scheduler overhead time: 0.1345006194896996 Adapter cache time: 0.027259161695837975 Engine time: 0.13633872754871845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 4320, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31485 . Total input tokens: 6969401 . Total output tokens: 6209593
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1575082633644342,
    "estimated_duration": 3599.2440911733593,
    "input_throughput": 721.7551614158855,
    "output_throughput": 632.9767979857378,
    "total_throughput": 1354.7319594016233,
    "itl": 21.96975736706265,
    "ttft": 3772.6438275678274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 10598,
    "finished_requests": 10587,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1576051441952586. Arrivals time: 0.03636030945926905 Scheduler time: 0.756773236207664 Scheduler overhead time: 0.13443215563893318 Adapter cache time: 0.027301370166242123 Engine time: 0.13534011971205473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 4320, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31485 . Total input tokens: 6969401 . Total output tokens: 6209593
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1574744610115886,
    "estimated_duration": 3599.249891184852,
    "input_throughput": 721.7539983434793,
    "output_throughput": 632.9757779752317,
    "total_throughput": 1354.7297763187112,
    "itl": 21.969746643797297,
    "ttft": 3772.784407799627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 10598,
    "finished_requests": 10587,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1577139091677964. Arrivals time: 0.03636148804798722 Scheduler time: 0.7580920639447868 Scheduler overhead time: 0.1363219553604722 Adapter cache time: 0.026977429166436195 Engine time: 0.13318250188603997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 4320, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31485 . Total input tokens: 6969401 . Total output tokens: 6209593
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 1.1585471592843533,
    "estimated_duration": 3599.237238906258,
    "input_throughput": 721.7565355012317,
    "output_throughput": 632.9780030538678,
    "total_throughput": 1354.7345385550996,
    "itl": 21.969677989093494,
    "ttft": 3772.6809224059734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 10598,
    "finished_requests": 10587,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.158645689021796. Arrivals time: 0.036136113572865725 Scheduler time: 0.7609956050291657 Scheduler overhead time: 0.13412676053121686 Adapter cache time: 0.02733131730929017 Engine time: 0.13375967601314187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 4320, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31485 . Total input tokens: 6969401 . Total output tokens: 6209593
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 1.1692879092879593,
    "estimated_duration": 3599.24992139383,
    "input_throughput": 721.7539922857031,
    "output_throughput": 632.9757726625828,
    "total_throughput": 1354.729764948286,
    "itl": 21.96976846209205,
    "ttft": 3772.724506541685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 10598,
    "finished_requests": 10587,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.16940772626549. Arrivals time: 0.03717011399567127 Scheduler time: 0.7675826498307288 Scheduler overhead time: 0.13478891598060727 Adapter cache time: 0.027260446455329657 Engine time: 0.13602768070995808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 4320, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31485 . Total input tokens: 6969401 . Total output tokens: 6209593
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 1.1578028108924627,
    "estimated_duration": 3599.249980403049,
    "input_throughput": 721.7539804526435,
    "output_throughput": 632.9757622850302,
    "total_throughput": 1354.7297427376739,
    "itl": 21.969647059157076,
    "ttft": 3772.7892472978842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 10598,
    "finished_requests": 10587,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1578967040404677. Arrivals time: 0.03602204658091068 Scheduler time: 0.7589617166668177 Scheduler overhead time: 0.1346682603470981 Adapter cache time: 0.02736051892861724 Engine time: 0.13426544424146414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [5 5 6]
Adapter prompts. [1080, 33, 33, 1080, 4320, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 4320]
Prompts retrieved: 31485 . Total input tokens: 6969401 . Total output tokens: 6209593
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 1.1630010157823563,
    "estimated_duration": 3599.2440683366617,
    "input_throughput": 721.7551659953206,
    "output_throughput": 632.9768020018867,
    "total_throughput": 1354.7319679972072,
    "itl": 21.96978668591541,
    "ttft": 3772.688848819639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 10598,
    "finished_requests": 10587,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1630985168740153. Arrivals time: 0.0372428665868938 Scheduler time: 0.7632308648899198 Scheduler overhead time: 0.1340787773951888 Adapter cache time: 0.026963234413415194 Engine time: 0.13437075586989522 
