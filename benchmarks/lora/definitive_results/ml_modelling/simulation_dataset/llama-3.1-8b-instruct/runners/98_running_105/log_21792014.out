INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.291674904059619,
    "estimated_duration": 3600.064815413414,
    "input_throughput": 6119.258993805147,
    "output_throughput": 5340.076911307592,
    "total_throughput": 11459.335905112739,
    "itl": 108.95576090524555,
    "ttft": 1943414.7357316143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 802,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.303152043623953,
    "arrivals": 501405,
    "finished_requests": 89485,
    "scheduler_time": 141.08630206205802
}
#Debug simulation 
Total elapsed time: 6.291824398096651. Arrivals time: 0.2644187896512449 Scheduler time: 5.888008711859584 Scheduler overhead time: 0.04812989057973027 Adapter cache time: 0.01784795429557562 Engine time: 0.05031648185104132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.153696455061436,
    "estimated_duration": 3600.0931212559244,
    "input_throughput": 5975.4087673419235,
    "output_throughput": 5218.968611968951,
    "total_throughput": 11194.377379310874,
    "itl": 101.41454440754063,
    "ttft": 1958264.1030628858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.751564092561609,
    "arrivals": 501405,
    "finished_requests": 87423,
    "scheduler_time": 143.98013176987948
}
#Debug simulation 
Total elapsed time: 6.153800345025957. Arrivals time: 0.30237737530842423 Scheduler time: 5.704765365924686 Scheduler overhead time: 0.05147921992465854 Adapter cache time: 0.018317504320293665 Engine time: 0.052227004896849394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.939439403824508,
    "estimated_duration": 3600.0659061913716,
    "input_throughput": 5683.970386433322,
    "output_throughput": 4967.502114126397,
    "total_throughput": 10651.47250055972,
    "itl": 89.34173650509517,
    "ttft": 1989111.538864501,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 758,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.643197826752458,
    "arrivals": 501405,
    "finished_requests": 83113,
    "scheduler_time": 149.47110579117216
}
#Debug simulation 
Total elapsed time: 5.9397577829658985. Arrivals time: 0.2943176804110408 Scheduler time: 5.483446047641337 Scheduler overhead time: 0.05698701040819287 Adapter cache time: 0.019260482862591743 Engine time: 0.058338798116892576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.182429657317698,
    "estimated_duration": 3600.0979497507474,
    "input_throughput": 5976.029902600161,
    "output_throughput": 5219.555484955899,
    "total_throughput": 11195.58538755606,
    "itl": 101.40434404003288,
    "ttft": 1958299.5765235934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.34371045721228,
    "arrivals": 501405,
    "finished_requests": 87432,
    "scheduler_time": 143.9947389517457
}
#Debug simulation 
Total elapsed time: 6.182517200242728. Arrivals time: 0.30423468723893166 Scheduler time: 5.731905122287571 Scheduler overhead time: 0.05121841374784708 Adapter cache time: 0.018202349543571472 Engine time: 0.052562343422323465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.905536450911313,
    "estimated_duration": 3600.0285020843075,
    "input_throughput": 5684.165552620611,
    "output_throughput": 4967.631503374467,
    "total_throughput": 10651.797055995079,
    "itl": 89.34079513957026,
    "ttft": 1989078.250599565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 757,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.5800285843992805,
    "arrivals": 501405,
    "finished_requests": 83114,
    "scheduler_time": 149.47160462118342
}
#Debug simulation 
Total elapsed time: 5.905630462802947. Arrivals time: 0.293416497297585 Scheduler time: 5.451417888049036 Scheduler overhead time: 0.056801021099090576 Adapter cache time: 0.019051548093557358 Engine time: 0.05791469383984804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.173227761872113,
    "estimated_duration": 3600.0558308805566,
    "input_throughput": 5976.568423033954,
    "output_throughput": 5220.038211297257,
    "total_throughput": 11196.60663433121,
    "itl": 101.39406165908802,
    "ttft": 1958247.0144249902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 788,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.030529701169556,
    "arrivals": 501405,
    "finished_requests": 87437,
    "scheduler_time": 144.00412572451665
}
#Debug simulation 
Total elapsed time: 6.173347523901612. Arrivals time: 0.306464284658432 Scheduler time: 5.71998928161338 Scheduler overhead time: 0.051572649739682674 Adapter cache time: 0.01835508830845356 Engine time: 0.05255701579153538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334687122 . Total output tokens: 295243649
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.967872024979442,
    "estimated_duration": 3600.0053400957104,
    "input_throughput": 5684.066012928741,
    "output_throughput": 4967.585686838051,
    "total_throughput": 10651.651699766791,
    "itl": 89.33839261198807,
    "ttft": 1989111.8340291602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 758,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.544195816330632,
    "arrivals": 501405,
    "finished_requests": 83113,
    "scheduler_time": 149.47163794350806
}
#Debug simulation 
Total elapsed time: 5.967990091070533. Arrivals time: 0.3001037761569023 Scheduler time: 5.505884336307645 Scheduler overhead time: 0.056829091627150774 Adapter cache time: 0.01935972273349762 Engine time: 0.05826138565316796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.363070657942444,
    "estimated_duration": 3600.0301556256013,
    "input_throughput": 6133.110014508508,
    "output_throughput": 5380.531596306569,
    "total_throughput": 11513.641610815077,
    "itl": 108.09505352592186,
    "ttft": 1936672.7213418076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.093081190777076,
    "arrivals": 500396,
    "finished_requests": 89806,
    "scheduler_time": 142.17041800785134
}
#Debug simulation 
Total elapsed time: 6.363186141010374. Arrivals time: 0.3095376193523407 Scheduler time: 5.9153836304321885 Scheduler overhead time: 0.04900766536593437 Adapter cache time: 0.01621861057356 Engine time: 0.04989470774307847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.22924301866442,
    "estimated_duration": 3600.050112519787,
    "input_throughput": 5987.252767688114,
    "output_throughput": 5253.290484549068,
    "total_throughput": 11240.543252237181,
    "itl": 100.66340335296604,
    "ttft": 1952397.2658776485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.426163737331523,
    "arrivals": 500396,
    "finished_requests": 87676,
    "scheduler_time": 145.02094604478495
}
#Debug simulation 
Total elapsed time: 6.229363514576107. Arrivals time: 0.30952729284763336 Scheduler time: 5.773281322792172 Scheduler overhead time: 0.05177063215523958 Adapter cache time: 0.017038557212799788 Engine time: 0.05312757892534137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.015352117829025,
    "estimated_duration": 3600.067510853399,
    "input_throughput": 5699.791722832387,
    "output_throughput": 4998.718481180398,
    "total_throughput": 10698.510204012786,
    "itl": 88.8102971071309,
    "ttft": 1985052.3238319908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.417995592360416,
    "arrivals": 500396,
    "finished_requests": 83463,
    "scheduler_time": 150.39466604856833
}
#Debug simulation 
Total elapsed time: 6.015445502009243. Arrivals time: 0.3229949064552784 Scheduler time: 5.532094458118081 Scheduler overhead time: 0.05713578267022967 Adapter cache time: 0.017468122765421867 Engine time: 0.05847662827000022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.2204945483244956,
    "estimated_duration": 3600.087870294486,
    "input_throughput": 5987.763848174263,
    "output_throughput": 5253.644544640761,
    "total_throughput": 11241.408392815023,
    "itl": 100.65476018905353,
    "ttft": 1952300.130757774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1437976958230065,
    "arrivals": 500396,
    "finished_requests": 87683,
    "scheduler_time": 145.03368658055282
}
#Debug simulation 
Total elapsed time: 6.220622165128589. Arrivals time: 0.307395554613322 Scheduler time: 5.767501980531961 Scheduler overhead time: 0.051672860980033875 Adapter cache time: 0.016818552743643522 Engine time: 0.05265635950490832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.014529515989125,
    "estimated_duration": 3600.0253035698865,
    "input_throughput": 5699.85854811969,
    "output_throughput": 4998.777086970731,
    "total_throughput": 10698.63563509042,
    "itl": 88.80940920457954,
    "ttft": 1985036.008896001,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.375743688331017,
    "arrivals": 500396,
    "finished_requests": 83463,
    "scheduler_time": 150.39456811424708
}
#Debug simulation 
Total elapsed time: 6.014649411197752. Arrivals time: 0.2978821573778987 Scheduler time: 5.55509368237108 Scheduler overhead time: 0.0575584308244288 Adapter cache time: 0.017545144073665142 Engine time: 0.05902167782187462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.26165025215596,
    "estimated_duration": 3600.090264915717,
    "input_throughput": 5988.656787333287,
    "output_throughput": 5254.635469658947,
    "total_throughput": 11243.292256992234,
    "itl": 100.6485485392254,
    "ttft": 1952297.4561219485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.881423931866866,
    "arrivals": 500396,
    "finished_requests": 87697,
    "scheduler_time": 145.04338028711388
}
#Debug simulation 
Total elapsed time: 6.261771104764193. Arrivals time: 0.30606465227901936 Scheduler time: 5.808823637198657 Scheduler overhead time: 0.05190984904766083 Adapter cache time: 0.016970261465758085 Engine time: 0.0531540890224278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 334059749 . Total output tokens: 294664217
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.010227820836008,
    "estimated_duration": 3600.0795081627894,
    "input_throughput": 5700.066888376091,
    "output_throughput": 4998.895151952915,
    "total_throughput": 10698.962040329006,
    "itl": 88.80792133629407,
    "ttft": 1985115.5386666146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.337841245010527,
    "arrivals": 500396,
    "finished_requests": 83468,
    "scheduler_time": 150.39831311286076
}
#Debug simulation 
Total elapsed time: 6.010345140006393. Arrivals time: 0.32243591267615557 Scheduler time: 5.526766030583531 Scheduler overhead time: 0.0573698659427464 Adapter cache time: 0.017716077156364918 Engine time: 0.058762111235409975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.3427430600859225,
    "estimated_duration": 3600.086804835582,
    "input_throughput": 6194.152588223058,
    "output_throughput": 5413.219751763744,
    "total_throughput": 11607.372339986803,
    "itl": 107.39778956258324,
    "ttft": 1935214.037697348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.38555342435841,
    "arrivals": 499947,
    "finished_requests": 90472,
    "scheduler_time": 143.05206132661698
}
#Debug simulation 
Total elapsed time: 6.3428349192254245. Arrivals time: 0.2655083783902228 Scheduler time: 5.940063416957855 Scheduler overhead time: 0.04899514466524124 Adapter cache time: 0.015007272362709045 Engine time: 0.049937423784285784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.180788348894566,
    "estimated_duration": 3600.0015517955085,
    "input_throughput": 6035.137954083341,
    "output_throughput": 5284.386888808934,
    "total_throughput": 11319.524842892275,
    "itl": 100.09943630670499,
    "ttft": 1951058.2582358595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.645351544632581,
    "arrivals": 499947,
    "finished_requests": 88178,
    "scheduler_time": 145.84994330911763
}
#Debug simulation 
Total elapsed time: 6.180906634777784. Arrivals time: 0.26496390579268336 Scheduler time: 5.771141417790204 Scheduler overhead time: 0.05183566967025399 Adapter cache time: 0.015561566688120365 Engine time: 0.05283029284328222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.984851140994579,
    "estimated_duration": 3600.0116428845654,
    "input_throughput": 5735.29978460185,
    "output_throughput": 5024.554583247498,
    "total_throughput": 10759.854367849348,
    "itl": 88.40842606440002,
    "ttft": 1983717.855090261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5409793048259144,
    "arrivals": 499947,
    "finished_requests": 83808,
    "scheduler_time": 151.16876195403447
}
#Debug simulation 
Total elapsed time: 5.984963688999414. Arrivals time: 0.2538702394813299 Scheduler time: 5.571095020975918 Scheduler overhead time: 0.05734764225780964 Adapter cache time: 0.01644692150875926 Engine time: 0.05854456406086683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.1940770503133535,
    "estimated_duration": 3600.020991343372,
    "input_throughput": 6035.243697813113,
    "output_throughput": 5284.529741839342,
    "total_throughput": 11319.773439652454,
    "itl": 100.09427151981178,
    "ttft": 1950998.7333803126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4218479679292018,
    "arrivals": 499947,
    "finished_requests": 88181,
    "scheduler_time": 145.85864649679624
}
#Debug simulation 
Total elapsed time: 6.194170685019344. Arrivals time: 0.2642397847957909 Scheduler time: 5.784491709899157 Scheduler overhead time: 0.05182108981534839 Adapter cache time: 0.01565754972398281 Engine time: 0.05328651750460267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.96608254685998,
    "estimated_duration": 3600.019277648758,
    "input_throughput": 5735.494009211399,
    "output_throughput": 5024.75475959518,
    "total_throughput": 10760.24876880658,
    "itl": 88.40895620163437,
    "ttft": 1983655.545916934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5090832596272503,
    "arrivals": 499947,
    "finished_requests": 83809,
    "scheduler_time": 151.16974800782873
}
#Debug simulation 
Total elapsed time: 5.966169708874077. Arrivals time: 0.2548419991508126 Scheduler time: 5.5507072089239955 Scheduler overhead time: 0.057637129444628954 Adapter cache time: 0.016333251725882292 Engine time: 0.05897338408976793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.280876603908837,
    "estimated_duration": 3600.071757888258,
    "input_throughput": 6036.051351583776,
    "output_throughput": 5285.065487461476,
    "total_throughput": 11321.116839045251,
    "itl": 100.0883467562373,
    "ttft": 1950947.7006347699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.198344391225823,
    "arrivals": 499947,
    "finished_requests": 88191,
    "scheduler_time": 145.87002043698737
}
#Debug simulation 
Total elapsed time: 6.280968093778938. Arrivals time: 0.2910848008468747 Scheduler time: 5.843623171094805 Scheduler overhead time: 0.05215395521372557 Adapter cache time: 0.015749570447951555 Engine time: 0.05352287506684661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333759846 . Total output tokens: 294395541
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.942309617064893,
    "estimated_duration": 3600.0998647578003,
    "input_throughput": 5735.220070454648,
    "output_throughput": 5024.653392834971,
    "total_throughput": 10759.873463289618,
    "itl": 88.40579932528182,
    "ttft": 1983643.0901983688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 475,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4854704049788685,
    "arrivals": 499947,
    "finished_requests": 83809,
    "scheduler_time": 151.17554899673928
}
#Debug simulation 
Total elapsed time: 5.942397948354483. Arrivals time: 0.2545797359198332 Scheduler time: 5.528778868261725 Scheduler overhead time: 0.05702174548059702 Adapter cache time: 0.01617002161219716 Engine time: 0.05826670303940773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.429125966038555,
    "estimated_duration": 3600.0965351427094,
    "input_throughput": 6298.1741680160785,
    "output_throughput": 5462.626295719715,
    "total_throughput": 11760.800463735794,
    "itl": 105.65337910437597,
    "ttft": 1922863.3914624518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6714132489077937,
    "arrivals": 498492,
    "finished_requests": 91287,
    "scheduler_time": 144.78198889545573
}
#Debug simulation 
Total elapsed time: 6.429219409357756. Arrivals time: 0.26971258455887437 Scheduler time: 6.022113049868494 Scheduler overhead time: 0.04968610079959035 Adapter cache time: 0.01345828315243125 Engine time: 0.05068568652495742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.287332025356591,
    "estimated_duration": 3600.000481713349,
    "input_throughput": 6130.913901849148,
    "output_throughput": 5318.056510616997,
    "total_throughput": 11448.970412466144,
    "itl": 98.24761101162879,
    "ttft": 1939881.235457261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.904514429355044,
    "arrivals": 498492,
    "finished_requests": 88823,
    "scheduler_time": 147.5523538137354
}
#Debug simulation 
Total elapsed time: 6.287425781134516. Arrivals time: 0.2697415817528963 Scheduler time: 5.868531825952232 Scheduler overhead time: 0.05384604027494788 Adapter cache time: 0.014374100603163242 Engine time: 0.05511860363185406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.01347841322422,
    "estimated_duration": 3600.0667457693667,
    "input_throughput": 5810.658656421514,
    "output_throughput": 5046.797541004246,
    "total_throughput": 10857.456197425761,
    "itl": 86.85353875568867,
    "ttft": 1973128.8587388156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.734165982063875,
    "arrivals": 498492,
    "finished_requests": 84181,
    "scheduler_time": 152.76087704533748
}
#Debug simulation 
Total elapsed time: 6.013567605055869. Arrivals time: 0.2592461910098791 Scheduler time: 5.593447696883231 Scheduler overhead time: 0.05837480537593365 Adapter cache time: 0.014663103502243757 Engine time: 0.06000791070982814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.315266056917608,
    "estimated_duration": 3600.024907243567,
    "input_throughput": 6136.213934395821,
    "output_throughput": 5322.907339180671,
    "total_throughput": 11459.121273576493,
    "itl": 98.4683483016344,
    "ttft": 1939261.127830079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7198810399044264,
    "arrivals": 498492,
    "finished_requests": 88902,
    "scheduler_time": 147.46778524771
}
#Debug simulation 
Total elapsed time: 6.315359982661903. Arrivals time: 0.29204362304881215 Scheduler time: 5.877516454085708 Scheduler overhead time: 0.052513821981847286 Adapter cache time: 0.014138973783701658 Engine time: 0.05389855057001114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.229783482849598,
    "estimated_duration": 3600.0014529932755,
    "input_throughput": 5810.660432541518,
    "output_throughput": 5046.857685263812,
    "total_throughput": 10857.51811780533,
    "itl": 86.85175379666916,
    "ttft": 1973093.240765416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7095190380467256,
    "arrivals": 498492,
    "finished_requests": 84182,
    "scheduler_time": 152.75973265196484
}
#Debug simulation 
Total elapsed time: 6.229876339901239. Arrivals time: 0.25709444750100374 Scheduler time: 5.811772721353918 Scheduler overhead time: 0.058467794209718704 Adapter cache time: 0.0146714448928833 Engine time: 0.05998176848515868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.2650008029304445,
    "estimated_duration": 3600.086738911752,
    "input_throughput": 6136.5143681727095,
    "output_throughput": 5323.394792924676,
    "total_throughput": 11459.909161097386,
    "itl": 98.46098897873327,
    "ttft": 1939133.1648114575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5408005343470617,
    "arrivals": 498492,
    "finished_requests": 88913,
    "scheduler_time": 147.47772223148093
}
#Debug simulation 
Total elapsed time: 6.26509262714535. Arrivals time: 0.2654364285990596 Scheduler time: 5.852744405623525 Scheduler overhead time: 0.05292989034205675 Adapter cache time: 0.014167157001793385 Engine time: 0.054730976931750774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332778274 . Total output tokens: 293519737
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.0595925422385335,
    "estimated_duration": 3600.0288061552947,
    "input_throughput": 5810.616282912499,
    "output_throughput": 5046.819339038438,
    "total_throughput": 10857.435621950937,
    "itl": 86.85349010743701,
    "ttft": 1973121.7198453704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6844578596763466,
    "arrivals": 498492,
    "finished_requests": 84182,
    "scheduler_time": 152.7605635839969
}
#Debug simulation 
Total elapsed time: 6.059680690057576. Arrivals time: 0.253693756647408 Scheduler time: 5.6448487169109285 Scheduler overhead time: 0.05849645892158151 Adapter cache time: 0.014733978547155857 Engine time: 0.05996314994990826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.4698171080090106,
    "estimated_duration": 3600.0398459592757,
    "input_throughput": 6262.761515072144,
    "output_throughput": 5476.321608531169,
    "total_throughput": 11739.083123603312,
    "itl": 106.04596267811324,
    "ttft": 1920496.6012710764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2283818437671337,
    "arrivals": 497997,
    "finished_requests": 91333,
    "scheduler_time": 144.75091857483673
}
#Debug simulation 
Total elapsed time: 6.469908287283033. Arrivals time: 0.31275789346545935 Scheduler time: 6.020431952085346 Scheduler overhead time: 0.04957461217418313 Adapter cache time: 0.012886390555649996 Engine time: 0.05071211280301213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.368455014657229,
    "estimated_duration": 3600.0415627789707,
    "input_throughput": 6101.7573316690805,
    "output_throughput": 5339.668907922612,
    "total_throughput": 11441.426239591692,
    "itl": 98.9041419360818,
    "ttft": 1937033.7959645444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3918363938434073,
    "arrivals": 497997,
    "finished_requests": 89024,
    "scheduler_time": 147.39803592205507
}
#Debug simulation 
Total elapsed time: 6.368543625809252. Arrivals time: 0.3098674463108182 Scheduler time: 5.913084514904767 Scheduler overhead time: 0.0529068224132061 Adapter cache time: 0.013278324156999588 Engine time: 0.05429783929139376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.030911903362721,
    "estimated_duration": 3600.0814810330903,
    "input_throughput": 5777.8389488092835,
    "output_throughput": 5063.99010579295,
    "total_throughput": 10841.829054602234,
    "itl": 87.51482304934389,
    "ttft": 1968814.9731383105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3445007231365995,
    "arrivals": 497997,
    "finished_requests": 84345,
    "scheduler_time": 152.44257229484546
}
#Debug simulation 
Total elapsed time: 6.030999160371721. Arrivals time: 0.3001293670386076 Scheduler time: 5.572349824011326 Scheduler overhead time: 0.05766761861741543 Adapter cache time: 0.014083344023674726 Engine time: 0.059189609717577696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.334343351889402,
    "estimated_duration": 3600.096067391852,
    "input_throughput": 6101.668285733846,
    "output_throughput": 5339.51834622187,
    "total_throughput": 11441.186631955716,
    "itl": 98.90078026691111,
    "ttft": 1937003.7827801465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2377438658056747,
    "arrivals": 497997,
    "finished_requests": 89025,
    "scheduler_time": 147.4048328198587
}
#Debug simulation 
Total elapsed time: 6.3344394117593765. Arrivals time: 0.2646401533856988 Scheduler time: 5.924565287772566 Scheduler overhead time: 0.052649824880063534 Adapter cache time: 0.013416240457445383 Engine time: 0.054124656599015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.042222851887345,
    "estimated_duration": 3600.002275109194,
    "input_throughput": 5777.453571017295,
    "output_throughput": 5063.613466590679,
    "total_throughput": 10841.067037607974,
    "itl": 87.51277572761856,
    "ttft": 1968834.909373044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3229605367686696,
    "arrivals": 497997,
    "finished_requests": 84341,
    "scheduler_time": 152.43914131100837
}
#Debug simulation 
Total elapsed time: 6.0423101070337. Arrivals time: 0.3006274038925767 Scheduler time: 5.583176223561168 Scheduler overhead time: 0.05772105138748884 Adapter cache time: 0.014199311845004559 Engine time: 0.05886854603886604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.344846535008401,
    "estimated_duration": 3600.0747476219653,
    "input_throughput": 6102.151632965712,
    "output_throughput": 5339.798850759481,
    "total_throughput": 11441.950483725193,
    "itl": 98.89601053055476,
    "ttft": 1937164.2188404603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1003099894476973,
    "arrivals": 497997,
    "finished_requests": 89029,
    "scheduler_time": 147.40878988143248
}
#Debug simulation 
Total elapsed time: 6.34496529167518. Arrivals time: 0.29084146628156304 Scheduler time: 5.909855025354773 Scheduler overhead time: 0.052476788870990276 Adapter cache time: 0.013441498391330242 Engine time: 0.05326971597969532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332464970 . Total output tokens: 293251279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.07079018233344,
    "estimated_duration": 3600.0806995576418,
    "input_throughput": 5777.478266683221,
    "output_throughput": 5063.890929511678,
    "total_throughput": 10841.369196194899,
    "itl": 87.51240963917468,
    "ttft": 1968812.9230138084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.302663053460429,
    "arrivals": 497997,
    "finished_requests": 84344,
    "scheduler_time": 152.44505672017888
}
#Debug simulation 
Total elapsed time: 6.070908376015723. Arrivals time: 0.3004461764357984 Scheduler time: 5.609730421565473 Scheduler overhead time: 0.05855998117476702 Adapter cache time: 0.014436826575547457 Engine time: 0.05967944301664829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.749164822045714,
    "estimated_duration": 3600.102756818897,
    "input_throughput": 6305.495296489378,
    "output_throughput": 5521.192405508858,
    "total_throughput": 11826.687701998237,
    "itl": 105.26763280617713,
    "ttft": 1920741.7279084725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7192263483069816,
    "arrivals": 497033,
    "finished_requests": 92177,
    "scheduler_time": 145.92301514256565
}
#Debug simulation 
Total elapsed time: 6.749258432071656. Arrivals time: 0.5514442375861108 Scheduler time: 6.060893533285707 Scheduler overhead time: 0.050037245731800795 Adapter cache time: 0.012009065132588148 Engine time: 0.050996719393879175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.641130652278662,
    "estimated_duration": 3600.063470987719,
    "input_throughput": 6145.525260400464,
    "output_throughput": 5376.895478648085,
    "total_throughput": 11522.42073904855,
    "itl": 98.20015098818499,
    "ttft": 1938081.9981793864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8278131376858822,
    "arrivals": 497033,
    "finished_requests": 89731,
    "scheduler_time": 148.51720863063107
}
#Debug simulation 
Total elapsed time: 6.6412254041060805. Arrivals time: 0.5441123778000474 Scheduler time: 5.952499881386757 Scheduler overhead time: 0.05280127888545394 Adapter cache time: 0.012173594906926155 Engine time: 0.054225347470492125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.348105814307928,
    "estimated_duration": 3600.0981342616265,
    "input_throughput": 5825.29231645549,
    "output_throughput": 5095.211107005606,
    "total_throughput": 10920.503423461098,
    "itl": 86.91528469492073,
    "ttft": 1972445.481637315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.851250386149626,
    "arrivals": 497033,
    "finished_requests": 84997,
    "scheduler_time": 153.50469663022648
}
#Debug simulation 
Total elapsed time: 6.348200897220522. Arrivals time: 0.2667606370523572 Scheduler time: 5.921613189391792 Scheduler overhead time: 0.05853612767532468 Adapter cache time: 0.0131315803155303 Engine time: 0.060085633769631386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.638202960137278,
    "estimated_duration": 3600.0825379246726,
    "input_throughput": 6145.474379249614,
    "output_throughput": 5376.785058699957,
    "total_throughput": 11522.25943794957,
    "itl": 98.1951788039939,
    "ttft": 1938039.634585308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.707037913007659,
    "arrivals": 497033,
    "finished_requests": 89729,
    "scheduler_time": 148.5226168633429
}
#Debug simulation 
Total elapsed time: 6.6382655231282115. Arrivals time: 0.2705206824466586 Scheduler time: 6.222601057030261 Scheduler overhead time: 0.05334960808977485 Adapter cache time: 0.012124287430197 Engine time: 0.05433974089100957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.348074147012085,
    "estimated_duration": 3600.084798259368,
    "input_throughput": 5825.435003681007,
    "output_throughput": 5095.238592398972,
    "total_throughput": 10920.67359607998,
    "itl": 86.91531716105962,
    "ttft": 1972418.2112379796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.833852543313991,
    "arrivals": 497033,
    "finished_requests": 84998,
    "scheduler_time": 153.50559952940569
}
#Debug simulation 
Total elapsed time: 6.34814023738727. Arrivals time: 0.26355553697794676 Scheduler time: 5.925176813732833 Scheduler overhead time: 0.058480636682361364 Adapter cache time: 0.012894722167402506 Engine time: 0.060063487850129604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.601221272721887,
    "estimated_duration": 3600.009385873243,
    "input_throughput": 6145.617588336754,
    "output_throughput": 5376.9762589951115,
    "total_throughput": 11522.593847331866,
    "itl": 98.19235487647477,
    "ttft": 1938043.245646354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.595980235142628,
    "arrivals": 497033,
    "finished_requests": 89731,
    "scheduler_time": 148.5225626590933
}
#Debug simulation 
Total elapsed time: 6.601326907984912. Arrivals time: 0.27194241946563125 Scheduler time: 6.185232070740312 Scheduler overhead time: 0.05286793317645788 Adapter cache time: 0.012216254603117704 Engine time: 0.053769735153764486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331812623 . Total output tokens: 292669292
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.347238597925752,
    "estimated_duration": 3600.0823348661006,
    "input_throughput": 5825.317881453399,
    "output_throughput": 5095.233467954074,
    "total_throughput": 10920.551349407473,
    "itl": 86.91689061987533,
    "ttft": 1972434.2024176924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8174902863614295,
    "arrivals": 497033,
    "finished_requests": 84997,
    "scheduler_time": 153.50390114471324
}
#Debug simulation 
Total elapsed time: 6.347347577102482. Arrivals time: 0.26073361095041037 Scheduler time: 5.927126469556242 Scheduler overhead time: 0.05849806638434529 Adapter cache time: 0.01312448550015688 Engine time: 0.05976012255996466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 20.591370570939034,
    "estimated_duration": 3600.0304688359033,
    "input_throughput": 5692.480710205377,
    "output_throughput": 4969.308775262882,
    "total_throughput": 10661.789485468258,
    "itl": 116.55199649945597,
    "ttft": 1953705.3862540706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0549329727609025,
    "arrivals": 432423,
    "finished_requests": 82794,
    "scheduler_time": 131.2720990591919
}
#Debug simulation 
Total elapsed time: 20.591491361614317. Arrivals time: 0.31930717965587974 Scheduler time: 20.129663043189794 Scheduler overhead time: 0.05234636180102825 Adapter cache time: 0.014309358783066273 Engine time: 0.052857253700494766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 15.896378037985414,
    "estimated_duration": 3600.1002561803534,
    "input_throughput": 5517.620229019777,
    "output_throughput": 4817.168346972615,
    "total_throughput": 10334.788575992392,
    "itl": 109.25966130199434,
    "ttft": 1971582.944440628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 552,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.036177898347383,
    "arrivals": 432423,
    "finished_requests": 80267,
    "scheduler_time": 132.97748730695017
}
#Debug simulation 
Total elapsed time: 15.896475323941559. Arrivals time: 0.34609085181728005 Scheduler time: 15.404392719268799 Scheduler overhead time: 0.05300152441486716 Adapter cache time: 0.015623771585524082 Engine time: 0.05353541160002351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.381976552773267,
    "estimated_duration": 3600.0651319112058,
    "input_throughput": 5210.290456617951,
    "output_throughput": 4556.273678107602,
    "total_throughput": 9766.564134725553,
    "itl": 96.96764573939105,
    "ttft": 2009214.1754333915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 924,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.92743539575955,
    "arrivals": 432423,
    "finished_requests": 75782,
    "scheduler_time": 137.10978138344254
}
#Debug simulation 
Total elapsed time: 11.382040000986308. Arrivals time: 0.5889881933107972 Scheduler time: 10.634746241383255 Scheduler overhead time: 0.05555224372074008 Adapter cache time: 0.021086473017930984 Engine time: 0.0558638796210289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 14.293315021321177,
    "estimated_duration": 3600.0627362061878,
    "input_throughput": 5526.099809295264,
    "output_throughput": 4824.315650205182,
    "total_throughput": 10350.415459500446,
    "itl": 109.08290316843133,
    "ttft": 1972331.1440869684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 712,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.884077627174549,
    "arrivals": 432423,
    "finished_requests": 80386,
    "scheduler_time": 133.18689549357504
}
#Debug simulation 
Total elapsed time: 14.29342309711501. Arrivals time: 0.3496270594187081 Scheduler time: 13.798178722150624 Scheduler overhead time: 0.05237363977357745 Adapter cache time: 0.017376802396029234 Engine time: 0.05224266229197383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 10.47089755628258,
    "estimated_duration": 3600.0082844560598,
    "input_throughput": 5202.3819169710005,
    "output_throughput": 4546.919814234053,
    "total_throughput": 9749.301731205054,
    "itl": 97.18870245136179,
    "ttft": 2009822.1838334417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1000,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.43900303112345,
    "arrivals": 432423,
    "finished_requests": 75651,
    "scheduler_time": 136.8240358932371
}
#Debug simulation 
Total elapsed time: 10.471004019025713. Arrivals time: 0.3213212778791785 Scheduler time: 9.990795658435673 Scheduler overhead time: 0.055583507753908634 Adapter cache time: 0.021674327086657286 Engine time: 0.055904383305460215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 17.182010338176042,
    "estimated_duration": 3600.0616986187397,
    "input_throughput": 5532.340461731976,
    "output_throughput": 4830.156940552353,
    "total_throughput": 10362.497402284329,
    "itl": 108.97262853073596,
    "ttft": 1973602.1714931692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 546,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4856208335514953,
    "arrivals": 432423,
    "finished_requests": 80477,
    "scheduler_time": 133.3039204773872
}
#Debug simulation 
Total elapsed time: 17.182154619134963. Arrivals time: 0.6300619198009372 Scheduler time: 16.405987256206572 Scheduler overhead time: 0.0530727868899703 Adapter cache time: 0.0158595722168684 Engine time: 0.05341128213331103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288495134 . Total output tokens: 254538121
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.21441355207935,
    "estimated_duration": 3600.038781129356,
    "input_throughput": 5191.079078914096,
    "output_throughput": 4538.641940649944,
    "total_throughput": 9729.72101956404,
    "itl": 97.38181278093344,
    "ttft": 2012491.8158029697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.486526679974048,
    "arrivals": 432423,
    "finished_requests": 75521,
    "scheduler_time": 136.52760715389377
}
#Debug simulation 
Total elapsed time: 10.214517413638532. Arrivals time: 0.3108139457181096 Scheduler time: 9.744095690082759 Scheduler overhead time: 0.05528190406039357 Adapter cache time: 0.023670013062655926 Engine time: 0.05485799303278327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 20.474393936339766,
    "estimated_duration": 3600.0686867002764,
    "input_throughput": 5603.390311558094,
    "output_throughput": 4903.552830871226,
    "total_throughput": 10506.94314242932,
    "itl": 116.6868137664109,
    "ttft": 1926108.2118661818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.253305243719407,
    "arrivals": 386952,
    "finished_requests": 81965,
    "scheduler_time": 129.7414079017277
}
#Debug simulation 
Total elapsed time: 20.474517888389528. Arrivals time: 0.3215192691422999 Scheduler time: 20.008351556956768 Scheduler overhead time: 0.053367730230093 Adapter cache time: 0.014830341096967459 Engine time: 0.05334427021443844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 14.848999246023595,
    "estimated_duration": 3600.102487450073,
    "input_throughput": 5452.196449524608,
    "output_throughput": 4768.369806093769,
    "total_throughput": 10220.566255618378,
    "itl": 109.2990218745086,
    "ttft": 1944805.0045300173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 629,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.612421290143399,
    "arrivals": 386952,
    "finished_requests": 79634,
    "scheduler_time": 131.70721399338882
}
#Debug simulation 
Total elapsed time: 14.849093781784177. Arrivals time: 0.30186508828774095 Scheduler time: 14.40127058699727 Scheduler overhead time: 0.053148137871176004 Adapter cache time: 0.015997170004993677 Engine time: 0.05294596403837204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.554415807593614,
    "estimated_duration": 3600.057291635394,
    "input_throughput": 5178.161204076746,
    "output_throughput": 4527.121842718334,
    "total_throughput": 9705.28304679508,
    "itl": 96.67616048482137,
    "ttft": 1982483.350489311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.37610676676503,
    "arrivals": 386952,
    "finished_requests": 75660,
    "scheduler_time": 136.27944703202715
}
#Debug simulation 
Total elapsed time: 10.554507837630808. Arrivals time: 0.2709245537407696 Scheduler time: 10.126486791297793 Scheduler overhead time: 0.05606193980202079 Adapter cache time: 0.01973872911185026 Engine time: 0.055468097794801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 16.439838686957955,
    "estimated_duration": 3600.1112439065755,
    "input_throughput": 5483.394446050146,
    "output_throughput": 4796.522060041129,
    "total_throughput": 10279.916506091276,
    "itl": 108.91865705623859,
    "ttft": 1943560.064398108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.030832523792045,
    "arrivals": 386952,
    "finished_requests": 80113,
    "scheduler_time": 132.30022462378327
}
#Debug simulation 
Total elapsed time: 16.439908428117633. Arrivals time: 0.560008151922375 Scheduler time: 15.731979717034847 Scheduler overhead time: 0.05369602469727397 Adapter cache time: 0.01603796100243926 Engine time: 0.054115922655910254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 10.316343028098345,
    "estimated_duration": 3600.073602165946,
    "input_throughput": 5179.472438780569,
    "output_throughput": 4526.5505100217215,
    "total_throughput": 9706.02294880229,
    "itl": 97.0526289304904,
    "ttft": 1983291.2202125988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.152396422061162,
    "arrivals": 386952,
    "finished_requests": 75671,
    "scheduler_time": 135.9864825655099
}
#Debug simulation 
Total elapsed time: 10.316437172237784. Arrivals time: 0.270521754398942 Scheduler time: 9.890217977110296 Scheduler overhead time: 0.055826632771641016 Adapter cache time: 0.01933648530393839 Engine time: 0.05484403856098652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.669793871231377,
    "estimated_duration": 3600.0428957920485,
    "input_throughput": 5462.0490836328745,
    "output_throughput": 4774.854771895179,
    "total_throughput": 10236.903855528053,
    "itl": 109.27866707340831,
    "ttft": 1945046.4482888016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 695,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4368250536964995,
    "arrivals": 386952,
    "finished_requests": 79804,
    "scheduler_time": 131.78192348889374
}
#Debug simulation 
Total elapsed time: 13.669883484952152. Arrivals time: 0.298749677836895 Scheduler time: 13.226297506131232 Scheduler overhead time: 0.05240448843687773 Adapter cache time: 0.016845012083649635 Engine time: 0.051946640480309725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258114440 . Total output tokens: 227948153
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.43658202001825,
    "estimated_duration": 3600.0493668302793,
    "input_throughput": 5174.386543593806,
    "output_throughput": 4532.767564342487,
    "total_throughput": 9707.154107936294,
    "itl": 97.0376151685219,
    "ttft": 1982153.0050835132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 879,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.4903327593021345,
    "arrivals": 386952,
    "finished_requests": 75608,
    "scheduler_time": 136.08122933997103
}
#Debug simulation 
Total elapsed time: 10.436676704790443. Arrivals time: 0.27815205650404096 Scheduler time: 10.001925263553858 Scheduler overhead time: 0.05581096652895212 Adapter cache time: 0.01948254555463791 Engine time: 0.05565656675025821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 20.252328617032617,
    "estimated_duration": 3600.0932240413244,
    "input_throughput": 5633.681334849564,
    "output_throughput": 4921.467278036415,
    "total_throughput": 10555.14861288598,
    "itl": 116.85904734016344,
    "ttft": 1924356.7534903558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.94252201921775,
    "arrivals": 379429,
    "finished_requests": 82017,
    "scheduler_time": 129.94316033251837
}
#Debug simulation 
Total elapsed time: 20.252437146846205. Arrivals time: 0.3305217269808054 Scheduler time: 19.77795504871756 Scheduler overhead time: 0.05302404938265681 Adapter cache time: 0.014783541671931744 Engine time: 0.052768674213439226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 15.486758498009294,
    "estimated_duration": 3600.0425915539167,
    "input_throughput": 5476.008546746315,
    "output_throughput": 4795.053269786146,
    "total_throughput": 10271.061816532461,
    "itl": 109.19298038956848,
    "ttft": 1944338.3972217073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.231312483241787,
    "arrivals": 379429,
    "finished_requests": 79794,
    "scheduler_time": 132.1617734177008
}
#Debug simulation 
Total elapsed time: 15.48688475927338. Arrivals time: 0.29594725044444203 Scheduler time: 15.04563280614093 Scheduler overhead time: 0.05290925083681941 Adapter cache time: 0.015689777210354805 Engine time: 0.05286162905395031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.489107222296298,
    "estimated_duration": 3600.0589836395106,
    "input_throughput": 5182.1909265329205,
    "output_throughput": 4525.9747337605195,
    "total_throughput": 9708.16566029344,
    "itl": 97.41673871061819,
    "ttft": 1984231.2755210458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 970,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.281846707002275,
    "arrivals": 379429,
    "finished_requests": 75392,
    "scheduler_time": 135.75728429339097
}
#Debug simulation 
Total elapsed time: 9.489199090283364. Arrivals time: 0.2602259237319231 Scheduler time: 9.074058061931282 Scheduler overhead time: 0.0549481688067317 Adapter cache time: 0.019973344169557095 Engine time: 0.05456807650625706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 15.446340800262988,
    "estimated_duration": 3600.09262789336,
    "input_throughput": 5473.123621135799,
    "output_throughput": 4785.206321227548,
    "total_throughput": 10258.329942363347,
    "itl": 109.08488798296285,
    "ttft": 1945869.7943407197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 659,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.512412513964803,
    "arrivals": 379429,
    "finished_requests": 79692,
    "scheduler_time": 132.1031760007408
}
#Debug simulation 
Total elapsed time: 15.446435181424022. Arrivals time: 0.31301213754341006 Scheduler time: 14.986114277970046 Scheduler overhead time: 0.0533508500084281 Adapter cache time: 0.01655231835320592 Engine time: 0.053335356060415506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.995029513724148,
    "estimated_duration": 3600.085211468796,
    "input_throughput": 5186.402516395503,
    "output_throughput": 4530.882476902556,
    "total_throughput": 9717.284993298059,
    "itl": 96.87350472205988,
    "ttft": 1982648.3431375576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 880,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.539624074571788,
    "arrivals": 379429,
    "finished_requests": 75490,
    "scheduler_time": 136.24604475946816
}
#Debug simulation 
Total elapsed time: 9.995121812913567. Arrivals time: 0.26784323202446103 Scheduler time: 9.57102577900514 Scheduler overhead time: 0.055673106107860804 Adapter cache time: 0.019527807366102934 Engine time: 0.05515673290938139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 14.687459978740662,
    "estimated_duration": 3600.0632922616237,
    "input_throughput": 5478.600346387096,
    "output_throughput": 4792.080471774739,
    "total_throughput": 10270.680818161834,
    "itl": 109.4430523377076,
    "ttft": 1944847.6844300658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 696,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.44320897463707,
    "arrivals": 379429,
    "finished_requests": 79784,
    "scheduler_time": 131.98452247779022
}
#Debug simulation 
Total elapsed time: 14.68752751685679. Arrivals time: 0.29459331557154655 Scheduler time: 14.247257322072983 Scheduler overhead time: 0.052578480914235115 Adapter cache time: 0.016620791517198086 Engine time: 0.052650618832558393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 253045891 . Total output tokens: 223500132
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.40150059107691,
    "estimated_duration": 3600.0320031971646,
    "input_throughput": 5173.0992900787405,
    "output_throughput": 4524.46450074181,
    "total_throughput": 9697.563790820552,
    "itl": 96.84778934190375,
    "ttft": 1982905.9947993131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 787,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.790415816660997,
    "arrivals": 379429,
    "finished_requests": 75323,
    "scheduler_time": 136.13833425717962
}
#Debug simulation 
Total elapsed time: 10.401590968016535. Arrivals time: 0.2648385767824948 Scheduler time: 9.981268065050244 Scheduler overhead time: 0.05591053981333971 Adapter cache time: 0.01804641541093588 Engine time: 0.055762695614248514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 19.651677502319217,
    "estimated_duration": 3600.0853394243486,
    "input_throughput": 5666.449563460043,
    "output_throughput": 4964.756752921298,
    "total_throughput": 10631.206316381342,
    "itl": 116.21029470692739,
    "ttft": 1911206.435137625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.074770199856753,
    "arrivals": 375670,
    "finished_requests": 82985,
    "scheduler_time": 130.88671434049874
}
#Debug simulation 
Total elapsed time: 19.6517710480839. Arrivals time: 0.31905384035781026 Scheduler time: 19.188600279856473 Scheduler overhead time: 0.05322858924046159 Adapter cache time: 0.01439035777002573 Engine time: 0.053346600383520126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.434849073644727,
    "estimated_duration": 3600.0220960850065,
    "input_throughput": 5487.443541383623,
    "output_throughput": 4810.231309088915,
    "total_throughput": 10297.674850472536,
    "itl": 109.30486918409227,
    "ttft": 1935939.608075458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.455316377771093,
    "arrivals": 375670,
    "finished_requests": 80342,
    "scheduler_time": 132.22002739823597
}
#Debug simulation 
Total elapsed time: 13.434944139793515. Arrivals time: 0.29259663075208664 Scheduler time: 13.00011300155893 Scheduler overhead time: 0.05187382781878114 Adapter cache time: 0.014527149498462677 Engine time: 0.05211251135915518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.26067490177229,
    "estimated_duration": 3600.0843678953943,
    "input_throughput": 5176.369244616958,
    "output_throughput": 4539.826662327512,
    "total_throughput": 9716.195906944471,
    "itl": 97.34063304115668,
    "ttft": 1975603.3592661058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.124039685786727,
    "arrivals": 375670,
    "finished_requests": 75771,
    "scheduler_time": 135.90581541302393
}
#Debug simulation 
Total elapsed time: 9.260766219813377. Arrivals time: 0.26335338968783617 Scheduler time: 8.843784081749618 Scheduler overhead time: 0.05500307772308588 Adapter cache time: 0.017847198992967606 Engine time: 0.05526616657152772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 13.845896271057427,
    "estimated_duration": 3600.008362539907,
    "input_throughput": 5466.526468320627,
    "output_throughput": 4790.513594205248,
    "total_throughput": 10257.040062525875,
    "itl": 108.77485463837857,
    "ttft": 1934554.353288553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.840419785743573,
    "arrivals": 375670,
    "finished_requests": 80006,
    "scheduler_time": 132.31365998581984
}
#Debug simulation 
Total elapsed time: 13.84600402507931. Arrivals time: 0.3008221215568483 Scheduler time: 13.401943562552333 Scheduler overhead time: 0.0520950173959136 Adapter cache time: 0.014793290756642818 Engine time: 0.05254071019589901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.442358930129558,
    "estimated_duration": 3600.093347421625,
    "input_throughput": 5145.74962709444,
    "output_throughput": 4514.286556385968,
    "total_throughput": 9660.036183480408,
    "itl": 97.28373002495309,
    "ttft": 1976320.7411566945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 766,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.688546310067203,
    "arrivals": 375670,
    "finished_requests": 75378,
    "scheduler_time": 135.59303716437503
}
#Debug simulation 
Total elapsed time: 9.442479473073035. Arrivals time: 0.27877758955582976 Scheduler time: 9.009778921026736 Scheduler overhead time: 0.055434920359402895 Adapter cache time: 0.017782905604690313 Engine time: 0.055039575789123774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.005330141168088,
    "estimated_duration": 3600.104000394833,
    "input_throughput": 5487.879793981841,
    "output_throughput": 4800.374655316806,
    "total_throughput": 10288.254449298647,
    "itl": 109.44235751626537,
    "ttft": 1934557.5746118114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.792049038698879,
    "arrivals": 375670,
    "finished_requests": 80294,
    "scheduler_time": 132.01864803220042
}
#Debug simulation 
Total elapsed time: 13.00542654376477. Arrivals time: 0.2899274677038193 Scheduler time: 12.574423391837627 Scheduler overhead time: 0.05139538086950779 Adapter cache time: 0.014677640050649643 Engine time: 0.05136113194748759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250503986 . Total output tokens: 221259781
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.862559522036463,
    "estimated_duration": 3600.0571699384045,
    "input_throughput": 5156.127006815726,
    "output_throughput": 4522.484847172181,
    "total_throughput": 9678.611853987908,
    "itl": 97.56827763812197,
    "ttft": 1976354.660871462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 953,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.036594976652437,
    "arrivals": 375670,
    "finished_requests": 75472,
    "scheduler_time": 135.50491472979888
}
#Debug simulation 
Total elapsed time: 8.86268555233255. Arrivals time: 0.2609862363897264 Scheduler time: 8.447057736571878 Scheduler overhead time: 0.05493419570848346 Adapter cache time: 0.019190522376447916 Engine time: 0.05495015811175108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 20.559878641273826,
    "estimated_duration": 3600.091859530423,
    "input_throughput": 5752.99764787154,
    "output_throughput": 5026.299524023877,
    "total_throughput": 10779.297171895418,
    "itl": 115.33964336655094,
    "ttft": 1902039.0109085187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 431,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.849948292770448,
    "arrivals": 373765,
    "finished_requests": 84099,
    "scheduler_time": 132.09004655365618
}
#Debug simulation 
Total elapsed time: 20.559992300346494. Arrivals time: 0.32635139022022486 Scheduler time: 20.088785031810403 Scheduler overhead time: 0.05344456899911165 Adapter cache time: 0.01353658502921462 Engine time: 0.054447962902486324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 14.316031584050506,
    "estimated_duration": 3600.053096505695,
    "input_throughput": 5510.365949673048,
    "output_throughput": 4808.189083878035,
    "total_throughput": 10318.555033551082,
    "itl": 108.81561487952897,
    "ttft": 1930204.7209855572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.121114275474106,
    "arrivals": 373765,
    "finished_requests": 80513,
    "scheduler_time": 132.43834193381716
}
#Debug simulation 
Total elapsed time: 14.31613673409447. Arrivals time: 0.3093658001162112 Scheduler time: 13.8626159587875 Scheduler overhead time: 0.052447544410824776 Adapter cache time: 0.014849330298602581 Engine time: 0.052988345734775066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.396040249150246,
    "estimated_duration": 3600.0919668050296,
    "input_throughput": 5222.356865701205,
    "output_throughput": 4556.228327288265,
    "total_throughput": 9778.585192989469,
    "itl": 96.97585655862925,
    "ttft": 1971867.1988456072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.352183477240638,
    "arrivals": 373765,
    "finished_requests": 76253,
    "scheduler_time": 136.31863178657898
}
#Debug simulation 
Total elapsed time: 9.396131461020559. Arrivals time: 0.26918594632297754 Scheduler time: 8.972904541064054 Scheduler overhead time: 0.05513545172289014 Adapter cache time: 0.018039445858448744 Engine time: 0.055033883545547724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.579736184328794,
    "estimated_duration": 3600.083812701448,
    "input_throughput": 5521.077573214912,
    "output_throughput": 4818.846977615817,
    "total_throughput": 10339.924550830729,
    "itl": 108.55118760144019,
    "ttft": 1932297.0987312952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 716,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.91377797385677,
    "arrivals": 373765,
    "finished_requests": 80658,
    "scheduler_time": 132.73742493550589
}
#Debug simulation 
Total elapsed time: 12.579843412153423. Arrivals time: 0.3273245212621987 Scheduler time: 12.109189942013472 Scheduler overhead time: 0.05175698781386018 Adapter cache time: 0.015763533301651478 Engine time: 0.052048454992473125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.458323827013373,
    "estimated_duration": 3600.085456976451,
    "input_throughput": 5205.170606071692,
    "output_throughput": 4542.788274180398,
    "total_throughput": 9747.95888025209,
    "itl": 97.18113619449883,
    "ttft": 1972210.7739150205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.751656918367398,
    "arrivals": 373765,
    "finished_requests": 76002,
    "scheduler_time": 135.982587957862
}
#Debug simulation 
Total elapsed time: 9.458452234975994. Arrivals time: 0.26367892185226083 Scheduler time: 9.040921146515757 Scheduler overhead time: 0.05555835505947471 Adapter cache time: 0.017687669023871422 Engine time: 0.05493565835058689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.161506067030132,
    "estimated_duration": 3600.0042107594295,
    "input_throughput": 5539.716298218711,
    "output_throughput": 4834.014901423937,
    "total_throughput": 10373.731199642647,
    "itl": 108.94867141886199,
    "ttft": 1926692.4520315123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.213387820776532,
    "arrivals": 373765,
    "finished_requests": 80920,
    "scheduler_time": 132.72384323261264
}
#Debug simulation 
Total elapsed time: 13.161597287748009. Arrivals time: 0.32734494702890515 Scheduler time: 12.691328978165984 Scheduler overhead time: 0.05216307332739234 Adapter cache time: 0.01516757719218731 Engine time: 0.05196647625416517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249289724 . Total output tokens: 220134166
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.990268115885556,
    "estimated_duration": 3600.00496054652,
    "input_throughput": 5204.531161855849,
    "output_throughput": 4543.508461587476,
    "total_throughput": 9748.039623443325,
    "itl": 97.25305177537271,
    "ttft": 1973388.0723489814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 840,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.198460359796909,
    "arrivals": 373765,
    "finished_requests": 76011,
    "scheduler_time": 135.94377624835724
}
#Debug simulation 
Total elapsed time: 8.990361164789647. Arrivals time: 0.2612547567114234 Scheduler time: 8.576238654088229 Scheduler overhead time: 0.05479679349809885 Adapter cache time: 0.01820608740672469 Engine time: 0.054250652436167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 15.125540495384485,
    "estimated_duration": 3600.0595716561725,
    "input_throughput": 5723.470845378524,
    "output_throughput": 4991.99294964233,
    "total_throughput": 10715.463795020854,
    "itl": 115.38795180800882,
    "ttft": 1910880.705490494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7029490578920172,
    "arrivals": 372804,
    "finished_requests": 83143,
    "scheduler_time": 131.64120978163558
}
#Debug simulation 
Total elapsed time: 15.125689810141921. Arrivals time: 0.3026268584653735 Scheduler time: 14.684294356964529 Scheduler overhead time: 0.0508575476706028 Adapter cache time: 0.013096415903419256 Engine time: 0.052068207412958145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.393876615911722,
    "estimated_duration": 3600.0590614406497,
    "input_throughput": 5560.168224570667,
    "output_throughput": 4854.559800751235,
    "total_throughput": 10414.728025321903,
    "itl": 107.7358046637994,
    "ttft": 1928227.9479418313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 552,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.033401456400757,
    "arrivals": 372804,
    "finished_requests": 80818,
    "scheduler_time": 133.79392317187964
}
#Debug simulation 
Total elapsed time: 13.39394685300067. Arrivals time: 0.546274317894131 Scheduler time: 12.704796251375228 Scheduler overhead time: 0.05212823348119855 Adapter cache time: 0.013863753527402878 Engine time: 0.05291188973933458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.375628758221865,
    "estimated_duration": 3600.0494715161044,
    "input_throughput": 5205.907904400909,
    "output_throughput": 4546.846961274258,
    "total_throughput": 9752.754865675166,
    "itl": 96.76687495686276,
    "ttft": 1973364.832327658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.133185481824942,
    "arrivals": 372804,
    "finished_requests": 75575,
    "scheduler_time": 136.43300430230065
}
#Debug simulation 
Total elapsed time: 9.37573026213795. Arrivals time: 0.2677604779601097 Scheduler time: 8.954929084982723 Scheduler overhead time: 0.055655643343925476 Adapter cache time: 0.016120594460517168 Engine time: 0.05540183046832681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 11.877667318098247,
    "estimated_duration": 3600.054209760247,
    "input_throughput": 5549.404213368911,
    "output_throughput": 4841.493484388604,
    "total_throughput": 10390.897697757515,
    "itl": 108.48596373528957,
    "ttft": 1924980.2861677848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.281203139130952,
    "arrivals": 372804,
    "finished_requests": 80545,
    "scheduler_time": 133.1911115627531
}
#Debug simulation 
Total elapsed time: 11.877816404215991. Arrivals time: 0.5370870451442897 Scheduler time: 11.20093493303284 Scheduler overhead time: 0.0507693556137383 Adapter cache time: 0.013987726997584105 Engine time: 0.05137399071827531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.47543907398358,
    "estimated_duration": 3600.104406082492,
    "input_throughput": 5200.480010626392,
    "output_throughput": 4538.681426125728,
    "total_throughput": 9739.16143675212,
    "itl": 96.79640532165907,
    "ttft": 1975227.2911706278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.833281754106317,
    "arrivals": 372804,
    "finished_requests": 75482,
    "scheduler_time": 136.31737346240675
}
#Debug simulation 
Total elapsed time: 9.475530344061553. Arrivals time: 0.27130533242598176 Scheduler time: 9.051866760477424 Scheduler overhead time: 0.05557945463806391 Adapter cache time: 0.015753441490232944 Engine time: 0.05520368227735162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.324738780036569,
    "estimated_duration": 3600.0395004406178,
    "input_throughput": 5534.20038795728,
    "output_throughput": 4826.499541983736,
    "total_throughput": 10360.699929941016,
    "itl": 107.88346365924393,
    "ttft": 1926548.4853265896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.504772596373207,
    "arrivals": 372804,
    "finished_requests": 80324,
    "scheduler_time": 133.3134848557133
}
#Debug simulation 
Total elapsed time: 13.324832568876445. Arrivals time: 0.5580995501950383 Scheduler time: 12.624362953007221 Scheduler overhead time: 0.05216105608269572 Adapter cache time: 0.01374142337590456 Engine time: 0.052537128794938326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248633863 . Total output tokens: 219571672
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.144498310983181,
    "estimated_duration": 3600.0563584073225,
    "input_throughput": 5215.007802907237,
    "output_throughput": 4553.3842718095875,
    "total_throughput": 9768.392074716825,
    "itl": 96.6803961986896,
    "ttft": 1974630.8427089986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 751,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.543173934798717,
    "arrivals": 372804,
    "finished_requests": 75713,
    "scheduler_time": 136.6008831756988
}
#Debug simulation 
Total elapsed time: 9.144618707243353. Arrivals time: 0.25649344408884645 Scheduler time: 8.734718536492437 Scheduler overhead time: 0.055398535914719105 Adapter cache time: 0.016952393110841513 Engine time: 0.05520561849698424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 17.141592574771494,
    "estimated_duration": 3600.07525372707,
    "input_throughput": 5685.907531740683,
    "output_throughput": 4979.2528590734255,
    "total_throughput": 10665.160390814108,
    "itl": 115.49189098061237,
    "ttft": 1902502.8688977389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2548314798949343,
    "arrivals": 372307,
    "finished_requests": 82811,
    "scheduler_time": 131.39281719092386
}
#Debug simulation 
Total elapsed time: 17.14168989704922. Arrivals time: 0.3561501852236688 Scheduler time: 16.647583475802094 Scheduler overhead time: 0.051395855844020844 Adapter cache time: 0.011696573346853256 Engine time: 0.05187345435842872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.521877062041312,
    "estimated_duration": 3600.1164108014927,
    "input_throughput": 5536.852625152266,
    "output_throughput": 4842.7278483804885,
    "total_throughput": 10379.580473532755,
    "itl": 107.95808073956016,
    "ttft": 1929739.4361180563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 411,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.008328716182154,
    "arrivals": 372307,
    "finished_requests": 80549,
    "scheduler_time": 133.5804486337609
}
#Debug simulation 
Total elapsed time: 13.522010724060237. Arrivals time: 0.33170420955866575 Scheduler time: 13.049245361238718 Scheduler overhead time: 0.0523232095874846 Adapter cache time: 0.012344071175903082 Engine time: 0.05250955978408456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.663730880245566,
    "estimated_duration": 3600.0284497166535,
    "input_throughput": 5158.977285710555,
    "output_throughput": 4518.218738321421,
    "total_throughput": 9677.196024031977,
    "itl": 97.0064432912819,
    "ttft": 1977006.6398665826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.245611425326241,
    "arrivals": 372307,
    "finished_requests": 75048,
    "scheduler_time": 135.89557857941708
}
#Debug simulation 
Total elapsed time: 8.66382020805031. Arrivals time: 0.2838403698988259 Scheduler time: 8.228617242537439 Scheduler overhead time: 0.055448644794523716 Adapter cache time: 0.015274310950189829 Engine time: 0.05497729778289795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 13.187549120280892,
    "estimated_duration": 3600.0741840849564,
    "input_throughput": 5534.924554635169,
    "output_throughput": 4849.823394524799,
    "total_throughput": 10384.74794915997,
    "itl": 108.20981791749621,
    "ttft": 1925787.6431154679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9341673876205423,
    "arrivals": 372307,
    "finished_requests": 80549,
    "scheduler_time": 133.5251849074722
}
#Debug simulation 
Total elapsed time: 13.187644361983985. Arrivals time: 0.32073296373710036 Scheduler time: 12.726227321662009 Scheduler overhead time: 0.052171275950968266 Adapter cache time: 0.012308891862630844 Engine time: 0.05229629063978791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.608257093001157,
    "estimated_duration": 3600.013200909095,
    "input_throughput": 5162.293848063387,
    "output_throughput": 4520.596756670322,
    "total_throughput": 9682.89060473371,
    "itl": 97.26703075469563,
    "ttft": 1973334.8245134805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.296645457800512,
    "arrivals": 372307,
    "finished_requests": 75090,
    "scheduler_time": 135.6944719232079
}
#Debug simulation 
Total elapsed time: 8.608379459939897. Arrivals time: 0.2847786224447191 Scheduler time: 8.173819944262505 Scheduler overhead time: 0.05483846087008715 Adapter cache time: 0.015050362795591354 Engine time: 0.05429603531956673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.092492877040058,
    "estimated_duration": 3600.0721591565507,
    "input_throughput": 5516.333040572372,
    "output_throughput": 4829.778746455369,
    "total_throughput": 10346.111787027741,
    "itl": 108.38851150461538,
    "ttft": 1926107.6731943646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2047283121663934,
    "arrivals": 372307,
    "finished_requests": 80247,
    "scheduler_time": 133.10937406642907
}
#Debug simulation 
Total elapsed time: 12.092608058359474. Arrivals time: 0.3143168375827372 Scheduler time: 11.638649734202772 Scheduler overhead time: 0.051336200907826424 Adapter cache time: 0.01331967581063509 Engine time: 0.05145820090547204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_64_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248315419 . Total output tokens: 219313736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.902594794984907,
    "estimated_duration": 3600.050009092865,
    "input_throughput": 5171.793712024438,
    "output_throughput": 4532.074265299221,
    "total_throughput": 9703.86797732366,
    "itl": 97.03001403494326,
    "ttft": 1973531.228071982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.678789581023189,
    "arrivals": 372307,
    "finished_requests": 75243,
    "scheduler_time": 136.0644218525972
}
#Debug simulation 
Total elapsed time: 8.902685458771884. Arrivals time: 0.2870406722649932 Scheduler time: 8.463770207483321 Scheduler overhead time: 0.05535578867420554 Adapter cache time: 0.01571471057832241 Engine time: 0.055065253749489784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.088707841001451,
    "estimated_duration": 3600.0016262280083,
    "input_throughput": 5575.79498124612,
    "output_throughput": 4908.543893774567,
    "total_throughput": 10484.338875020687,
    "itl": 117.67600704825453,
    "ttft": 1868471.0997158743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3260417430708586,
    "arrivals": 324936,
    "finished_requests": 81661,
    "scheduler_time": 128.56023299235673
}
#Debug simulation 
Total elapsed time: 11.088838413823396. Arrivals time: 0.5403539454564452 Scheduler time: 10.416873378679156 Scheduler overhead time: 0.04805984394624829 Adapter cache time: 0.013839242048561573 Engine time: 0.047732237726449966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.85201581614092,
    "estimated_duration": 3600.1172455762776,
    "input_throughput": 5428.2507115603175,
    "output_throughput": 4775.460582881103,
    "total_throughput": 10203.71129444142,
    "itl": 109.96337195571819,
    "ttft": 1890544.8578644826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2088270945474555,
    "arrivals": 324936,
    "finished_requests": 79474,
    "scheduler_time": 130.6273871171012
}
#Debug simulation 
Total elapsed time: 9.85215389309451. Arrivals time: 0.3017321522347629 Scheduler time: 9.411818146705627 Scheduler overhead time: 0.05021969182416797 Adapter cache time: 0.015285841654986143 Engine time: 0.04976056329905987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.124883693177253,
    "estimated_duration": 3600.104662013235,
    "input_throughput": 5135.148484728146,
    "output_throughput": 4515.801490867943,
    "total_throughput": 9650.94997559609,
    "itl": 97.51711241394136,
    "ttft": 1933027.130466458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 826,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.188953203372681,
    "arrivals": 324936,
    "finished_requests": 75133,
    "scheduler_time": 134.6524932478696
}
#Debug simulation 
Total elapsed time: 8.124998717103153. Arrivals time: 0.25302927661687136 Scheduler time: 7.718448926694691 Scheduler overhead time: 0.05480930209159851 Adapter cache time: 0.01891506602987647 Engine time: 0.05432416312396526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 9.70847249077633,
    "estimated_duration": 3600.0225319229294,
    "input_throughput": 5429.592961341627,
    "output_throughput": 4774.957891949108,
    "total_throughput": 10204.550853290735,
    "itl": 109.95572710336795,
    "ttft": 1890683.2772404766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 587,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.987523820498019,
    "arrivals": 324936,
    "finished_requests": 79475,
    "scheduler_time": 130.63167808232404
}
#Debug simulation 
Total elapsed time: 9.708566877990961. Arrivals time: 0.269466879311949 Scheduler time: 9.300118416547775 Scheduler overhead time: 0.05021149991080165 Adapter cache time: 0.015278758015483618 Engine time: 0.05031590210273862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_64_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.20649958588183,
    "estimated_duration": 3600.041323550671,
    "input_throughput": 5135.864379957793,
    "output_throughput": 4515.562055817382,
    "total_throughput": 9651.426435775176,
    "itl": 97.54252471550285,
    "ttft": 1932899.089999529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 801,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.947078830194683,
    "arrivals": 324936,
    "finished_requests": 75143,
    "scheduler_time": 134.6435498356341
}
#Debug simulation 
Total elapsed time: 8.206610206048936. Arrivals time: 0.26592523232102394 Scheduler time: 7.786747236736119 Scheduler overhead time: 0.055020066909492016 Adapter cache time: 0.018386803567409515 Engine time: 0.054834394715726376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_64_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_64_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.831006365828216,
    "estimated_duration": 3600.117225607203,
    "input_throughput": 5431.369251233772,
    "output_throughput": 4776.221140160196,
    "total_throughput": 10207.59039139397,
    "itl": 109.91564228585487,
    "ttft": 1890748.9586577886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6388349361251873,
    "arrivals": 324936,
    "finished_requests": 79484,
    "scheduler_time": 130.65346594547543
}
#Debug simulation 
Total elapsed time: 9.831135970074683. Arrivals time: 0.27217232529073954 Scheduler time: 9.421132091898471 Scheduler overhead time: 0.050209110137075186 Adapter cache time: 0.014976565726101398 Engine time: 0.04940660623833537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_64_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_64_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216736162 . Total output tokens: 191416247
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.20475015277043,
    "estimated_duration": 3600.039208962647,
    "input_throughput": 5137.414879803298,
    "output_throughput": 4516.136368604948,
    "total_throughput": 9653.551248408246,
    "itl": 97.47418556254601,
    "ttft": 1933524.88294573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 803,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.9088106697239215,
    "arrivals": 324936,
    "finished_requests": 75158,
    "scheduler_time": 134.67351623142778
}
#Debug simulation 
Total elapsed time: 8.204846172127873. Arrivals time: 0.25569062773138285 Scheduler time: 7.796416447497904 Scheduler overhead time: 0.05477402778342366 Adapter cache time: 0.01835168292745948 Engine time: 0.05419723456725478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.81988789793104,
    "estimated_duration": 3600.043882074879,
    "input_throughput": 5624.491162682679,
    "output_throughput": 4906.471859398465,
    "total_throughput": 10530.963022081143,
    "itl": 117.63719110504637,
    "ttft": 1865539.8732624154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.425227878550111,
    "arrivals": 317347,
    "finished_requests": 81741,
    "scheduler_time": 128.53360380354675
}
#Debug simulation 
Total elapsed time: 9.819976826198399. Arrivals time: 0.2861403371207416 Scheduler time: 9.40300263138488 Scheduler overhead time: 0.047526062931865454 Adapter cache time: 0.01399054890498519 Engine time: 0.047304346691817045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.937668192666024,
    "estimated_duration": 3600.0015537121085,
    "input_throughput": 5461.555420643114,
    "output_throughput": 4771.744607245174,
    "total_throughput": 10233.300027888288,
    "itl": 109.95268970030627,
    "ttft": 1886323.7117615277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.451982751898473,
    "arrivals": 317347,
    "finished_requests": 79389,
    "scheduler_time": 130.59526838284455
}
#Debug simulation 
Total elapsed time: 8.93778046965599. Arrivals time: 0.2736855745315552 Scheduler time: 8.525372544769198 Scheduler overhead time: 0.05046740313991904 Adapter cache time: 0.015399269293993711 Engine time: 0.049650806467980146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.513483901042491,
    "estimated_duration": 3600.053195569266,
    "input_throughput": 5164.446465091763,
    "output_throughput": 4511.162785035444,
    "total_throughput": 9675.609250127207,
    "itl": 97.59917077717002,
    "ttft": 1928960.9455076023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 845,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.332145759104774,
    "arrivals": 317347,
    "finished_requests": 75045,
    "scheduler_time": 134.5709378694982
}
#Debug simulation 
Total elapsed time: 7.513601331040263. Arrivals time: 0.24867953453212976 Scheduler time: 7.1109243533574045 Scheduler overhead time: 0.0548083302564919 Adapter cache time: 0.01899900659918785 Engine time: 0.05466267140582204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.996269260067493,
    "estimated_duration": 3600.0071579063033,
    "input_throughput": 5461.7688625472865,
    "output_throughput": 4772.695788191871,
    "total_throughput": 10234.464650739157,
    "itl": 109.95025152224132,
    "ttft": 1886167.2545945512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.049974808930411,
    "arrivals": 317347,
    "finished_requests": 79389,
    "scheduler_time": 130.6086610381028
}
#Debug simulation 
Total elapsed time: 8.996361684054136. Arrivals time: 0.262722862418741 Scheduler time: 8.595166258979589 Scheduler overhead time: 0.05011720675975084 Adapter cache time: 0.01513803331181407 Engine time: 0.05007045157253742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_64_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.472575687803328,
    "estimated_duration": 3600.0549199043294,
    "input_throughput": 5161.998473204918,
    "output_throughput": 4507.4717916890095,
    "total_throughput": 9669.470264893927,
    "itl": 97.37375149465068,
    "ttft": 1930131.3085685028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 836,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.208303873618159,
    "arrivals": 317347,
    "finished_requests": 74977,
    "scheduler_time": 134.66169526400856
}
#Debug simulation 
Total elapsed time: 7.472665234934539. Arrivals time: 0.24958288623020053 Scheduler time: 7.067854365333915 Scheduler overhead time: 0.05479529080912471 Adapter cache time: 0.018597705755382776 Engine time: 0.056310463696718216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_64_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_64_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.01542882900685,
    "estimated_duration": 3600.073695501914,
    "input_throughput": 5462.896502527873,
    "output_throughput": 4773.076179376469,
    "total_throughput": 10235.972681904343,
    "itl": 109.95566159156974,
    "ttft": 1886414.7924509104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7984329596394497,
    "arrivals": 317347,
    "finished_requests": 79413,
    "scheduler_time": 130.61565950132479
}
#Debug simulation 
Total elapsed time: 9.015560219064355. Arrivals time: 0.2663291268981993 Scheduler time: 8.609142826870084 Scheduler overhead time: 0.050399553030729294 Adapter cache time: 0.01585522945970297 Engine time: 0.05055521568283439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_64_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_64_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211729314 . Total output tokens: 186966896
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.60158542310819,
    "estimated_duration": 3600.0811446066996,
    "input_throughput": 5164.446648057757,
    "output_throughput": 4509.3691913918055,
    "total_throughput": 9673.815839449562,
    "itl": 97.4699542278811,
    "ttft": 1929663.661233156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 813,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.9769884071685615,
    "arrivals": 317347,
    "finished_requests": 75027,
    "scheduler_time": 134.63223351818553
}
#Debug simulation 
Total elapsed time: 7.6017083008773625. Arrivals time: 0.24771322682499886 Scheduler time: 7.200531456619501 Scheduler overhead time: 0.05453479569405317 Adapter cache time: 0.019093578681349754 Engine time: 0.05428051017224789 
