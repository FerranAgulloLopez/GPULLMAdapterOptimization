INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.766842224169523,
    "estimated_duration": 3600.0491666886205,
    "input_throughput": 5731.199782190246,
    "output_throughput": 4956.004536018938,
    "total_throughput": 10687.204318209184,
    "itl": 117.08784255980267,
    "ttft": 2076986.6976517506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7375373392272953,
    "arrivals": 863203,
    "finished_requests": 83331,
    "scheduler_time": 132.12892028829242
}
#Debug simulation 
Total elapsed time: 21.76709476299584. Arrivals time: 0.4315575114451349 Scheduler time: 21.191571473609656 Scheduler overhead time: 0.05272608436644077 Adapter cache time: 0.014273195993155241 Engine time: 0.053624133579432964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 18.785589403938502,
    "estimated_duration": 3600.0531102919626,
    "input_throughput": 5554.82443934784,
    "output_throughput": 4803.661632257839,
    "total_throughput": 10358.48607160568,
    "itl": 109.70461848945148,
    "ttft": 2094490.5528469817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.241209642793987,
    "arrivals": 863203,
    "finished_requests": 80793,
    "scheduler_time": 133.93219062101198
}
#Debug simulation 
Total elapsed time: 18.78571624867618. Arrivals time: 0.40704749850556254 Scheduler time: 18.230533338617533 Scheduler overhead time: 0.05470042210072279 Adapter cache time: 0.014813079964369535 Engine time: 0.05447621177881956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.71899575786665,
    "estimated_duration": 3600.003939292205,
    "input_throughput": 5251.198976109113,
    "output_throughput": 4542.280585174861,
    "total_throughput": 9793.479561283973,
    "itl": 97.16049532793593,
    "ttft": 2125703.1048624665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 777,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.825692929560344,
    "arrivals": 863203,
    "finished_requests": 76391,
    "scheduler_time": 138.32915626376527
}
#Debug simulation 
Total elapsed time: 11.719119139015675. Arrivals time: 0.3938462012447417 Scheduler time: 11.16745396470651 Scheduler overhead time: 0.056105084251612425 Adapter cache time: 0.01970302825793624 Engine time: 0.0560443508438766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 18.0320579810068,
    "estimated_duration": 3600.110913368547,
    "input_throughput": 5574.985460995256,
    "output_throughput": 4825.110230769637,
    "total_throughput": 10400.095691764895,
    "itl": 109.3796908574114,
    "ttft": 2096567.064152658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6400156477605865,
    "arrivals": 863203,
    "finished_requests": 81130,
    "scheduler_time": 134.42837180199413
}
#Debug simulation 
Total elapsed time: 18.03222649078816. Arrivals time: 0.4076849650591612 Scheduler time: 17.47443148912862 Scheduler overhead time: 0.05496095959097147 Adapter cache time: 0.016016085166484118 Engine time: 0.054916521068662405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 11.679906512144953,
    "estimated_duration": 3600.055454558265,
    "input_throughput": 5251.329663842552,
    "output_throughput": 4542.297252475654,
    "total_throughput": 9793.626916318206,
    "itl": 97.1591727179459,
    "ttft": 2125706.2784432676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 777,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.77080687775745,
    "arrivals": 863203,
    "finished_requests": 76392,
    "scheduler_time": 138.3331466388434
}
#Debug simulation 
Total elapsed time: 11.680040232371539. Arrivals time: 0.3780689360573888 Scheduler time: 11.14430671511218 Scheduler overhead time: 0.05570725677534938 Adapter cache time: 0.01977977715432644 Engine time: 0.05633116001263261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 18.390029795933515,
    "estimated_duration": 3600.0965320058417,
    "input_throughput": 5575.635214652469,
    "output_throughput": 4825.648936230194,
    "total_throughput": 10401.284150882662,
    "itl": 109.373042402903,
    "ttft": 2096494.8896892765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.402629861324079,
    "arrivals": 863203,
    "finished_requests": 81136,
    "scheduler_time": 134.4366252796245
}
#Debug simulation 
Total elapsed time: 18.390104661229998. Arrivals time: 0.6687393309548497 Scheduler time: 17.571667782962322 Scheduler overhead time: 0.05475846724584699 Adapter cache time: 0.01626751199364662 Engine time: 0.054561125580221415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577522286 . Total output tokens: 509068587
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.071861269883811,
    "estimated_duration": 3600.003737095953,
    "input_throughput": 5251.405104165344,
    "output_throughput": 4542.362506876516,
    "total_throughput": 9793.76761104186,
    "itl": 97.15782985657056,
    "ttft": 2125685.5721157813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 777,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.7192347007803885,
    "arrivals": 863203,
    "finished_requests": 76392,
    "scheduler_time": 138.3330013535085
}
#Debug simulation 
Total elapsed time: 12.071964017115533. Arrivals time: 0.7510432596318424 Scheduler time: 11.163772967644036 Scheduler overhead time: 0.055778523441404104 Adapter cache time: 0.019710758235305548 Engine time: 0.05581945087760687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 20.930095246061683,
    "estimated_duration": 3600.0153599827645,
    "input_throughput": 5644.351195239701,
    "output_throughput": 4915.793192639247,
    "total_throughput": 10560.144387878947,
    "itl": 117.18248762139187,
    "ttft": 2071114.318900348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5788395224604916,
    "arrivals": 802767,
    "finished_requests": 82467,
    "scheduler_time": 131.39919424119827
}
#Debug simulation 
Total elapsed time: 20.93022931087762. Arrivals time: 0.32855941727757454 Scheduler time: 20.456006209366024 Scheduler overhead time: 0.054316740948706865 Adapter cache time: 0.013987195212393999 Engine time: 0.054053920321166515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 14.741147900931537,
    "estimated_duration": 3600.055691200212,
    "input_throughput": 5508.4367301520015,
    "output_throughput": 4797.032457640272,
    "total_throughput": 10305.469187792272,
    "itl": 109.87854948302972,
    "ttft": 2083932.368406513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.426447068136191,
    "arrivals": 802767,
    "finished_requests": 80426,
    "scheduler_time": 133.62223154231455
}
#Debug simulation 
Total elapsed time: 14.741266513243318. Arrivals time: 0.32026620069518685 Scheduler time: 14.27501526568085 Scheduler overhead time: 0.05287799704819918 Adapter cache time: 0.016506886575371027 Engine time: 0.05263140611350536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.239954382646829,
    "estimated_duration": 3600.021366962819,
    "input_throughput": 5195.206665058987,
    "output_throughput": 4532.528098233513,
    "total_throughput": 9727.734763292501,
    "itl": 97.25283361923105,
    "ttft": 2116427.813896283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 943,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.087716015437638,
    "arrivals": 802767,
    "finished_requests": 75907,
    "scheduler_time": 137.9345661275424
}
#Debug simulation 
Total elapsed time: 11.240072285756469. Arrivals time: 0.30427417857572436 Scheduler time: 10.776438677217811 Scheduler overhead time: 0.05608704034239054 Adapter cache time: 0.021140010561794043 Engine time: 0.05620572715997696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 14.849002007395029,
    "estimated_duration": 3600.014087477251,
    "input_throughput": 5510.225937449294,
    "output_throughput": 4800.859824443455,
    "total_throughput": 10311.08576189275,
    "itl": 109.62855634143197,
    "ttft": 2085252.6031822818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8862216001795544,
    "arrivals": 802767,
    "finished_requests": 80487,
    "scheduler_time": 133.7786055801863
}
#Debug simulation 
Total elapsed time: 14.849177417345345. Arrivals time: 0.3202582043595612 Scheduler time: 14.382005564402789 Scheduler overhead time: 0.053521462716162205 Adapter cache time: 0.016316011548042297 Engine time: 0.053095103707164526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 11.308904025238007,
    "estimated_duration": 3600.064687581762,
    "input_throughput": 5195.299702396034,
    "output_throughput": 4532.474668103979,
    "total_throughput": 9727.774370500012,
    "itl": 97.24998985894965,
    "ttft": 2116453.2082974734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 943,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.023302573510466,
    "arrivals": 802767,
    "finished_requests": 75909,
    "scheduler_time": 137.9386138605085
}
#Debug simulation 
Total elapsed time: 11.309035279322416. Arrivals time: 0.2926588160917163 Scheduler time: 10.856925983913243 Scheduler overhead time: 0.056282468140125275 Adapter cache time: 0.021125984378159046 Engine time: 0.056037806905806065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 14.852802817709744,
    "estimated_duration": 3600.1190468543114,
    "input_throughput": 5510.644993068986,
    "output_throughput": 4801.765101380427,
    "total_throughput": 10312.410094449413,
    "itl": 109.6226973085242,
    "ttft": 2085267.6527644044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6196831733034758,
    "arrivals": 802767,
    "finished_requests": 80498,
    "scheduler_time": 133.7914300226202
}
#Debug simulation 
Total elapsed time: 14.852910143788904. Arrivals time: 0.3175948718562722 Scheduler time: 14.388565011322498 Scheduler overhead time: 0.053244783077389 Adapter cache time: 0.01631623227149248 Engine time: 0.053285069297999144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 537235884 . Total output tokens: 473502401
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.231864139903337,
    "estimated_duration": 3600.105182298819,
    "input_throughput": 5195.241264605797,
    "output_throughput": 4532.42368590486,
    "total_throughput": 9727.664950510658,
    "itl": 97.24827710489387,
    "ttft": 2116542.6455633603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 943,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.956403725463918,
    "arrivals": 802767,
    "finished_requests": 75909,
    "scheduler_time": 137.9426045166238
}
#Debug simulation 
Total elapsed time: 11.231968033127487. Arrivals time: 0.2881351187825203 Scheduler time: 10.783964217174798 Scheduler overhead time: 0.05605838680639863 Adapter cache time: 0.021121229510754347 Engine time: 0.05683215847238898 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.351349358912557,
    "estimated_duration": 3600.024275050517,
    "input_throughput": 5724.148346113814,
    "output_throughput": 4934.809502013902,
    "total_throughput": 10658.957848127715,
    "itl": 117.42394936172025,
    "ttft": 2059604.9410167488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8830103379301986,
    "arrivals": 757465,
    "finished_requests": 83029,
    "scheduler_time": 131.52411915073318
}
#Debug simulation 
Total elapsed time: 21.35145941283554. Arrivals time: 0.33301310427486897 Scheduler time: 20.871814863290638 Scheduler overhead time: 0.05422830441966653 Adapter cache time: 0.015544977504760027 Engine time: 0.05346506694331765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 14.667615683749318,
    "estimated_duration": 3600.004731904174,
    "input_throughput": 5553.613533564705,
    "output_throughput": 4792.4550896004885,
    "total_throughput": 10346.068623165193,
    "itl": 109.83124290064131,
    "ttft": 2074495.0647799487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.360936306952505,
    "arrivals": 757465,
    "finished_requests": 80575,
    "scheduler_time": 133.4887430204777
}
#Debug simulation 
Total elapsed time: 14.667771404143423. Arrivals time: 0.32197768706828356 Scheduler time: 14.200924130622298 Scheduler overhead time: 0.05273481411859393 Adapter cache time: 0.015909262467175722 Engine time: 0.0525151789188385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.740311329253018,
    "estimated_duration": 3600.097917780781,
    "input_throughput": 5253.190449791427,
    "output_throughput": 4521.481462935919,
    "total_throughput": 9774.671912727346,
    "itl": 97.19551835795167,
    "ttft": 2107143.3480919325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.479691844824727,
    "arrivals": 757465,
    "finished_requests": 76127,
    "scheduler_time": 137.79059125406584
}
#Debug simulation 
Total elapsed time: 9.740443343296647. Arrivals time: 0.27964456053450704 Scheduler time: 9.303218487650156 Scheduler overhead time: 0.05600589653477073 Adapter cache time: 0.01983496593311429 Engine time: 0.05580732459202409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 14.744316214695573,
    "estimated_duration": 3600.0832242104907,
    "input_throughput": 5553.804385837436,
    "output_throughput": 4792.97947446315,
    "total_throughput": 10346.783860300586,
    "itl": 109.82366499187751,
    "ttft": 2074462.5102741634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.074962786450044,
    "arrivals": 757465,
    "finished_requests": 80581,
    "scheduler_time": 133.5017626681366
}
#Debug simulation 
Total elapsed time: 14.744445873890072. Arrivals time: 0.31378350080922246 Scheduler time: 14.284333022776991 Scheduler overhead time: 0.05300586065277457 Adapter cache time: 0.016197632998228073 Engine time: 0.05306654889136553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.74709592293948,
    "estimated_duration": 3600.035882736736,
    "input_throughput": 5253.280971639415,
    "output_throughput": 4521.55937613202,
    "total_throughput": 9774.840347771435,
    "itl": 97.19390690767348,
    "ttft": 2107121.326936408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.417763809016932,
    "arrivals": 757465,
    "finished_requests": 76127,
    "scheduler_time": 137.79048424582882
}
#Debug simulation 
Total elapsed time: 9.747265352867544. Arrivals time: 0.28117751656100154 Scheduler time: 9.30795443058014 Scheduler overhead time: 0.05648005520924926 Adapter cache time: 0.01984464842826128 Engine time: 0.05584342870861292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 14.685724169015884,
    "estimated_duration": 3600.0617302395735,
    "input_throughput": 5554.193927299509,
    "output_throughput": 4793.29280802406,
    "total_throughput": 10347.486735323568,
    "itl": 109.8169669959182,
    "ttft": 2074371.4379168495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8112008015205907,
    "arrivals": 757465,
    "finished_requests": 80585,
    "scheduler_time": 133.51001664828863
}
#Debug simulation 
Total elapsed time: 14.685829747002572. Arrivals time: 0.3151184720918536 Scheduler time: 14.224422318395227 Scheduler overhead time: 0.05373038584366441 Adapter cache time: 0.01594962552189827 Engine time: 0.052670236211270094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506810648 . Total output tokens: 446884663
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.834914930164814,
    "estimated_duration": 3600.086846008489,
    "input_throughput": 5253.35104650845,
    "output_throughput": 4521.562866753581,
    "total_throughput": 9774.913913262031,
    "itl": 97.19200472445188,
    "ttft": 2107194.11082011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.359563882388201,
    "arrivals": 757465,
    "finished_requests": 76129,
    "scheduler_time": 137.7945440217297
}
#Debug simulation 
Total elapsed time: 9.835036137141287. Arrivals time: 0.2847844287753105 Scheduler time: 9.392813289072365 Scheduler overhead time: 0.055592676624655724 Adapter cache time: 0.019811066798865795 Engine time: 0.05589655181393027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 15.012963713146746,
    "estimated_duration": 3600.0151576832377,
    "input_throughput": 5697.928786833428,
    "output_throughput": 4940.454198377836,
    "total_throughput": 10638.382985211263,
    "itl": 116.64774584380598,
    "ttft": 2064341.0446527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 674,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.456763687534334,
    "arrivals": 749804,
    "finished_requests": 82951,
    "scheduler_time": 131.8118508626594
}
#Debug simulation 
Total elapsed time: 15.013106211088598. Arrivals time: 0.3134579863399267 Scheduler time: 14.558915473520756 Scheduler overhead time: 0.050648991484194994 Adapter cache time: 0.015936681535094976 Engine time: 0.051368963439017534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 14.561302159447223,
    "estimated_duration": 3600.027924734784,
    "input_throughput": 5504.928410092818,
    "output_throughput": 4782.625679568424,
    "total_throughput": 10287.554089661242,
    "itl": 109.15944103551361,
    "ttft": 2075333.1756339185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.554682670873596,
    "arrivals": 749804,
    "finished_requests": 80162,
    "scheduler_time": 133.6121172681495
}
#Debug simulation 
Total elapsed time: 14.561436475254595. Arrivals time: 0.30127537343651056 Scheduler time: 14.114456225186586 Scheduler overhead time: 0.053406130988150835 Adapter cache time: 0.01579419896006584 Engine time: 0.052634178195148706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.604684612713754,
    "estimated_duration": 3600.10164880059,
    "input_throughput": 5204.233054431119,
    "output_throughput": 4522.961735101254,
    "total_throughput": 9727.194789532372,
    "itl": 97.38642196562265,
    "ttft": 2106596.2594101913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.425469901668869,
    "arrivals": 749804,
    "finished_requests": 75842,
    "scheduler_time": 137.51242451089064
}
#Debug simulation 
Total elapsed time: 9.604791382793337. Arrivals time: 0.3170852027833462 Scheduler time: 9.13344077533111 Scheduler overhead time: 0.054763125255703926 Adapter cache time: 0.01790922973304987 Engine time: 0.05583244934678078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 13.272002052981406,
    "estimated_duration": 3600.0236017801813,
    "input_throughput": 5531.690122851581,
    "output_throughput": 4798.1229877099895,
    "total_throughput": 10329.81311056157,
    "itl": 109.18139554906803,
    "ttft": 2076746.367353287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.726972714802243,
    "arrivals": 749804,
    "finished_requests": 80521,
    "scheduler_time": 133.82817794748232
}
#Debug simulation 
Total elapsed time: 13.27210090495646. Arrivals time: 0.3457698142156005 Scheduler time: 12.781820414587855 Scheduler overhead time: 0.05210045864805579 Adapter cache time: 0.016412220895290375 Engine time: 0.052220541052520275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.561799221206456,
    "estimated_duration": 3600.0509937460683,
    "input_throughput": 5204.306281368618,
    "output_throughput": 4523.025376109031,
    "total_throughput": 9727.331657477649,
    "itl": 97.38513797521541,
    "ttft": 2106576.9236178496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.374933310574883,
    "arrivals": 749804,
    "finished_requests": 75842,
    "scheduler_time": 137.51230604746374
}
#Debug simulation 
Total elapsed time: 9.561903249006718. Arrivals time: 0.3245315598323941 Scheduler time: 9.083438351284713 Scheduler overhead time: 0.05509034777060151 Adapter cache time: 0.017769469413906336 Engine time: 0.05542987259104848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.395954182837158,
    "estimated_duration": 3600.074707978508,
    "input_throughput": 5532.198805725145,
    "output_throughput": 4798.928467154226,
    "total_throughput": 10331.127272879372,
    "itl": 109.17164242582231,
    "ttft": 2076800.8837581505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.404905448993647,
    "arrivals": 749804,
    "finished_requests": 80534,
    "scheduler_time": 133.84111924607726
}
#Debug simulation 
Total elapsed time: 13.39607465500012. Arrivals time: 0.36279344791546464 Scheduler time: 12.88731586560607 Scheduler overhead time: 0.05282470304518938 Adapter cache time: 0.016631615813821554 Engine time: 0.05257875984534621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501767467 . Total output tokens: 442433043
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.580378605052829,
    "estimated_duration": 3600.001747254474,
    "input_throughput": 5204.377474063382,
    "output_throughput": 4523.087249170991,
    "total_throughput": 9727.464723234372,
    "itl": 97.38390516023519,
    "ttft": 2106557.4497905304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.325846539717198,
    "arrivals": 749804,
    "finished_requests": 75842,
    "scheduler_time": 137.512146326727
}
#Debug simulation 
Total elapsed time: 9.580488504841924. Arrivals time: 0.28679823596030474 Scheduler time: 9.139944952912629 Scheduler overhead time: 0.055027724243700504 Adapter cache time: 0.017869677860289812 Engine time: 0.0551768415607512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 17.02009696373716,
    "estimated_duration": 3600.1001799356072,
    "input_throughput": 5735.427062579497,
    "output_throughput": 5002.172189642262,
    "total_throughput": 10737.59925222176,
    "itl": 115.42528722353207,
    "ttft": 2054052.177155377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5773132862849644,
    "arrivals": 746017,
    "finished_requests": 83716,
    "scheduler_time": 133.54215225385693
}
#Debug simulation 
Total elapsed time: 17.020200063940138. Arrivals time: 0.32341114711016417 Scheduler time: 16.55329696275294 Scheduler overhead time: 0.05239480175077915 Adapter cache time: 0.015013682655990124 Engine time: 0.05276930006220937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.100496367085725,
    "estimated_duration": 3600.072182389591,
    "input_throughput": 5526.137808379939,
    "output_throughput": 4828.412631566201,
    "total_throughput": 10354.55043994614,
    "itl": 109.1056687870921,
    "ttft": 2072640.1315946924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.181997430194176,
    "arrivals": 746017,
    "finished_requests": 80723,
    "scheduler_time": 134.46564304525393
}
#Debug simulation 
Total elapsed time: 12.100605266168714. Arrivals time: 0.29378360603004694 Scheduler time: 11.662395553197712 Scheduler overhead time: 0.052177194971591234 Adapter cache time: 0.01620510872453451 Engine time: 0.052295306231826544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.852731524966657,
    "estimated_duration": 3600.040449451757,
    "input_throughput": 5183.025930400742,
    "output_throughput": 4529.9138242970275,
    "total_throughput": 9712.93975469777,
    "itl": 97.60708170417537,
    "ttft": 2109610.7107416983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 830,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.240054367207949,
    "arrivals": 746017,
    "finished_requests": 75767,
    "scheduler_time": 137.5963716411643
}
#Debug simulation 
Total elapsed time: 8.852830776944757. Arrivals time: 0.3460385212674737 Scheduler time: 8.352505387738347 Scheduler overhead time: 0.05519828153774142 Adapter cache time: 0.018309398088604212 Engine time: 0.055212436709553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.070206398144364,
    "estimated_duration": 3600.110635680584,
    "input_throughput": 5526.332386237587,
    "output_throughput": 4828.676604465984,
    "total_throughput": 10355.008990703573,
    "itl": 109.09629348701483,
    "ttft": 2072567.5790611326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.857153722438953,
    "arrivals": 746017,
    "finished_requests": 80729,
    "scheduler_time": 134.4786275404997
}
#Debug simulation 
Total elapsed time: 12.070311981253326. Arrivals time: 0.37549144262447953 Scheduler time: 11.551992233842611 Scheduler overhead time: 0.05141806183382869 Adapter cache time: 0.016198159661144018 Engine time: 0.05154859321191907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.896608917973936,
    "estimated_duration": 3600.0944784816375,
    "input_throughput": 5183.075364141496,
    "output_throughput": 4530.153055004938,
    "total_throughput": 9713.228419146433,
    "itl": 97.60640064232557,
    "ttft": 2109571.1819670796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 830,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.18371849516875,
    "arrivals": 746017,
    "finished_requests": 75772,
    "scheduler_time": 137.60049944494187
}
#Debug simulation 
Total elapsed time: 8.896728312131017. Arrivals time: 0.3552619507536292 Scheduler time: 8.387237552087754 Scheduler overhead time: 0.05471267970278859 Adapter cache time: 0.01846807450056076 Engine time: 0.05544683337211609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.877488786820322,
    "estimated_duration": 3600.0650596134574,
    "input_throughput": 5510.760131132226,
    "output_throughput": 4817.614879955035,
    "total_throughput": 10328.37501108726,
    "itl": 108.20930104027468,
    "ttft": 2073697.9102872915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6260670942440463,
    "arrivals": 746017,
    "finished_requests": 80531,
    "scheduler_time": 134.86613389122314
}
#Debug simulation 
Total elapsed time: 12.87762859184295. Arrivals time: 0.38935474352911115 Scheduler time: 12.342211668379605 Scheduler overhead time: 0.05249677272513509 Adapter cache time: 0.017387954983860254 Engine time: 0.052193115931004286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 499243981 . Total output tokens: 440168023
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.970683089923114,
    "estimated_duration": 3600.06763959968,
    "input_throughput": 5187.509755254671,
    "output_throughput": 4531.062089103935,
    "total_throughput": 9718.571844358607,
    "itl": 97.26251483221996,
    "ttft": 2108286.4386731265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 832,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.123495196476604,
    "arrivals": 746017,
    "finished_requests": 75791,
    "scheduler_time": 137.86503137420334
}
#Debug simulation 
Total elapsed time: 8.97081406088546. Arrivals time: 0.26974874129518867 Scheduler time: 8.547465928364545 Scheduler overhead time: 0.0546778766438365 Adapter cache time: 0.018109497148543596 Engine time: 0.05515302997082472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 17.519587837159634,
    "estimated_duration": 3600.0416189054076,
    "input_throughput": 5769.74329711097,
    "output_throughput": 4994.364205563488,
    "total_throughput": 10764.107502674458,
    "itl": 115.09628580097434,
    "ttft": 2051817.6646212824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.638351203748043,
    "arrivals": 744176,
    "finished_requests": 83708,
    "scheduler_time": 133.42956776480855
}
#Debug simulation 
Total elapsed time: 17.519908642861992. Arrivals time: 0.32805532589554787 Scheduler time: 17.04917225241661 Scheduler overhead time: 0.05324612790718675 Adapter cache time: 0.01292871031910181 Engine time: 0.05304219387471676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.464224508963525,
    "estimated_duration": 3600.0017993576357,
    "input_throughput": 5584.274986636712,
    "output_throughput": 4835.925638455632,
    "total_throughput": 10420.200625092344,
    "itl": 108.2289146254742,
    "ttft": 2073540.4536922206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.202443173606885,
    "arrivals": 744176,
    "finished_requests": 81003,
    "scheduler_time": 134.9523480052969
}
#Debug simulation 
Total elapsed time: 12.464383646845818. Arrivals time: 0.35885067749768496 Scheduler time: 11.963190137408674 Scheduler overhead time: 0.05226998310536146 Adapter cache time: 0.014336953405290842 Engine time: 0.05190592026337981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.578016341663897,
    "estimated_duration": 3600.0890084743805,
    "input_throughput": 5208.650940535056,
    "output_throughput": 4517.948573413931,
    "total_throughput": 9726.599513948988,
    "itl": 96.52281547718151,
    "ttft": 2104104.5499219066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.69523490778639,
    "arrivals": 744176,
    "finished_requests": 75569,
    "scheduler_time": 138.08638498992275
}
#Debug simulation 
Total elapsed time: 8.578122594859451. Arrivals time: 0.2650081808678806 Scheduler time: 8.160095065832138 Scheduler overhead time: 0.05480344081297517 Adapter cache time: 0.01739022508263588 Engine time: 0.055110767018049955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.293725616764277,
    "estimated_duration": 3600.1127645572187,
    "input_throughput": 5571.657976237593,
    "output_throughput": 4827.7918322754695,
    "total_throughput": 10399.449808513062,
    "itl": 108.31553635981304,
    "ttft": 2071790.5190143134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9236940887756586,
    "arrivals": 744176,
    "finished_requests": 80856,
    "scheduler_time": 134.78585392216053
}
#Debug simulation 
Total elapsed time: 12.293839357793331. Arrivals time: 0.31299529457464814 Scheduler time: 11.837859965395182 Scheduler overhead time: 0.052178974729031324 Adapter cache time: 0.014785205014050007 Engine time: 0.052021026611328125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.652968210168183,
    "estimated_duration": 3600.0402462521206,
    "input_throughput": 5208.721491244899,
    "output_throughput": 4518.009768622158,
    "total_throughput": 9726.731259867058,
    "itl": 96.52155357868621,
    "ttft": 2104085.532695312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.646562371281936,
    "arrivals": 744176,
    "finished_requests": 75569,
    "scheduler_time": 138.08629530416795
}
#Debug simulation 
Total elapsed time: 8.653093533124775. Arrivals time: 0.30372075363993645 Scheduler time: 8.195575133897364 Scheduler overhead time: 0.05544987693428993 Adapter cache time: 0.017245354130864143 Engine time: 0.05538183497264981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.236788971815258,
    "estimated_duration": 3600.0772043178645,
    "input_throughput": 5572.082447548764,
    "output_throughput": 4828.123124457669,
    "total_throughput": 10400.205572006435,
    "itl": 108.30784181433398,
    "ttft": 2071726.9940101523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6516027780063283,
    "arrivals": 744176,
    "finished_requests": 80860,
    "scheduler_time": 134.79389762856246
}
#Debug simulation 
Total elapsed time: 12.236883294768631. Arrivals time: 0.29407287715002894 Scheduler time: 11.800146345049143 Scheduler overhead time: 0.05205019563436508 Adapter cache time: 0.014403922483325005 Engine time: 0.05234031239524484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497961282 . Total output tokens: 439081150
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.575416695792228,
    "estimated_duration": 3600.0905242491926,
    "input_throughput": 5208.655691765056,
    "output_throughput": 4517.966392912334,
    "total_throughput": 9726.62208467739,
    "itl": 96.52022120942776,
    "ttft": 2104059.67199713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.5914692023024255,
    "arrivals": 744176,
    "finished_requests": 75570,
    "scheduler_time": 138.09024749363752
}
#Debug simulation 
Total elapsed time: 8.575555054936558. Arrivals time: 0.2578471675515175 Scheduler time: 8.164052728563547 Scheduler overhead time: 0.05498361308127642 Adapter cache time: 0.017443250864744186 Engine time: 0.05535745434463024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 17.267761595081538,
    "estimated_duration": 3600.062575565219,
    "input_throughput": 5743.987379650863,
    "output_throughput": 5017.149735838748,
    "total_throughput": 10761.137115489611,
    "itl": 115.0812341014523,
    "ttft": 2046223.5619004865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.327567979246386,
    "arrivals": 743225,
    "finished_requests": 83703,
    "scheduler_time": 133.8791006429635
}
#Debug simulation 
Total elapsed time: 17.267868116963655. Arrivals time: 0.3969980990514159 Scheduler time: 16.72874563606456 Scheduler overhead time: 0.05335643235594034 Adapter cache time: 0.012452587950974703 Engine time: 0.05314859328791499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 14.319214415270835,
    "estimated_duration": 3600.0408139117535,
    "input_throughput": 5570.629900223012,
    "output_throughput": 4867.868700899004,
    "total_throughput": 10438.498601122015,
    "itl": 107.67582098413772,
    "ttft": 2070326.3028157272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1718175494950307,
    "arrivals": 743225,
    "finished_requests": 81224,
    "scheduler_time": 135.8821256954823
}
#Debug simulation 
Total elapsed time: 14.31933305086568. Arrivals time: 0.3881314154714346 Scheduler time: 13.786266087554395 Scheduler overhead time: 0.05375810759142041 Adapter cache time: 0.013570383656769991 Engine time: 0.05334149906411767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.973767425864935,
    "estimated_duration": 3600.0132019429075,
    "input_throughput": 5203.035086063292,
    "output_throughput": 4548.985540153503,
    "total_throughput": 9752.020626216794,
    "itl": 96.8902607527717,
    "ttft": 2105006.950841984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 703,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.294706949298301,
    "arrivals": 743225,
    "finished_requests": 75903,
    "scheduler_time": 138.35270636409678
}
#Debug simulation 
Total elapsed time: 8.973896763753146. Arrivals time: 0.3421697597950697 Scheduler time: 8.478474777191877 Scheduler overhead time: 0.054854735266417265 Adapter cache time: 0.017161940690129995 Engine time: 0.05535003961995244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.220187419094145,
    "estimated_duration": 3600.080649125551,
    "input_throughput": 5544.97577848674,
    "output_throughput": 4845.787830958007,
    "total_throughput": 10390.763609444746,
    "itl": 108.41588578112963,
    "ttft": 2068713.9111182329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3491317264409695,
    "arrivals": 743225,
    "finished_requests": 80836,
    "scheduler_time": 135.0301327589767
}
#Debug simulation 
Total elapsed time: 12.220297446008772. Arrivals time: 0.40925350319594145 Scheduler time: 11.669465254992247 Scheduler overhead time: 0.05174937704578042 Adapter cache time: 0.013436639215797186 Engine time: 0.05255133705213666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.959794172085822,
    "estimated_duration": 3600.0781843090163,
    "input_throughput": 5202.941447671684,
    "output_throughput": 4549.0017609557235,
    "total_throughput": 9751.943208627408,
    "itl": 96.88914051094852,
    "ttft": 2104970.0970199937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 703,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.248312701736608,
    "arrivals": 743225,
    "finished_requests": 75904,
    "scheduler_time": 138.35685391342113
}
#Debug simulation 
Total elapsed time: 8.95989820593968. Arrivals time: 0.3414401514455676 Scheduler time: 8.464926759712398 Scheduler overhead time: 0.055255618412047625 Adapter cache time: 0.01698706578463316 Engine time: 0.05546062486246228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.239054124802351,
    "estimated_duration": 3600.085674280549,
    "input_throughput": 5545.386361948076,
    "output_throughput": 4846.050227259243,
    "total_throughput": 10391.436589207318,
    "itl": 108.40843949374398,
    "ttft": 2068735.7362537659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.108969498057836,
    "arrivals": 743225,
    "finished_requests": 80842,
    "scheduler_time": 135.0384719193443
}
#Debug simulation 
Total elapsed time: 12.23916291585192. Arrivals time: 0.3783441139385104 Scheduler time: 11.718388127628714 Scheduler overhead time: 0.052623226307332516 Adapter cache time: 0.013509299606084824 Engine time: 0.05238712299615145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 497311740 . Total output tokens: 438508024
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.038951553869992,
    "estimated_duration": 3600.0252328980173,
    "input_throughput": 5203.017975771676,
    "output_throughput": 4549.068670503934,
    "total_throughput": 9752.086646275611,
    "itl": 96.88776553741913,
    "ttft": 2104948.791836923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 703,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.19549782169986,
    "arrivals": 743225,
    "finished_requests": 75904,
    "scheduler_time": 138.356717382459
}
#Debug simulation 
Total elapsed time: 9.039059509057552. Arrivals time: 0.35044021299108863 Scheduler time: 8.53368671797216 Scheduler overhead time: 0.05557251209393144 Adapter cache time: 0.01738628838211298 Engine time: 0.05592617578804493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 17.22888923389837,
    "estimated_duration": 3600.1048718511033,
    "input_throughput": 5784.750928461655,
    "output_throughput": 5012.237599267999,
    "total_throughput": 10796.988527729654,
    "itl": 115.1155974792195,
    "ttft": 2062989.7733992885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 366,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.420141705693688,
    "arrivals": 742746,
    "finished_requests": 83981,
    "scheduler_time": 133.68261775397335
}
#Debug simulation 
Total elapsed time: 17.22901876317337. Arrivals time: 0.3143500746227801 Scheduler time: 16.774420712143183 Scheduler overhead time: 0.052185914013534784 Adapter cache time: 0.012328492011874914 Engine time: 0.052538986783474684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.650157251395285,
    "estimated_duration": 3600.080402575198,
    "input_throughput": 5608.327243346412,
    "output_throughput": 4861.955579514141,
    "total_throughput": 10470.282822860552,
    "itl": 107.29794326534866,
    "ttft": 2079923.361096889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1804207284562325,
    "arrivals": 742746,
    "finished_requests": 81435,
    "scheduler_time": 135.88718907244314
}
#Debug simulation 
Total elapsed time: 13.650258413050324. Arrivals time: 0.3021809123456478 Scheduler time: 13.203347750473768 Scheduler overhead time: 0.05350565491244197 Adapter cache time: 0.013285085558891296 Engine time: 0.05373347597196698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.60651112627238,
    "estimated_duration": 3600.0120240245437,
    "input_throughput": 5204.671227473727,
    "output_throughput": 4522.144618228367,
    "total_throughput": 9726.815845702095,
    "itl": 96.68045295397437,
    "ttft": 2110323.0383900944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.822572150239755,
    "arrivals": 742746,
    "finished_requests": 75611,
    "scheduler_time": 138.03234376853408
}
#Debug simulation 
Total elapsed time: 8.606623034924269. Arrivals time: 0.2721419776789844 Scheduler time: 8.18122909963131 Scheduler overhead time: 0.05511155864223838 Adapter cache time: 0.01669042371213436 Engine time: 0.055613304022699594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.505135853774846,
    "estimated_duration": 3600.0690793344406,
    "input_throughput": 5599.195614248212,
    "output_throughput": 4853.001321638517,
    "total_throughput": 10452.19693588673,
    "itl": 107.84018378496694,
    "ttft": 2080261.8668837755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 458,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1598333562444862,
    "arrivals": 742746,
    "finished_requests": 81292,
    "scheduler_time": 135.4033660918867
}
#Debug simulation 
Total elapsed time: 12.505261057056487. Arrivals time: 0.29404271161183715 Scheduler time: 12.068985065445304 Scheduler overhead time: 0.052282368298619986 Adapter cache time: 0.013407886028289795 Engine time: 0.05258113704621792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.619666859973222,
    "estimated_duration": 3600.078774714341,
    "input_throughput": 5204.616946607438,
    "output_throughput": 4522.15382461785,
    "total_throughput": 9726.770771225289,
    "itl": 96.67971152242264,
    "ttft": 2110300.46678133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7825985351531175,
    "arrivals": 742746,
    "finished_requests": 75613,
    "scheduler_time": 138.03637371171752
}
#Debug simulation 
Total elapsed time: 8.619766189716756. Arrivals time: 0.2615896565839648 Scheduler time: 8.204509789124131 Scheduler overhead time: 0.055391752161085606 Adapter cache time: 0.016588581260293722 Engine time: 0.05575820570811629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.712651558686048,
    "estimated_duration": 3600.105672049748,
    "input_throughput": 5602.39693978663,
    "output_throughput": 4859.230976417367,
    "total_throughput": 10461.627916203997,
    "itl": 108.02304647201771,
    "ttft": 2078750.2448972582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1025855771172655,
    "arrivals": 742746,
    "finished_requests": 81368,
    "scheduler_time": 135.41332075489558
}
#Debug simulation 
Total elapsed time: 11.71274738991633. Arrivals time: 0.2887660264968872 Scheduler time: 11.283164795022458 Scheduler overhead time: 0.05172623647376895 Adapter cache time: 0.013590151444077492 Engine time: 0.051827655639499426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 497002899 . Total output tokens: 438238368
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.592332580126822,
    "estimated_duration": 3600.029553275154,
    "input_throughput": 5204.688106783414,
    "output_throughput": 4522.215653810133,
    "total_throughput": 9726.903760593546,
    "itl": 96.67848619128692,
    "ttft": 2110281.742578201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.733511764295434,
    "arrivals": 742746,
    "finished_requests": 75613,
    "scheduler_time": 138.03623904338787
}
#Debug simulation 
Total elapsed time: 8.592432484962046. Arrivals time: 0.26342036621645093 Scheduler time: 8.176402693148702 Scheduler overhead time: 0.054797525983303785 Adapter cache time: 0.016522227320820093 Engine time: 0.05536216078326106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.665063044056296,
    "estimated_duration": 3600.023063788732,
    "input_throughput": 5650.2915785746545,
    "output_throughput": 4910.776038583038,
    "total_throughput": 10561.067617157692,
    "itl": 117.77912499737002,
    "ttft": 2049474.3795053256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6169877404766653,
    "arrivals": 679344,
    "finished_requests": 82300,
    "scheduler_time": 130.81467183305284
}
#Debug simulation 
Total elapsed time: 12.665211088955402. Arrivals time: 0.3686731164343655 Scheduler time: 12.161448318976909 Scheduler overhead time: 0.04879666678607464 Adapter cache time: 0.01487246435135603 Engine time: 0.04911869391798973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.548947173170745,
    "estimated_duration": 3600.0809364880097,
    "input_throughput": 5484.374753880136,
    "output_throughput": 4774.379882905515,
    "total_throughput": 10258.754636785652,
    "itl": 110.25550539177686,
    "ttft": 2063817.7207257568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8912836439302243,
    "arrivals": 679344,
    "finished_requests": 80006,
    "scheduler_time": 132.8463695726166
}
#Debug simulation 
Total elapsed time: 11.549046299885958. Arrivals time: 0.6003240048885345 Scheduler time: 10.807987626641989 Scheduler overhead time: 0.05104548344388604 Adapter cache time: 0.015285407193005085 Engine time: 0.05089022219181061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.807545105926692,
    "estimated_duration": 3600.0926372739577,
    "input_throughput": 5186.420984471778,
    "output_throughput": 4517.834577811255,
    "total_throughput": 9704.255562283033,
    "itl": 97.86796194354397,
    "ttft": 2095023.8337370378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.917011432261231,
    "arrivals": 679344,
    "finished_requests": 75631,
    "scheduler_time": 137.03269723364946
}
#Debug simulation 
Total elapsed time: 8.807645337190479. Arrivals time: 0.3365745171904564 Scheduler time: 8.316245817579329 Scheduler overhead time: 0.054946082178503275 Adapter cache time: 0.01890158047899604 Engine time: 0.05539530701935291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 11.227773954160511,
    "estimated_duration": 3600.045965288658,
    "input_throughput": 5484.708026058994,
    "output_throughput": 4774.562093298658,
    "total_throughput": 10259.27011935765,
    "itl": 110.24620015743787,
    "ttft": 2063796.95697148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6247452170541443,
    "arrivals": 679344,
    "finished_requests": 80012,
    "scheduler_time": 132.85440926614712
}
#Debug simulation 
Total elapsed time: 11.227875992190093. Arrivals time: 0.35578799014911056 Scheduler time: 10.730336039327085 Scheduler overhead time: 0.05094716092571616 Adapter cache time: 0.015212556347250938 Engine time: 0.05197464535012841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.758135051000863,
    "estimated_duration": 3600.0332774307267,
    "input_throughput": 5186.50650177477,
    "output_throughput": 4517.909070998295,
    "total_throughput": 9704.415572773065,
    "itl": 97.8663751907847,
    "ttft": 2095000.5859954718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.857775919749426,
    "arrivals": 679344,
    "finished_requests": 75631,
    "scheduler_time": 137.03257290293018
}
#Debug simulation 
Total elapsed time: 8.758233507163823. Arrivals time: 0.33550025755539536 Scheduler time: 8.267976185306907 Scheduler overhead time: 0.054435544181615114 Adapter cache time: 0.01901768147945404 Engine time: 0.055768380872905254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.19300582818687,
    "estimated_duration": 3600.053632632658,
    "input_throughput": 5484.69690035164,
    "output_throughput": 4774.773310093621,
    "total_throughput": 10259.47021044526,
    "itl": 110.24032356909986,
    "ttft": 2063687.3934801365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.402629861324079,
    "arrivals": 679344,
    "finished_requests": 80014,
    "scheduler_time": 132.8620815264621
}
#Debug simulation 
Total elapsed time: 11.193106643855572. Arrivals time: 0.28848126996308565 Scheduler time: 10.76366786006838 Scheduler overhead time: 0.05145649518817663 Adapter cache time: 0.015196534339338541 Engine time: 0.050875287503004074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 454339313 . Total output tokens: 400632838
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.813026300165802,
    "estimated_duration": 3600.0937062282733,
    "input_throughput": 5186.530274947253,
    "output_throughput": 4518.007954032033,
    "total_throughput": 9704.538228979285,
    "itl": 97.86540821144945,
    "ttft": 2094986.6959288456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.8095176175982015,
    "arrivals": 679344,
    "finished_requests": 75633,
    "scheduler_time": 137.03658170851938
}
#Debug simulation 
Total elapsed time: 8.81312312791124. Arrivals time: 0.3343779989518225 Scheduler time: 8.324458525981754 Scheduler overhead time: 0.055090582463890314 Adapter cache time: 0.01883409544825554 Engine time: 0.05475184507668018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.759540096856654,
    "estimated_duration": 3600.0361765114235,
    "input_throughput": 5617.270774094353,
    "output_throughput": 4904.57043604211,
    "total_throughput": 10521.841210136463,
    "itl": 117.74691137281158,
    "ttft": 2033897.7486522312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.425227878550111,
    "arrivals": 634153,
    "finished_requests": 81904,
    "scheduler_time": 130.56631128639714
}
#Debug simulation 
Total elapsed time: 9.759640601929277. Arrivals time: 0.3135179574601352 Scheduler time: 9.314979737158865 Scheduler overhead time: 0.04735478153452277 Adapter cache time: 0.014171960763633251 Engine time: 0.04748563840985298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.953086057677865,
    "estimated_duration": 3600.087450557249,
    "input_throughput": 5458.239631639615,
    "output_throughput": 4767.833347310252,
    "total_throughput": 10226.072978949867,
    "itl": 109.76668018581839,
    "ttft": 2050446.481501172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.344003802151425,
    "arrivals": 634153,
    "finished_requests": 79615,
    "scheduler_time": 132.80856972571516
}
#Debug simulation 
Total elapsed time: 8.953186821658164. Arrivals time: 0.2703777328133583 Scheduler time: 8.543095458764583 Scheduler overhead time: 0.04988111974671483 Adapter cache time: 0.015583277214318514 Engine time: 0.050707328133285046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.896783500909805,
    "estimated_duration": 3600.0333956501513,
    "input_throughput": 5161.8610045266005,
    "output_throughput": 4513.232854904035,
    "total_throughput": 9675.093859430635,
    "itl": 97.61971204203934,
    "ttft": 2081242.9516561097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 817,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.123103768588067,
    "arrivals": 634153,
    "finished_requests": 75348,
    "scheduler_time": 136.91015211286725
}
#Debug simulation 
Total elapsed time: 7.89684941386804. Arrivals time: 0.2554370700381696 Scheduler time: 7.487758303526789 Scheduler overhead time: 0.05428133485838771 Adapter cache time: 0.01874772971495986 Engine time: 0.055242838338017464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.932306681293994,
    "estimated_duration": 3600.01706393065,
    "input_throughput": 5458.636348391085,
    "output_throughput": 4768.470453097468,
    "total_throughput": 10227.106801488553,
    "itl": 109.75717081601188,
    "ttft": 2050417.5735573685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.035818746075957,
    "arrivals": 634153,
    "finished_requests": 79623,
    "scheduler_time": 132.81688882397435
}
#Debug simulation 
Total elapsed time: 8.93240462616086. Arrivals time: 0.33503955183550715 Scheduler time: 8.456200960092247 Scheduler overhead time: 0.04977556364610791 Adapter cache time: 0.01760860253125429 Engine time: 0.050394958816468716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.6280238148756325,
    "estimated_duration": 3600.0788323876727,
    "input_throughput": 5162.315567316085,
    "output_throughput": 4513.389221875317,
    "total_throughput": 9675.704789191403,
    "itl": 97.61724348156785,
    "ttft": 2081286.3153443437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 817,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.061382849956887,
    "arrivals": 634153,
    "finished_requests": 75353,
    "scheduler_time": 136.91419027894656
}
#Debug simulation 
Total elapsed time: 7.628119715955108. Arrivals time: 0.2580571644939482 Scheduler time: 7.215347467456013 Scheduler overhead time: 0.055118138901889324 Adapter cache time: 0.018820991273969412 Engine time: 0.05516141979023814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.979789856355637,
    "estimated_duration": 3600.0928063053825,
    "input_throughput": 5462.789171866632,
    "output_throughput": 4772.579465147971,
    "total_throughput": 10235.368637014602,
    "itl": 109.9899489275321,
    "ttft": 2050024.3639909972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 596,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.80481688058002,
    "arrivals": 634153,
    "finished_requests": 79691,
    "scheduler_time": 132.7549991833297
}
#Debug simulation 
Total elapsed time: 8.979887821245939. Arrivals time: 0.3335993350483477 Scheduler time: 8.50605530012399 Scheduler overhead time: 0.05035352287814021 Adapter cache time: 0.015720213763415813 Engine time: 0.05066405236721039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423981411 . Total output tokens: 373861399
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.659326361026615,
    "estimated_duration": 3600.080760462333,
    "input_throughput": 5162.968343413771,
    "output_throughput": 4512.9395924755645,
    "total_throughput": 9675.907935889336,
    "itl": 97.61134366977636,
    "ttft": 2081317.056301014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 800,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.892712247222689,
    "arrivals": 634153,
    "finished_requests": 75346,
    "scheduler_time": 136.92199636358606
}
#Debug simulation 
Total elapsed time: 7.659427700098604. Arrivals time: 0.2619798881933093 Scheduler time: 7.243328862823546 Scheduler overhead time: 0.05460132658481598 Adapter cache time: 0.01866224640980363 Engine time: 0.055091134272515774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.52152555808425,
    "estimated_duration": 3600.0761322554204,
    "input_throughput": 5647.119186688803,
    "output_throughput": 4916.381862433418,
    "total_throughput": 10563.50104912222,
    "itl": 118.16956224126122,
    "ttft": 2030295.2600174116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1475066992082046,
    "arrivals": 626696,
    "finished_requests": 82333,
    "scheduler_time": 130.39842400705535
}
#Debug simulation 
Total elapsed time: 9.521628031972796. Arrivals time: 0.3421978345140815 Scheduler time: 9.048584556672722 Scheduler overhead time: 0.04730773624032736 Adapter cache time: 0.013806211296468973 Engine time: 0.04773673880845308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.505184649024159,
    "estimated_duration": 3600.070721738103,
    "input_throughput": 5490.69046911851,
    "output_throughput": 4780.7730265061355,
    "total_throughput": 10271.463495624646,
    "itl": 110.3945996534345,
    "ttft": 2045884.181685317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.225759599348535,
    "arrivals": 626696,
    "finished_requests": 80034,
    "scheduler_time": 132.55780363974102
}
#Debug simulation 
Total elapsed time: 8.50528510613367. Arrivals time: 0.30478769121691585 Scheduler time: 8.062258685939014 Scheduler overhead time: 0.04954634187743068 Adapter cache time: 0.015483201947063208 Engine time: 0.050053305458277464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.245610446669161,
    "estimated_duration": 3600.100269375893,
    "input_throughput": 5190.300158845104,
    "output_throughput": 4520.760751705796,
    "total_throughput": 9711.060910550901,
    "itl": 97.87520715932494,
    "ttft": 2076327.7108478742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.822743275370473,
    "arrivals": 626696,
    "finished_requests": 75611,
    "scheduler_time": 136.76516386680478
}
#Debug simulation 
Total elapsed time: 7.245756135787815. Arrivals time: 0.31135006668046117 Scheduler time: 6.782058481592685 Scheduler overhead time: 0.05412715906277299 Adapter cache time: 0.017920642625540495 Engine time: 0.05488014733418822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.631377577781677,
    "estimated_duration": 3600.0173058432674,
    "input_throughput": 5486.495569879883,
    "output_throughput": 4778.119808502078,
    "total_throughput": 10264.61537838196,
    "itl": 110.28812025358945,
    "ttft": 2044998.2591013494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7685251036193153,
    "arrivals": 626696,
    "finished_requests": 79974,
    "scheduler_time": 132.60225147335245
}
#Debug simulation 
Total elapsed time: 8.631480432115495. Arrivals time: 0.3273856984451413 Scheduler time: 8.165918068028986 Scheduler overhead time: 0.04967471957206726 Adapter cache time: 0.014972074422985315 Engine time: 0.05037612095475197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.23532212106511,
    "estimated_duration": 3600.0407053025656,
    "input_throughput": 5190.386034379455,
    "output_throughput": 4520.8355494503085,
    "total_throughput": 9711.221583829763,
    "itl": 97.87367166299249,
    "ttft": 2076304.5809819878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.763300645682054,
    "arrivals": 626696,
    "finished_requests": 75611,
    "scheduler_time": 136.76504242316588
}
#Debug simulation 
Total elapsed time: 7.235419546253979. Arrivals time: 0.31333736050873995 Scheduler time: 6.769309056457132 Scheduler overhead time: 0.05438148695975542 Adapter cache time: 0.018129332456737757 Engine time: 0.05481618037447333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.40555828763172,
    "estimated_duration": 3600.045841850825,
    "input_throughput": 5478.906065779291,
    "output_throughput": 4771.120911940801,
    "total_throughput": 10250.026977720092,
    "itl": 109.91995191241375,
    "ttft": 2045738.0611803543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7409776711743152,
    "arrivals": 626696,
    "finished_requests": 79874,
    "scheduler_time": 132.71948986241915
}
#Debug simulation 
Total elapsed time: 8.405691624619067. Arrivals time: 0.3621718594804406 Scheduler time: 7.904367247130722 Scheduler overhead time: 0.05004595732316375 Adapter cache time: 0.015196063090115786 Engine time: 0.050707829650491476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418865135 . Total output tokens: 369456308
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.177884486038238,
    "estimated_duration": 3600.019965575711,
    "input_throughput": 5188.374280866799,
    "output_throughput": 4521.882143894361,
    "total_throughput": 9710.25642476116,
    "itl": 97.8920539969899,
    "ttft": 2076016.7640525547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.815524374470138,
    "arrivals": 626696,
    "finished_requests": 75606,
    "scheduler_time": 136.7616696500531
}
#Debug simulation 
Total elapsed time: 7.177984624169767. Arrivals time: 0.2537177442573011 Scheduler time: 6.771302932873368 Scheduler overhead time: 0.05427373619750142 Adapter cache time: 0.01852940721437335 Engine time: 0.05477816425263882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.710096103139222,
    "estimated_duration": 3600.038797729529,
    "input_throughput": 5641.286980798201,
    "output_throughput": 4911.9203968447155,
    "total_throughput": 10553.207377642917,
    "itl": 118.12933887527178,
    "ttft": 2036785.337597341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 478,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.160731517272105,
    "arrivals": 623022,
    "finished_requests": 82303,
    "scheduler_time": 130.43053376577075
}
#Debug simulation 
Total elapsed time: 8.710196101106703. Arrivals time: 0.2686940925195813 Scheduler time: 8.311099946964532 Scheduler overhead time: 0.04709498258307576 Adapter cache time: 0.013533596880733967 Engine time: 0.04773574601858854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.906832939013839,
    "estimated_duration": 3600.020065074746,
    "input_throughput": 5484.136100110898,
    "output_throughput": 4779.538360612651,
    "total_throughput": 10263.674460723549,
    "itl": 110.33411774174166,
    "ttft": 2051476.424601472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.14886921725702,
    "arrivals": 623022,
    "finished_requests": 79976,
    "scheduler_time": 132.60750769496113
}
#Debug simulation 
Total elapsed time: 7.906929941382259. Arrivals time: 0.2628652323037386 Scheduler time: 7.506536777131259 Scheduler overhead time: 0.049444102216511965 Adapter cache time: 0.0151735027320683 Engine time: 0.049757182598114014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.83404395589605,
    "estimated_duration": 3600.087457634764,
    "input_throughput": 5176.68312764937,
    "output_throughput": 4521.630152478218,
    "total_throughput": 9698.313280127588,
    "itl": 97.94174361396988,
    "ttft": 2083221.6141920462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 716,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.363810968869393,
    "arrivals": 623022,
    "finished_requests": 75599,
    "scheduler_time": 136.78901435040714
}
#Debug simulation 
Total elapsed time: 6.834144604858011. Arrivals time: 0.2520476938225329 Scheduler time: 6.429964893031865 Scheduler overhead time: 0.05441153421998024 Adapter cache time: 0.017525251023471355 Engine time: 0.054746967274695635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.004378348123282,
    "estimated_duration": 3600.0452789956403,
    "input_throughput": 5481.635776954876,
    "output_throughput": 4778.009071263359,
    "total_throughput": 10259.644848218235,
    "itl": 110.20240368834057,
    "ttft": 2052186.2740436664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 546,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.706347968308247,
    "arrivals": 623022,
    "finished_requests": 79948,
    "scheduler_time": 132.65817802964307
}
#Debug simulation 
Total elapsed time: 8.004477115813643. Arrivals time: 0.2693621311336756 Scheduler time: 7.596938451286405 Scheduler overhead time: 0.049763543996959925 Adapter cache time: 0.014955959282815456 Engine time: 0.050172058399766684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.80677907820791,
    "estimated_duration": 3600.0355685384243,
    "input_throughput": 5176.7577417481525,
    "output_throughput": 4521.6953249739145,
    "total_throughput": 9698.453066722066,
    "itl": 97.94038005428075,
    "ttft": 2083201.6088402069,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 716,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.312031674715718,
    "arrivals": 623022,
    "finished_requests": 75599,
    "scheduler_time": 136.78890454822135
}
#Debug simulation 
Total elapsed time: 6.806904579047114. Arrivals time: 0.24569408036768436 Scheduler time: 6.409723718650639 Scheduler overhead time: 0.05400599120184779 Adapter cache time: 0.017580570187419653 Engine time: 0.054508276749402285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.941268336959183,
    "estimated_duration": 3600.108332147741,
    "input_throughput": 5486.197130133879,
    "output_throughput": 4780.679471868098,
    "total_throughput": 10266.876602001976,
    "itl": 110.31778557328018,
    "ttft": 2051587.8828723899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6324510151846168,
    "arrivals": 623022,
    "finished_requests": 79997,
    "scheduler_time": 132.62857497995478
}
#Debug simulation 
Total elapsed time: 7.941395631991327. Arrivals time: 0.26408070093020797 Scheduler time: 7.538793134968728 Scheduler overhead time: 0.04968873085454106 Adapter cache time: 0.015224747825413942 Engine time: 0.05028481362387538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 416339382 . Total output tokens: 367196825
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.192952774930745,
    "estimated_duration": 3600.0992198952645,
    "input_throughput": 5176.674269700317,
    "output_throughput": 4521.6712111822235,
    "total_throughput": 9698.34548088254,
    "itl": 97.93931582184935,
    "ttft": 2083177.9482038342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 716,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.266258778683869,
    "arrivals": 623022,
    "finished_requests": 75600,
    "scheduler_time": 136.79294195969698
}
#Debug simulation 
Total elapsed time: 7.193021409679204. Arrivals time: 0.5572987925261259 Scheduler time: 6.483491474762559 Scheduler overhead time: 0.05441437615081668 Adapter cache time: 0.017591344192624092 Engine time: 0.05469039594754577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.499207413755357,
    "estimated_duration": 3600.075650020982,
    "input_throughput": 5645.202761192408,
    "output_throughput": 4906.156624763444,
    "total_throughput": 10551.359385955851,
    "itl": 117.59585238166098,
    "ttft": 2031553.1124689602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9359096101857998,
    "arrivals": 621138,
    "finished_requests": 82331,
    "scheduler_time": 130.54640716117433
}
#Debug simulation 
Total elapsed time: 8.499305866658688. Arrivals time: 0.3382976888678968 Scheduler time: 8.031289688311517 Scheduler overhead time: 0.04713127203285694 Adapter cache time: 0.013213973958045244 Engine time: 0.04748007049784064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.775168078020215,
    "estimated_duration": 3600.0567263629086,
    "input_throughput": 5491.046531361587,
    "output_throughput": 4773.417283721902,
    "total_throughput": 10264.463815083489,
    "itl": 109.96782229661547,
    "ttft": 2046646.889824035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7053094219230163,
    "arrivals": 621138,
    "finished_requests": 80048,
    "scheduler_time": 132.69427251433768
}
#Debug simulation 
Total elapsed time: 7.775306022725999. Arrivals time: 0.3603447121568024 Scheduler time: 7.277848057448864 Scheduler overhead time: 0.04948672652244568 Adapter cache time: 0.014232978224754333 Engine time: 0.050168330781161785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.725433596875519,
    "estimated_duration": 3600.028269741299,
    "input_throughput": 5200.435829173463,
    "output_throughput": 4515.26843181376,
    "total_throughput": 9715.704260987224,
    "itl": 97.55958274003083,
    "ttft": 2079531.7942642274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.349367628516658,
    "arrivals": 621138,
    "finished_requests": 75791,
    "scheduler_time": 136.88437210489613
}
#Debug simulation 
Total elapsed time: 6.725536308716983. Arrivals time: 0.25740903336554766 Scheduler time: 6.300575408618897 Scheduler overhead time: 0.05465206550434232 Adapter cache time: 0.01931984629482031 Engine time: 0.067928456235677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.664060065988451,
    "estimated_duration": 3600.024426678098,
    "input_throughput": 5490.736355430701,
    "output_throughput": 4773.577888152653,
    "total_throughput": 10264.314243583354,
    "itl": 109.9898330514866,
    "ttft": 2046443.2147906853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6133655961463162,
    "arrivals": 621138,
    "finished_requests": 80045,
    "scheduler_time": 132.69104066547789
}
#Debug simulation 
Total elapsed time: 7.6641606548801064. Arrivals time: 0.32216544449329376 Scheduler time: 7.20477509777993 Scheduler overhead time: 0.0493820677511394 Adapter cache time: 0.014564466662704945 Engine time: 0.050115860998630524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.656251480802894,
    "estimated_duration": 3600.087070532931,
    "input_throughput": 5200.507552509969,
    "output_throughput": 4515.316346944255,
    "total_throughput": 9715.823899454224,
    "itl": 97.5578644898303,
    "ttft": 2079561.9330760546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.298416803069442,
    "arrivals": 621138,
    "finished_requests": 75793,
    "scheduler_time": 136.88845346166346
}
#Debug simulation 
Total elapsed time: 6.6563532329164445. Arrivals time: 0.24783317325636744 Scheduler time: 6.256260050460696 Scheduler overhead time: 0.05434906296432018 Adapter cache time: 0.017271395307034254 Engine time: 0.05517388740554452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.674560128711164,
    "estimated_duration": 3600.042195425568,
    "input_throughput": 5491.143971900903,
    "output_throughput": 4773.8446015542895,
    "total_throughput": 10264.988573455192,
    "itl": 109.98282951308893,
    "ttft": 2046468.7689808835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.389862019442938,
    "arrivals": 621138,
    "finished_requests": 80051,
    "scheduler_time": 132.6991492125377
}
#Debug simulation 
Total elapsed time: 7.674670273903757. Arrivals time: 0.32312315283343196 Scheduler time: 7.214054486248642 Scheduler overhead time: 0.04968531057238579 Adapter cache time: 0.014722730498760939 Engine time: 0.049810986034572124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 415103891 . Total output tokens: 366047870
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.049988692160696,
    "estimated_duration": 3600.040500452862,
    "input_throughput": 5200.5748262123325,
    "output_throughput": 4515.374757021527,
    "total_throughput": 9715.94958323386,
    "itl": 97.55669586947876,
    "ttft": 2079542.6896329708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.252022555507748,
    "arrivals": 621138,
    "finished_requests": 75793,
    "scheduler_time": 136.8882776291556
}
#Debug simulation 
Total elapsed time: 7.050056139938533. Arrivals time: 0.5600211387500167 Scheduler time: 6.337353595532477 Scheduler overhead time: 0.05470316065475345 Adapter cache time: 0.01728088641539216 Engine time: 0.05507261399179697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.520671417936683,
    "estimated_duration": 3600.0021710865626,
    "input_throughput": 5647.207705395343,
    "output_throughput": 4902.645376647917,
    "total_throughput": 10549.85308204326,
    "itl": 117.3206819617661,
    "ttft": 2033717.7515674701,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2680562979588346,
    "arrivals": 620208,
    "finished_requests": 82342,
    "scheduler_time": 130.63057984667176
}
#Debug simulation 
Total elapsed time: 8.520805679261684. Arrivals time: 0.2727305884473026 Scheduler time: 8.119067643769085 Scheduler overhead time: 0.04738183878362179 Adapter cache time: 0.011852131690829992 Engine time: 0.04768404318019748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.733881411142647,
    "estimated_duration": 3600.0993001403763,
    "input_throughput": 5483.48291371581,
    "output_throughput": 4763.272501769979,
    "total_throughput": 10246.75541548579,
    "itl": 109.2895943083239,
    "ttft": 2051021.4368446628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0371980258170557,
    "arrivals": 620208,
    "finished_requests": 79959,
    "scheduler_time": 132.92629210114902
}
#Debug simulation 
Total elapsed time: 7.733980954159051. Arrivals time: 0.2653548209927976 Scheduler time: 7.331218586303294 Scheduler overhead time: 0.05002995673567057 Adapter cache time: 0.01322473119944334 Engine time: 0.05076711531728506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.5407849769108,
    "estimated_duration": 3600.0075948857007,
    "input_throughput": 5182.9129545468095,
    "output_throughput": 4507.289379903427,
    "total_throughput": 9690.202334450238,
    "itl": 97.13304956691437,
    "ttft": 2082089.8532032548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.505123593076165,
    "arrivals": 620208,
    "finished_requests": 75596,
    "scheduler_time": 137.08964591827325
}
#Debug simulation 
Total elapsed time: 6.540886441245675. Arrivals time: 0.2546417061239481 Scheduler time: 6.134874088689685 Scheduler overhead time: 0.054645876865834 Adapter cache time: 0.01618930883705616 Engine time: 0.055072924587875605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.985310707706958,
    "estimated_duration": 3600.009970995994,
    "input_throughput": 5483.830644651836,
    "output_throughput": 4763.616250555958,
    "total_throughput": 10247.446895207793,
    "itl": 109.2835095765966,
    "ttft": 2050937.9476830068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8234119959268678,
    "arrivals": 620208,
    "finished_requests": 79961,
    "scheduler_time": 132.93019923328077
}
#Debug simulation 
Total elapsed time: 7.985431551933289. Arrivals time: 0.5867416951805353 Scheduler time: 7.262470546644181 Scheduler overhead time: 0.04985244246199727 Adapter cache time: 0.013076442759484053 Engine time: 0.04993941728025675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.565956763923168,
    "estimated_duration": 3600.0726271138433,
    "input_throughput": 5182.898494733607,
    "output_throughput": 4507.304068753964,
    "total_throughput": 9690.202563487572,
    "itl": 97.1320811946868,
    "ttft": 2082067.8581909528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.462871689046766,
    "arrivals": 620208,
    "finished_requests": 75597,
    "scheduler_time": 137.09366885091424
}
#Debug simulation 
Total elapsed time: 6.566049009095877. Arrivals time: 0.23798775067552924 Scheduler time: 6.176477466709912 Scheduler overhead time: 0.05455006845295429 Adapter cache time: 0.016310275066643953 Engine time: 0.05508238496258855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.680479015689343,
    "estimated_duration": 3600.0792684807775,
    "input_throughput": 5484.015913997205,
    "output_throughput": 4763.6417759315145,
    "total_throughput": 10247.65768992872,
    "itl": 109.2791548629675,
    "ttft": 2050904.7056569972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6429432693961896,
    "arrivals": 620208,
    "finished_requests": 79964,
    "scheduler_time": 132.93858251893727
}
#Debug simulation 
Total elapsed time: 7.68057001568377. Arrivals time: 0.24869689159095287 Scheduler time: 7.2950802724808455 Scheduler overhead time: 0.049821384251117706 Adapter cache time: 0.013146542012691498 Engine time: 0.05051431059837341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 414440281 . Total output tokens: 365487013
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.664932987652719,
    "estimated_duration": 3600.032497968942,
    "input_throughput": 5182.956267902271,
    "output_throughput": 4507.354311149885,
    "total_throughput": 9690.310579052157,
    "itl": 97.13106147823265,
    "ttft": 2082051.303448573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4228980739601305,
    "arrivals": 620208,
    "finished_requests": 75597,
    "scheduler_time": 137.0935133211006
}
#Debug simulation 
Total elapsed time: 6.665040371008217. Arrivals time: 0.25181584153324366 Scheduler time: 6.261100369039923 Scheduler overhead time: 0.05471417307853699 Adapter cache time: 0.016329921782016754 Engine time: 0.055302010383456945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.207235232926905,
    "estimated_duration": 3600.1210521559915,
    "input_throughput": 5660.714099542722,
    "output_throughput": 4908.722719036582,
    "total_throughput": 10569.436818579305,
    "itl": 117.66025502079759,
    "ttft": 2027694.6165823762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.459816159885389,
    "arrivals": 619792,
    "finished_requests": 82575,
    "scheduler_time": 130.53370361496172
}
#Debug simulation 
Total elapsed time: 8.207357762847096. Arrivals time: 0.35285398038104177 Scheduler time: 7.724846432451159 Scheduler overhead time: 0.04734481032937765 Adapter cache time: 0.012338591739535332 Engine time: 0.047991165425628424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.422403448726982,
    "estimated_duration": 3600.0435946493967,
    "input_throughput": 5495.375953058895,
    "output_throughput": 4768.963638528387,
    "total_throughput": 10264.339591587282,
    "itl": 109.59147952216566,
    "ttft": 2043928.5529251576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1881928703701163,
    "arrivals": 619792,
    "finished_requests": 80174,
    "scheduler_time": 132.81797316218845
}
#Debug simulation 
Total elapsed time: 7.422498602885753. Arrivals time: 0.25437396857887506 Scheduler time: 7.03177257347852 Scheduler overhead time: 0.0494494060985744 Adapter cache time: 0.013608974870294333 Engine time: 0.050080135464668274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.532438729889691,
    "estimated_duration": 3600.0745805096567,
    "input_throughput": 5186.551717869312,
    "output_throughput": 4509.338247570911,
    "total_throughput": 9695.889965440223,
    "itl": 97.16977702955727,
    "ttft": 2075999.0508326436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.488503487729497,
    "arrivals": 619792,
    "finished_requests": 75763,
    "scheduler_time": 137.05772607386504
}
#Debug simulation 
Total elapsed time: 6.532532353885472. Arrivals time: 0.24440638488158584 Scheduler time: 6.135891265235841 Scheduler overhead time: 0.05493654729798436 Adapter cache time: 0.016362208873033524 Engine time: 0.055176286958158016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.362893280107528,
    "estimated_duration": 3600.080483319983,
    "input_throughput": 5495.718246208293,
    "output_throughput": 4769.287819967304,
    "total_throughput": 10265.006066175596,
    "itl": 109.5855198723617,
    "ttft": 2043879.6987777604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9841243872931176,
    "arrivals": 619792,
    "finished_requests": 80179,
    "scheduler_time": 132.82633337656935
}
#Debug simulation 
Total elapsed time: 7.363015742041171. Arrivals time: 0.2522345660254359 Scheduler time: 6.974726069252938 Scheduler overhead time: 0.04938384285196662 Adapter cache time: 0.013526496011763811 Engine time: 0.05000638822093606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.522966460790485,
    "estimated_duration": 3600.0346904260805,
    "input_throughput": 5186.609187310383,
    "output_throughput": 4509.38821316709,
    "total_throughput": 9695.997400477472,
    "itl": 97.16877000830591,
    "ttft": 2075983.6103525222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.448736989819476,
    "arrivals": 619792,
    "finished_requests": 75763,
    "scheduler_time": 137.05760248819877
}
#Debug simulation 
Total elapsed time: 6.5230628750287. Arrivals time: 0.23929499834775925 Scheduler time: 6.131509250961244 Scheduler overhead time: 0.05499871354550123 Adapter cache time: 0.016424849163740873 Engine time: 0.055224867071956396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.401453626807779,
    "estimated_duration": 3600.0063745091766,
    "input_throughput": 5496.312489918221,
    "output_throughput": 4769.475165815775,
    "total_throughput": 10265.787655733995,
    "itl": 109.57868983238845,
    "ttft": 2043808.6503259882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.789773451029311,
    "arrivals": 619792,
    "finished_requests": 80185,
    "scheduler_time": 132.8298519040092
}
#Debug simulation 
Total elapsed time: 7.401553582865745. Arrivals time: 0.3075603856705129 Scheduler time: 6.957714318763465 Scheduler overhead time: 0.049523137509822845 Adapter cache time: 0.013501696288585663 Engine time: 0.05002649128437042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 414137857 . Total output tokens: 365218995
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.568977572955191,
    "estimated_duration": 3600.101296833384,
    "input_throughput": 5186.513228509347,
    "output_throughput": 4509.304783806844,
    "total_throughput": 9695.818012316191,
    "itl": 97.16773386610892,
    "ttft": 2075951.2500440292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.407520671673151,
    "arrivals": 619792,
    "finished_requests": 75763,
    "scheduler_time": 137.06159717628682
}
#Debug simulation 
Total elapsed time: 6.569081236608326. Arrivals time: 0.28472330514341593 Scheduler time: 6.1322123827412724 Scheduler overhead time: 0.054757225792855024 Adapter cache time: 0.0162448612973094 Engine time: 0.055394891649484634 
