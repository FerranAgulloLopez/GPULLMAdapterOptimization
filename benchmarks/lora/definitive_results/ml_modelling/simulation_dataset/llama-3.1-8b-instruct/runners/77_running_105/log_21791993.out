INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.876226233318448,
    "estimated_duration": 3600.0087791031046,
    "input_throughput": 5520.834870005959,
    "output_throughput": 4854.500106067513,
    "total_throughput": 10375.334976073473,
    "itl": 113.10624127450612,
    "ttft": 982922.0890529996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 100828,
    "finished_requests": 80951,
    "scheduler_time": 54.26969705415528
}
#Debug simulation 
Total elapsed time: 5.876313692424446. Arrivals time: 0.21543735451996326 Scheduler time: 5.50036747334525 Scheduler overhead time: 0.04710790375247598 Adapter cache time: 0.04079396044835448 Engine time: 0.04987418092787266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.854144572746009,
    "estimated_duration": 3600.012650026475,
    "input_throughput": 5407.70266456059,
    "output_throughput": 4756.769118540198,
    "total_throughput": 10164.47178310079,
    "itl": 101.52003636620972,
    "ttft": 1062555.687069924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.701702552586794,
    "arrivals": 100828,
    "finished_requests": 79256,
    "scheduler_time": 49.69515880567187
}
#Debug simulation 
Total elapsed time: 5.854251112788916. Arrivals time: 0.21908096969127655 Scheduler time: 5.458019479177892 Scheduler overhead time: 0.051444845739752054 Adapter cache time: 0.04618271114304662 Engine time: 0.054585103411227465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.533617548178881,
    "estimated_duration": 3600.0339650480196,
    "input_throughput": 5032.544741493387,
    "output_throughput": 4434.393162673136,
    "total_throughput": 9466.937904166523,
    "itl": 78.015133404025,
    "ttft": 1286196.4920321875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 100828,
    "finished_requests": 73765,
    "scheduler_time": 34.0258486750052
}
#Debug simulation 
Total elapsed time: 5.533694874960929. Arrivals time: 0.21842706063762307 Scheduler time: 5.098389635328203 Scheduler overhead time: 0.06428418215364218 Adapter cache time: 0.05381040275096893 Engine time: 0.06779460003599524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 5.845431437715888,
    "estimated_duration": 3600.0777694162584,
    "input_throughput": 5407.826232364078,
    "output_throughput": 4756.860017159234,
    "total_throughput": 10164.686249523313,
    "itl": 101.51956438676986,
    "ttft": 1062595.8889020341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407829,
    "arrivals": 100828,
    "finished_requests": 79257,
    "scheduler_time": 49.6961569568631
}
#Debug simulation 
Total elapsed time: 5.8455130979418755. Arrivals time: 0.2146317521110177 Scheduler time: 5.45373140508309 Scheduler overhead time: 0.05182807520031929 Adapter cache time: 0.04579535638913512 Engine time: 0.05445867404341698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.526543078944087,
    "estimated_duration": 3600.0254841875394,
    "input_throughput": 5032.520763971404,
    "output_throughput": 4434.367220487259,
    "total_throughput": 9466.887984458665,
    "itl": 78.01406447000302,
    "ttft": 1286186.1813616657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 100828,
    "finished_requests": 73764,
    "scheduler_time": 34.02688338490949
}
#Debug simulation 
Total elapsed time: 5.526628978084773. Arrivals time: 0.2220216249115765 Scheduler time: 5.086329691577703 Scheduler overhead time: 0.06455065542832017 Adapter cache time: 0.05414691334590316 Engine time: 0.0684996317140758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 5.8346550720743835,
    "estimated_duration": 3600.050733181325,
    "input_throughput": 5407.7904571072695,
    "output_throughput": 4756.743520907899,
    "total_throughput": 10164.53397801517,
    "itl": 101.51756997324381,
    "ttft": 1062509.7232120351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 100828,
    "finished_requests": 79257,
    "scheduler_time": 49.6982432738938
}
#Debug simulation 
Total elapsed time: 5.834736497141421. Arrivals time: 0.2188659543171525 Scheduler time: 5.4386133956722915 Scheduler overhead time: 0.051615793257951736 Adapter cache time: 0.04583012219518423 Engine time: 0.05500103905797005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 270, 8640, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 270, 540, 540, 540, 270, 270, 8640, 540, 540, 270, 8640, 8640, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 8640, 270, 270, 540, 270, 8640, 270, 270, 8640, 270, 540, 540, 270, 540, 8640, 8640, 8640, 540, 8640, 8640, 270, 270, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 8640, 270, 270, 8640, 270, 270, 540]
Prompts retrieved: 302400 . Total input tokens: 67425015 . Total output tokens: 59324753
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.544507713057101,
    "estimated_duration": 3600.0226539952055,
    "input_throughput": 5032.433054246033,
    "output_throughput": 4434.3701510555165,
    "total_throughput": 9466.803205301549,
    "itl": 78.01431876182171,
    "ttft": 1286095.670064184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 100828,
    "finished_requests": 73763,
    "scheduler_time": 34.0266231591869
}
#Debug simulation 
Total elapsed time: 5.544610309414566. Arrivals time: 0.221332848072052 Scheduler time: 5.104877841193229 Scheduler overhead time: 0.06456476263701916 Adapter cache time: 0.0545313018374145 Engine time: 0.06811789842322469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.09828951023519,
    "estimated_duration": 3600.0056756208346,
    "input_throughput": 5841.640234740271,
    "output_throughput": 5022.260693209048,
    "total_throughput": 10863.900927949318,
    "itl": 108.59174322700176,
    "ttft": 760662.4824258595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 99385,
    "finished_requests": 84428,
    "scheduler_time": 58.97687496622689
}
#Debug simulation 
Total elapsed time: 6.0984182371757925. Arrivals time: 0.21656381152570248 Scheduler time: 5.720909749157727 Scheduler overhead time: 0.04903885116800666 Adapter cache time: 0.0366917634382844 Engine time: 0.05157738830894232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.036767999641597,
    "estimated_duration": 3600.021406940345,
    "input_throughput": 5708.562721427324,
    "output_throughput": 4911.01579727161,
    "total_throughput": 10619.578518698934,
    "itl": 97.66521665674605,
    "ttft": 855996.7516050814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.701702552586794,
    "arrivals": 99385,
    "finished_requests": 82513,
    "scheduler_time": 54.254484598890436
}
#Debug simulation 
Total elapsed time: 6.036880563013256. Arrivals time: 0.21866172831505537 Scheduler time: 5.640791262499988 Scheduler overhead time: 0.054152888245880604 Adapter cache time: 0.04047760833054781 Engine time: 0.05674267094582319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.687796249985695,
    "estimated_duration": 3600.0852927619385,
    "input_throughput": 5287.84999574143,
    "output_throughput": 4551.855766569368,
    "total_throughput": 9839.705762310798,
    "itl": 75.44943326923298,
    "ttft": 1148935.3905405968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 99385,
    "finished_requests": 76401,
    "scheduler_time": 37.976193960105476
}
#Debug simulation 
Total elapsed time: 5.687894133850932. Arrivals time: 0.22547910967841744 Scheduler time: 5.2461330238729715 Scheduler overhead time: 0.06646969029679894 Adapter cache time: 0.04736940562725067 Engine time: 0.07033474929630756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.036769073922187,
    "estimated_duration": 3600.0464697309585,
    "input_throughput": 5708.7001995104465,
    "output_throughput": 4911.298548132725,
    "total_throughput": 10619.99874764317,
    "itl": 97.66266570315992,
    "ttft": 855832.7176242565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 99385,
    "finished_requests": 82517,
    "scheduler_time": 54.25788113475907
}
#Debug simulation 
Total elapsed time: 6.036849376279861. Arrivals time: 0.22054002061486244 Scheduler time: 5.638795243110508 Scheduler overhead time: 0.05386620108038187 Adapter cache time: 0.04081593547016382 Engine time: 0.05688279354944825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.687044557649642,
    "estimated_duration": 3600.0588742927625,
    "input_throughput": 5287.719913674932,
    "output_throughput": 4551.679728631971,
    "total_throughput": 9839.399642306904,
    "itl": 75.45046039327582,
    "ttft": 1149006.5604934085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 99385,
    "finished_requests": 76397,
    "scheduler_time": 37.97582612847099
}
#Debug simulation 
Total elapsed time: 5.68712502066046. Arrivals time: 0.23540229769423604 Scheduler time: 5.236128069460392 Scheduler overhead time: 0.0663621136918664 Adapter cache time: 0.04682290740311146 Engine time: 0.07025031419470906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.010472119320184,
    "estimated_duration": 3600.0579858967512,
    "input_throughput": 5708.81026931032,
    "output_throughput": 4911.373947104832,
    "total_throughput": 10620.184216415153,
    "itl": 97.66235194591486,
    "ttft": 855806.1597029878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 99385,
    "finished_requests": 82518,
    "scheduler_time": 54.25861879418574
}
#Debug simulation 
Total elapsed time: 6.010552175343037. Arrivals time: 0.2182890549302101 Scheduler time: 5.616267144214362 Scheduler overhead time: 0.053539091255515814 Adapter cache time: 0.03997307410463691 Engine time: 0.05645579379051924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 135, 8640, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 135, 540, 540, 540, 135, 135, 8640, 540, 540, 135, 8640, 8640, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 8640, 135, 135, 540, 135, 8640, 135, 135, 8640, 135, 540, 540, 135, 540, 8640, 8640, 8640, 540, 8640, 8640, 135, 135, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 8640, 135, 135, 8640, 135, 135, 540]
Prompts retrieved: 298080 . Total input tokens: 66447292 . Total output tokens: 58494075
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.676342369057238,
    "estimated_duration": 3600.0278574210997,
    "input_throughput": 5287.507417688694,
    "output_throughput": 4551.510890734577,
    "total_throughput": 9839.01830842327,
    "itl": 75.44981329802265,
    "ttft": 1149117.5957120322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 99385,
    "finished_requests": 76394,
    "scheduler_time": 37.975451284891776
}
#Debug simulation 
Total elapsed time: 5.676423494238406. Arrivals time: 0.2242298605851829 Scheduler time: 5.236097032669932 Scheduler overhead time: 0.06645599706098437 Adapter cache time: 0.0472547453828156 Engine time: 0.07032981654629111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.232459594961256,
    "estimated_duration": 3600.0268948840594,
    "input_throughput": 5868.749766848735,
    "output_throughput": 5136.581347844497,
    "total_throughput": 11005.331114693232,
    "itl": 106.53502098610177,
    "ttft": 672859.9317286401,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 98626,
    "finished_requests": 85629,
    "scheduler_time": 61.44728063057692
}
#Debug simulation 
Total elapsed time: 6.232566982973367. Arrivals time: 0.2144081019796431 Scheduler time: 5.858765232376754 Scheduler overhead time: 0.04986674711108208 Adapter cache time: 0.03296741144731641 Engine time: 0.05249908473342657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.123991864267737,
    "estimated_duration": 3600.0379903796043,
    "input_throughput": 5728.846488596446,
    "output_throughput": 5016.503450313788,
    "total_throughput": 10745.349938910233,
    "itl": 96.00625156588362,
    "ttft": 777028.5550624167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867946,
    "arrivals": 98626,
    "finished_requests": 83566,
    "scheduler_time": 56.60118682359283
}
#Debug simulation 
Total elapsed time: 6.124102636240423. Arrivals time: 0.21642593201249838 Scheduler time: 5.7324373209849 Scheduler overhead time: 0.05461547011509538 Adapter cache time: 0.036649237386882305 Engine time: 0.05752993002533913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.748906126711518,
    "estimated_duration": 3600.005244759986,
    "input_throughput": 5280.771195450705,
    "output_throughput": 4629.584088595172,
    "total_throughput": 9910.355284045878,
    "itl": 74.53765413400897,
    "ttft": 1093537.3372135398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 98626,
    "finished_requests": 77077,
    "scheduler_time": 39.87996185656151
}
#Debug simulation 
Total elapsed time: 5.74902507988736. Arrivals time: 0.2222640342079103 Scheduler time: 5.312749517150223 Scheduler overhead time: 0.06713224947452545 Adapter cache time: 0.043208274990320206 Engine time: 0.07107187295332551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.144486020319164,
    "estimated_duration": 3600.0974654169936,
    "input_throughput": 5729.2776648787,
    "output_throughput": 5016.773066144763,
    "total_throughput": 10746.050731023463,
    "itl": 96.00504331406886,
    "ttft": 776780.1817245163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407825,
    "arrivals": 98626,
    "finished_requests": 83572,
    "scheduler_time": 56.60563518226136
}
#Debug simulation 
Total elapsed time: 6.144564235117286. Arrivals time: 0.21746647357940674 Scheduler time: 5.752195224631578 Scheduler overhead time: 0.054553674068301916 Adapter cache time: 0.036323273088783026 Engine time: 0.0576809118501842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.760565138887614,
    "estimated_duration": 3600.029928677185,
    "input_throughput": 5280.496378257922,
    "output_throughput": 4629.264570065299,
    "total_throughput": 9909.76094832322,
    "itl": 74.53801773679155,
    "ttft": 1093637.5347109921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 98626,
    "finished_requests": 77074,
    "scheduler_time": 39.8778932442184
}
#Debug simulation 
Total elapsed time: 5.760658658109605. Arrivals time: 0.23547752341255546 Scheduler time: 5.310409835074097 Scheduler overhead time: 0.06742211943492293 Adapter cache time: 0.04376631323248148 Engine time: 0.07101290998980403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.128449460025877,
    "estimated_duration": 3600.0849768192675,
    "input_throughput": 5729.404203737353,
    "output_throughput": 5016.982409109042,
    "total_throughput": 10746.386612846394,
    "itl": 96.00104917760362,
    "ttft": 776546.463889137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 98626,
    "finished_requests": 83577,
    "scheduler_time": 56.60943720319862
}
#Debug simulation 
Total elapsed time: 6.128530205693096. Arrivals time: 0.21595388324931264 Scheduler time: 5.737669330555946 Scheduler overhead time: 0.05464512389153242 Adapter cache time: 0.03623184049502015 Engine time: 0.057824833784252405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 66, 8640, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 66, 540, 540, 540, 66, 66, 8640, 540, 540, 66, 8640, 8640, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 8640, 66, 66, 540, 66, 8640, 66, 66, 8640, 66, 540, 540, 66, 540, 8640, 8640, 8640, 540, 8640, 8640, 66, 66, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 8640, 66, 66, 8640, 66, 66, 540]
Prompts retrieved: 295872 . Total input tokens: 65965944 . Total output tokens: 58061176
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.77630234695971,
    "estimated_duration": 3600.010287726364,
    "input_throughput": 5280.845464474137,
    "output_throughput": 4629.517881329678,
    "total_throughput": 9910.363345803815,
    "itl": 74.53518195565802,
    "ttft": 1093541.1186165714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 98626,
    "finished_requests": 77077,
    "scheduler_time": 39.87848248084288
}
#Debug simulation 
Total elapsed time: 5.776410934980959. Arrivals time: 0.2210651459172368 Scheduler time: 5.340431293472648 Scheduler overhead time: 0.06729293754324317 Adapter cache time: 0.04357745684683323 Engine time: 0.07153361244127154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.298344017937779,
    "estimated_duration": 3600.0694423479986,
    "input_throughput": 5984.173456928977,
    "output_throughput": 5174.959066303233,
    "total_throughput": 11159.13252323221,
    "itl": 105.32119421002037,
    "ttft": 602591.562429166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 98351,
    "finished_requests": 86635,
    "scheduler_time": 62.935714879475064
}
#Debug simulation 
Total elapsed time: 6.298441670835018. Arrivals time: 0.21770916692912579 Scheduler time: 5.920203017536551 Scheduler overhead time: 0.050427718088030815 Adapter cache time: 0.030499227344989777 Engine time: 0.05514263175427914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.168451129924506,
    "estimated_duration": 3600.059403847283,
    "input_throughput": 5836.392026627602,
    "output_throughput": 5048.304197585669,
    "total_throughput": 10884.69622421327,
    "itl": 95.02286165781125,
    "ttft": 712524.6666002721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867943,
    "arrivals": 98351,
    "finished_requests": 84485,
    "scheduler_time": 58.02884242246532
}
#Debug simulation 
Total elapsed time: 6.168548251036555. Arrivals time: 0.21775389602407813 Scheduler time: 5.777452878654003 Scheduler overhead time: 0.05499378126114607 Adapter cache time: 0.03365001222118735 Engine time: 0.05812360160052776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.791025488637388,
    "estimated_duration": 3600.001652167892,
    "input_throughput": 5368.894758245684,
    "output_throughput": 4648.4198111194555,
    "total_throughput": 10017.31456936514,
    "itl": 73.92902364261776,
    "ttft": 1043720.8652529335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 98351,
    "finished_requests": 77752,
    "scheduler_time": 41.24690911391554
}
#Debug simulation 
Total elapsed time: 5.791107874829322. Arrivals time: 0.22331405011937022 Scheduler time: 5.354582108557224 Scheduler overhead time: 0.06771323224529624 Adapter cache time: 0.040529183112084866 Engine time: 0.07215901743620634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.171266767196357,
    "estimated_duration": 3600.0609403488706,
    "input_throughput": 5836.424812843264,
    "output_throughput": 5048.379263890674,
    "total_throughput": 10884.804076733937,
    "itl": 95.01825036687381,
    "ttft": 712462.3676159696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407825,
    "arrivals": 98351,
    "finished_requests": 84487,
    "scheduler_time": 58.0276167046755
}
#Debug simulation 
Total elapsed time: 6.171349281445146. Arrivals time: 0.21700567845255136 Scheduler time: 5.780850120354444 Scheduler overhead time: 0.054961780086159706 Adapter cache time: 0.03350119246169925 Engine time: 0.05848262319341302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.766528103966266,
    "estimated_duration": 3600.03171857865,
    "input_throughput": 5368.950751253703,
    "output_throughput": 4648.433766191108,
    "total_throughput": 10017.38451744481,
    "itl": 73.92719022410176,
    "ttft": 1043671.434226246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 98351,
    "finished_requests": 77756,
    "scheduler_time": 41.24562368072651
}
#Debug simulation 
Total elapsed time: 5.766629104036838. Arrivals time: 0.22110479418188334 Scheduler time: 5.333180947229266 Scheduler overhead time: 0.06775948638096452 Adapter cache time: 0.040288377553224564 Engine time: 0.07156948884949088 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.15133038489148,
    "estimated_duration": 3600.0865762640738,
    "input_throughput": 5836.1104809327335,
    "output_throughput": 5048.265538897108,
    "total_throughput": 10884.376019829842,
    "itl": 95.01990580029005,
    "ttft": 712640.8338257355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 98351,
    "finished_requests": 84484,
    "scheduler_time": 58.02723520990048
}
#Debug simulation 
Total elapsed time: 6.151416663080454. Arrivals time: 0.2173926280811429 Scheduler time: 5.7607726524583995 Scheduler overhead time: 0.05509455455467105 Adapter cache time: 0.03343321941792965 Engine time: 0.05820092745125294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 540, 8640, 540, 540, 540, 540, 8640, 540, 8640, 540, 33, 8640, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 33, 540, 540, 540, 33, 33, 8640, 540, 540, 33, 8640, 8640, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 8640, 33, 33, 540, 33, 8640, 33, 33, 8640, 33, 540, 540, 33, 540, 8640, 8640, 8640, 540, 8640, 8640, 33, 33, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 8640, 33, 33, 8640, 33, 33, 540]
Prompts retrieved: 294816 . Total input tokens: 65742382 . Total output tokens: 57850757
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.767177768051624,
    "estimated_duration": 3600.0469366817365,
    "input_throughput": 5368.814168243912,
    "output_throughput": 4648.312728784677,
    "total_throughput": 10017.12689702859,
    "itl": 73.92961136903656,
    "ttft": 1043739.5546589103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 98351,
    "finished_requests": 77754,
    "scheduler_time": 41.245690935273075
}
#Debug simulation 
Total elapsed time: 5.767285967245698. Arrivals time: 0.2213734476827085 Scheduler time: 5.333828433416784 Scheduler overhead time: 0.06766394386067986 Adapter cache time: 0.040018999483436346 Engine time: 0.07169326767325401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.339723824057728,
    "estimated_duration": 3600.0916098271873,
    "input_throughput": 6120.197591598967,
    "output_throughput": 5223.89539995699,
    "total_throughput": 11344.092991555957,
    "itl": 103.93374459217942,
    "ttft": 444919.17702950124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 96609,
    "finished_requests": 88282,
    "scheduler_time": 65.39908644411382
}
#Debug simulation 
Total elapsed time: 6.339823946822435. Arrivals time: 0.2138197449967265 Scheduler time: 5.959838272538036 Scheduler overhead time: 0.05139553966000676 Adapter cache time: 0.03611428523436189 Engine time: 0.05402405373752117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.239184862934053,
    "estimated_duration": 3600.0113461029573,
    "input_throughput": 5975.922832378967,
    "output_throughput": 5109.576396172261,
    "total_throughput": 11085.499228551227,
    "itl": 93.62062123774307,
    "ttft": 553389.96012833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867943,
    "arrivals": 96609,
    "finished_requests": 86228,
    "scheduler_time": 60.9103497265436
}
#Debug simulation 
Total elapsed time: 6.239288843236864. Arrivals time: 0.2159415939822793 Scheduler time: 5.8426829553209245 Scheduler overhead time: 0.05637221643701196 Adapter cache time: 0.038076522294431925 Engine time: 0.05917746154591441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.857116271276027,
    "estimated_duration": 3600.067303038755,
    "input_throughput": 5502.447963480962,
    "output_throughput": 4721.452564415299,
    "total_throughput": 10223.900527896261,
    "itl": 72.76947811941385,
    "ttft": 895138.861959623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 96609,
    "finished_requests": 79538,
    "scheduler_time": 44.877886606080494
}
#Debug simulation 
Total elapsed time: 5.857193113304675. Arrivals time: 0.21892610983923078 Scheduler time: 5.420667095575482 Scheduler overhead time: 0.06895823637023568 Adapter cache time: 0.04216748848557472 Engine time: 0.07317503495141864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.231806630268693,
    "estimated_duration": 3600.036874921767,
    "input_throughput": 5975.880177746099,
    "output_throughput": 5109.492941068758,
    "total_throughput": 11085.373118814858,
    "itl": 93.62081350396133,
    "ttft": 553519.8377995762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407829,
    "arrivals": 96609,
    "finished_requests": 86227,
    "scheduler_time": 60.90801521618894
}
#Debug simulation 
Total elapsed time: 6.231890524271876. Arrivals time: 0.21200296375900507 Scheduler time: 5.839896527584642 Scheduler overhead time: 0.0560469925403595 Adapter cache time: 0.03794991038739681 Engine time: 0.05902911722660065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.871169027872384,
    "estimated_duration": 3600.020414189535,
    "input_throughput": 5502.519630700372,
    "output_throughput": 4721.514059476972,
    "total_throughput": 10224.033690177343,
    "itl": 72.76879168937067,
    "ttft": 894931.8853359042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 96609,
    "finished_requests": 79538,
    "scheduler_time": 44.879990498968226
}
#Debug simulation 
Total elapsed time: 5.871253743767738. Arrivals time: 0.21955118887126446 Scheduler time: 5.434784701094031 Scheduler overhead time: 0.06895087286829948 Adapter cache time: 0.042044220957905054 Engine time: 0.0727840200997889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.231031130999327,
    "estimated_duration": 3600.0301904673593,
    "input_throughput": 5975.950439795363,
    "output_throughput": 5109.550205636471,
    "total_throughput": 11085.500645431834,
    "itl": 93.619788740487,
    "ttft": 553410.5980602095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 96609,
    "finished_requests": 86229,
    "scheduler_time": 60.91155651640891
}
#Debug simulation 
Total elapsed time: 6.231114162132144. Arrivals time: 0.21325699472799897 Scheduler time: 5.838231081608683 Scheduler overhead time: 0.055858610197901726 Adapter cache time: 0.03772579599171877 Engine time: 0.059112862683832645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 135, 8640, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 135, 270, 270, 270, 135, 135, 8640, 270, 270, 135, 8640, 8640, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 8640, 135, 135, 270, 135, 8640, 135, 135, 8640, 135, 270, 270, 135, 270, 8640, 8640, 8640, 270, 8640, 8640, 135, 135, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 8640, 135, 135, 8640, 135, 135, 270]
Prompts retrieved: 289440 . Total input tokens: 64543752 . Total output tokens: 56775769
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.869697909802198,
    "estimated_duration": 3600.0363968048096,
    "input_throughput": 5502.494646326775,
    "output_throughput": 4721.491986882706,
    "total_throughput": 10223.986633209483,
    "itl": 72.76788207274161,
    "ttft": 895107.9601617347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 96609,
    "finished_requests": 79536,
    "scheduler_time": 44.87978384169884
}
#Debug simulation 
Total elapsed time: 5.86977931112051. Arrivals time: 0.21926226932555437 Scheduler time: 5.432474736124277 Scheduler overhead time: 0.0692345486022532 Adapter cache time: 0.04236997710540891 Engine time: 0.07304872665554285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.47300266334787,
    "estimated_duration": 3600.0787651385704,
    "input_throughput": 6074.865697878197,
    "output_throughput": 5346.240806278351,
    "total_throughput": 11421.106504156549,
    "itl": 102.48195399140226,
    "ttft": 367258.09305133857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 95912,
    "finished_requests": 89116,
    "scheduler_time": 67.93792830219796
}
#Debug simulation 
Total elapsed time: 6.473117073997855. Arrivals time: 0.20954112242907286 Scheduler time: 6.099586847703904 Scheduler overhead time: 0.05203088093549013 Adapter cache time: 0.032089517917484045 Engine time: 0.05470713507384062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.371286879759282,
    "estimated_duration": 3600.0653643352807,
    "input_throughput": 5924.053549493769,
    "output_throughput": 5215.538913823824,
    "total_throughput": 11139.592463317593,
    "itl": 92.50617145380205,
    "ttft": 485387.72031507245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867944,
    "arrivals": 95912,
    "finished_requests": 86915,
    "scheduler_time": 63.270585048470025
}
#Debug simulation 
Total elapsed time: 6.371367600746453. Arrivals time: 0.2129034628160298 Scheduler time: 5.980197149794549 Scheduler overhead time: 0.05703336838632822 Adapter cache time: 0.034370748326182365 Engine time: 0.05955797107890248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.953577037900686,
    "estimated_duration": 3600.0127908544023,
    "input_throughput": 5439.526784390246,
    "output_throughput": 4798.8393385402005,
    "total_throughput": 10238.366122930445,
    "itl": 72.08859616797065,
    "ttft": 851219.7519152645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 95912,
    "finished_requests": 79864,
    "scheduler_time": 46.68684289186437
}
#Debug simulation 
Total elapsed time: 5.953663416206837. Arrivals time: 0.218054611235857 Scheduler time: 5.520124556031078 Scheduler overhead time: 0.06970278173685074 Adapter cache time: 0.03842621948570013 Engine time: 0.07368731591850519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.367307806853205,
    "estimated_duration": 3600.045235148823,
    "input_throughput": 5922.432527189789,
    "output_throughput": 5213.6836550677635,
    "total_throughput": 11136.116182257552,
    "itl": 92.4224571374603,
    "ttft": 486440.95788860513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407827,
    "arrivals": 95912,
    "finished_requests": 86894,
    "scheduler_time": 63.22741736307659
}
#Debug simulation 
Total elapsed time: 6.367403730284423. Arrivals time: 0.21621956583112478 Scheduler time: 5.972679767757654 Scheduler overhead time: 0.056871114764362574 Adapter cache time: 0.03402758203446865 Engine time: 0.060066025238484144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.951719357166439,
    "estimated_duration": 3600.047548420271,
    "input_throughput": 5439.627598416898,
    "output_throughput": 4798.862450464273,
    "total_throughput": 10238.49004888117,
    "itl": 72.08753129831021,
    "ttft": 851201.9798552817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 95912,
    "finished_requests": 79866,
    "scheduler_time": 46.68826215400883
}
#Debug simulation 
Total elapsed time: 5.951794983353466. Arrivals time: 0.2172021851874888 Scheduler time: 5.517224014271051 Scheduler overhead time: 0.06967818597331643 Adapter cache time: 0.037885737139731646 Engine time: 0.07620788225904107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.375729050952941,
    "estimated_duration": 3600.0489329998472,
    "input_throughput": 5923.852257816215,
    "output_throughput": 5215.397720817812,
    "total_throughput": 11139.249978634027,
    "itl": 92.50669262913786,
    "ttft": 485367.99197240104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 95912,
    "finished_requests": 86913,
    "scheduler_time": 63.27344570051365
}
#Debug simulation 
Total elapsed time: 6.375836858060211. Arrivals time: 0.21610140288248658 Scheduler time: 5.97980240592733 Scheduler overhead time: 0.056904828641563654 Adapter cache time: 0.034695481415838 Engine time: 0.06089974148198962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 66, 8640, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 66, 270, 270, 270, 66, 66, 8640, 270, 270, 66, 8640, 8640, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 8640, 66, 66, 270, 66, 8640, 66, 66, 8640, 66, 270, 270, 66, 270, 8640, 8640, 8640, 270, 8640, 8640, 66, 66, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 8640, 66, 66, 8640, 66, 66, 270]
Prompts retrieved: 287232 . Total input tokens: 64047455 . Total output tokens: 56345187
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.95608155336231,
    "estimated_duration": 3600.0625426128063,
    "input_throughput": 5439.721607110487,
    "output_throughput": 4798.912184303713,
    "total_throughput": 10238.633791414199,
    "itl": 72.08766661666061,
    "ttft": 851263.0726875362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 95912,
    "finished_requests": 79867,
    "scheduler_time": 46.68766082624766
}
#Debug simulation 
Total elapsed time: 5.956199089065194. Arrivals time: 0.2184362057596445 Scheduler time: 5.522184491157532 Scheduler overhead time: 0.0697224116884172 Adapter cache time: 0.03852313617244363 Engine time: 0.07350049633532763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.510073299054056,
    "estimated_duration": 3600.053263969339,
    "input_throughput": 6279.764309673988,
    "output_throughput": 5398.465126755281,
    "total_throughput": 11678.229436429268,
    "itl": 100.28278902687453,
    "ttft": 261642.01506793377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 95515,
    "finished_requests": 90756,
    "scheduler_time": 69.73382628050055
}
#Debug simulation 
Total elapsed time: 6.510181661229581. Arrivals time: 0.20696521317586303 Scheduler time: 6.138656641356647 Scheduler overhead time: 0.05315101891756058 Adapter cache time: 0.030375038739293814 Engine time: 0.055700724478811026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.413904850836843,
    "estimated_duration": 3600.0205892144168,
    "input_throughput": 6118.490562523862,
    "output_throughput": 5264.7868339375045,
    "total_throughput": 11383.277396461366,
    "itl": 90.73487313242836,
    "ttft": 387493.7805001313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867946,
    "arrivals": 95515,
    "finished_requests": 88452,
    "scheduler_time": 65.00059907395261
}
#Debug simulation 
Total elapsed time: 6.413986430969089. Arrivals time: 0.2111370749771595 Scheduler time: 6.0245380802080035 Scheduler overhead time: 0.05762063758447766 Adapter cache time: 0.03230377333238721 Engine time: 0.060724347829818726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.976169181056321,
    "estimated_duration": 3600.041422332369,
    "input_throughput": 5610.165170520461,
    "output_throughput": 4831.884403355077,
    "total_throughput": 10442.049573875538,
    "itl": 71.05163006499734,
    "ttft": 776590.7480346579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 95515,
    "finished_requests": 81045,
    "scheduler_time": 48.1939535279689
}
#Debug simulation 
Total elapsed time: 5.976251594722271. Arrivals time: 0.21697316598147154 Scheduler time: 5.543922604527324 Scheduler overhead time: 0.07069814531132579 Adapter cache time: 0.03596683265641332 Engine time: 0.07458571204915643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.391283496748656,
    "estimated_duration": 3600.0715082433235,
    "input_throughput": 6118.566797787954,
    "output_throughput": 5264.794589940949,
    "total_throughput": 11383.361387728903,
    "itl": 90.73484835346599,
    "ttft": 387531.60989407246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407825,
    "arrivals": 95515,
    "finished_requests": 88454,
    "scheduler_time": 65.00233929818887
}
#Debug simulation 
Total elapsed time: 6.391370797064155. Arrivals time: 0.21242078905925155 Scheduler time: 6.000349693931639 Scheduler overhead time: 0.05780250206589699 Adapter cache time: 0.03213901398703456 Engine time: 0.060671028681099415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 5.974251528270543,
    "estimated_duration": 3600.0786351680063,
    "input_throughput": 5609.841074779055,
    "output_throughput": 4831.482243217246,
    "total_throughput": 10441.3233179963,
    "itl": 71.05037062305598,
    "ttft": 776812.0138037404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 95515,
    "finished_requests": 81041,
    "scheduler_time": 48.19231001496098
}
#Debug simulation 
Total elapsed time: 5.9743299800902605. Arrivals time: 0.21722802752628922 Scheduler time: 5.542091648094356 Scheduler overhead time: 0.07041470566764474 Adapter cache time: 0.03610386000946164 Engine time: 0.0746727054938674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.397185985930264,
    "estimated_duration": 3600.0307215680755,
    "input_throughput": 6118.519730410995,
    "output_throughput": 5264.853682066444,
    "total_throughput": 11383.373412477438,
    "itl": 90.73369147342859,
    "ttft": 387426.9242111618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 95515,
    "finished_requests": 88453,
    "scheduler_time": 65.0025999519031
}
#Debug simulation 
Total elapsed time: 6.397268718108535. Arrivals time: 0.21138752065598965 Scheduler time: 6.006425277329981 Scheduler overhead time: 0.05848844675347209 Adapter cache time: 0.032393140718340874 Engine time: 0.06068122386932373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 8640, 270, 270, 270, 270, 8640, 270, 8640, 270, 33, 8640, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 33, 270, 270, 270, 33, 33, 8640, 270, 270, 33, 8640, 8640, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 8640, 33, 33, 270, 33, 8640, 33, 33, 8640, 33, 270, 270, 33, 270, 8640, 8640, 8640, 270, 8640, 8640, 33, 33, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 8640, 33, 33, 8640, 33, 33, 270]
Prompts retrieved: 286176 . Total input tokens: 63824976 . Total output tokens: 56126545
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 5.994261065032333,
    "estimated_duration": 3600.0431689861352,
    "input_throughput": 5610.245503163794,
    "output_throughput": 4832.028168400831,
    "total_throughput": 10442.273671564626,
    "itl": 71.05011896249525,
    "ttft": 776577.735966854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 95515,
    "finished_requests": 81046,
    "scheduler_time": 48.19409539701015
}
#Debug simulation 
Total elapsed time: 5.994345034938306. Arrivals time: 0.22877349983900785 Scheduler time: 5.55052108829841 Scheduler overhead time: 0.07059688633307815 Adapter cache time: 0.035939380060881376 Engine time: 0.07436460442841053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.6464454960078,
    "estimated_duration": 3600.0091780330663,
    "input_throughput": 6393.167312027504,
    "output_throughput": 5548.475298863956,
    "total_throughput": 11941.642610891458,
    "itl": 97.50148090893519,
    "ttft": 84514.15874303241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 94463,
    "finished_requests": 92829,
    "scheduler_time": 73.92092884146066
}
#Debug simulation 
Total elapsed time: 6.646553071215749. Arrivals time: 0.20469791535288095 Scheduler time: 6.2783519038930535 Scheduler overhead time: 0.05379873141646385 Adapter cache time: 0.027394090313464403 Engine time: 0.05642239190638065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.52377083292231,
    "estimated_duration": 3600.040919481293,
    "input_throughput": 6225.5953477410085,
    "output_throughput": 5403.77191123788,
    "total_throughput": 11629.36725897889,
    "itl": 88.30751469100217,
    "ttft": 223616.25029888513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867939,
    "arrivals": 94463,
    "finished_requests": 90346,
    "scheduler_time": 69.15097124505164
}
#Debug simulation 
Total elapsed time: 6.523879155050963. Arrivals time: 0.20919851819053292 Scheduler time: 6.137842973694205 Scheduler overhead time: 0.058647113386541605 Adapter cache time: 0.028332794085144997 Engine time: 0.06150554260239005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.095748278778046,
    "estimated_duration": 3600.0127823251805,
    "input_throughput": 5696.345052073668,
    "output_throughput": 4938.479131876066,
    "total_throughput": 10634.824183949735,
    "itl": 69.409910574039,
    "ttft": 643154.120438962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 94463,
    "finished_requests": 82649,
    "scheduler_time": 51.94542442816192
}
#Debug simulation 
Total elapsed time: 6.095825990661979. Arrivals time: 0.21439359802752733 Scheduler time: 5.667962486390024 Scheduler overhead time: 0.071908222977072 Adapter cache time: 0.03090042807161808 Engine time: 0.07603555917739868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.553536832798272,
    "estimated_duration": 3600.067269699158,
    "input_throughput": 6225.678666796954,
    "output_throughput": 5403.837079307049,
    "total_throughput": 11629.515746104002,
    "itl": 88.3059465580784,
    "ttft": 223552.8047984635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407823,
    "arrivals": 94463,
    "finished_requests": 90349,
    "scheduler_time": 69.15324142438745
}
#Debug simulation 
Total elapsed time: 6.553616747725755. Arrivals time: 0.20816307794302702 Scheduler time: 6.16784986667335 Scheduler overhead time: 0.05898090731352568 Adapter cache time: 0.02828155644237995 Engine time: 0.0618716087192297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.092442197725177,
    "estimated_duration": 3600.006994884564,
    "input_throughput": 5696.412265070494,
    "output_throughput": 4938.552626498463,
    "total_throughput": 10634.964891568956,
    "itl": 69.4087945743289,
    "ttft": 643133.6502707353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 94463,
    "finished_requests": 82650,
    "scheduler_time": 51.94423805817363
}
#Debug simulation 
Total elapsed time: 6.09255447704345. Arrivals time: 0.21530235558748245 Scheduler time: 5.663061442319304 Scheduler overhead time: 0.07211619336158037 Adapter cache time: 0.03126424504444003 Engine time: 0.07615669257938862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.546578589826822,
    "estimated_duration": 3600.001499974746,
    "input_throughput": 6225.663517128319,
    "output_throughput": 5403.831081774957,
    "total_throughput": 11629.494598903277,
    "itl": 88.30497035797222,
    "ttft": 223631.78326881904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 94463,
    "finished_requests": 90346,
    "scheduler_time": 69.15096435796822
}
#Debug simulation 
Total elapsed time: 6.546654481906444. Arrivals time: 0.2090505864471197 Scheduler time: 6.159911645576358 Scheduler overhead time: 0.05917361844331026 Adapter cache time: 0.028235417790710926 Engine time: 0.06195619376376271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 66, 8640, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 66, 135, 135, 135, 66, 66, 8640, 135, 135, 66, 8640, 8640, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 8640, 66, 66, 135, 66, 8640, 66, 66, 8640, 66, 135, 135, 66, 135, 8640, 8640, 8640, 135, 8640, 8640, 66, 66, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 8640, 66, 66, 8640, 66, 66, 135]
Prompts retrieved: 282912 . Total input tokens: 63106199 . Total output tokens: 55484330
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.09277207730338,
    "estimated_duration": 3600.063535692186,
    "input_throughput": 5696.667793963487,
    "output_throughput": 4938.569506824438,
    "total_throughput": 10635.237300787925,
    "itl": 69.40743654104239,
    "ttft": 643042.708271884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 94463,
    "finished_requests": 82653,
    "scheduler_time": 51.94836390999925
}
#Debug simulation 
Total elapsed time: 6.092882900033146. Arrivals time: 0.2199911237694323 Scheduler time: 5.659624754451215 Scheduler overhead time: 0.07186253415420651 Adapter cache time: 0.031042333226650953 Engine time: 0.07578419102355838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.635392841883004,
    "estimated_duration": 3600.026833061197,
    "input_throughput": 6472.162870016009,
    "output_throughput": 5576.146493033308,
    "total_throughput": 12048.309363049317,
    "itl": 94.60821999603965,
    "ttft": 25858.40501818516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 94079,
    "finished_requests": 93452,
    "scheduler_time": 74.80893230123026
}
#Debug simulation 
Total elapsed time: 6.635471905115992. Arrivals time: 0.19944688957184553 Scheduler time: 6.276383985765278 Scheduler overhead time: 0.05455830926075578 Adapter cache time: 0.02306925831362605 Engine time: 0.05616053519770503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.573268535081297,
    "estimated_duration": 3600.087090786918,
    "input_throughput": 6327.088324694139,
    "output_throughput": 5451.070350553785,
    "total_throughput": 11778.158675247925,
    "itl": 87.43388675362807,
    "ttft": 143972.959929884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867946,
    "arrivals": 94079,
    "finished_requests": 91378,
    "scheduler_time": 70.88020548272222
}
#Debug simulation 
Total elapsed time: 6.5733490120619535. Arrivals time: 0.20672196336090565 Scheduler time: 6.190683906432241 Scheduler overhead time: 0.0591877200640738 Adapter cache time: 0.02569051319733262 Engine time: 0.062394816894084215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.1130585628561676,
    "estimated_duration": 3600.0401910611586,
    "input_throughput": 5754.298535732119,
    "output_throughput": 4971.18366745922,
    "total_throughput": 10725.482203191337,
    "itl": 68.96402706552601,
    "ttft": 591761.1978335725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 94079,
    "finished_requests": 83217,
    "scheduler_time": 53.41724055248985
}
#Debug simulation 
Total elapsed time: 6.113150685094297. Arrivals time: 0.21345818461850286 Scheduler time: 5.687271981500089 Scheduler overhead time: 0.07261714804917574 Adapter cache time: 0.028110558167099953 Engine time: 0.07684040209278464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.5886869970709085,
    "estimated_duration": 3600.055725065037,
    "input_throughput": 6327.346502279069,
    "output_throughput": 5451.334784448499,
    "total_throughput": 11778.681286727568,
    "itl": 87.43101230926482,
    "ttft": 143777.9820654979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 94079,
    "finished_requests": 91380,
    "scheduler_time": 70.87826922258753
}
#Debug simulation 
Total elapsed time: 6.588767752982676. Arrivals time: 0.2073222086764872 Scheduler time: 6.205528697930276 Scheduler overhead time: 0.059547172393649817 Adapter cache time: 0.025611468590795994 Engine time: 0.06217169435694814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.124779514037073,
    "estimated_duration": 3600.06061536751,
    "input_throughput": 5754.356999315473,
    "output_throughput": 4971.209074537494,
    "total_throughput": 10725.566073852968,
    "itl": 68.96484626229048,
    "ttft": 591864.5261266563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 94079,
    "finished_requests": 83218,
    "scheduler_time": 53.42020960593671
}
#Debug simulation 
Total elapsed time: 6.1248792903497815. Arrivals time: 0.2151127988472581 Scheduler time: 5.696996582672 Scheduler overhead time: 0.0730148539878428 Adapter cache time: 0.028050498571246862 Engine time: 0.07685596868395805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.5629434189759195,
    "estimated_duration": 3600.054798664279,
    "input_throughput": 6327.230632281324,
    "output_throughput": 5451.130912585328,
    "total_throughput": 11778.361544866651,
    "itl": 87.43297849063256,
    "ttft": 143905.68167585437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 94079,
    "finished_requests": 91376,
    "scheduler_time": 70.88110102176634
}
#Debug simulation 
Total elapsed time: 6.563021905254573. Arrivals time: 0.20697086723521352 Scheduler time: 6.17978203156963 Scheduler overhead time: 0.05952286859974265 Adapter cache time: 0.025867571588605642 Engine time: 0.062260786537081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 8640, 135, 135, 135, 135, 8640, 135, 8640, 135, 33, 8640, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 33, 135, 135, 135, 33, 33, 8640, 135, 135, 33, 8640, 8640, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 8640, 33, 33, 135, 33, 8640, 33, 33, 8640, 33, 135, 135, 33, 135, 8640, 8640, 8640, 135, 8640, 8640, 33, 33, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 8640, 33, 33, 8640, 33, 33, 135]
Prompts retrieved: 281856 . Total input tokens: 62870153 . Total output tokens: 55279064
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.128407274838537,
    "estimated_duration": 3600.0093103031777,
    "input_throughput": 5753.9512302693265,
    "output_throughput": 4971.1846435455045,
    "total_throughput": 10725.135873814832,
    "itl": 68.96557725916425,
    "ttft": 591977.7729907752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 94079,
    "finished_requests": 83213,
    "scheduler_time": 53.41512833361923
}
#Debug simulation 
Total elapsed time: 6.128519373945892. Arrivals time: 0.21446877345442772 Scheduler time: 5.701285294257104 Scheduler overhead time: 0.07283547800034285 Adapter cache time: 0.028152443934231997 Engine time: 0.07690136274322867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.555435087066144,
    "estimated_duration": 3600.0645135139366,
    "input_throughput": 6440.032647462235,
    "output_throughput": 5541.065423999596,
    "total_throughput": 11981.09807146183,
    "itl": 82.73932337064304,
    "ttft": 17303.749841201494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 93393,
    "finished_requests": 92948,
    "scheduler_time": 73.18404213810213
}
#Debug simulation 
Total elapsed time: 6.555541413836181. Arrivals time: 0.2013656673952937 Scheduler time: 6.185429089702666 Scheduler overhead time: 0.05979731446132064 Adapter cache time: 0.019142271485179663 Engine time: 0.06155603565275669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.587586430832744,
    "estimated_duration": 3600.083938152364,
    "input_throughput": 6439.997899576411,
    "output_throughput": 5541.035526587698,
    "total_throughput": 11981.03342616411,
    "itl": 82.70519260453581,
    "ttft": 18312.347175183782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867939,
    "arrivals": 93393,
    "finished_requests": 92948,
    "scheduler_time": 73.21900449673043
}
#Debug simulation 
Total elapsed time: 6.587683881167322. Arrivals time: 0.2004829323850572 Scheduler time: 6.215523967985064 Scheduler overhead time: 0.060418311040848494 Adapter cache time: 0.020069407299160957 Engine time: 0.06245647184550762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.23523928783834,
    "estimated_duration": 3600.008956579846,
    "input_throughput": 5876.683434726551,
    "output_throughput": 5069.60960934354,
    "total_throughput": 10946.293044070091,
    "itl": 67.53416430716473,
    "ttft": 465926.0411137652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 93393,
    "finished_requests": 84934,
    "scheduler_time": 56.89099862603555
}
#Debug simulation 
Total elapsed time: 6.235321878921241. Arrivals time: 0.21407201187685132 Scheduler time: 5.809349870309234 Scheduler overhead time: 0.07448440371081233 Adapter cache time: 0.023754792287945747 Engine time: 0.07815334433689713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 6.545926054008305,
    "estimated_duration": 3600.0504860469196,
    "input_throughput": 6439.829132912289,
    "output_throughput": 5540.961738540469,
    "total_throughput": 11980.790871452758,
    "itl": 82.70380471905402,
    "ttft": 18388.777464257335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 93393,
    "finished_requests": 92946,
    "scheduler_time": 73.21785875071846
}
#Debug simulation 
Total elapsed time: 6.54601184790954. Arrivals time: 0.19905247120186687 Scheduler time: 6.174896183889359 Scheduler overhead time: 0.06069782143458724 Adapter cache time: 0.020118228625506163 Engine time: 0.06239201873540878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 6.221117457840592,
    "estimated_duration": 3600.0197508525243,
    "input_throughput": 5876.8185910618595,
    "output_throughput": 5069.657741649332,
    "total_throughput": 10946.476332711192,
    "itl": 67.53367906412163,
    "ttft": 465956.19997272536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 93393,
    "finished_requests": 84936,
    "scheduler_time": 56.890189345195495
}
#Debug simulation 
Total elapsed time: 6.2211981527507305. Arrivals time: 0.21334416512399912 Scheduler time: 5.79683069512248 Scheduler overhead time: 0.0740688731893897 Adapter cache time: 0.02363496646285057 Engine time: 0.07795179216191173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 6.585274233948439,
    "estimated_duration": 3600.0542788321554,
    "input_throughput": 6439.91595802295,
    "output_throughput": 5540.998678072994,
    "total_throughput": 11980.914636095944,
    "itl": 82.70124595755284,
    "ttft": 18349.132773519566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 93393,
    "finished_requests": 92947,
    "scheduler_time": 73.21811858399572
}
#Debug simulation 
Total elapsed time: 6.585353125818074. Arrivals time: 0.19895018311217427 Scheduler time: 6.215310829691589 Scheduler overhead time: 0.060330528765916824 Adapter cache time: 0.019951175898313522 Engine time: 0.06211323710158467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 8640, 66, 66, 66, 66, 8640, 66, 8640, 66, 33, 8640, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 33, 66, 66, 66, 33, 33, 8640, 66, 66, 33, 8640, 8640, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 8640, 33, 33, 66, 33, 8640, 33, 33, 8640, 33, 66, 66, 33, 66, 8640, 8640, 8640, 66, 8640, 8640, 33, 33, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 8640, 33, 33, 8640, 33, 33, 66]
Prompts retrieved: 279648 . Total input tokens: 62363452 . Total output tokens: 54854868
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 6.2254461189731956,
    "estimated_duration": 3600.003696986469,
    "input_throughput": 5876.6586872423195,
    "output_throughput": 5069.629793790902,
    "total_throughput": 10946.288481033222,
    "itl": 67.5329536777312,
    "ttft": 465973.933726127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 93393,
    "finished_requests": 84934,
    "scheduler_time": 56.889935318105024
}
#Debug simulation 
Total elapsed time: 6.225531518924981. Arrivals time: 0.21305180341005325 Scheduler time: 5.801067764405161 Scheduler overhead time: 0.07402284629642963 Adapter cache time: 0.02366116503253579 Engine time: 0.0781396678648889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.621481542941183,
    "estimated_duration": 3600.0278638745103,
    "input_throughput": 4378.24194589329,
    "output_throughput": 3805.9774863113703,
    "total_throughput": 8184.21943220466,
    "itl": 87.66605204091903,
    "ttft": 20091.636030648235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 63713,
    "finished_requests": 63359,
    "scheduler_time": 48.29761164760161
}
#Debug simulation 
Total elapsed time: 4.621559484861791. Arrivals time: 0.14317390322685242 Scheduler time: 4.294684969820082 Scheduler overhead time: 0.05469855107367039 Adapter cache time: 0.04699884541332722 Engine time: 0.05617532134056091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.607391661033034,
    "estimated_duration": 3600.02637283425,
    "input_throughput": 4378.243759250841,
    "output_throughput": 3805.9790626513945,
    "total_throughput": 8184.222821902236,
    "itl": 87.66875669579876,
    "ttft": 20091.596594240327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.701702552586794,
    "arrivals": 63713,
    "finished_requests": 63359,
    "scheduler_time": 48.29807086838406
}
#Debug simulation 
Total elapsed time: 4.607470429036766. Arrivals time: 0.1421873583458364 Scheduler time: 4.28105052607134 Scheduler overhead time: 0.054879166185855865 Adapter cache time: 0.0472177998162806 Engine time: 0.056128987576812506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.721998818684369,
    "estimated_duration": 3600.0289779908944,
    "input_throughput": 4374.805618589617,
    "output_throughput": 3801.9610630019265,
    "total_throughput": 8176.766681591543,
    "itl": 87.66587453811542,
    "ttft": 26405.834220925775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 63713,
    "finished_requests": 63297,
    "scheduler_time": 48.293632228757374
}
#Debug simulation 
Total elapsed time: 4.722077905666083. Arrivals time: 0.1450449381954968 Scheduler time: 4.382663751486689 Scheduler overhead time: 0.05604607192799449 Adapter cache time: 0.05404406599700451 Engine time: 0.057574689388275146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.626424151007086,
    "estimated_duration": 3600.0278685552744,
    "input_throughput": 4378.24194020069,
    "output_throughput": 3805.97748136283,
    "total_throughput": 8184.21942156352,
    "itl": 87.66694685364561,
    "ttft": 20091.536466875044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407824,
    "arrivals": 63713,
    "finished_requests": 63359,
    "scheduler_time": 48.29782498356727
}
#Debug simulation 
Total elapsed time: 4.6265041497536. Arrivals time: 0.1431520814076066 Scheduler time: 4.299663155339658 Scheduler overhead time: 0.054422018118202686 Adapter cache time: 0.04719441756606102 Engine time: 0.056280385702848434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.725778417196125,
    "estimated_duration": 3600.0733842992263,
    "input_throughput": 4374.772211224168,
    "output_throughput": 3801.941388109871,
    "total_throughput": 8176.713599334039,
    "itl": 87.66308515930851,
    "ttft": 26282.847851267732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 63713,
    "finished_requests": 63298,
    "scheduler_time": 48.29386918642883
}
#Debug simulation 
Total elapsed time: 4.725862215273082. Arrivals time: 0.14511547796428204 Scheduler time: 4.3852038802579045 Scheduler overhead time: 0.05674737272784114 Adapter cache time: 0.054036141373217106 Engine time: 0.05767499469220638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.636377816088498,
    "estimated_duration": 3600.026416492191,
    "input_throughput": 4378.243706155368,
    "output_throughput": 3805.979016495842,
    "total_throughput": 8184.2227226512105,
    "itl": 87.66497192081518,
    "ttft": 20091.58948688021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 63713,
    "finished_requests": 63359,
    "scheduler_time": 48.297417455840645
}
#Debug simulation 
Total elapsed time: 4.636455417145044. Arrivals time: 0.14263159222900867 Scheduler time: 4.3099190182983875 Scheduler overhead time: 0.054667636286467314 Adapter cache time: 0.04696315107867122 Engine time: 0.05641863588243723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 540, 4320, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 4320, 1080, 1080, 540, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 4320, 540, 540, 1080, 540, 4320, 540, 540, 4320, 540, 1080, 1080, 540, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 540, 540, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 4320, 540, 540, 4320, 540, 540, 1080]
Prompts retrieved: 190080 . Total input tokens: 42409519 . Total output tokens: 37280410
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.710883338004351,
    "estimated_duration": 3600.01750632018,
    "input_throughput": 4374.902892097004,
    "output_throughput": 3801.967067096447,
    "total_throughput": 8176.869959193451,
    "itl": 87.66298370573354,
    "ttft": 26226.398643426783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 63713,
    "finished_requests": 63299,
    "scheduler_time": 48.29313413768854
}
#Debug simulation 
Total elapsed time: 4.710983349010348. Arrivals time: 0.14478630060330033 Scheduler time: 4.371853307820857 Scheduler overhead time: 0.05647861259058118 Adapter cache time: 0.05295878043398261 Engine time: 0.05810423707589507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.395260279066861,
    "estimated_duration": 3600.0487804532327,
    "input_throughput": 4195.988143832385,
    "output_throughput": 3633.105215411375,
    "total_throughput": 7829.09335924376,
    "itl": 69.92329668299658,
    "ttft": 16269.289793815713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 60841,
    "finished_requests": 60568,
    "scheduler_time": 43.804630867410495
}
#Debug simulation 
Total elapsed time: 4.395354441832751. Arrivals time: 0.141127519775182 Scheduler time: 4.043687687255442 Scheduler overhead time: 0.06379085872322321 Adapter cache time: 0.0508776237256825 Engine time: 0.06546369381248951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.357004892081022,
    "estimated_duration": 3600.0519996348844,
    "input_throughput": 4195.984391762125,
    "output_throughput": 3633.1019666733987,
    "total_throughput": 7829.086358435523,
    "itl": 69.92438123012089,
    "ttft": 16269.430314283007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867943,
    "arrivals": 60841,
    "finished_requests": 60568,
    "scheduler_time": 43.80498681051822
}
#Debug simulation 
Total elapsed time: 4.35710663208738. Arrivals time: 0.1388765056617558 Scheduler time: 4.00796093326062 Scheduler overhead time: 0.06320207845419645 Adapter cache time: 0.050882451236248016 Engine time: 0.06608509458601475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.386602159123868,
    "estimated_duration": 3600.0282088898384,
    "input_throughput": 4195.79934476625,
    "output_throughput": 3632.940977435605,
    "total_throughput": 7828.740322201855,
    "itl": 69.92465173640254,
    "ttft": 16447.184761537053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 60841,
    "finished_requests": 60565,
    "scheduler_time": 43.80486894050598
}
#Debug simulation 
Total elapsed time: 4.386696795001626. Arrivals time: 0.14110670005902648 Scheduler time: 4.036473053973168 Scheduler overhead time: 0.06350363232195377 Adapter cache time: 0.05089448671787977 Engine time: 0.06436887290328741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.417058133054525,
    "estimated_duration": 3600.0467201862125,
    "input_throughput": 4195.990545150107,
    "output_throughput": 3633.1072945974074,
    "total_throughput": 7829.097839747515,
    "itl": 69.92370029602725,
    "ttft": 16269.245435328341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407826,
    "arrivals": 60841,
    "finished_requests": 60568,
    "scheduler_time": 43.80477556456414
}
#Debug simulation 
Total elapsed time: 4.417135753203183. Arrivals time: 0.1403472009114921 Scheduler time: 4.066684616263956 Scheduler overhead time: 0.06362002668902278 Adapter cache time: 0.051165647339075804 Engine time: 0.0650734486989677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.396735435817391,
    "estimated_duration": 3600.0590535414563,
    "input_throughput": 4195.976170207578,
    "output_throughput": 3633.094848022994,
    "total_throughput": 7829.071018230573,
    "itl": 69.92358603771049,
    "ttft": 16269.777019672361,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 60841,
    "finished_requests": 60568,
    "scheduler_time": 43.805102939236406
}
#Debug simulation 
Total elapsed time: 4.396811570972204. Arrivals time: 0.13913219049572945 Scheduler time: 4.048967466689646 Scheduler overhead time: 0.06324481824412942 Adapter cache time: 0.05076695326715708 Engine time: 0.06432440597563982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.32834094716236,
    "estimated_duration": 3600.025031168354,
    "input_throughput": 4195.80304837431,
    "output_throughput": 3632.9441842118067,
    "total_throughput": 7828.747232586117,
    "itl": 69.92441439965152,
    "ttft": 16446.822013171717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 60841,
    "finished_requests": 60565,
    "scheduler_time": 43.80427445909134
}
#Debug simulation 
Total elapsed time: 4.3284205459058285. Arrivals time: 0.13731901673600078 Scheduler time: 3.9846617272123694 Scheduler overhead time: 0.06280889408662915 Adapter cache time: 0.050161649473011494 Engine time: 0.0636441521346569 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 270, 4320, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 4320, 1080, 1080, 270, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 4320, 270, 270, 1080, 270, 4320, 270, 270, 4320, 270, 1080, 1080, 270, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 270, 270, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 4320, 270, 270, 4320, 270, 270, 1080]
Prompts retrieved: 181440 . Total input tokens: 40482527 . Total output tokens: 35591514
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.34247076837346,
    "estimated_duration": 3600.055731083828,
    "input_throughput": 4195.980042634585,
    "output_throughput": 3633.098200972113,
    "total_throughput": 7829.078243606697,
    "itl": 69.92387064052947,
    "ttft": 16269.865883558925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 60841,
    "finished_requests": 60568,
    "scheduler_time": 43.8051109469025
}
#Debug simulation 
Total elapsed time: 4.3425479689612985. Arrivals time: 0.13835807098075747 Scheduler time: 3.9958546250127256 Scheduler overhead time: 0.06327376887202263 Adapter cache time: 0.050577189307659864 Engine time: 0.06431377632543445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.205236244015396,
    "estimated_duration": 3600.0096848559338,
    "input_throughput": 4093.695653652044,
    "output_throughput": 3542.3796368232097,
    "total_throughput": 7636.0752904752535,
    "itl": 62.441988010080344,
    "ttft": 15679.938016045902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 59397,
    "finished_requests": 59139,
    "scheduler_time": 41.39318620216355
}
#Debug simulation 
Total elapsed time: 4.205333749763668. Arrivals time: 0.13680705660954118 Scheduler time: 3.857475583907217 Scheduler overhead time: 0.06603138474747539 Adapter cache time: 0.04748195968568325 Engine time: 0.06608283426612616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.243989568669349,
    "estimated_duration": 3600.046019147688,
    "input_throughput": 4093.6543370879103,
    "output_throughput": 3542.343884542671,
    "total_throughput": 7635.998221630581,
    "itl": 62.44359748189008,
    "ttft": 15680.021063184751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867944,
    "arrivals": 59397,
    "finished_requests": 59139,
    "scheduler_time": 41.393978560302834
}
#Debug simulation 
Total elapsed time: 4.244096819777042. Arrivals time: 0.136747679207474 Scheduler time: 3.8949155560694635 Scheduler overhead time: 0.0661160983145237 Adapter cache time: 0.04808770306408405 Engine time: 0.06667286669835448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.263238111976534,
    "estimated_duration": 3600.019802546216,
    "input_throughput": 4093.68414850846,
    "output_throughput": 3542.3696811279656,
    "total_throughput": 7636.053829636426,
    "itl": 62.4437372694603,
    "ttft": 15679.910847158004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 59397,
    "finished_requests": 59139,
    "scheduler_time": 41.39377207021817
}
#Debug simulation 
Total elapsed time: 4.2633299101144075. Arrivals time: 0.13987647742033005 Scheduler time: 3.908710048068315 Scheduler overhead time: 0.06693043652921915 Adapter cache time: 0.048254788387566805 Engine time: 0.0677058040164411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.246656482573599,
    "estimated_duration": 3600.0094875541777,
    "input_throughput": 4093.695878010714,
    "output_throughput": 3542.37983096651,
    "total_throughput": 7636.075708977224,
    "itl": 62.442730464099014,
    "ttft": 15679.850735543141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407825,
    "arrivals": 59397,
    "finished_requests": 59139,
    "scheduler_time": 41.39333915283293
}
#Debug simulation 
Total elapsed time: 4.246737238951027. Arrivals time: 0.1402274528518319 Scheduler time: 3.892630038317293 Scheduler overhead time: 0.06685440242290497 Adapter cache time: 0.04817889118567109 Engine time: 0.06706696143373847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.257029936183244,
    "estimated_duration": 3600.0094694980066,
    "input_throughput": 4093.6958985430138,
    "output_throughput": 3542.3798487336344,
    "total_throughput": 7636.075747276648,
    "itl": 62.44362543787445,
    "ttft": 15679.928827352442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 59397,
    "finished_requests": 59139,
    "scheduler_time": 41.393626293229055
}
#Debug simulation 
Total elapsed time: 4.257109955884516. Arrivals time: 0.1368748163804412 Scheduler time: 3.906482091639191 Scheduler overhead time: 0.0665252823382616 Adapter cache time: 0.04777933889999986 Engine time: 0.06786704389378428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.24103882862255,
    "estimated_duration": 3600.0094993797966,
    "input_throughput": 4093.6958645633918,
    "output_throughput": 3542.3798193301977,
    "total_throughput": 7636.0756838935895,
    "itl": 62.44164669921444,
    "ttft": 15679.93715440004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 59397,
    "finished_requests": 59139,
    "scheduler_time": 41.39302357585589
}
#Debug simulation 
Total elapsed time: 4.241114028729498. Arrivals time: 0.13695093523710966 Scheduler time: 3.8911593267694116 Scheduler overhead time: 0.06637096684426069 Adapter cache time: 0.04770162981003523 Engine time: 0.06720901560038328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 135, 4320, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 4320, 1080, 1080, 135, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 135, 4320, 1080, 1080, 1080, 4320, 4320, 135, 135, 1080, 135, 4320, 135, 135, 4320, 135, 1080, 1080, 135, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 135, 135, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 4320, 135, 135, 4320, 135, 135, 1080]
Prompts retrieved: 177120 . Total input tokens: 39503486 . Total output tokens: 34757848
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.258876702748239,
    "estimated_duration": 3600.0125694629646,
    "input_throughput": 4093.6923734681454,
    "output_throughput": 3542.3767983961184,
    "total_throughput": 7636.069171864264,
    "itl": 62.44402621551234,
    "ttft": 15679.855939697683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 59397,
    "finished_requests": 59139,
    "scheduler_time": 41.39369013519668
}
#Debug simulation 
Total elapsed time: 4.258951679803431. Arrivals time: 0.13730244617909193 Scheduler time: 3.9084670045413077 Scheduler overhead time: 0.06641280045732856 Adapter cache time: 0.04800422163680196 Engine time: 0.06695757620036602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.193021161947399,
    "estimated_duration": 3600.0419211098288,
    "input_throughput": 4036.9227132554356,
    "output_throughput": 3488.7743740850824,
    "total_throughput": 7525.697087340518,
    "itl": 59.46651543647332,
    "ttft": 14819.747534752178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 58689,
    "finished_requests": 58449,
    "scheduler_time": 40.10277606652374
}
#Debug simulation 
Total elapsed time: 4.1931237312965095. Arrivals time: 0.13576575508341193 Scheduler time: 3.8437510221265256 Scheduler overhead time: 0.06803900841623545 Adapter cache time: 0.0446483064442873 Engine time: 0.06859798217192292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.153268271125853,
    "estimated_duration": 3600.04449279143,
    "input_throughput": 4036.919829491113,
    "output_throughput": 3488.7718818889757,
    "total_throughput": 7525.691711380088,
    "itl": 59.470068476659605,
    "ttft": 14819.809060584657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867944,
    "arrivals": 58689,
    "finished_requests": 58449,
    "scheduler_time": 40.10355921423634
}
#Debug simulation 
Total elapsed time: 4.153376603964716. Arrivals time: 0.1355528486892581 Scheduler time: 3.804618177935481 Scheduler overhead time: 0.06765312561765313 Adapter cache time: 0.04432316171005368 Engine time: 0.06893400754779577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.176845517940819,
    "estimated_duration": 3600.0062881235913,
    "input_throughput": 4036.962670855498,
    "output_throughput": 3488.8089060940033,
    "total_throughput": 7525.771576949502,
    "itl": 59.46893216785053,
    "ttft": 14820.025342890904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206212392449379,
    "arrivals": 58689,
    "finished_requests": 58449,
    "scheduler_time": 40.10300495260976
}
#Debug simulation 
Total elapsed time: 4.176943833939731. Arrivals time: 0.13764199521392584 Scheduler time: 3.8256691307760775 Scheduler overhead time: 0.06830551009625196 Adapter cache time: 0.04428164241835475 Engine time: 0.06859600124880672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.2024728283286095,
    "estimated_duration": 3600.04000614077,
    "input_throughput": 4036.9248606154856,
    "output_throughput": 3488.7762298686202,
    "total_throughput": 7525.701090484106,
    "itl": 59.46873326329337,
    "ttft": 14819.85012023131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.657279481440783,
    "arrivals": 58689,
    "finished_requests": 58449,
    "scheduler_time": 40.10317058486498
}
#Debug simulation 
Total elapsed time: 4.2025537779554725. Arrivals time: 0.1370267542079091 Scheduler time: 3.852687413804233 Scheduler overhead time: 0.06791252130642533 Adapter cache time: 0.044460272416472435 Engine time: 0.06834349920973182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.1729526608251035,
    "estimated_duration": 3600.015837677721,
    "input_throughput": 4036.9519622377356,
    "output_throughput": 3488.7996515320797,
    "total_throughput": 7525.751613769815,
    "itl": 59.47080484524411,
    "ttft": 14819.831464529458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932675,
    "arrivals": 58689,
    "finished_requests": 58449,
    "scheduler_time": 40.10332353553463
}
#Debug simulation 
Total elapsed time: 4.173028032761067. Arrivals time: 0.13597028190270066 Scheduler time: 3.824087052606046 Scheduler overhead time: 0.06814058870077133 Adapter cache time: 0.04439862258732319 Engine time: 0.06794673250988126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.164955468848348,
    "estimated_duration": 3600.0408984802634,
    "input_throughput": 4036.9238599858854,
    "output_throughput": 3488.7753651082185,
    "total_throughput": 7525.699225094104,
    "itl": 59.46593440649872,
    "ttft": 14819.728104709495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 58689,
    "finished_requests": 58449,
    "scheduler_time": 40.102599680992114
}
#Debug simulation 
Total elapsed time: 4.165029048919678. Arrivals time: 0.13590795174241066 Scheduler time: 3.8137864572927356 Scheduler overhead time: 0.06888401228934526 Adapter cache time: 0.044488861691206694 Engine time: 0.06947754183784127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 66, 4320, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 4320, 1080, 1080, 66, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 66, 4320, 1080, 1080, 1080, 4320, 4320, 66, 66, 1080, 66, 4320, 66, 66, 4320, 66, 1080, 1080, 66, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 66, 66, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 4320, 66, 66, 4320, 66, 66, 1080]
Prompts retrieved: 174912 . Total input tokens: 39000675 . Total output tokens: 34334508
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.2127640740945935,
    "estimated_duration": 3600.0631668195974,
    "input_throughput": 4036.8988894266995,
    "output_throughput": 3488.753785144176,
    "total_throughput": 7525.652674570876,
    "itl": 59.47048973500991,
    "ttft": 14819.976992656868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.707365739941597,
    "arrivals": 58689,
    "finished_requests": 58449,
    "scheduler_time": 40.10377763387041
}
#Debug simulation 
Total elapsed time: 4.212874376215041. Arrivals time: 0.1366747277788818 Scheduler time: 3.8607981614768505 Scheduler overhead time: 0.06878451677039266 Adapter cache time: 0.04478048114106059 Engine time: 0.06916837021708488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.202466918155551,
    "estimated_duration": 3599.3027051913878,
    "input_throughput": 4017.058631702716,
    "output_throughput": 3513.2452132357143,
    "total_throughput": 7530.30384493843,
    "itl": 58.86097293143134,
    "ttft": 13917.635471921198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6347912670671934,
    "arrivals": 58348,
    "finished_requests": 58124,
    "scheduler_time": 40.31826901913419
}
#Debug simulation 
Total elapsed time: 4.202545489184558. Arrivals time: 0.13463067170232534 Scheduler time: 3.8558679819107056 Scheduler overhead time: 0.06856611929833889 Adapter cache time: 0.041794313583523035 Engine time: 0.06905232463032007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.177040410693735,
    "estimated_duration": 3599.292615680206,
    "input_throughput": 4016.948757378996,
    "output_throughput": 3513.1208685044226,
    "total_throughput": 7530.069625883419,
    "itl": 58.86279362949191,
    "ttft": 13979.416597680041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7017025525867946,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.31858309203877
}
#Debug simulation 
Total elapsed time: 4.177117506042123. Arrivals time: 0.13436980405822396 Scheduler time: 3.8308994248509407 Scheduler overhead time: 0.06826029671356082 Adapter cache time: 0.04145773593336344 Engine time: 0.06947484379634261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.229050962720066,
    "estimated_duration": 3599.2762666183994,
    "input_throughput": 4016.96700364259,
    "output_throughput": 3513.1368262209076,
    "total_throughput": 7530.103829863498,
    "itl": 58.863403761484065,
    "ttft": 13979.450942535845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.720621239244938,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.31859010181959
}
#Debug simulation 
Total elapsed time: 4.229127566795796. Arrivals time: 0.13545332616195083 Scheduler time: 3.8806528677232563 Scheduler overhead time: 0.06876507215201855 Adapter cache time: 0.042184669990092516 Engine time: 0.06948515493422747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 4.206834768876433,
    "estimated_duration": 3599.305691847709,
    "input_throughput": 4017.055298400523,
    "output_throughput": 3513.2422979912412,
    "total_throughput": 7530.297596391764,
    "itl": 58.86219992492228,
    "ttft": 13917.721233547873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6572794814407829,
    "arrivals": 58348,
    "finished_requests": 58124,
    "scheduler_time": 40.31850274303957
}
#Debug simulation 
Total elapsed time: 4.206910301931202. Arrivals time: 0.14157972810789943 Scheduler time: 3.8532539922744036 Scheduler overhead time: 0.0681535997428 Adapter cache time: 0.04182333592325449 Engine time: 0.06948025105521083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 4.218510226812214,
    "estimated_duration": 3599.2762890566687,
    "input_throughput": 4016.966978600393,
    "output_throughput": 3513.1368043196408,
    "total_throughput": 7530.103782920033,
    "itl": 58.86221995540705,
    "ttft": 13979.351900912385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7139934895932676,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.318405202434874
}
#Debug simulation 
Total elapsed time: 4.2185878599993885. Arrivals time: 0.13462720904499292 Scheduler time: 3.871535554062575 Scheduler overhead time: 0.06837615789845586 Adapter cache time: 0.04172498686239123 Engine time: 0.06950128963217139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 4.192603080999106,
    "estimated_duration": 3599.293190132591,
    "input_throughput": 4016.94811626818,
    "output_throughput": 3513.1203078052645,
    "total_throughput": 7530.0684240734445,
    "itl": 58.86046475366552,
    "ttft": 13979.394573354646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6128564102947712,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.31793041704367
}
#Debug simulation 
Total elapsed time: 4.192679382394999. Arrivals time: 0.13449996104463935 Scheduler time: 3.846092422027141 Scheduler overhead time: 0.06843013782054186 Adapter cache time: 0.04183521633967757 Engine time: 0.06910674134269357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_96_slots_96_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 1080, 33, 4320, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 4320, 1080, 1080, 33, 4320, 4320, 1080, 4320, 4320, 1080, 4320, 33, 4320, 1080, 1080, 1080, 4320, 4320, 33, 33, 1080, 33, 4320, 33, 33, 4320, 33, 1080, 1080, 33, 1080, 4320, 4320, 4320, 1080, 4320, 4320, 33, 33, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 4320, 33, 33, 4320, 33, 33, 1080]
Prompts retrieved: 173856 . Total input tokens: 38764817 . Total output tokens: 34137669
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 4.1994485929608345,
    "estimated_duration": 3599.2715937827106,
    "input_throughput": 4016.9722187607845,
    "output_throughput": 3513.1413872301873,
    "total_throughput": 7530.113605990972,
    "itl": 58.86219046763725,
    "ttft": 13979.398895556586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7073657399415971,
    "arrivals": 58348,
    "finished_requests": 58123,
    "scheduler_time": 40.31830408223389
}
#Debug simulation 
Total elapsed time: 4.199546760879457. Arrivals time: 0.1418454353697598 Scheduler time: 3.8449568655341864 Scheduler overhead time: 0.06872012838721275 Adapter cache time: 0.041834627743810415 Engine time: 0.06952101131901145 
