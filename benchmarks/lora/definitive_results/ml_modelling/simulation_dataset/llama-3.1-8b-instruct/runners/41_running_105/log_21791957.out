INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.604300954844803,
    "estimated_duration": 3600.115029733462,
    "input_throughput": 6160.116500955778,
    "output_throughput": 5363.537787132759,
    "total_throughput": 11523.654288088537,
    "itl": 108.07007808472993,
    "ttft": 1761268.6083699886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 291201,
    "finished_requests": 89532,
    "scheduler_time": 43.321676938349384
}
#Debug simulation 
Total elapsed time: 6.604449859820306. Arrivals time: 0.32544666761532426 Scheduler time: 6.129292383790016 Scheduler overhead time: 0.05071903020143509 Adapter cache time: 0.02364840917289257 Engine time: 0.05169502552598715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.505922870244831,
    "estimated_duration": 3600.087022005183,
    "input_throughput": 6041.075081536929,
    "output_throughput": 5263.158608162312,
    "total_throughput": 11304.23368969924,
    "itl": 100.17328133811341,
    "ttft": 1777557.9334524542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 291201,
    "finished_requests": 87854,
    "scheduler_time": 38.97730243357931
}
#Debug simulation 
Total elapsed time: 6.506219930015504. Arrivals time: 0.3177320808172226 Scheduler time: 6.028353495988995 Scheduler overhead time: 0.05386135680601001 Adapter cache time: 0.025487049482762814 Engine time: 0.05533325346186757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.300617472734302,
    "estimated_duration": 3600.092963278533,
    "input_throughput": 5810.221073000015,
    "output_throughput": 5062.161223582276,
    "total_throughput": 10872.382296582291,
    "itl": 87.46758730493245,
    "ttft": 1811331.424810201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 291201,
    "finished_requests": 84493,
    "scheduler_time": 30.384037177119854
}
#Debug simulation 
Total elapsed time: 6.3007865799590945. Arrivals time: 0.3141218591481447 Scheduler time: 5.804951688274741 Scheduler overhead time: 0.06034490466117859 Adapter cache time: 0.030810938216745853 Engine time: 0.062165481969714165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.504293305333704,
    "estimated_duration": 3600.089797372185,
    "input_throughput": 6041.070424375196,
    "output_throughput": 5263.154550708873,
    "total_throughput": 11304.22497508407,
    "itl": 100.17615837675334,
    "ttft": 1777497.3328450902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 291201,
    "finished_requests": 87854,
    "scheduler_time": 38.98176744549245
}
#Debug simulation 
Total elapsed time: 6.504429564345628. Arrivals time: 0.31724862288683653 Scheduler time: 6.026622941251844 Scheduler overhead time: 0.05407900782302022 Adapter cache time: 0.025768591556698084 Engine time: 0.05533647397533059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.348862327169627,
    "estimated_duration": 3600.0460859994582,
    "input_throughput": 5810.2970074042405,
    "output_throughput": 5062.231583888324,
    "total_throughput": 10872.528591292565,
    "itl": 87.46459041137477,
    "ttft": 1811278.7712493027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 291201,
    "finished_requests": 84494,
    "scheduler_time": 30.384633276484177
}
#Debug simulation 
Total elapsed time: 6.349014677107334. Arrivals time: 0.35402158135548234 Scheduler time: 5.813571460545063 Scheduler overhead time: 0.060399440582841635 Adapter cache time: 0.03066158527508378 Engine time: 0.06189093505963683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.489176528993994,
    "estimated_duration": 3600.02580517981,
    "input_throughput": 6041.011975166596,
    "output_throughput": 5263.029774047315,
    "total_throughput": 11304.041749213911,
    "itl": 100.17414151146825,
    "ttft": 1777431.3667225603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 291201,
    "finished_requests": 87851,
    "scheduler_time": 38.97892329902265
}
#Debug simulation 
Total elapsed time: 6.489337359089404. Arrivals time: 0.2995173027738929 Scheduler time: 6.0299142152071 Scheduler overhead time: 0.05385997053235769 Adapter cache time: 0.0256126937456429 Engine time: 0.05517796287313104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 195024558 . Total output tokens: 171496667
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.641947566065937,
    "estimated_duration": 3600.095274266992,
    "input_throughput": 5810.366228226848,
    "output_throughput": 5062.231583221268,
    "total_throughput": 10872.597811448115,
    "itl": 87.46647081416519,
    "ttft": 1811219.7166139672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 291201,
    "finished_requests": 84496,
    "scheduler_time": 30.38427967487899
}
#Debug simulation 
Total elapsed time: 6.642044479958713. Arrivals time: 0.3086436428129673 Scheduler time: 6.151863351929933 Scheduler overhead time: 0.060460742097347975 Adapter cache time: 0.03028590651229024 Engine time: 0.0623245220631361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.8771627261303365,
    "estimated_duration": 3600.0066686334744,
    "input_throughput": 6322.416066145717,
    "output_throughput": 5524.199489205102,
    "total_throughput": 11846.61555535082,
    "itl": 104.43884253322034,
    "ttft": 1735178.0926402186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 287382,
    "finished_requests": 91985,
    "scheduler_time": 44.5212530220728
}
#Debug simulation 
Total elapsed time: 6.877316736150533. Arrivals time: 0.3407722944393754 Scheduler time: 6.385613870341331 Scheduler overhead time: 0.05227195704355836 Adapter cache time: 0.0210290071554482 Engine time: 0.05324089573696256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.706493272911757,
    "estimated_duration": 3600.0918106381614,
    "input_throughput": 6186.931937175163,
    "output_throughput": 5412.425856035599,
    "total_throughput": 11599.357793210762,
    "itl": 96.91687942159888,
    "ttft": 1753026.4144482992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489668,
    "arrivals": 287382,
    "finished_requests": 90082,
    "scheduler_time": 39.952393563619346
}
#Debug simulation 
Total elapsed time: 6.706673463340849. Arrivals time: 0.351657509803772 Scheduler time: 6.1937777367420495 Scheduler overhead time: 0.05584819009527564 Adapter cache time: 0.021786144003272057 Engine time: 0.05752845574170351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.507389212027192,
    "estimated_duration": 3600.0566453555234,
    "input_throughput": 5928.396717738956,
    "output_throughput": 5188.766966791788,
    "total_throughput": 11117.163684530744,
    "itl": 84.83697929195283,
    "ttft": 1788760.6651308788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 287382,
    "finished_requests": 86269,
    "scheduler_time": 30.96917863903809
}
#Debug simulation 
Total elapsed time: 6.507596922572702. Arrivals time: 0.3545203427784145 Scheduler time: 5.9705100655555725 Scheduler overhead time: 0.062329089269042015 Adapter cache time: 0.026492621283978224 Engine time: 0.06433227425441146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.771099629811943,
    "estimated_duration": 3600.0346307342747,
    "input_throughput": 6186.858817926744,
    "output_throughput": 5412.031549270425,
    "total_throughput": 11598.89036719717,
    "itl": 96.91829038625836,
    "ttft": 1753110.7777187312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 287382,
    "finished_requests": 90077,
    "scheduler_time": 39.952805022867416
}
#Debug simulation 
Total elapsed time: 6.771254847757518. Arrivals time: 0.39498979737982154 Scheduler time: 6.214878622908145 Scheduler overhead time: 0.05567456968128681 Adapter cache time: 0.02248394163325429 Engine time: 0.057098912075161934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.4877784927375615,
    "estimated_duration": 3600.0213620924756,
    "input_throughput": 5928.422599024501,
    "output_throughput": 5188.701432911156,
    "total_throughput": 11117.124031935658,
    "itl": 84.83642262517417,
    "ttft": 1788762.8105704016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 287382,
    "finished_requests": 86268,
    "scheduler_time": 30.9682731465637
}
#Debug simulation 
Total elapsed time: 6.48801040276885. Arrivals time: 0.33318550512194633 Scheduler time: 5.973274893127382 Scheduler overhead time: 0.0620917733758688 Adapter cache time: 0.02629006700590253 Engine time: 0.06376786297187209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.744204403832555,
    "estimated_duration": 3600.0788527453765,
    "input_throughput": 6186.813375772274,
    "output_throughput": 5412.113400000034,
    "total_throughput": 11598.926775772308,
    "itl": 96.91703705563789,
    "ttft": 1753027.5842139316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 287382,
    "finished_requests": 90080,
    "scheduler_time": 39.95274317356803
}
#Debug simulation 
Total elapsed time: 6.744337580632418. Arrivals time: 0.3682102905586362 Scheduler time: 6.214372029528022 Scheduler overhead time: 0.05585935106500983 Adapter cache time: 0.022653096355497837 Engine time: 0.057196386624127626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192481990 . Total output tokens: 169267683
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.540145505685359,
    "estimated_duration": 3600.0117002813163,
    "input_throughput": 5928.3873989443555,
    "output_throughput": 5188.698691879345,
    "total_throughput": 11117.0860908237,
    "itl": 84.83611266407233,
    "ttft": 1788750.2441050517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 287382,
    "finished_requests": 86267,
    "scheduler_time": 30.967920873581274
}
#Debug simulation 
Total elapsed time: 6.540297850035131. Arrivals time: 0.3855659165419638 Scheduler time: 5.973088019061834 Scheduler overhead time: 0.062154457438737154 Adapter cache time: 0.026284626685082912 Engine time: 0.06397272692993283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.99818178685382,
    "estimated_duration": 3600.0686105462714,
    "input_throughput": 6443.091371105931,
    "output_throughput": 5637.602833608313,
    "total_throughput": 12080.694204714244,
    "itl": 102.61620587320408,
    "ttft": 1712537.3236661232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 285504,
    "finished_requests": 93772,
    "scheduler_time": 45.520105202993086
}
#Debug simulation 
Total elapsed time: 6.998336993157864. Arrivals time: 0.33368138689547777 Scheduler time: 6.513782458379865 Scheduler overhead time: 0.053512814454734325 Adapter cache time: 0.01835276046767831 Engine time: 0.0541658834554255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.820868601091206,
    "estimated_duration": 3600.091682944564,
    "input_throughput": 6287.105716565311,
    "output_throughput": 5505.432290487919,
    "total_throughput": 11792.53800705323,
    "itl": 94.77097714070024,
    "ttft": 1734124.5185260174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 285504,
    "finished_requests": 91488,
    "scheduler_time": 40.431529531052504
}
#Debug simulation 
Total elapsed time: 6.821001735050231. Arrivals time: 0.3576021329499781 Scheduler time: 6.300394320394844 Scheduler overhead time: 0.05694140214473009 Adapter cache time: 0.02076809899881482 Engine time: 0.058620212599635124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.586557134054601,
    "estimated_duration": 3600.0326700861447,
    "input_throughput": 6020.514252577981,
    "output_throughput": 5274.0407490651105,
    "total_throughput": 11294.555001643092,
    "itl": 83.10978047011332,
    "ttft": 1771436.5291386435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 285504,
    "finished_requests": 87646,
    "scheduler_time": 31.307353063517443
}
#Debug simulation 
Total elapsed time: 6.58670201478526. Arrivals time: 0.3325972091406584 Scheduler time: 6.07269295444712 Scheduler overhead time: 0.0631837067194283 Adapter cache time: 0.0237408634275198 Engine time: 0.0647561214864254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.76895547285676,
    "estimated_duration": 3600.061773059953,
    "input_throughput": 6299.396629720646,
    "output_throughput": 5514.64815091921,
    "total_throughput": 11814.044780639855,
    "itl": 95.04377530523999,
    "ttft": 1732245.100806364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 285504,
    "finished_requests": 91658,
    "scheduler_time": 40.703972113896825
}
#Debug simulation 
Total elapsed time: 6.769109630957246. Arrivals time: 0.32486521219834685 Scheduler time: 6.282868151552975 Scheduler overhead time: 0.05671753967180848 Adapter cache time: 0.020216969307512045 Engine time: 0.057934388518333435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.6259276806376874,
    "estimated_duration": 3600.021872399409,
    "input_throughput": 6020.294811585367,
    "output_throughput": 5274.048234419587,
    "total_throughput": 11294.343046004955,
    "itl": 83.11008304937656,
    "ttft": 1771373.8078592417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 285504,
    "finished_requests": 87643,
    "scheduler_time": 31.30873234304717
}
#Debug simulation 
Total elapsed time: 6.6260626330040395. Arrivals time: 0.3222337136976421 Scheduler time: 6.121733907610178 Scheduler overhead time: 0.06338305026292801 Adapter cache time: 0.023965660482645035 Engine time: 0.06494883354753256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.857888880651444,
    "estimated_duration": 3600.0175802772524,
    "input_throughput": 6303.865604526364,
    "output_throughput": 5518.1613858869205,
    "total_throughput": 11822.026990413284,
    "itl": 95.26827752449739,
    "ttft": 1731777.298579561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 285504,
    "finished_requests": 91721,
    "scheduler_time": 40.85944920172461
}
#Debug simulation 
Total elapsed time: 6.858092225622386. Arrivals time: 0.337058890145272 Scheduler time: 6.359625714831054 Scheduler overhead time: 0.056763737462460995 Adapter cache time: 0.020037169568240643 Engine time: 0.057998260483145714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 191217258 . Total output tokens: 168150479
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.52918513212353,
    "estimated_duration": 3600.0006605638378,
    "input_throughput": 6027.658060651961,
    "output_throughput": 5280.270142234581,
    "total_throughput": 11307.928202886542,
    "itl": 83.43405183798815,
    "ttft": 1769679.2155294043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 285504,
    "finished_requests": 87752,
    "scheduler_time": 31.60187045815819
}
#Debug simulation 
Total elapsed time: 6.5293175908736885. Arrivals time: 0.31224094750359654 Scheduler time: 6.037427441217005 Scheduler overhead time: 0.0627078004181385 Adapter cache time: 0.02325692819431424 Engine time: 0.06418648129329085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.052137951832265,
    "estimated_duration": 3600.0247051394163,
    "input_throughput": 6565.033558314075,
    "output_throughput": 5706.868614170905,
    "total_throughput": 12271.902172484979,
    "itl": 100.9984388393421,
    "ttft": 1695617.8563817116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 284495,
    "finished_requests": 95460,
    "scheduler_time": 46.06749009865539
}
#Debug simulation 
Total elapsed time: 7.052282440941781. Arrivals time: 0.37674091290682554 Scheduler time: 6.5239074868150055 Scheduler overhead time: 0.053897150326520205 Adapter cache time: 0.017371194902807474 Engine time: 0.05515179689973593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.908860333729535,
    "estimated_duration": 3600.0831750373586,
    "input_throughput": 6418.333098584651,
    "output_throughput": 5577.7047428332335,
    "total_throughput": 11996.037841417885,
    "itl": 93.90906245202999,
    "ttft": 1715365.3883960175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 284495,
    "finished_requests": 93295,
    "scheduler_time": 41.297615649202065
}
#Debug simulation 
Total elapsed time: 6.909049530047923. Arrivals time: 0.3519019144587219 Scheduler time: 6.394543669652194 Scheduler overhead time: 0.057377046905457973 Adapter cache time: 0.019389308989048004 Engine time: 0.05896295187994838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.658414797857404,
    "estimated_duration": 3600.0115178891124,
    "input_throughput": 6141.432295462175,
    "output_throughput": 5335.633762433883,
    "total_throughput": 11477.066057896058,
    "itl": 82.46628714523563,
    "ttft": 1755678.1717019351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 284495,
    "finished_requests": 89271,
    "scheduler_time": 31.96005041463666
}
#Debug simulation 
Total elapsed time: 6.658541108015925. Arrivals time: 0.32503657322376966 Scheduler time: 6.153400952927768 Scheduler overhead time: 0.06337390514090657 Adapter cache time: 0.021996148861944675 Engine time: 0.06488095223903656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.881698064971715,
    "estimated_duration": 3600.0976848179243,
    "input_throughput": 6418.38222819463,
    "output_throughput": 5577.660596455608,
    "total_throughput": 11996.04282465024,
    "itl": 93.90835458538726,
    "ttft": 1715329.5579791858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 284495,
    "finished_requests": 93296,
    "scheduler_time": 41.29724717042792
}
#Debug simulation 
Total elapsed time: 6.881975699216127. Arrivals time: 0.33884319942444563 Scheduler time: 6.381137047894299 Scheduler overhead time: 0.05753506952896714 Adapter cache time: 0.018565760925412178 Engine time: 0.05890125688165426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.636115319095552,
    "estimated_duration": 3600.079183633088,
    "input_throughput": 6141.372139956059,
    "output_throughput": 5335.454033157078,
    "total_throughput": 11476.826173113137,
    "itl": 82.4652912134967,
    "ttft": 1755740.4246668657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 284495,
    "finished_requests": 89270,
    "scheduler_time": 31.95888519043651
}
#Debug simulation 
Total elapsed time: 6.636238246224821. Arrivals time: 0.31894561601802707 Scheduler time: 6.137034320738167 Scheduler overhead time: 0.06346601573750377 Adapter cache time: 0.02204380463808775 Engine time: 0.06496260687708855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.922265592962503,
    "estimated_duration": 3600.0280463793006,
    "input_throughput": 6418.641938981551,
    "output_throughput": 5577.922655407083,
    "total_throughput": 11996.564594388634,
    "itl": 93.90868816693131,
    "ttft": 1715270.9123257294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 284495,
    "finished_requests": 93299,
    "scheduler_time": 41.29774156534074
}
#Debug simulation 
Total elapsed time: 6.92242216411978. Arrivals time: 0.3494916995987296 Scheduler time: 6.411026733461767 Scheduler overhead time: 0.05766913387924433 Adapter cache time: 0.01887800730764866 Engine time: 0.058432592544704676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190532798 . Total output tokens: 167610224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.658284476026893,
    "estimated_duration": 3600.01840902451,
    "input_throughput": 6141.4138729337465,
    "output_throughput": 5335.498271855983,
    "total_throughput": 11476.912144789729,
    "itl": 82.46478235315706,
    "ttft": 1755686.2489242626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 284495,
    "finished_requests": 89269,
    "scheduler_time": 31.958082626799975
}
#Debug simulation 
Total elapsed time: 6.658446755260229. Arrivals time: 0.339548135176301 Scheduler time: 6.13718172442168 Scheduler overhead time: 0.06393035082146525 Adapter cache time: 0.022171305026859045 Engine time: 0.06554954964667559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.117576566059142,
    "estimated_duration": 3600.0377596257003,
    "input_throughput": 6611.375932477618,
    "output_throughput": 5742.8353201910695,
    "total_throughput": 12354.211252668687,
    "itl": 100.69840326637511,
    "ttft": 1688354.26624223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 284045,
    "finished_requests": 96056,
    "scheduler_time": 46.45931618222866
}
#Debug simulation 
Total elapsed time: 7.117723093833774. Arrivals time: 0.34837470203638077 Scheduler time: 6.618740948848426 Scheduler overhead time: 0.05421992298215628 Adapter cache time: 0.015865548513829708 Engine time: 0.055204703006893396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.974847188685089,
    "estimated_duration": 3600.0172705603977,
    "input_throughput": 6463.600936108235,
    "output_throughput": 5610.570028419849,
    "total_throughput": 12074.170964528084,
    "itl": 93.6448976363768,
    "ttft": 1709371.070499817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489668,
    "arrivals": 284045,
    "finished_requests": 93836,
    "scheduler_time": 41.632862394871196
}
#Debug simulation 
Total elapsed time: 6.975006642751396. Arrivals time: 0.3461034460924566 Scheduler time: 6.468642293475568 Scheduler overhead time: 0.05737953260540962 Adapter cache time: 0.01730041392147541 Engine time: 0.05871784780174494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.073865632060915,
    "estimated_duration": 3600.0173352701077,
    "input_throughput": 6166.997803729811,
    "output_throughput": 5358.905306091399,
    "total_throughput": 11525.90310982121,
    "itl": 82.31597793791602,
    "ttft": 1749803.9008676466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 284045,
    "finished_requests": 89555,
    "scheduler_time": 32.17078659570147
}
#Debug simulation 
Total elapsed time: 7.073983415029943. Arrivals time: 0.34491700446233153 Scheduler time: 6.548212070949376 Scheduler overhead time: 0.06387510523200035 Adapter cache time: 0.021196768153458834 Engine time: 0.06582966540008783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.947105762083083,
    "estimated_duration": 3600.0556150689968,
    "input_throughput": 6463.629312447172,
    "output_throughput": 5610.699711263455,
    "total_throughput": 12074.329023710627,
    "itl": 93.64488195142732,
    "ttft": 1709488.328636615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 284045,
    "finished_requests": 93838,
    "scheduler_time": 41.63464327206253
}
#Debug simulation 
Total elapsed time: 6.947245425079018. Arrivals time: 0.3366980920545757 Scheduler time: 6.4506626133807 Scheduler overhead time: 0.05733579071238637 Adapter cache time: 0.017116249538958073 Engine time: 0.05850491765886545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.702816929202527,
    "estimated_duration": 3600.0579135407884,
    "input_throughput": 6166.768294614677,
    "output_throughput": 5358.840180719615,
    "total_throughput": 11525.608475334293,
    "itl": 82.3166320377799,
    "ttft": 1749876.0624113919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 284045,
    "finished_requests": 89553,
    "scheduler_time": 32.17231193328083
}
#Debug simulation 
Total elapsed time: 6.702967113349587. Arrivals time: 0.3238843260332942 Scheduler time: 6.199261207599193 Scheduler overhead time: 0.06356168584898114 Adapter cache time: 0.0208498970605433 Engine time: 0.0654452326707542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.949219993781298,
    "estimated_duration": 3600.031552363259,
    "input_throughput": 6463.575294145658,
    "output_throughput": 5610.547770544073,
    "total_throughput": 12074.123064689731,
    "itl": 93.64581759311461,
    "ttft": 1709397.898353586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 284045,
    "finished_requests": 93836,
    "scheduler_time": 41.635259895119525
}
#Debug simulation 
Total elapsed time: 6.949440797790885. Arrivals time: 0.3496411028318107 Scheduler time: 6.4381504859775305 Scheduler overhead time: 0.057617980055511 Adapter cache time: 0.018061772920191288 Engine time: 0.058907336089760065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190221411 . Total output tokens: 167329595
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.707414174918085,
    "estimated_duration": 3600.0309495856045,
    "input_throughput": 6166.950593176305,
    "output_throughput": 5358.954206274456,
    "total_throughput": 11525.904799450762,
    "itl": 82.31587849874622,
    "ttft": 1749794.0746095062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 284045,
    "finished_requests": 89555,
    "scheduler_time": 32.171336935624694
}
#Debug simulation 
Total elapsed time: 6.70754918595776. Arrivals time: 0.33133659372106194 Scheduler time: 6.196553114335984 Scheduler overhead time: 0.06351087940856814 Adapter cache time: 0.02088120486587286 Engine time: 0.06542247394099832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.077812109608203,
    "estimated_duration": 3600.039271806484,
    "input_throughput": 6629.023518408669,
    "output_throughput": 5773.1192442161755,
    "total_throughput": 12402.142762624844,
    "itl": 100.21095656612933,
    "ttft": 1686039.4697296764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 283764,
    "finished_requests": 96396,
    "scheduler_time": 46.73349555485799
}
#Debug simulation 
Total elapsed time: 7.0780383287929. Arrivals time: 0.3396963872946799 Scheduler time: 6.58806137368083 Scheduler overhead time: 0.05440750904381275 Adapter cache time: 0.01498386263847351 Engine time: 0.05549033312126994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.2200610539875925,
    "estimated_duration": 3600.048328417783,
    "input_throughput": 6475.05557522472,
    "output_throughput": 5641.1703808780685,
    "total_throughput": 12116.22595610279,
    "itl": 93.24675775815264,
    "ttft": 1706986.5125875948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4604923001350833,
    "arrivals": 283764,
    "finished_requests": 94177,
    "scheduler_time": 41.89908221942724
}
#Debug simulation 
Total elapsed time: 7.220176948234439. Arrivals time: 0.3544084229506552 Scheduler time: 6.704910451546311 Scheduler overhead time: 0.057117072865366936 Adapter cache time: 0.017551246099174023 Engine time: 0.059265711810439825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.662507108878344,
    "estimated_duration": 3600.053575322065,
    "input_throughput": 6191.114808063949,
    "output_throughput": 5385.005137948726,
    "total_throughput": 11576.119946012674,
    "itl": 81.97455021730246,
    "ttft": 1749219.0934713923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4729076882544906,
    "arrivals": 283764,
    "finished_requests": 90021,
    "scheduler_time": 32.43519296576305
}
#Debug simulation 
Total elapsed time: 6.66265603620559. Arrivals time: 0.3258148403838277 Scheduler time: 6.157880628481507 Scheduler overhead time: 0.06347810151055455 Adapter cache time: 0.019654502160847187 Engine time: 0.06572676170617342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.922870809677988,
    "estimated_duration": 3600.031122600234,
    "input_throughput": 6475.13207695914,
    "output_throughput": 5641.214841868234,
    "total_throughput": 12116.346918827374,
    "itl": 93.24511308186995,
    "ttft": 1706998.7685124564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4327278806688265,
    "arrivals": 283764,
    "finished_requests": 94178,
    "scheduler_time": 41.8983735817948
}
#Debug simulation 
Total elapsed time: 6.9229871719144285. Arrivals time: 0.3271988620981574 Scheduler time: 6.435822461266071 Scheduler overhead time: 0.05724714370444417 Adapter cache time: 0.01679970556870103 Engine time: 0.05905759194865823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.850261173676699,
    "estimated_duration": 3600.0729434276263,
    "input_throughput": 6191.107888713831,
    "output_throughput": 5385.038388011689,
    "total_throughput": 11576.14627672552,
    "itl": 81.97502338415504,
    "ttft": 1749249.1725723192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46876534472219655,
    "arrivals": 283764,
    "finished_requests": 90021,
    "scheduler_time": 32.43617973515786
}
#Debug simulation 
Total elapsed time: 6.850340953562409. Arrivals time: 0.28786544874310493 Scheduler time: 6.385803205892444 Scheduler overhead time: 0.06279335590079427 Adapter cache time: 0.018698162399232388 Engine time: 0.06536620762199163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.859476124402136,
    "estimated_duration": 3600.037830504325,
    "input_throughput": 6475.034735047348,
    "output_throughput": 5641.187386398939,
    "total_throughput": 12116.222121446286,
    "itl": 93.24479598453165,
    "ttft": 1706958.4282783945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40218701925594397,
    "arrivals": 283764,
    "finished_requests": 94177,
    "scheduler_time": 41.89890526879208
}
#Debug simulation 
Total elapsed time: 6.859594841022044. Arrivals time: 0.2956244070082903 Scheduler time: 6.407170274760574 Scheduler overhead time: 0.056526228319853544 Adapter cache time: 0.015278609935194254 Engine time: 0.05828590504825115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 190063199 . Total output tokens: 167189867
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.610059607308358,
    "estimated_duration": 3600.054930758762,
    "input_throughput": 6191.024422869705,
    "output_throughput": 5384.863668153577,
    "total_throughput": 11575.888091023282,
    "itl": 81.97224274124754,
    "ttft": 1749255.6887730553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4642087668366731,
    "arrivals": 283764,
    "finished_requests": 90018,
    "scheduler_time": 32.43425602446754
}
#Debug simulation 
Total elapsed time: 6.610187242273241. Arrivals time: 0.28794439835473895 Scheduler time: 6.145150757860392 Scheduler overhead time: 0.06296209618449211 Adapter cache time: 0.018695103470236063 Engine time: 0.06553841568529606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.0569373848848045,
    "estimated_duration": 3600.049627509098,
    "input_throughput": 6634.731315225358,
    "output_throughput": 5797.390913869354,
    "total_throughput": 12432.122229094713,
    "itl": 99.9193996282526,
    "ttft": 1646910.9132926383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 264567,
    "finished_requests": 96734,
    "scheduler_time": 47.24468266299471
}
#Debug simulation 
Total elapsed time: 7.057062073145062. Arrivals time: 0.32270301738753915 Scheduler time: 6.573873384390026 Scheduler overhead time: 0.05355979362502694 Adapter cache time: 0.02633228525519371 Engine time: 0.05528977792710066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.972159696277231,
    "estimated_duration": 3600.070522359333,
    "input_throughput": 6529.770418106726,
    "output_throughput": 5704.2438120189345,
    "total_throughput": 12234.01423012566,
    "itl": 92.32445958785779,
    "ttft": 1663206.9513558901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46826444204896656,
    "arrivals": 264567,
    "finished_requests": 95161,
    "scheduler_time": 42.70397809997114
}
#Debug simulation 
Total elapsed time: 6.972286358010024. Arrivals time: 0.29780590161681175 Scheduler time: 6.502768303733319 Scheduler overhead time: 0.05738944560289383 Adapter cache time: 0.028016854543238878 Engine time: 0.05923708342015743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.038718148134649,
    "estimated_duration": 3600.0450234586547,
    "input_throughput": 6293.03546271612,
    "output_throughput": 5500.91148053872,
    "total_throughput": 11793.94694325484,
    "itl": 80.34443207180125,
    "ttft": 1696632.5813179745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 264567,
    "finished_requests": 91725,
    "scheduler_time": 33.47427019650342
}
#Debug simulation 
Total elapsed time: 7.03883113199845. Arrivals time: 0.28862902522087097 Scheduler time: 6.558633788488805 Scheduler overhead time: 0.06430117040872574 Adapter cache time: 0.03036898048594594 Engine time: 0.06647795205935836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.98088304605335,
    "estimated_duration": 3600.011389566258,
    "input_throughput": 6529.919340847502,
    "output_throughput": 5704.5158411219045,
    "total_throughput": 12234.435181969407,
    "itl": 92.32079218805164,
    "ttft": 1663202.4486111056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 264567,
    "finished_requests": 95162,
    "scheduler_time": 42.703890796788585
}
#Debug simulation 
Total elapsed time: 6.981029557995498. Arrivals time: 0.301060251891613 Scheduler time: 6.508850494399667 Scheduler overhead time: 0.05745522445067763 Adapter cache time: 0.02722257887944579 Engine time: 0.059417294804006815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.765559606719762,
    "estimated_duration": 3600.00669578067,
    "input_throughput": 6293.178295071783,
    "output_throughput": 5501.143379319583,
    "total_throughput": 11794.321674391367,
    "itl": 80.34241585974752,
    "ttft": 1696523.7657865891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 264567,
    "finished_requests": 91727,
    "scheduler_time": 33.47348940472325
}
#Debug simulation 
Total elapsed time: 6.765707899816334. Arrivals time: 0.31334754964336753 Scheduler time: 6.260093721095473 Scheduler overhead time: 0.0644176285713911 Adapter cache time: 0.030105712357908487 Engine time: 0.06723409611731768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.24285811977461,
    "estimated_duration": 3600.0993098904314,
    "input_throughput": 6529.860422299147,
    "output_throughput": 5704.590410486494,
    "total_throughput": 12234.45083278564,
    "itl": 92.31775306282127,
    "ttft": 1663168.7643037946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 264567,
    "finished_requests": 95164,
    "scheduler_time": 42.70356965195236
}
#Debug simulation 
Total elapsed time: 7.242982281837612. Arrivals time: 0.30178117007017136 Scheduler time: 6.769539033994079 Scheduler overhead time: 0.057456367649137974 Adapter cache time: 0.02753681829199195 Engine time: 0.059473063331097364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177368541 . Total output tokens: 155936618
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.766934422310442,
    "estimated_duration": 3600.085776144543,
    "input_throughput": 6293.144499535493,
    "output_throughput": 5501.105871207679,
    "total_throughput": 11794.250370743173,
    "itl": 80.34449799977038,
    "ttft": 1696481.0593676665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337263,
    "arrivals": 264567,
    "finished_requests": 91729,
    "scheduler_time": 33.474724236006935
}
#Debug simulation 
Total elapsed time: 6.767147170379758. Arrivals time: 0.28882394125685096 Scheduler time: 6.285227857995778 Scheduler overhead time: 0.06446788785979152 Adapter cache time: 0.03053158149123192 Engine time: 0.06760788802057505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.199729808140546,
    "estimated_duration": 3600.0726509185633,
    "input_throughput": 6855.293321289774,
    "output_throughput": 5939.08819994079,
    "total_throughput": 12794.381521230565,
    "itl": 97.30842670718451,
    "ttft": 1622984.5863048467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 262719,
    "finished_requests": 99528,
    "scheduler_time": 48.41392105471147
}
#Debug simulation 
Total elapsed time: 7.199882246088237. Arrivals time: 0.31153459707275033 Scheduler time: 6.726481903810054 Scheduler overhead time: 0.0548228332772851 Adapter cache time: 0.024072730913758278 Engine time: 0.05706806154921651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.095416355878115,
    "estimated_duration": 3600.012747152347,
    "input_throughput": 6740.755854043943,
    "output_throughput": 5835.6187812439875,
    "total_throughput": 12576.374635287932,
    "itl": 90.02321105786893,
    "ttft": 1641562.1105484776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489666,
    "arrivals": 262719,
    "finished_requests": 97848,
    "scheduler_time": 43.68252838111412
}
#Debug simulation 
Total elapsed time: 7.0955740148201585. Arrivals time: 0.3000482125207782 Scheduler time: 6.622861109673977 Scheduler overhead time: 0.05877556838095188 Adapter cache time: 0.025472549255937338 Engine time: 0.06072017876431346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.909825815353543,
    "estimated_duration": 3600.0089719423336,
    "input_throughput": 6482.166067327736,
    "output_throughput": 5615.178228040207,
    "total_throughput": 12097.344295367942,
    "itl": 78.4719800886918,
    "ttft": 1678434.2601025053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 262719,
    "finished_requests": 94087,
    "scheduler_time": 34.1317395438237
}
#Debug simulation 
Total elapsed time: 6.909956165123731. Arrivals time: 0.30539311142638326 Scheduler time: 6.4108070195652544 Scheduler overhead time: 0.06617750274017453 Adapter cache time: 0.028080491349101067 Engine time: 0.06840957468375564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.091443742159754,
    "estimated_duration": 3600.0663015374203,
    "input_throughput": 6740.910574240365,
    "output_throughput": 5835.914463860776,
    "total_throughput": 12576.82503810114,
    "itl": 90.02129072536965,
    "ttft": 1641538.2949527998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4391118016093971,
    "arrivals": 262719,
    "finished_requests": 97854,
    "scheduler_time": 43.68309483193043
}
#Debug simulation 
Total elapsed time: 7.0915959272533655. Arrivals time: 0.3004586868919432 Scheduler time: 6.618832696229219 Scheduler overhead time: 0.058357883244752884 Adapter cache time: 0.025821720249950886 Engine time: 0.06056564021855593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.874192600138485,
    "estimated_duration": 3600.0544114426243,
    "input_throughput": 6482.079250199109,
    "output_throughput": 5615.017632997286,
    "total_throughput": 12097.096883196395,
    "itl": 78.47194480847665,
    "ttft": 1678384.7484030132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 262719,
    "finished_requests": 94086,
    "scheduler_time": 34.13369767240956
}
#Debug simulation 
Total elapsed time: 6.87431984115392. Arrivals time: 0.29636202612891793 Scheduler time: 6.385080485139042 Scheduler overhead time: 0.06583438208326697 Adapter cache time: 0.027751803398132324 Engine time: 0.0682101552374661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.080939243081957,
    "estimated_duration": 3600.098294681473,
    "input_throughput": 6740.815948234306,
    "output_throughput": 5835.894545167937,
    "total_throughput": 12576.710493402243,
    "itl": 90.0229004265128,
    "ttft": 1641501.9744638554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 262719,
    "finished_requests": 97853,
    "scheduler_time": 43.68418484068128
}
#Debug simulation 
Total elapsed time: 7.081091887317598. Arrivals time: 0.29975564824417233 Scheduler time: 6.608113700989634 Scheduler overhead time: 0.05876131355762482 Adapter cache time: 0.02596340235322714 Engine time: 0.06094200722873211 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 176096664 . Total output tokens: 154814895
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.909178924746811,
    "estimated_duration": 3600.0410167712116,
    "input_throughput": 6482.201144732971,
    "output_throughput": 5615.171856605734,
    "total_throughput": 12097.373001338707,
    "itl": 78.47118878934693,
    "ttft": 1678379.3566745238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 262719,
    "finished_requests": 94088,
    "scheduler_time": 34.13392150670129
}
#Debug simulation 
Total elapsed time: 6.909326447639614. Arrivals time: 0.30580657813698053 Scheduler time: 6.410214871633798 Scheduler overhead time: 0.06569774122908711 Adapter cache time: 0.028232363052666187 Engine time: 0.06819115299731493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.345390825998038,
    "estimated_duration": 3600.0728202137298,
    "input_throughput": 6870.676021084355,
    "output_throughput": 6004.977143411382,
    "total_throughput": 12875.653164495736,
    "itl": 96.21335553927311,
    "ttft": 1617414.9232650413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 261754,
    "finished_requests": 99903,
    "scheduler_time": 48.87259675976197
}
#Debug simulation 
Total elapsed time: 7.345547199714929. Arrivals time: 0.3172817090526223 Scheduler time: 6.867229836527258 Scheduler overhead time: 0.05552629753947258 Adapter cache time: 0.021942798513919115 Engine time: 0.0573707208968699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.499416267964989,
    "estimated_duration": 3600.0691457868074,
    "input_throughput": 6744.826284355469,
    "output_throughput": 5891.599339091151,
    "total_throughput": 12636.425623446621,
    "itl": 89.08159251003396,
    "ttft": 1635865.5872391008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489668,
    "arrivals": 261754,
    "finished_requests": 98046,
    "scheduler_time": 44.01922580295519
}
#Debug simulation 
Total elapsed time: 7.499545065220445. Arrivals time: 0.3177278321236372 Scheduler time: 7.009333572816104 Scheduler overhead time: 0.059311825316399336 Adapter cache time: 0.023427669890224934 Engine time: 0.06170585611835122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.994516556151211,
    "estimated_duration": 3600.066452831844,
    "input_throughput": 6479.934552777446,
    "output_throughput": 5661.860764810743,
    "total_throughput": 12141.795317588188,
    "itl": 77.81551838703345,
    "ttft": 1673437.1262757592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 261754,
    "finished_requests": 94169,
    "scheduler_time": 34.34394013314036
}
#Debug simulation 
Total elapsed time: 6.994668849278241. Arrivals time: 0.3099646824412048 Scheduler time: 6.491317859385163 Scheduler overhead time: 0.0667528067715466 Adapter cache time: 0.026093542110174894 Engine time: 0.06908102752640843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.261053243651986,
    "estimated_duration": 3600.0042564185514,
    "input_throughput": 6744.815080901101,
    "output_throughput": 5891.519145341282,
    "total_throughput": 12636.334226242383,
    "itl": 89.08191626624854,
    "ttft": 1635846.0461135944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 261754,
    "finished_requests": 98042,
    "scheduler_time": 44.01873931136403
}
#Debug simulation 
Total elapsed time: 7.261210185009986. Arrivals time: 0.3192579559981823 Scheduler time: 6.768732108641416 Scheduler overhead time: 0.05963318329304457 Adapter cache time: 0.02353185834363103 Engine time: 0.061709937173873186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.282033155206591,
    "estimated_duration": 3600.04090789673,
    "input_throughput": 6479.654703042934,
    "output_throughput": 5661.727886283421,
    "total_throughput": 12141.382589326355,
    "itl": 77.81563184136316,
    "ttft": 1673450.9101917879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 261754,
    "finished_requests": 94164,
    "scheduler_time": 34.34218218702813
}
#Debug simulation 
Total elapsed time: 7.282151653897017. Arrivals time: 0.3138212743215263 Scheduler time: 6.774999278131872 Scheduler overhead time: 0.06696198647841811 Adapter cache time: 0.02593533881008625 Engine time: 0.0689759161323309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.2141965590417385,
    "estimated_duration": 3600.0204451577906,
    "input_throughput": 6745.111415314666,
    "output_throughput": 5891.751539502809,
    "total_throughput": 12636.862954817476,
    "itl": 89.083405436287,
    "ttft": 1635882.9417475031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 261754,
    "finished_requests": 98047,
    "scheduler_time": 44.020206335966854
}
#Debug simulation 
Total elapsed time: 7.214411687105894. Arrivals time: 0.315299229696393 Scheduler time: 6.726126857101917 Scheduler overhead time: 0.059767977334558964 Adapter cache time: 0.023622183594852686 Engine time: 0.06159381614997983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175472810 . Total output tokens: 154240655
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.0421938402578235,
    "estimated_duration": 3600.0225690411025,
    "input_throughput": 6479.687711017144,
    "output_throughput": 5661.756727661028,
    "total_throughput": 12141.444438678172,
    "itl": 77.81360240853604,
    "ttft": 1673423.2798236497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337263,
    "arrivals": 261754,
    "finished_requests": 94164,
    "scheduler_time": 34.342164558619324
}
#Debug simulation 
Total elapsed time: 7.042315368074924. Arrivals time: 0.30986645678058267 Scheduler time: 6.538625749759376 Scheduler overhead time: 0.06709579797461629 Adapter cache time: 0.025825513992458582 Engine time: 0.06919720489531755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.384951732121408,
    "estimated_duration": 3600.0932826153535,
    "input_throughput": 7021.4808383065965,
    "output_throughput": 6062.539019584603,
    "total_throughput": 13084.0198578912,
    "itl": 95.22750282719402,
    "ttft": 1598412.9244562744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 261307,
    "finished_requests": 101586,
    "scheduler_time": 49.42582172298672
}
#Debug simulation 
Total elapsed time: 7.385081602726132. Arrivals time: 0.32156428042799234 Scheduler time: 6.901887551881373 Scheduler overhead time: 0.05620722891762853 Adapter cache time: 0.02082265540957451 Engine time: 0.05806894460693002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.27428764430806,
    "estimated_duration": 3600.040613519171,
    "input_throughput": 6879.055171489143,
    "output_throughput": 5946.1731958280325,
    "total_throughput": 12825.228367317175,
    "itl": 88.27512909143144,
    "ttft": 1617583.7012256475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489666,
    "arrivals": 261307,
    "finished_requests": 99544,
    "scheduler_time": 44.522535637469915
}
#Debug simulation 
Total elapsed time: 7.2744232141412795. Arrivals time: 0.3392055402509868 Scheduler time: 6.762880535330623 Scheduler overhead time: 0.059718912467360497 Adapter cache time: 0.022160277236253023 Engine time: 0.06219080090522766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.025910294149071,
    "estimated_duration": 3600.009622153979,
    "input_throughput": 6592.360713136149,
    "output_throughput": 5707.519744823939,
    "total_throughput": 12299.880457960087,
    "itl": 77.21623176650525,
    "ttft": 1657522.5742504091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 261307,
    "finished_requests": 95480,
    "scheduler_time": 34.76106317162902
}
#Debug simulation 
Total elapsed time: 7.026038314215839. Arrivals time: 0.33205212093889713 Scheduler time: 6.501933715771884 Scheduler overhead time: 0.06678589060902596 Adapter cache time: 0.024261340964585543 Engine time: 0.06937555223703384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.272455932572484,
    "estimated_duration": 3600.0922416870535,
    "input_throughput": 6878.522364859064,
    "output_throughput": 5945.855428962293,
    "total_throughput": 12824.377793821357,
    "itl": 88.27556114090146,
    "ttft": 1617600.9388225654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4391118016093969,
    "arrivals": 261307,
    "finished_requests": 99539,
    "scheduler_time": 44.52309665666015
}
#Debug simulation 
Total elapsed time: 7.27258971799165. Arrivals time: 0.3399593587964773 Scheduler time: 6.761136548127979 Scheduler overhead time: 0.059489828534424305 Adapter cache time: 0.02206231513991952 Engine time: 0.061682894825935364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.02710925694555,
    "estimated_duration": 3600.0732472917493,
    "input_throughput": 6592.105596143932,
    "output_throughput": 5707.398596808292,
    "total_throughput": 12299.504192952225,
    "itl": 77.21616044405457,
    "ttft": 1657531.6866515935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 261307,
    "finished_requests": 95479,
    "scheduler_time": 34.76074892569344
}
#Debug simulation 
Total elapsed time: 7.027238185983151. Arrivals time: 0.3290106006897986 Scheduler time: 6.50594393350184 Scheduler overhead time: 0.06704771937802434 Adapter cache time: 0.02432609023526311 Engine time: 0.0692582493647933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.249373387079686,
    "estimated_duration": 3600.0859292971545,
    "input_throughput": 6879.102745426676,
    "output_throughput": 5946.129737014141,
    "total_throughput": 12825.232482440817,
    "itl": 88.27117616249916,
    "ttft": 1617569.3058497412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 261307,
    "finished_requests": 99547,
    "scheduler_time": 44.52233992880544
}
#Debug simulation 
Total elapsed time: 7.249502282124013. Arrivals time: 0.33680458134040236 Scheduler time: 6.740932107903063 Scheduler overhead time: 0.059781627263873816 Adapter cache time: 0.021849979180842638 Engine time: 0.06196444155648351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 175139655 . Total output tokens: 153960539
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.024651841260493,
    "estimated_duration": 3600.0733091983134,
    "input_throughput": 6592.241035581831,
    "output_throughput": 5707.418220484949,
    "total_throughput": 12299.659256066781,
    "itl": 77.21609915164407,
    "ttft": 1657498.24615025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 261307,
    "finished_requests": 95479,
    "scheduler_time": 34.760658997637435
}
#Debug simulation 
Total elapsed time: 7.024782345164567. Arrivals time: 0.3342093429528177 Scheduler time: 6.499096138868481 Scheduler overhead time: 0.06658644368872046 Adapter cache time: 0.0244436077773571 Engine time: 0.06891850661486387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.414513415191323,
    "estimated_duration": 3600.0668847223187,
    "input_throughput": 6963.257573458546,
    "output_throughput": 6077.475974918616,
    "total_throughput": 13040.733548377162,
    "itl": 94.93209662999226,
    "ttft": 1607125.1030723627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4165817690128457,
    "arrivals": 261056,
    "finished_requests": 101065,
    "scheduler_time": 49.37316248008886
}
#Debug simulation 
Total elapsed time: 7.414647130295634. Arrivals time: 0.35869120387360454 Scheduler time: 6.894323585554957 Scheduler overhead time: 0.05659095011651516 Adapter cache time: 0.020034179091453552 Engine time: 0.058398088440299034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.300160665996373,
    "estimated_duration": 3600.0275451761736,
    "input_throughput": 6820.190871289143,
    "output_throughput": 5955.663319501287,
    "total_throughput": 12775.85419079043,
    "itl": 88.06146927114497,
    "ttft": 1627181.554474076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46049230013508324,
    "arrivals": 261056,
    "finished_requests": 99039,
    "scheduler_time": 44.402719637878555
}
#Debug simulation 
Total elapsed time: 7.3002911997027695. Arrivals time: 0.321157643571496 Scheduler time: 6.807152756024152 Scheduler overhead time: 0.06002437323331833 Adapter cache time: 0.021306995768100023 Engine time: 0.06226454908028245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.042942208703607,
    "estimated_duration": 3600.01972166319,
    "input_throughput": 6545.014144843184,
    "output_throughput": 5716.612571914519,
    "total_throughput": 12261.626716757703,
    "itl": 77.04869113839432,
    "ttft": 1666976.6787918594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47290768825449053,
    "arrivals": 261056,
    "finished_requests": 94964,
    "scheduler_time": 34.570013125569936
}
#Debug simulation 
Total elapsed time: 7.043071245774627. Arrivals time: 0.343202434014529 Scheduler time: 6.508258428424597 Scheduler overhead time: 0.06708352454006672 Adapter cache time: 0.023530873004347086 Engine time: 0.06927405623719096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.3047319441102445,
    "estimated_duration": 3600.03252815057,
    "input_throughput": 6820.181708917349,
    "output_throughput": 5955.65563153802,
    "total_throughput": 12775.83734045537,
    "itl": 88.06004965744191,
    "ttft": 1627062.3217438746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43133965969551363,
    "arrivals": 261056,
    "finished_requests": 99040,
    "scheduler_time": 44.40299644346173
}
#Debug simulation 
Total elapsed time: 7.304860949981958. Arrivals time: 0.3556095617823303 Scheduler time: 6.778054367750883 Scheduler overhead time: 0.059990279376506805 Adapter cache time: 0.0209366949275136 Engine time: 0.06187891634181142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.145363645162433,
    "estimated_duration": 3600.067839475407,
    "input_throughput": 6544.958887061817,
    "output_throughput": 5716.628385258125,
    "total_throughput": 12261.587272319943,
    "itl": 77.04809230959778,
    "ttft": 1666953.0470939628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4685582275455818,
    "arrivals": 261056,
    "finished_requests": 94964,
    "scheduler_time": 34.56945745550195
}
#Debug simulation 
Total elapsed time: 7.145603391807526. Arrivals time: 0.4197269445285201 Scheduler time: 6.531603653449565 Scheduler overhead time: 0.06752120610326529 Adapter cache time: 0.02465251600369811 Engine time: 0.0700530856847763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.358293583616614,
    "estimated_duration": 3600.025697860525,
    "input_throughput": 6820.152149078143,
    "output_throughput": 5955.593042777957,
    "total_throughput": 12775.7451918561,
    "itl": 88.05897668611702,
    "ttft": 1627108.9984275883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40218701925594397,
    "arrivals": 261056,
    "finished_requests": 99038,
    "scheduler_time": 44.401932196381466
}
#Debug simulation 
Total elapsed time: 7.358457881957293. Arrivals time: 0.39357865229249 Scheduler time: 6.789955916348845 Scheduler overhead time: 0.06071209954097867 Adapter cache time: 0.022587931249290705 Engine time: 0.06291203200817108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174989276 . Total output tokens: 153819412
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.10979030514136,
    "estimated_duration": 3600.0272176538742,
    "input_throughput": 6544.826073663683,
    "output_throughput": 5716.495391779737,
    "total_throughput": 12261.32146544342,
    "itl": 77.04837111725804,
    "ttft": 1666964.217738392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46420876683667306,
    "arrivals": 261056,
    "finished_requests": 94961,
    "scheduler_time": 34.571180155181246
}
#Debug simulation 
Total elapsed time: 7.10993520822376. Arrivals time: 0.33955781953409314 Scheduler time: 6.576714518480003 Scheduler overhead time: 0.06763409404084086 Adapter cache time: 0.023966407869011164 Engine time: 0.07007114449515939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.58123331097886,
    "estimated_duration": 3600.0443806758276,
    "input_throughput": 7006.648900051814,
    "output_throughput": 6140.451245173293,
    "total_throughput": 13147.100145225106,
    "itl": 94.21377991737334,
    "ttft": 1586106.7407338498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 258928,
    "finished_requests": 102115,
    "scheduler_time": 50.11673599245762
}
#Debug simulation 
Total elapsed time: 7.58140668598935. Arrivals time: 0.34478639578446746 Scheduler time: 7.070097382180393 Scheduler overhead time: 0.057338721584528685 Adapter cache time: 0.023221994750201702 Engine time: 0.058996138628572226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.43589144712314,
    "estimated_duration": 3600.0779790559195,
    "input_throughput": 6870.427569595656,
    "output_throughput": 6022.779263710834,
    "total_throughput": 12893.20683330649,
    "itl": 87.27037192286247,
    "ttft": 1604314.0754929923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489666,
    "arrivals": 258928,
    "finished_requests": 100120,
    "scheduler_time": 45.1495291012015
}
#Debug simulation 
Total elapsed time: 7.436025706119835. Arrivals time: 0.3544273586012423 Scheduler time: 6.904072319623083 Scheduler overhead time: 0.061107974499464035 Adapter cache time: 0.024730476550757885 Engine time: 0.06298516085371375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.1881251791492105,
    "estimated_duration": 3600.053876431621,
    "input_throughput": 6598.0837552199255,
    "output_throughput": 5785.357307109067,
    "total_throughput": 12383.441062328991,
    "itl": 76.24757238598126,
    "ttft": 1644108.6419882618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 258928,
    "finished_requests": 96174,
    "scheduler_time": 35.26137216586926
}
#Debug simulation 
Total elapsed time: 7.18825078709051. Arrivals time: 0.33977833064273 Scheduler time: 6.651961992494762 Scheduler overhead time: 0.06820160942152143 Adapter cache time: 0.02581425290554762 Engine time: 0.07032371312379837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.714176569133997,
    "estimated_duration": 3600.056666197393,
    "input_throughput": 6870.601852533116,
    "output_throughput": 6022.890751578831,
    "total_throughput": 12893.492604111947,
    "itl": 87.27025546398585,
    "ttft": 1604214.9371059733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 258928,
    "finished_requests": 100122,
    "scheduler_time": 45.15240230260539
}
#Debug simulation 
Total elapsed time: 7.71431151824072. Arrivals time: 0.35361727699637413 Scheduler time: 7.183184262830764 Scheduler overhead time: 0.06099020270630717 Adapter cache time: 0.02465904038399458 Engine time: 0.06308062840253115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.190888213925064,
    "estimated_duration": 3600.0132687368373,
    "input_throughput": 6598.128736435049,
    "output_throughput": 5785.489509406174,
    "total_throughput": 12383.618245841222,
    "itl": 76.24996347075171,
    "ttft": 1644040.2079170954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 258928,
    "finished_requests": 96174,
    "scheduler_time": 35.26103749575997
}
#Debug simulation 
Total elapsed time: 7.191030067857355. Arrivals time: 0.3789516887627542 Scheduler time: 6.614938331767917 Scheduler overhead time: 0.06822326080873609 Adapter cache time: 0.026279767975211143 Engine time: 0.07051980262622237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.465827594976872,
    "estimated_duration": 3600.0442248311115,
    "input_throughput": 6870.623096626089,
    "output_throughput": 6022.955176617923,
    "total_throughput": 12893.578273244011,
    "itl": 87.2691138324303,
    "ttft": 1604219.6213277422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 258928,
    "finished_requests": 100122,
    "scheduler_time": 45.15191784456256
}
#Debug simulation 
Total elapsed time: 7.465976425912231. Arrivals time: 0.39434076100587845 Scheduler time: 6.894098780117929 Scheduler overhead time: 0.06121179321780801 Adapter cache time: 0.024362431839108467 Engine time: 0.06307419203221798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173556764 . Total output tokens: 152566588
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.545864419080317,
    "estimated_duration": 3600.027919225019,
    "input_throughput": 6598.040496617414,
    "output_throughput": 5785.470409486054,
    "total_throughput": 12383.51090610347,
    "itl": 76.24654428103612,
    "ttft": 1644108.5153850238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337263,
    "arrivals": 258928,
    "finished_requests": 96175,
    "scheduler_time": 35.260284475492234
}
#Debug simulation 
Total elapsed time: 7.545984521973878. Arrivals time: 0.33432189794257283 Scheduler time: 7.015433385502547 Scheduler overhead time: 0.0680992966517806 Adapter cache time: 0.02573006972670555 Engine time: 0.07032553432509303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.710211261641234,
    "estimated_duration": 3600.086754257436,
    "input_throughput": 7110.912249468918,
    "output_throughput": 6212.483344616834,
    "total_throughput": 13323.395594085752,
    "itl": 92.96376800401467,
    "ttft": 1577530.431997157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 257941,
    "finished_requests": 103432,
    "scheduler_time": 50.69447908318587
}
#Debug simulation 
Total elapsed time: 7.710349308792502. Arrivals time: 0.3636752348393202 Scheduler time: 7.179534069728106 Scheduler overhead time: 0.05790475616231561 Adapter cache time: 0.022018515970557928 Engine time: 0.05991493118926883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.525877628941089,
    "estimated_duration": 3600.0302145509436,
    "input_throughput": 6969.29400719755,
    "output_throughput": 6093.616356699489,
    "total_throughput": 13062.91036389704,
    "itl": 86.14565755994832,
    "ttft": 1597457.8425880184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 257941,
    "finished_requests": 101391,
    "scheduler_time": 45.662855349250606
}
#Debug simulation 
Total elapsed time: 7.526036879047751. Arrivals time: 0.3515324154868722 Scheduler time: 6.998166019096971 Scheduler overhead time: 0.061726595275104046 Adapter cache time: 0.021847726311534643 Engine time: 0.06361797358840704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.248559939209372,
    "estimated_duration": 3600.0718783920847,
    "input_throughput": 6672.958155138155,
    "output_throughput": 5840.476721090077,
    "total_throughput": 12513.434876228233,
    "itl": 75.41695840733817,
    "ttft": 1638610.93324134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 257941,
    "finished_requests": 97094,
    "scheduler_time": 35.591421527461414
}
#Debug simulation 
Total elapsed time: 7.248733422253281. Arrivals time: 0.3471379308030009 Scheduler time: 6.7049525659531355 Scheduler overhead time: 0.06901140604168177 Adapter cache time: 0.023509275633841753 Engine time: 0.0715489280410111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.4893229571171105,
    "estimated_duration": 3600.0771029382345,
    "input_throughput": 6969.656005289863,
    "output_throughput": 6093.801986100515,
    "total_throughput": 13063.457991390378,
    "itl": 86.14277127736997,
    "ttft": 1597428.3205269314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 257941,
    "finished_requests": 101398,
    "scheduler_time": 45.662064427520605
}
#Debug simulation 
Total elapsed time: 7.489532602950931. Arrivals time: 0.34844767255708575 Scheduler time: 6.964832837227732 Scheduler overhead time: 0.06151844095438719 Adapter cache time: 0.022052906453609467 Engine time: 0.06365253264084458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.267532200086862,
    "estimated_duration": 3600.039488225865,
    "input_throughput": 6672.896249768254,
    "output_throughput": 5840.5028802508605,
    "total_throughput": 12513.399130019116,
    "itl": 75.418482761993,
    "ttft": 1638440.7618273539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 257941,
    "finished_requests": 97092,
    "scheduler_time": 35.5917890465187
}
#Debug simulation 
Total elapsed time: 7.267664818093181. Arrivals time: 0.3497016914188862 Scheduler time: 6.720951770432293 Scheduler overhead time: 0.06891218619421124 Adapter cache time: 0.02395377866923809 Engine time: 0.07143618585541844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.513616237323731,
    "estimated_duration": 3600.058632038166,
    "input_throughput": 6969.377325334075,
    "output_throughput": 6093.522423433529,
    "total_throughput": 13062.899748767604,
    "itl": 86.14107651292316,
    "ttft": 1597456.8656320053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 257941,
    "finished_requests": 101393,
    "scheduler_time": 45.66218299103967
}
#Debug simulation 
Total elapsed time: 7.513841101899743. Arrivals time: 0.35677587054669857 Scheduler time: 6.980414309538901 Scheduler overhead time: 0.06181031744927168 Adapter cache time: 0.02198463724926114 Engine time: 0.06369999423623085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172932132 . Total output tokens: 152006803
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.276358782779425,
    "estimated_duration": 3600.0180714466946,
    "input_throughput": 6672.898169743452,
    "output_throughput": 5840.322349146108,
    "total_throughput": 12513.22051888956,
    "itl": 75.41904220950777,
    "ttft": 1638464.232630828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 257941,
    "finished_requests": 97090,
    "scheduler_time": 35.59054659121772
}
#Debug simulation 
Total elapsed time: 7.276518094819039. Arrivals time: 0.3313618996180594 Scheduler time: 6.748775705695152 Scheduler overhead time: 0.06878734612837434 Adapter cache time: 0.023782554548233747 Engine time: 0.07121867360547185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.682322251144797,
    "estimated_duration": 3600.0597369664542,
    "input_throughput": 7180.480572186926,
    "output_throughput": 6270.19262158714,
    "total_throughput": 13450.673193774066,
    "itl": 92.3488010317492,
    "ttft": 1566282.6575645567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 257455,
    "finished_requests": 104579,
    "scheduler_time": 51.29500814181625
}
#Debug simulation 
Total elapsed time: 7.682459921110421. Arrivals time: 0.3600745848380029 Scheduler time: 7.154690486844629 Scheduler overhead time: 0.058470805175602436 Adapter cache time: 0.020914054475724697 Engine time: 0.06081948662176728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.907625128980726,
    "estimated_duration": 3600.038783033938,
    "input_throughput": 7036.719748516439,
    "output_throughput": 6140.597458055662,
    "total_throughput": 13177.317206572101,
    "itl": 85.65025934991662,
    "ttft": 1587079.2047200508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46826444204896656,
    "arrivals": 257455,
    "finished_requests": 102414,
    "scheduler_time": 46.13285856168045
}
#Debug simulation 
Total elapsed time: 7.90772052295506. Arrivals time: 0.35032276390120387 Scheduler time: 7.381626113783568 Scheduler overhead time: 0.061763495206832886 Adapter cache time: 0.020895130932331085 Engine time: 0.06393741490319371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.274057731963694,
    "estimated_duration": 3600.0310437581825,
    "input_throughput": 6730.248352166013,
    "output_throughput": 5880.573734691946,
    "total_throughput": 12610.82208685796,
    "itl": 75.07702813179351,
    "ttft": 1629323.2608818877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 257455,
    "finished_requests": 98021,
    "scheduler_time": 35.95717016679329
}
#Debug simulation 
Total elapsed time: 7.2742061340250075. Arrivals time: 0.35760343400761485 Scheduler time: 6.720100629609078 Scheduler overhead time: 0.06929851789027452 Adapter cache time: 0.02292026486247778 Engine time: 0.07169688679277897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.54473148426041,
    "estimated_duration": 3600.0427388430476,
    "input_throughput": 7036.8084041527945,
    "output_throughput": 6140.926539973456,
    "total_throughput": 13177.73494412625,
    "itl": 85.64733518190128,
    "ttft": 1587003.3047808905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939695,
    "arrivals": 257455,
    "finished_requests": 102416,
    "scheduler_time": 46.13225837227321
}
#Debug simulation 
Total elapsed time: 7.54487288184464. Arrivals time: 0.34615765744820237 Scheduler time: 7.022816675249487 Scheduler overhead time: 0.06179602909833193 Adapter cache time: 0.020467996131628752 Engine time: 0.06442772643640637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.320060072932392,
    "estimated_duration": 3600.0025571845104,
    "input_throughput": 6730.139663830861,
    "output_throughput": 5880.650545025421,
    "total_throughput": 12610.790208856282,
    "itl": 75.07720917614363,
    "ttft": 1629238.7038376254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 257455,
    "finished_requests": 98020,
    "scheduler_time": 35.95627999867414
}
#Debug simulation 
Total elapsed time: 7.32020540907979. Arrivals time: 0.3518642457202077 Scheduler time: 6.771615664009005 Scheduler overhead time: 0.06952393427491188 Adapter cache time: 0.022662621922791004 Engine time: 0.07172329304739833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.909721728879958,
    "estimated_duration": 3600.0856382052884,
    "input_throughput": 7036.79205048828,
    "output_throughput": 6141.058080779831,
    "total_throughput": 13177.850131268111,
    "itl": 85.64749897778353,
    "ttft": 1586996.1974570365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 257455,
    "finished_requests": 102418,
    "scheduler_time": 46.13261935201014
}
#Debug simulation 
Total elapsed time: 7.909847977105528. Arrivals time: 0.36050262954086065 Scheduler time: 7.372926615644246 Scheduler overhead time: 0.062099166214466095 Adapter cache time: 0.020553034730255604 Engine time: 0.0645158370025456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172626489 . Total output tokens: 151701954
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.264527655206621,
    "estimated_duration": 3600.007394521517,
    "input_throughput": 6730.292286863574,
    "output_throughput": 5880.544865606794,
    "total_throughput": 12610.837152470369,
    "itl": 75.07876791872143,
    "ttft": 1629317.5605309168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 257455,
    "finished_requests": 98020,
    "scheduler_time": 35.95809764014005
}
#Debug simulation 
Total elapsed time: 7.264660485088825. Arrivals time: 0.3461730759590864 Scheduler time: 6.720963172148913 Scheduler overhead time: 0.06922183884307742 Adapter cache time: 0.023285165429115295 Engine time: 0.07236898876726627 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.735879824962467,
    "estimated_duration": 3600.0131392550047,
    "input_throughput": 7218.029211242656,
    "output_throughput": 6286.836498792241,
    "total_throughput": 13504.865710034897,
    "itl": 91.95557538183255,
    "ttft": 1559173.7380565705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 257175,
    "finished_requests": 104902,
    "scheduler_time": 51.417415031410556
}
#Debug simulation 
Total elapsed time: 7.735999884083867. Arrivals time: 0.36799894692376256 Scheduler time: 7.201235441956669 Scheduler overhead time: 0.05880041513592005 Adapter cache time: 0.019838734529912472 Engine time: 0.06064819032326341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.601792456116527,
    "estimated_duration": 3600.022424007208,
    "input_throughput": 7065.880431846185,
    "output_throughput": 6159.041358225608,
    "total_throughput": 13224.921790071792,
    "itl": 85.34363463854947,
    "ttft": 1580496.309484235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 257175,
    "finished_requests": 102714,
    "scheduler_time": 46.2557506895729
}
#Debug simulation 
Total elapsed time: 7.601949664298445. Arrivals time: 0.3623396665789187 Scheduler time: 7.062068301718682 Scheduler overhead time: 0.06239253794774413 Adapter cache time: 0.020723835565149784 Engine time: 0.06500201765447855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.344513122923672,
    "estimated_duration": 3600.069470475385,
    "input_throughput": 6758.914015286186,
    "output_throughput": 5898.096737893265,
    "total_throughput": 12657.010753179451,
    "itl": 74.76078756601055,
    "ttft": 1623134.3894801617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 257175,
    "finished_requests": 98266,
    "scheduler_time": 36.05842475460015
}
#Debug simulation 
Total elapsed time: 7.34472617181018. Arrivals time: 0.35509928362444043 Scheduler time: 6.793032257352024 Scheduler overhead time: 0.06961866654455662 Adapter cache time: 0.02192531106993556 Engine time: 0.07219338789582253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.588488072156906,
    "estimated_duration": 3600.0460843572873,
    "input_throughput": 7065.915658838393,
    "output_throughput": 6159.035045785668,
    "total_throughput": 13224.950704624061,
    "itl": 85.34298197155495,
    "ttft": 1580503.6974002875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 257175,
    "finished_requests": 102715,
    "scheduler_time": 46.256740540461436
}
#Debug simulation 
Total elapsed time: 7.588636536151171. Arrivals time: 0.347854879219085 Scheduler time: 7.065872923471034 Scheduler overhead time: 0.06185838580131531 Adapter cache time: 0.01973939873278141 Engine time: 0.06414608098566532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.324205304030329,
    "estimated_duration": 3600.054312226098,
    "input_throughput": 6758.587479463531,
    "output_throughput": 5898.19823770104,
    "total_throughput": 12656.785717164572,
    "itl": 74.76064445149375,
    "ttft": 1623132.92314683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 257175,
    "finished_requests": 98263,
    "scheduler_time": 36.05666554417394
}
#Debug simulation 
Total elapsed time: 7.324360062833875. Arrivals time: 0.35545514430850744 Scheduler time: 6.77303429832682 Scheduler overhead time: 0.06942482851445675 Adapter cache time: 0.021649299655109644 Engine time: 0.07196576287969947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.595860473811626,
    "estimated_duration": 3600.0275240405354,
    "input_throughput": 7065.85069978859,
    "output_throughput": 6158.970133404553,
    "total_throughput": 13224.820833193144,
    "itl": 85.34065692578858,
    "ttft": 1580457.0278975808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 257175,
    "finished_requests": 102714,
    "scheduler_time": 46.25576332100197
}
#Debug simulation 
Total elapsed time: 7.596071465872228. Arrivals time: 0.3575621107593179 Scheduler time: 7.06218182714656 Scheduler overhead time: 0.062160078436136246 Adapter cache time: 0.020479898434132338 Engine time: 0.06435413425788283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172468384 . Total output tokens: 151569475
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.224990865215659,
    "estimated_duration": 3600.007695455244,
    "input_throughput": 6758.857774309026,
    "output_throughput": 5898.190725204792,
    "total_throughput": 12657.04849951382,
    "itl": 74.7602943399833,
    "ttft": 1623110.0131340493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337263,
    "arrivals": 257175,
    "finished_requests": 98265,
    "scheduler_time": 36.056919366137734
}
#Debug simulation 
Total elapsed time: 7.225139115005732. Arrivals time: 0.306042920332402 Scheduler time: 6.725138960406184 Scheduler overhead time: 0.06913061067461967 Adapter cache time: 0.020421610679477453 Engine time: 0.07186413789168 
