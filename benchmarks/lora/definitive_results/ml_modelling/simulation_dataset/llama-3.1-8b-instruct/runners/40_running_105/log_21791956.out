INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.158383389003575,
    "estimated_duration": 3600.0112564511046,
    "input_throughput": 5821.846518519257,
    "output_throughput": 5045.941722389924,
    "total_throughput": 10867.788240909182,
    "itl": 114.58191732168888,
    "ttft": 1942121.4940109153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 435333,
    "finished_requests": 84192,
    "scheduler_time": 39.85881228227894
}
#Debug simulation 
Total elapsed time: 6.158535793423653. Arrivals time: 0.31534156762063503 Scheduler time: 5.716690585017204 Scheduler overhead time: 0.04676204873248935 Adapter cache time: 0.010381447151303291 Engine time: 0.0475291945040226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.0280397599563,
    "estimated_duration": 3600.1058488558488,
    "input_throughput": 5667.101706602327,
    "output_throughput": 4924.613815350596,
    "total_throughput": 10591.715521952923,
    "itl": 106.80910899756859,
    "ttft": 1958054.6499897877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489668,
    "arrivals": 435333,
    "finished_requests": 82096,
    "scheduler_time": 35.53038262260506
}
#Debug simulation 
Total elapsed time: 6.0282097538001835. Arrivals time: 0.32590611977502704 Scheduler time: 5.566798730287701 Scheduler overhead time: 0.04945699451491237 Adapter cache time: 0.012438996694982052 Engine time: 0.0505988341756165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.782103288918734,
    "estimated_duration": 3600.012949623708,
    "input_throughput": 5385.981459324112,
    "output_throughput": 4687.617860309061,
    "total_throughput": 10073.599319633173,
    "itl": 94.19350648047366,
    "ttft": 1991222.460370155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 435333,
    "finished_requests": 78083,
    "scheduler_time": 27.06062561044991
}
#Debug simulation 
Total elapsed time: 5.782249024603516. Arrivals time: 0.2935958025045693 Scheduler time: 5.33744768705219 Scheduler overhead time: 0.054554416332393885 Adapter cache time: 0.015063485130667686 Engine time: 0.05608280282467604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.052919826004654,
    "estimated_duration": 3600.0891724266817,
    "input_throughput": 5667.371562979118,
    "output_throughput": 4924.61690554446,
    "total_throughput": 10591.988468523577,
    "itl": 106.80597707793018,
    "ttft": 1958065.245962534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4391118016093973,
    "arrivals": 435333,
    "finished_requests": 82097,
    "scheduler_time": 35.53016807220651
}
#Debug simulation 
Total elapsed time: 6.053029369562864. Arrivals time: 0.358673854265362 Scheduler time: 5.558472656644881 Scheduler overhead time: 0.049656090792268515 Adapter cache time: 0.012525477912276983 Engine time: 0.05060541396960616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.816321634221822,
    "estimated_duration": 3600.0917252940626,
    "input_throughput": 5386.088044302858,
    "output_throughput": 4687.516398938857,
    "total_throughput": 10073.604443241715,
    "itl": 94.19427352029882,
    "ttft": 1991263.9061603544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 435333,
    "finished_requests": 78085,
    "scheduler_time": 27.06288912242854
}
#Debug simulation 
Total elapsed time: 5.816430977080017. Arrivals time: 0.29568803077563643 Scheduler time: 5.36843282636255 Scheduler overhead time: 0.05480652768164873 Adapter cache time: 0.015448668505996466 Engine time: 0.056355918291956186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.030641470104456,
    "estimated_duration": 3600.021891525512,
    "input_throughput": 5667.001372415244,
    "output_throughput": 4924.4586655798585,
    "total_throughput": 10591.460037995103,
    "itl": 106.8068910062366,
    "ttft": 1958070.3847770505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 435333,
    "finished_requests": 82091,
    "scheduler_time": 35.529382928962534
}
#Debug simulation 
Total elapsed time: 6.030814825091511. Arrivals time: 0.31403101049363613 Scheduler time: 5.5801353878341615 Scheduler overhead time: 0.049435882829129696 Adapter cache time: 0.012412788346409798 Engine time: 0.051623083651065826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 291126177 . Total output tokens: 256094142
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.837438743095845,
    "estimated_duration": 3600.031388098692,
    "input_throughput": 5386.263593174099,
    "output_throughput": 4687.757183393023,
    "total_throughput": 10074.020776567122,
    "itl": 94.19106277730323,
    "ttft": 1991209.08016238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 435333,
    "finished_requests": 78086,
    "scheduler_time": 27.06038974383998
}
#Debug simulation 
Total elapsed time: 5.837543166708201. Arrivals time: 0.3483485267497599 Scheduler time: 5.337369210086763 Scheduler overhead time: 0.05479587987065315 Adapter cache time: 0.0153812812641263 Engine time: 0.056068161968141794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.919216662179679,
    "estimated_duration": 3600.0448648141455,
    "input_throughput": 5867.097714930957,
    "output_throughput": 5099.441448474214,
    "total_throughput": 10966.53916340517,
    "itl": 113.15453860618331,
    "ttft": 1915233.6078440412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 404972,
    "finished_requests": 85204,
    "scheduler_time": 40.40332131882857
}
#Debug simulation 
Total elapsed time: 6.919336340855807. Arrivals time: 0.40219048876315355 Scheduler time: 6.379619183950126 Scheduler overhead time: 0.04770402004942298 Adapter cache time: 0.016523525584489107 Engine time: 0.050866634119302034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.106940445955843,
    "estimated_duration": 3600.0760581309264,
    "input_throughput": 5725.317650844588,
    "output_throughput": 4985.684666150807,
    "total_throughput": 10711.002316995395,
    "itl": 105.2323808397781,
    "ttft": 1930617.8912426033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 404972,
    "finished_requests": 83231,
    "scheduler_time": 36.099286306739906
}
#Debug simulation 
Total elapsed time: 6.1070778402499855. Arrivals time: 0.39954635268077254 Scheduler time: 5.563725037500262 Scheduler overhead time: 0.050690966192632914 Adapter cache time: 0.018031216226518154 Engine time: 0.0515306806191802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.829095731955022,
    "estimated_duration": 3600.0319388539406,
    "input_throughput": 5469.6367516863,
    "output_throughput": 4765.974938950706,
    "total_throughput": 10235.611690637006,
    "itl": 92.38538827339752,
    "ttft": 1962608.7923793127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 404972,
    "finished_requests": 79451,
    "scheduler_time": 27.62724745228417
}
#Debug simulation 
Total elapsed time: 5.829305106308311. Arrivals time: 0.3500154400244355 Scheduler time: 5.318655923940241 Scheduler overhead time: 0.05558024765923619 Adapter cache time: 0.02193468902260065 Engine time: 0.05715604359284043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.105366386007518,
    "estimated_duration": 3600.0333423524157,
    "input_throughput": 5725.130030749136,
    "output_throughput": 4985.417992898983,
    "total_throughput": 10710.54802364812,
    "itl": 105.23192103198149,
    "ttft": 1930659.4878397638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 404972,
    "finished_requests": 83227,
    "scheduler_time": 36.098120688205334
}
#Debug simulation 
Total elapsed time: 6.105471013579518. Arrivals time: 0.4142731372267008 Scheduler time: 5.547258028294891 Scheduler overhead time: 0.050685505382716656 Adapter cache time: 0.0186342247761786 Engine time: 0.0511636515147984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.835101779084653,
    "estimated_duration": 3600.0970907946157,
    "input_throughput": 5469.537766175584,
    "output_throughput": 4765.888687800069,
    "total_throughput": 10235.426453975653,
    "itl": 92.3845425206639,
    "ttft": 1962559.605055505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 404972,
    "finished_requests": 79451,
    "scheduler_time": 27.62803004016053
}
#Debug simulation 
Total elapsed time: 5.835203049238771. Arrivals time: 0.3532399875111878 Scheduler time: 5.320620604790747 Scheduler overhead time: 0.05586997698992491 Adapter cache time: 0.022012127097696066 Engine time: 0.05729958647862077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.057905296329409,
    "estimated_duration": 3600.0612565353617,
    "input_throughput": 5725.4653549525065,
    "output_throughput": 4985.765997013583,
    "total_throughput": 10711.23135196609,
    "itl": 105.23250589687153,
    "ttft": 1930601.645223456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 404972,
    "finished_requests": 83231,
    "scheduler_time": 36.09949494229576
}
#Debug simulation 
Total elapsed time: 6.05801439518109. Arrivals time: 0.3181336587294936 Scheduler time: 5.59456567466259 Scheduler overhead time: 0.05040359077975154 Adapter cache time: 0.02055058628320694 Engine time: 0.051145538687705994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270870873 . Total output tokens: 238330900
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.892824515234679,
    "estimated_duration": 3600.0015875520958,
    "input_throughput": 5469.587310207001,
    "output_throughput": 4765.827065000349,
    "total_throughput": 10235.414375207349,
    "itl": 92.38667732489169,
    "ttft": 1962629.7731777227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 404972,
    "finished_requests": 79449,
    "scheduler_time": 27.627252741682074
}
#Debug simulation 
Total elapsed time: 5.8929459461942315. Arrivals time: 0.37301887571811676 Scheduler time: 5.358092851936817 Scheduler overhead time: 0.05607565538957715 Adapter cache time: 0.022323378827422857 Engine time: 0.05725234281271696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.537230119574815,
    "estimated_duration": 3600.1162216373054,
    "input_throughput": 6201.3428527167125,
    "output_throughput": 5415.461001737671,
    "total_throughput": 11616.803854454383,
    "itl": 107.20620498774026,
    "ttft": 1854067.8365513182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 382341,
    "finished_requests": 90383,
    "scheduler_time": 43.148156234013356
}
#Debug simulation 
Total elapsed time: 6.537347766570747. Arrivals time: 0.33909332752227783 Scheduler time: 6.053702398203313 Scheduler overhead time: 0.050366964656859636 Adapter cache time: 0.019876786042004824 Engine time: 0.050907928962260485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.846566695719957,
    "estimated_duration": 3600.003802367589,
    "input_throughput": 6071.378587329677,
    "output_throughput": 5301.54356710627,
    "total_throughput": 11372.922154435948,
    "itl": 99.49611951795767,
    "ttft": 1869558.5343282796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 382341,
    "finished_requests": 88456,
    "scheduler_time": 38.63033402241589
}
#Debug simulation 
Total elapsed time: 6.846646167803556. Arrivals time: 0.41845696279779077 Scheduler time: 6.274807688314468 Scheduler overhead time: 0.053602200001478195 Adapter cache time: 0.020776868797838688 Engine time: 0.054121093824505806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.22590267797932,
    "estimated_duration": 3600.059664744787,
    "input_throughput": 5812.405334533088,
    "output_throughput": 5074.835614230067,
    "total_throughput": 10887.240948763156,
    "itl": 87.23682532558315,
    "ttft": 1903143.5839526132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 382341,
    "finished_requests": 84650,
    "scheduler_time": 29.716293478496922
}
#Debug simulation 
Total elapsed time: 6.22604179661721. Arrivals time: 0.37124241748824716 Scheduler time: 5.684825139120221 Scheduler overhead time: 0.05957152880728245 Adapter cache time: 0.021839480381458998 Engine time: 0.06086216261610389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.4659727178514,
    "estimated_duration": 3600.024710241615,
    "input_throughput": 6071.54638072832,
    "output_throughput": 5301.505555143558,
    "total_throughput": 11373.05193587188,
    "itl": 99.50073356155194,
    "ttft": 1869652.6731286864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 382341,
    "finished_requests": 88459,
    "scheduler_time": 38.63416976007223
}
#Debug simulation 
Total elapsed time: 6.4660786632448435. Arrivals time: 0.4067475972697139 Scheduler time: 5.905550207942724 Scheduler overhead time: 0.05368946632370353 Adapter cache time: 0.020706634037196636 Engine time: 0.05451167467981577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.227524531073868,
    "estimated_duration": 3600.0614129314895,
    "input_throughput": 5812.4958437752675,
    "output_throughput": 5074.965925407031,
    "total_throughput": 10887.461769182299,
    "itl": 87.2378182741645,
    "ttft": 1903024.2639325615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 382341,
    "finished_requests": 84652,
    "scheduler_time": 29.717135951811663
}
#Debug simulation 
Total elapsed time: 6.227671089116484. Arrivals time: 0.30683378549292684 Scheduler time: 5.751867614686489 Scheduler overhead time: 0.05914889508858323 Adapter cache time: 0.021696651354432106 Engine time: 0.06037776917219162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.898515533190221,
    "estimated_duration": 3600.1081163682466,
    "input_throughput": 6071.710152430353,
    "output_throughput": 5301.58578105539,
    "total_throughput": 11373.295933485744,
    "itl": 99.50254013900842,
    "ttft": 1869642.4225281067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 382341,
    "finished_requests": 88461,
    "scheduler_time": 38.63567428490741
}
#Debug simulation 
Total elapsed time: 6.89862531516701. Arrivals time: 0.4316904954612255 Scheduler time: 6.312704024836421 Scheduler overhead time: 0.053829886484891176 Adapter cache time: 0.020964741241186857 Engine time: 0.05473437486216426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255667918 . Total output tokens: 224932192
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.15304288174957,
    "estimated_duration": 3600.0349582117556,
    "input_throughput": 5812.4063357412515,
    "output_throughput": 5074.865719935981,
    "total_throughput": 10887.272055677233,
    "itl": 87.23619651462536,
    "ttft": 1903110.0214813822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 382341,
    "finished_requests": 84649,
    "scheduler_time": 29.716245502860293
}
#Debug simulation 
Total elapsed time: 6.153210519813001. Arrivals time: 0.35944994958117604 Scheduler time: 5.625490325503051 Scheduler overhead time: 0.05911565385758877 Adapter cache time: 0.02136317268013954 Engine time: 0.060254613403230906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.733425413724035,
    "estimated_duration": 3600.092635864657,
    "input_throughput": 6375.704828075126,
    "output_throughput": 5533.017901139601,
    "total_throughput": 11908.722729214727,
    "itl": 104.09386951901772,
    "ttft": 1838036.6932418356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 378554,
    "finished_requests": 92354,
    "scheduler_time": 43.8170251513948
}
#Debug simulation 
Total elapsed time: 6.733551531098783. Arrivals time: 0.3700835886411369 Scheduler time: 6.216144704259932 Scheduler overhead time: 0.05223372811451554 Adapter cache time: 0.017961659468710423 Engine time: 0.052961437962949276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.494710083119571,
    "estimated_duration": 3600.012205782365,
    "input_throughput": 6227.336941800159,
    "output_throughput": 5407.897497883358,
    "total_throughput": 11635.234439683518,
    "itl": 96.79421420952485,
    "ttft": 1853909.7340756163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 378554,
    "finished_requests": 90229,
    "scheduler_time": 39.12134637280801
}
#Debug simulation 
Total elapsed time: 6.494892098009586. Arrivals time: 0.3192033586092293 Scheduler time: 6.022521629463881 Scheduler overhead time: 0.05477135628461838 Adapter cache time: 0.017175445333123207 Engine time: 0.05570678785443306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.264674794860184,
    "estimated_duration": 3600.0488350846845,
    "input_throughput": 5940.501915301638,
    "output_throughput": 5163.059405987966,
    "total_throughput": 11103.561321289604,
    "itl": 85.16290339102484,
    "ttft": 1888595.4328886082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 378554,
    "finished_requests": 86098,
    "scheduler_time": 29.942573797851352
}
#Debug simulation 
Total elapsed time: 6.264784121885896. Arrivals time: 0.33042421797290444 Scheduler time: 5.763185477349907 Scheduler overhead time: 0.06141676427796483 Adapter cache time: 0.019121084362268448 Engine time: 0.062184945680201054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.481554651632905,
    "estimated_duration": 3600.091117098083,
    "input_throughput": 6227.303218389405,
    "output_throughput": 5407.779516339833,
    "total_throughput": 11635.082734729238,
    "itl": 96.79196540529121,
    "ttft": 1853950.4350162484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 378554,
    "finished_requests": 90230,
    "scheduler_time": 39.12210067321278
}
#Debug simulation 
Total elapsed time: 6.481694104615599. Arrivals time: 0.3221617112867534 Scheduler time: 6.00701272720471 Scheduler overhead time: 0.05476844357326627 Adapter cache time: 0.01706518605351448 Engine time: 0.0553715480491519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.273894850164652,
    "estimated_duration": 3600.0133316670363,
    "input_throughput": 5940.558278459728,
    "output_throughput": 5163.074213226029,
    "total_throughput": 11103.632491685757,
    "itl": 85.16175042150404,
    "ttft": 1888612.9111864641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 378554,
    "finished_requests": 86097,
    "scheduler_time": 29.940854051963846
}
#Debug simulation 
Total elapsed time: 6.274003187194467. Arrivals time: 0.33603677013888955 Scheduler time: 5.768125668633729 Scheduler overhead time: 0.0609658001922071 Adapter cache time: 0.018571654334664345 Engine time: 0.06218357291072607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.512731216847897,
    "estimated_duration": 3600.03620458063,
    "input_throughput": 6227.398205460988,
    "output_throughput": 5407.862002951133,
    "total_throughput": 11635.26020841212,
    "itl": 96.79224272442336,
    "ttft": 1853924.6637149143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 378554,
    "finished_requests": 90230,
    "scheduler_time": 39.12187392731903
}
#Debug simulation 
Total elapsed time: 6.5128324050456285. Arrivals time: 0.33637495758011937 Scheduler time: 6.023115967400372 Scheduler overhead time: 0.05489828949794173 Adapter cache time: 0.01729203388094902 Engine time: 0.05578243499621749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 253153768 . Total output tokens: 222683765
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.239621087908745,
    "estimated_duration": 3600.041460849103,
    "input_throughput": 5940.397696129864,
    "output_throughput": 5163.147758752744,
    "total_throughput": 11103.545454882607,
    "itl": 85.1605491012099,
    "ttft": 1888620.7876521803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 378554,
    "finished_requests": 86097,
    "scheduler_time": 29.940726748346403
}
#Debug simulation 
Total elapsed time: 6.239774367772043. Arrivals time: 0.3229669202119112 Scheduler time: 5.7459811065346 Scheduler overhead time: 0.06066845776513219 Adapter cache time: 0.018870954401791096 Engine time: 0.06323050986975431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.1991844600997865,
    "estimated_duration": 3600.0203344150254,
    "input_throughput": 6436.612809789935,
    "output_throughput": 5645.8611651988585,
    "total_throughput": 12082.473974988794,
    "itl": 102.8597239344642,
    "ttft": 1819772.953820987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 376669,
    "finished_requests": 94403,
    "scheduler_time": 45.08811369823701
}
#Debug simulation 
Total elapsed time: 7.19926743907854. Arrivals time: 0.7405708967708051 Scheduler time: 6.315638359636068 Scheduler overhead time: 0.052169489208608866 Adapter cache time: 0.014101943466812372 Engine time: 0.05283977882936597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.650680385995656,
    "estimated_duration": 3600.015340176876,
    "input_throughput": 6282.373507577553,
    "output_throughput": 5514.446224283207,
    "total_throughput": 11796.81973186076,
    "itl": 95.726035517969,
    "ttft": 1837297.9322685665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46826444204896667,
    "arrivals": 376669,
    "finished_requests": 92149,
    "scheduler_time": 40.30834078139555
}
#Debug simulation 
Total elapsed time: 6.65081345429644. Arrivals time: 0.3331192880868912 Scheduler time: 6.1654234188608825 Scheduler overhead time: 0.05546903144568205 Adapter cache time: 0.014777095522731543 Engine time: 0.05640760436654091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.444717298261821,
    "estimated_duration": 3600.0074092266677,
    "input_throughput": 5978.556028756455,
    "output_throughput": 5247.698644058684,
    "total_throughput": 11226.25467281514,
    "itl": 84.15331465070534,
    "ttft": 1873336.4246852053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 376669,
    "finished_requests": 87624,
    "scheduler_time": 30.7108396213953
}
#Debug simulation 
Total elapsed time: 6.444844170007855. Arrivals time: 0.33140899427235126 Scheduler time: 5.944427846930921 Scheduler overhead time: 0.06162568088620901 Adapter cache time: 0.01570684276521206 Engine time: 0.06305066682398319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.639028217643499,
    "estimated_duration": 3600.0521640348584,
    "input_throughput": 6282.307024865934,
    "output_throughput": 5514.355096941251,
    "total_throughput": 11796.662121807185,
    "itl": 95.72381731184447,
    "ttft": 1837267.1115678046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939695,
    "arrivals": 376669,
    "finished_requests": 92149,
    "scheduler_time": 40.30777464516602
}
#Debug simulation 
Total elapsed time: 6.639149527065456. Arrivals time: 0.32570989057421684 Scheduler time: 6.161671246401966 Scheduler overhead time: 0.05551018798723817 Adapter cache time: 0.014401336666196585 Engine time: 0.05618595751002431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.488455097656697,
    "estimated_duration": 3600.0353370872645,
    "input_throughput": 5978.6696475635945,
    "output_throughput": 5247.879043122305,
    "total_throughput": 11226.5486906859,
    "itl": 84.15234288026359,
    "ttft": 1873392.8052598138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 376669,
    "finished_requests": 87626,
    "scheduler_time": 30.70948408621075
}
#Debug simulation 
Total elapsed time: 6.4885716759599745. Arrivals time: 0.34781431779265404 Scheduler time: 5.971671108622104 Scheduler overhead time: 0.06161295250058174 Adapter cache time: 0.01587136834859848 Engine time: 0.063082130625844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.692642537876964,
    "estimated_duration": 3600.022162584509,
    "input_throughput": 6282.256602488095,
    "output_throughput": 5514.317719018763,
    "total_throughput": 11796.574321506858,
    "itl": 95.72641686455091,
    "ttft": 1837260.4358424963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 376669,
    "finished_requests": 92149,
    "scheduler_time": 40.309310516980574
}
#Debug simulation 
Total elapsed time: 6.69280753005296. Arrivals time: 0.38917909376323223 Scheduler time: 6.15157787501812 Scheduler overhead time: 0.05547097371891141 Adapter cache time: 0.014502241741865873 Engine time: 0.05618148948997259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251857675 . Total output tokens: 221573676
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.42456054314971,
    "estimated_duration": 3600.0720515196954,
    "input_throughput": 5982.327490059174,
    "output_throughput": 5251.632114423687,
    "total_throughput": 11233.959604482861,
    "itl": 84.288805500036,
    "ttft": 1873198.1679192674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 376669,
    "finished_requests": 87689,
    "scheduler_time": 30.842798728331847
}
#Debug simulation 
Total elapsed time: 6.424668225925416. Arrivals time: 0.3374304692260921 Scheduler time: 5.918709778226912 Scheduler overhead time: 0.06157953850924969 Adapter cache time: 0.01590172341093421 Engine time: 0.06278170505538583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.763859529048204,
    "estimated_duration": 3600.06559398383,
    "input_throughput": 6514.883795227134,
    "output_throughput": 5700.121140651687,
    "total_throughput": 12215.00493587882,
    "itl": 101.69336050631806,
    "ttft": 1812431.1392142875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 375735,
    "finished_requests": 95153,
    "scheduler_time": 45.43085082767714
}
#Debug simulation 
Total elapsed time: 6.764074600767344. Arrivals time: 0.3392253816127777 Scheduler time: 6.281079980079085 Scheduler overhead time: 0.052429585717618465 Adapter cache time: 0.012147406581789255 Engine time: 0.05338026350364089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.632290213834494,
    "estimated_duration": 3600.0590006311045,
    "input_throughput": 6358.6249547540865,
    "output_throughput": 5562.038843388337,
    "total_throughput": 11920.663798142423,
    "itl": 94.69572705591376,
    "ttft": 1831024.7850305194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489668,
    "arrivals": 375735,
    "finished_requests": 92834,
    "scheduler_time": 40.549630587985774
}
#Debug simulation 
Total elapsed time: 6.632422856986523. Arrivals time: 0.34997850051149726 Scheduler time: 6.130742454901338 Scheduler overhead time: 0.056106097530573606 Adapter cache time: 0.013068961910903454 Engine time: 0.05684226844459772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.352383375167847,
    "estimated_duration": 3600.029631730178,
    "input_throughput": 6046.0385681712505,
    "output_throughput": 5289.965624768321,
    "total_throughput": 11336.004192939572,
    "itl": 83.57440901512935,
    "ttft": 1867673.9067618265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48098376162815837,
    "arrivals": 375735,
    "finished_requests": 88223,
    "scheduler_time": 31.02043969467081
}
#Debug simulation 
Total elapsed time: 6.352493955288082. Arrivals time: 0.3102235565893352 Scheduler time: 5.876384174451232 Scheduler overhead time: 0.0615545348264277 Adapter cache time: 0.013216160237789154 Engine time: 0.062481132335960865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.5332385529764,
    "estimated_duration": 3600.0936920070594,
    "input_throughput": 6358.5636815018515,
    "output_throughput": 5561.985246233068,
    "total_throughput": 11920.548927734919,
    "itl": 94.69559324310488,
    "ttft": 1831037.7358975427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 375735,
    "finished_requests": 92834,
    "scheduler_time": 40.55080533528559
}
#Debug simulation 
Total elapsed time: 6.533333314117044. Arrivals time: 0.26794647378847003 Scheduler time: 6.116494459565729 Scheduler overhead time: 0.055140269454568624 Adapter cache time: 0.011607432272285223 Engine time: 0.05643145181238651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.308862132020295,
    "estimated_duration": 3600.011779821848,
    "input_throughput": 6045.877161290035,
    "output_throughput": 5290.101023208997,
    "total_throughput": 11335.978184499032,
    "itl": 83.57360511604637,
    "ttft": 1867615.9799956724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924963,
    "arrivals": 375735,
    "finished_requests": 88223,
    "scheduler_time": 31.01938154125285
}
#Debug simulation 
Total elapsed time: 6.308947777375579. Arrivals time: 0.2489172457717359 Scheduler time: 5.894962967373431 Scheduler overhead time: 0.06099558109417558 Adapter cache time: 0.012506457511335611 Engine time: 0.06306522153317928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.536235828883946,
    "estimated_duration": 3600.101283455532,
    "input_throughput": 6358.5502733489275,
    "output_throughput": 5561.97351780626,
    "total_throughput": 11920.523791155189,
    "itl": 94.69683695472622,
    "ttft": 1831079.9767126767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 375735,
    "finished_requests": 92834,
    "scheduler_time": 40.55179971741117
}
#Debug simulation 
Total elapsed time: 6.536350633949041. Arrivals time: 0.25735476706176996 Scheduler time: 6.129914166871458 Scheduler overhead time: 0.0552907707169652 Adapter cache time: 0.011653388384729624 Engine time: 0.05633597448468208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251201079 . Total output tokens: 221008641
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.255384934134781,
    "estimated_duration": 3600.00958788995,
    "input_throughput": 6046.0722308124505,
    "output_throughput": 5290.156744044255,
    "total_throughput": 11336.228974856705,
    "itl": 83.57325412766843,
    "ttft": 1867662.0712532718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337262,
    "arrivals": 375735,
    "finished_requests": 88225,
    "scheduler_time": 31.019831469137852
}
#Debug simulation 
Total elapsed time: 6.2554742051288486. Arrivals time: 0.2517162342555821 Scheduler time: 5.83919742424041 Scheduler overhead time: 0.06085340026766062 Adapter cache time: 0.012780191376805305 Engine time: 0.06238579470664263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.722862062044442,
    "estimated_duration": 3600.1056166523717,
    "input_throughput": 6544.4041116516555,
    "output_throughput": 5724.695660224589,
    "total_throughput": 12269.099771876245,
    "itl": 101.35517187413305,
    "ttft": 1805890.563785349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 375251,
    "finished_requests": 95584,
    "scheduler_time": 45.68078952887343
}
#Debug simulation 
Total elapsed time: 6.7229734542779624. Arrivals time: 0.2620336259715259 Scheduler time: 6.320909350179136 Scheduler overhead time: 0.052073122933506966 Adapter cache time: 0.010171945672482252 Engine time: 0.05345111805945635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.610737517010421,
    "estimated_duration": 3600.0259605607657,
    "input_throughput": 6379.122331779743,
    "output_throughput": 5583.88084425612,
    "total_throughput": 11963.003176035863,
    "itl": 94.45172020708213,
    "ttft": 1824408.5957352414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46826444204896667,
    "arrivals": 375251,
    "finished_requests": 93197,
    "scheduler_time": 40.7597481552185
}
#Debug simulation 
Total elapsed time: 6.610854210797697. Arrivals time: 0.2591793481260538 Scheduler time: 6.202910513151437 Scheduler overhead time: 0.055444028694182634 Adapter cache time: 0.010712376330047846 Engine time: 0.05670920619741082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.372214989271015,
    "estimated_duration": 3600.005748200284,
    "input_throughput": 6059.372547086946,
    "output_throughput": 5309.0054118815515,
    "total_throughput": 11368.377958968496,
    "itl": 83.36437985826487,
    "ttft": 1861524.6919759314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 375251,
    "finished_requests": 88505,
    "scheduler_time": 31.177594304411407
}
#Debug simulation 
Total elapsed time: 6.372331350110471. Arrivals time: 0.2514132601208985 Scheduler time: 5.955568566452712 Scheduler overhead time: 0.06139667425304651 Adapter cache time: 0.011865628883242607 Engine time: 0.06333598028868437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.6185980518348515,
    "estimated_duration": 3600.0305974406515,
    "input_throughput": 6378.974949914598,
    "output_throughput": 5583.755042051803,
    "total_throughput": 11962.7299919664,
    "itl": 94.45355769087591,
    "ttft": 1824457.1662930995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 375251,
    "finished_requests": 93195,
    "scheduler_time": 40.7616423959404
}
#Debug simulation 
Total elapsed time: 6.61872911080718. Arrivals time: 0.25994282215833664 Scheduler time: 6.209739030338824 Scheduler overhead time: 0.055583841167390347 Adapter cache time: 0.01077233487740159 Engine time: 0.05687269940972328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.304280408192426,
    "estimated_duration": 3600.043215069598,
    "input_throughput": 6059.361984513923,
    "output_throughput": 5309.107657373079,
    "total_throughput": 11368.469641887,
    "itl": 83.36395646662811,
    "ttft": 1861520.9594327009,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 375251,
    "finished_requests": 88509,
    "scheduler_time": 31.176835144501194
}
#Debug simulation 
Total elapsed time: 6.304399013053626. Arrivals time: 0.28065888676792383 Scheduler time: 5.859733798075467 Scheduler overhead time: 0.060963507276028395 Adapter cache time: 0.011756214313209057 Engine time: 0.06266199890524149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.609824298880994,
    "estimated_duration": 3600.092197892876,
    "input_throughput": 6379.288845280672,
    "output_throughput": 5584.103377065287,
    "total_throughput": 11963.39222234596,
    "itl": 94.45172367401156,
    "ttft": 1824346.1574580858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 375251,
    "finished_requests": 93201,
    "scheduler_time": 40.76111871232454
}
#Debug simulation 
Total elapsed time: 6.609940058086067. Arrivals time: 0.2639382933266461 Scheduler time: 6.1972576742991805 Scheduler overhead time: 0.055399826262146235 Adapter cache time: 0.010723436251282692 Engine time: 0.056721063796430826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250875413 . Total output tokens: 220723786
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.301928914152086,
    "estimated_duration": 3600.0027426519746,
    "input_throughput": 6059.429272526206,
    "output_throughput": 5309.091510836023,
    "total_throughput": 11368.52078336223,
    "itl": 83.36263910820439,
    "ttft": 1861527.7739192909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337263,
    "arrivals": 375251,
    "finished_requests": 88508,
    "scheduler_time": 31.176151648139694
}
#Debug simulation 
Total elapsed time: 6.302063458133489. Arrivals time: 0.25908124214038253 Scheduler time: 5.879071426577866 Scheduler overhead time: 0.0612341333180666 Adapter cache time: 0.011658559553325176 Engine time: 0.06244890671223402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.7262224038131535,
    "estimated_duration": 3600.095572851565,
    "input_throughput": 6540.914962806989,
    "output_throughput": 5745.668852789886,
    "total_throughput": 12286.583815596876,
    "itl": 101.0909123729365,
    "ttft": 1800164.0482837015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4165817690128457,
    "arrivals": 375015,
    "finished_requests": 95867,
    "scheduler_time": 45.90338261543905
}
#Debug simulation 
Total elapsed time: 6.726347623858601. Arrivals time: 0.2667077211663127 Scheduler time: 6.319678999017924 Scheduler overhead time: 0.05240846564993262 Adapter cache time: 0.009463852271437645 Engine time: 0.05361672164872289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.584675705060363,
    "estimated_duration": 3600.049923473601,
    "input_throughput": 6378.273215123757,
    "output_throughput": 5602.388141478754,
    "total_throughput": 11980.66135660251,
    "itl": 94.20533453048665,
    "ttft": 1819122.657180737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46049230013508324,
    "arrivals": 375015,
    "finished_requests": 93439,
    "scheduler_time": 40.947264393849295
}
#Debug simulation 
Total elapsed time: 6.58476402470842. Arrivals time: 0.26278636418282986 Scheduler time: 6.173637606203556 Scheduler overhead time: 0.05557308532297611 Adapter cache time: 0.009995108004659414 Engine time: 0.056955824606120586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.26910311030224,
    "estimated_duration": 3600.0816809942057,
    "input_throughput": 6058.569758332965,
    "output_throughput": 5323.484214587976,
    "total_throughput": 11382.053972920941,
    "itl": 83.16750208945656,
    "ttft": 1855715.2951260346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47290768825449053,
    "arrivals": 375015,
    "finished_requests": 88728,
    "scheduler_time": 31.289956573482815
}
#Debug simulation 
Total elapsed time: 6.269215910229832. Arrivals time: 0.25204061763361096 Scheduler time: 5.853671041782945 Scheduler overhead time: 0.06142269494011998 Adapter cache time: 0.010995719581842422 Engine time: 0.06264747399836779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.616116749122739,
    "estimated_duration": 3600.0323124349534,
    "input_throughput": 6378.228862192992,
    "output_throughput": 5602.102495118756,
    "total_throughput": 11980.331357311748,
    "itl": 94.20513040566979,
    "ttft": 1819117.893252375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43133965969551363,
    "arrivals": 375015,
    "finished_requests": 93436,
    "scheduler_time": 40.94813310605785
}
#Debug simulation 
Total elapsed time: 6.61623123427853. Arrivals time: 0.260383659042418 Scheduler time: 6.206285432912409 Scheduler overhead time: 0.05551932891830802 Adapter cache time: 0.009964600205421448 Engine time: 0.05814580572769046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.292519598733634,
    "estimated_duration": 3600.017957181486,
    "input_throughput": 6058.5339460573305,
    "output_throughput": 5323.375946436669,
    "total_throughput": 11381.909892494,
    "itl": 83.16612253130258,
    "ttft": 1855638.7224313782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4685582275455818,
    "arrivals": 375015,
    "finished_requests": 88726,
    "scheduler_time": 31.28864321690314
}
#Debug simulation 
Total elapsed time: 6.292633122764528. Arrivals time: 0.24975268356502056 Scheduler time: 5.878833933733404 Scheduler overhead time: 0.06137427408248186 Adapter cache time: 0.011137392371892929 Engine time: 0.06290367338806391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.5937116257846355,
    "estimated_duration": 3600.0528591147086,
    "input_throughput": 6378.251069804183,
    "output_throughput": 5602.262463712723,
    "total_throughput": 11980.513533516907,
    "itl": 94.20442308475268,
    "ttft": 1819101.974518045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40218701925594397,
    "arrivals": 375015,
    "finished_requests": 93439,
    "scheduler_time": 40.94840404360009
}
#Debug simulation 
Total elapsed time: 6.593796229921281. Arrivals time: 0.25777083495631814 Scheduler time: 6.1876909751445055 Scheduler overhead time: 0.05542269581928849 Adapter cache time: 0.00992988795042038 Engine time: 0.05695481086149812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250725759 . Total output tokens: 220591713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.2990322122350335,
    "estimated_duration": 3600.054004744935,
    "input_throughput": 6058.448559730787,
    "output_throughput": 5323.351253825927,
    "total_throughput": 11381.799813556714,
    "itl": 83.16723084367449,
    "ttft": 1855719.763790692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46420876683667306,
    "arrivals": 375015,
    "finished_requests": 88726,
    "scheduler_time": 31.289688913849854
}
#Debug simulation 
Total elapsed time: 6.2991182073019445. Arrivals time: 0.2556874118745327 Scheduler time: 5.87940191058442 Scheduler overhead time: 0.061473476234823465 Adapter cache time: 0.011054491624236107 Engine time: 0.06286701699718833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.289544385392219,
    "estimated_duration": 3600.0977759713783,
    "input_throughput": 5842.49492899529,
    "output_throughput": 5090.105363889897,
    "total_throughput": 10932.600292885187,
    "itl": 113.53771660848744,
    "ttft": 1867915.1219764263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 344340,
    "finished_requests": 85121,
    "scheduler_time": 40.63777080428974
}
#Debug simulation 
Total elapsed time: 6.289608480408788. Arrivals time: 0.4989716736599803 Scheduler time: 5.659165998920798 Scheduler overhead time: 0.04662309866398573 Adapter cache time: 0.01526087149977684 Engine time: 0.047661022283136845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.894319191109389,
    "estimated_duration": 3600.1147767913717,
    "input_throughput": 5713.385898860739,
    "output_throughput": 4977.449084545794,
    "total_throughput": 10690.834983406534,
    "itl": 105.57395425328268,
    "ttft": 1885831.5126067179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489668,
    "arrivals": 344340,
    "finished_requests": 83224,
    "scheduler_time": 36.36455182573823
}
#Debug simulation 
Total elapsed time: 5.894414378795773. Arrivals time: 0.26571005024015903 Scheduler time: 5.488544086925685 Scheduler overhead time: 0.04941414250060916 Adapter cache time: 0.017249972093850374 Engine time: 0.0505502843298018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.73434230312705,
    "estimated_duration": 3600.07775408145,
    "input_throughput": 5459.999017442134,
    "output_throughput": 4757.7633512446855,
    "total_throughput": 10217.76236868682,
    "itl": 92.65505724197651,
    "ttft": 1919878.9307772778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 344340,
    "finished_requests": 79563,
    "scheduler_time": 27.96131400525974
}
#Debug simulation 
Total elapsed time: 5.734432131052017. Arrivals time: 0.2329132603481412 Scheduler time: 5.338732970412821 Scheduler overhead time: 0.055431760381907225 Adapter cache time: 0.022639066446572542 Engine time: 0.05890295747667551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.93382807308808,
    "estimated_duration": 3600.0656695107846,
    "input_throughput": 5713.638274491587,
    "output_throughput": 4977.612534061115,
    "total_throughput": 10691.250808552702,
    "itl": 105.5708280888943,
    "ttft": 1885779.363215724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939695,
    "arrivals": 344340,
    "finished_requests": 83225,
    "scheduler_time": 36.363233607391415
}
#Debug simulation 
Total elapsed time: 5.933915832079947. Arrivals time: 0.24624858517199755 Scheduler time: 5.545593144372106 Scheduler overhead time: 0.0499546411447227 Adapter cache time: 0.017525825649499893 Engine time: 0.051055876072496176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.694386967923492,
    "estimated_duration": 3600.092734163754,
    "input_throughput": 5460.019630457081,
    "output_throughput": 4757.7943860878595,
    "total_throughput": 10217.81401654494,
    "itl": 92.6525591893946,
    "ttft": 1919842.955838943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 344340,
    "finished_requests": 79563,
    "scheduler_time": 27.961013047014088
}
#Debug simulation 
Total elapsed time: 5.694473761599511. Arrivals time: 0.23312057834118605 Scheduler time: 5.301581564825028 Scheduler overhead time: 0.055094115901738405 Adapter cache time: 0.022372146602720022 Engine time: 0.05660721054300666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.906324788928032,
    "estimated_duration": 3600.0201460520216,
    "input_throughput": 5713.788302701004,
    "output_throughput": 4977.509645231042,
    "total_throughput": 10691.297947932046,
    "itl": 105.57154256779384,
    "ttft": 1885763.0130300757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 344340,
    "finished_requests": 83225,
    "scheduler_time": 36.36438547955687
}
#Debug simulation 
Total elapsed time: 5.906470065005124. Arrivals time: 0.24108209740370512 Scheduler time: 5.524767829105258 Scheduler overhead time: 0.04934859508648515 Adapter cache time: 0.01743746967986226 Engine time: 0.0505914562381804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230359350 . Total output tokens: 202690231
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.704108054749668,
    "estimated_duration": 3600.0884034285,
    "input_throughput": 5460.210360745494,
    "output_throughput": 4758.283708723992,
    "total_throughput": 10218.494069469487,
    "itl": 92.65108192415777,
    "ttft": 1919893.783661327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 344340,
    "finished_requests": 79567,
    "scheduler_time": 27.95867457091179
}
#Debug simulation 
Total elapsed time: 5.704223131760955. Arrivals time: 0.2342682033777237 Scheduler time: 5.309430493507534 Scheduler overhead time: 0.05521566513925791 Adapter cache time: 0.022726375609636307 Engine time: 0.05660908808931708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.328234273940325,
    "estimated_duration": 3600.033964826406,
    "input_throughput": 6182.857222313264,
    "output_throughput": 5367.590747420012,
    "total_throughput": 11550.447969733275,
    "itl": 107.63997919565291,
    "ttft": 1798327.4276727582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 321710,
    "finished_requests": 89790,
    "scheduler_time": 42.99978674289925
}
#Debug simulation 
Total elapsed time: 6.328352476935834. Arrivals time: 0.24770502699539065 Scheduler time: 5.938701209146529 Scheduler overhead time: 0.049401416908949614 Adapter cache time: 0.01920382771641016 Engine time: 0.05027031246572733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.2067216341383755,
    "estimated_duration": 3600.069133438718,
    "input_throughput": 6053.565693385309,
    "output_throughput": 5258.135968605346,
    "total_throughput": 11311.701661990654,
    "itl": 99.86427916050387,
    "ttft": 1815098.5516505959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 321710,
    "finished_requests": 87897,
    "scheduler_time": 38.53057612703292
}
#Debug simulation 
Total elapsed time: 6.206838076002896. Arrivals time: 0.2466663671657443 Scheduler time: 5.8085114187560976 Scheduler overhead time: 0.05248335050418973 Adapter cache time: 0.020916856825351715 Engine time: 0.053675204049795866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.265683846082538,
    "estimated_duration": 3600.081184806182,
    "input_throughput": 5796.763719683593,
    "output_throughput": 5035.4528327049275,
    "total_throughput": 10832.21655238852,
    "itl": 87.54916422867804,
    "ttft": 1850000.0065570273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 321710,
    "finished_requests": 84176,
    "scheduler_time": 29.74165940876011
}
#Debug simulation 
Total elapsed time: 6.265743640251458. Arrivals time: 0.49058291455730796 Scheduler time: 5.605780579149723 Scheduler overhead time: 0.058661478105932474 Adapter cache time: 0.02348015457391739 Engine time: 0.05975152039900422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.1717198309488595,
    "estimated_duration": 3600.1037080047795,
    "input_throughput": 6053.6225530231495,
    "output_throughput": 5258.288242616057,
    "total_throughput": 11311.910795639205,
    "itl": 99.86094439403293,
    "ttft": 1815061.6723405956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.439111801609397,
    "arrivals": 321710,
    "finished_requests": 87900,
    "scheduler_time": 38.53111374625948
}
#Debug simulation 
Total elapsed time: 6.1718322592787445. Arrivals time: 0.24663742911070585 Scheduler time: 5.773630229290575 Scheduler overhead time: 0.052542674355208874 Adapter cache time: 0.02074631303548813 Engine time: 0.053937016520649195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.009242702741176,
    "estimated_duration": 3600.0657279671386,
    "input_throughput": 5796.700554071242,
    "output_throughput": 5035.357510051957,
    "total_throughput": 10832.058064123199,
    "itl": 87.55111288396209,
    "ttft": 1849890.8398850323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 321710,
    "finished_requests": 84176,
    "scheduler_time": 29.744097857898208
}
#Debug simulation 
Total elapsed time: 6.00932700606063. Arrivals time: 0.24029562436044216 Scheduler time: 5.5995869981125 Scheduler overhead time: 0.05854266043752432 Adapter cache time: 0.02341712499037385 Engine time: 0.06002820190042257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.440937953069806,
    "estimated_duration": 3600.0478496256173,
    "input_throughput": 6053.704536806755,
    "output_throughput": 5258.368719173731,
    "total_throughput": 11312.073255980486,
    "itl": 99.861746955167,
    "ttft": 1815058.2789209837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 321710,
    "finished_requests": 87898,
    "scheduler_time": 38.529717055831334
}
#Debug simulation 
Total elapsed time: 6.4409998948685825. Arrivals time: 0.2533158529549837 Scheduler time: 6.036209312733263 Scheduler overhead time: 0.052366788033396006 Adapter cache time: 0.02086257701739669 Engine time: 0.053646435495465994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 215250992 . Total output tokens: 189293034
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.982234595809132,
    "estimated_duration": 3600.011750851785,
    "input_throughput": 5796.702467724572,
    "output_throughput": 5035.3974527196815,
    "total_throughput": 10832.099920444254,
    "itl": 87.55306624680244,
    "ttft": 1849933.5072103147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 321710,
    "finished_requests": 84173,
    "scheduler_time": 29.744114965545783
}
#Debug simulation 
Total elapsed time: 5.98231905605644. Arrivals time: 0.23738326970487833 Scheduler time: 5.576475859619677 Scheduler overhead time: 0.058260923717170954 Adapter cache time: 0.023281477857381105 Engine time: 0.059697851072996855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.499035543762147,
    "estimated_duration": 3600.009127729645,
    "input_throughput": 6308.946503789435,
    "output_throughput": 5506.796870957486,
    "total_throughput": 11815.743374746922,
    "itl": 105.33259355179821,
    "ttft": 1781591.4865933878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 317887,
    "finished_requests": 91856,
    "scheduler_time": 44.276112657797775
}
#Debug simulation 
Total elapsed time: 6.4991253558546305. Arrivals time: 0.27651935210451484 Scheduler time: 6.080396024044603 Scheduler overhead time: 0.050253660418093204 Adapter cache time: 0.016898542642593384 Engine time: 0.051516633946448565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.363359151408076,
    "estimated_duration": 3600.0721544619864,
    "input_throughput": 6171.934352054836,
    "output_throughput": 5387.7070146941705,
    "total_throughput": 11559.641366749007,
    "itl": 97.84766047822387,
    "ttft": 1799602.739858914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46826444204896667,
    "arrivals": 317887,
    "finished_requests": 89829,
    "scheduler_time": 39.65764567989967
}
#Debug simulation 
Total elapsed time: 6.3634704812429845. Arrivals time: 0.24709018971771002 Scheduler time: 5.965206478256732 Scheduler overhead time: 0.053457348607480526 Adapter cache time: 0.01799706695601344 Engine time: 0.05477159284055233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.176016158889979,
    "estimated_duration": 3600.013560862117,
    "input_throughput": 5890.290589605967,
    "output_throughput": 5152.008926199257,
    "total_throughput": 11042.299515805225,
    "itl": 85.93292407318438,
    "ttft": 1835551.0132089905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 317887,
    "finished_requests": 85773,
    "scheduler_time": 30.63506565668524
}
#Debug simulation 
Total elapsed time: 6.1761033232323825. Arrivals time: 0.26881972840055823 Scheduler time: 5.73775393050164 Scheduler overhead time: 0.05991860944777727 Adapter cache time: 0.020377227570861578 Engine time: 0.06139596179127693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.340096542146057,
    "estimated_duration": 3600.086486881749,
    "input_throughput": 6171.762839854747,
    "output_throughput": 5387.608622925033,
    "total_throughput": 11559.37146277978,
    "itl": 97.84725754913008,
    "ttft": 1799601.9743849097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 317887,
    "finished_requests": 89828,
    "scheduler_time": 39.65683777189429
}
#Debug simulation 
Total elapsed time: 6.340213925112039. Arrivals time: 0.27562597999349236 Scheduler time: 5.913078826852143 Scheduler overhead time: 0.05368958739563823 Adapter cache time: 0.01795363798737526 Engine time: 0.05482976231724024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.144748312886804,
    "estimated_duration": 3600.045319483602,
    "input_throughput": 5890.195016500983,
    "output_throughput": 5151.84764470143,
    "total_throughput": 11042.042661202413,
    "itl": 85.93411471101146,
    "ttft": 1835568.6737490126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 317887,
    "finished_requests": 85772,
    "scheduler_time": 30.636933790295924
}
#Debug simulation 
Total elapsed time: 6.144881586078554. Arrivals time: 0.268225138541311 Scheduler time: 5.705803920980543 Scheduler overhead time: 0.06027372879907489 Adapter cache time: 0.020217431243509054 Engine time: 0.062496250960975885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.390494836028665,
    "estimated_duration": 3600.0878630729603,
    "input_throughput": 6171.760480599611,
    "output_throughput": 5387.606563425399,
    "total_throughput": 11559.36704402501,
    "itl": 97.84583034033768,
    "ttft": 1799634.7975754645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 317887,
    "finished_requests": 89828,
    "scheduler_time": 39.656812276493234
}
#Debug simulation 
Total elapsed time: 6.390606883913279. Arrivals time: 0.28644510824233294 Scheduler time: 5.952923487871885 Scheduler overhead time: 0.05347229726612568 Adapter cache time: 0.017998676281422377 Engine time: 0.054809242486953735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212764649 . Total output tokens: 187066859
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.339304508175701,
    "estimated_duration": 3600.0354898539263,
    "input_throughput": 5890.788037998062,
    "output_throughput": 5152.100320197839,
    "total_throughput": 11042.888358195902,
    "itl": 85.93291514673109,
    "ttft": 1835625.6012186045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337263,
    "arrivals": 317887,
    "finished_requests": 85776,
    "scheduler_time": 30.63723402509633
}
#Debug simulation 
Total elapsed time: 6.339522039983422. Arrivals time: 0.38660514634102583 Scheduler time: 5.780503393150866 Scheduler overhead time: 0.06053021224215627 Adapter cache time: 0.021527017932385206 Engine time: 0.0622184369713068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.843259782996029,
    "estimated_duration": 3600.112139913821,
    "input_throughput": 6478.340977611408,
    "output_throughput": 5628.363287729433,
    "total_throughput": 12106.70426534084,
    "itl": 102.77931713599904,
    "ttft": 1758850.2283098698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 315978,
    "finished_requests": 94159,
    "scheduler_time": 45.14323374394736
}
#Debug simulation 
Total elapsed time: 6.843369499780238. Arrivals time: 0.39618506329134107 Scheduler time: 6.302356351632625 Scheduler overhead time: 0.0519450125284493 Adapter cache time: 0.015376995783299208 Engine time: 0.05318709649145603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.6211768463253975,
    "estimated_duration": 3600.0576751776434,
    "input_throughput": 6327.115856241395,
    "output_throughput": 5496.557773626105,
    "total_throughput": 11823.6736298675,
    "itl": 95.60854002603294,
    "ttft": 1777413.100073002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 315978,
    "finished_requests": 91899,
    "scheduler_time": 40.36172422392464
}
#Debug simulation 
Total elapsed time: 6.621285529341549. Arrivals time: 0.3665368868969381 Scheduler time: 6.09993296302855 Scheduler overhead time: 0.055484813172370195 Adapter cache time: 0.016863520722836256 Engine time: 0.05662916740402579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.453464805148542,
    "estimated_duration": 3600.0856967008704,
    "input_throughput": 6030.500890546968,
    "output_throughput": 5243.707397659914,
    "total_throughput": 11274.208288206883,
    "itl": 84.18877283025921,
    "ttft": 1816996.9988659036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 315978,
    "finished_requests": 87701,
    "scheduler_time": 31.11283117423355
}
#Debug simulation 
Total elapsed time: 6.45360057707876. Arrivals time: 0.31076146569103 Scheduler time: 5.971634835470468 Scheduler overhead time: 0.06137473834678531 Adapter cache time: 0.018093904945999384 Engine time: 0.06312752608209848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.654566781129688,
    "estimated_duration": 3600.025023516685,
    "input_throughput": 6327.173242187446,
    "output_throughput": 5496.607626540929,
    "total_throughput": 11823.780868728376,
    "itl": 95.609513048403,
    "ttft": 1777419.302029509,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939695,
    "arrivals": 315978,
    "finished_requests": 91899,
    "scheduler_time": 40.36266407726561
}
#Debug simulation 
Total elapsed time: 6.65475125098601. Arrivals time: 0.3408646979369223 Scheduler time: 6.159259868320078 Scheduler overhead time: 0.05536589212715626 Adapter cache time: 0.016751940362155437 Engine time: 0.05680329352617264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.840581300202757,
    "estimated_duration": 3600.079980992576,
    "input_throughput": 6030.902678449346,
    "output_throughput": 5243.959884134289,
    "total_throughput": 11274.862562583634,
    "itl": 84.18628913025763,
    "ttft": 1816928.8248771871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 315978,
    "finished_requests": 87705,
    "scheduler_time": 31.111830563775786
}
#Debug simulation 
Total elapsed time: 6.8406536458060145. Arrivals time: 0.7038595750927925 Scheduler time: 5.96586408931762 Scheduler overhead time: 0.061610450968146324 Adapter cache time: 0.018123666290193796 Engine time: 0.06282014586031437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.9979652101174,
    "estimated_duration": 3600.075059483466,
    "input_throughput": 6327.085303401467,
    "output_throughput": 5496.53123144581,
    "total_throughput": 11823.616534847277,
    "itl": 95.60675461049857,
    "ttft": 1777404.9427586233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 315978,
    "finished_requests": 91899,
    "scheduler_time": 40.36162766628282
}
#Debug simulation 
Total elapsed time: 6.998041423037648. Arrivals time: 0.33822590159252286 Scheduler time: 6.505017378833145 Scheduler overhead time: 0.055635270196944475 Adapter cache time: 0.01698401290923357 Engine time: 0.05660636303946376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211495412 . Total output tokens: 185964859
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.86349336290732,
    "estimated_duration": 3600.0660583206004,
    "input_throughput": 6030.926279760637,
    "output_throughput": 5244.124606093195,
    "total_throughput": 11275.050885853832,
    "itl": 84.18626925254607,
    "ttft": 1816895.2704033605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 315978,
    "finished_requests": 87706,
    "scheduler_time": 31.111508389884925
}
#Debug simulation 
Total elapsed time: 6.8635745560750365. Arrivals time: 0.708188442979008 Scheduler time: 5.98303098930046 Scheduler overhead time: 0.06176399393007159 Adapter cache time: 0.018572949338704348 Engine time: 0.0631817034445703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.783304765354842,
    "estimated_duration": 3600.0445094746765,
    "input_throughput": 6534.599763443696,
    "output_throughput": 5686.655524986088,
    "total_throughput": 12221.255288429784,
    "itl": 101.75917333049757,
    "ttft": 1751034.433519723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 314998,
    "finished_requests": 95024,
    "scheduler_time": 45.67698158961821
}
#Debug simulation 
Total elapsed time: 6.783451121300459. Arrivals time: 0.34385111602023244 Scheduler time: 6.295687676407397 Scheduler overhead time: 0.05295063694939017 Adapter cache time: 0.013226756826043129 Engine time: 0.05344326002523303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.6250062878243625,
    "estimated_duration": 3600.005583835537,
    "input_throughput": 6378.2373291588165,
    "output_throughput": 5553.018053572348,
    "total_throughput": 11931.255382731164,
    "itl": 94.72343674786175,
    "ttft": 1771568.108789065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489667,
    "arrivals": 314998,
    "finished_requests": 92763,
    "scheduler_time": 40.832744840533735
}
#Debug simulation 
Total elapsed time: 6.625146985985339. Arrivals time: 0.3404097929596901 Scheduler time: 6.131857448723167 Scheduler overhead time: 0.05599504802376032 Adapter cache time: 0.014107065740972757 Engine time: 0.056990440003573895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.423376040067524,
    "estimated_duration": 3600.0701975144657,
    "input_throughput": 6070.0474715985565,
    "output_throughput": 5286.775800410909,
    "total_throughput": 11356.823272009466,
    "itl": 83.47855747382941,
    "ttft": 1810909.2161310152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281584,
    "arrivals": 314998,
    "finished_requests": 88248,
    "scheduler_time": 31.380686562654844
}
#Debug simulation 
Total elapsed time: 6.423499169293791. Arrivals time: 0.3188305264338851 Scheduler time: 5.934100387617946 Scheduler overhead time: 0.06194358877837658 Adapter cache time: 0.016346307937055826 Engine time: 0.06332568731158972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.9758162489160895,
    "estimated_duration": 3600.0134892057818,
    "input_throughput": 6378.048045888173,
    "output_throughput": 5553.083637031138,
    "total_throughput": 11931.131682919311,
    "itl": 94.72315740148622,
    "ttft": 1771558.8808467097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939706,
    "arrivals": 314998,
    "finished_requests": 92761,
    "scheduler_time": 40.83286353578538
}
#Debug simulation 
Total elapsed time: 6.975967112928629. Arrivals time: 0.6630623904056847 Scheduler time: 6.1608418840914965 Scheduler overhead time: 0.0556760816834867 Adapter cache time: 0.013861334882676601 Engine time: 0.05676804156973958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.4032311597839,
    "estimated_duration": 3600.068269916846,
    "input_throughput": 6069.665451234545,
    "output_throughput": 5286.544468902418,
    "total_throughput": 11356.209920136962,
    "itl": 83.47960301802316,
    "ttft": 1810971.4894838592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4766343009192497,
    "arrivals": 314998,
    "finished_requests": 88245,
    "scheduler_time": 31.38275605040599
}
#Debug simulation 
Total elapsed time: 6.403372618835419. Arrivals time: 0.3387427651323378 Scheduler time: 5.893451869953424 Scheduler overhead time: 0.062219871673732996 Adapter cache time: 0.016575542744249105 Engine time: 0.06364815402776003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.692141534760594,
    "estimated_duration": 3600.0753500271157,
    "input_throughput": 6378.209278266092,
    "output_throughput": 5553.069604459647,
    "total_throughput": 11931.27888272574,
    "itl": 94.72323402489533,
    "ttft": 1771521.6239249182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 314998,
    "finished_requests": 92765,
    "scheduler_time": 40.83428630411914
}
#Debug simulation 
Total elapsed time: 6.692319054622203. Arrivals time: 0.34119359962642193 Scheduler time: 6.197990143671632 Scheduler overhead time: 0.055962301790714264 Adapter cache time: 0.014035771135240793 Engine time: 0.057033529970794916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210856106 . Total output tokens: 185420046
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.409067173022777,
    "estimated_duration": 3600.015767153037,
    "input_throughput": 6070.001192600628,
    "output_throughput": 5286.8221227406975,
    "total_throughput": 11356.823315341326,
    "itl": 83.47978191445203,
    "ttft": 1810916.774484131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47207772303372625,
    "arrivals": 314998,
    "finished_requests": 88248,
    "scheduler_time": 31.380684593548466
}
#Debug simulation 
Total elapsed time: 6.409330890048295. Arrivals time: 0.3284346414729953 Scheduler time: 5.9098439095541835 Scheduler overhead time: 0.06234076479449868 Adapter cache time: 0.016431857366114855 Engine time: 0.06339288130402565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.9902518088929355,
    "estimated_duration": 3600.0949317313116,
    "input_throughput": 6622.080098464269,
    "output_throughput": 5715.380674727067,
    "total_throughput": 12337.460773191337,
    "itl": 101.00911411155006,
    "ttft": 1737055.0743075896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42319417804479564,
    "arrivals": 314533,
    "finished_requests": 95985,
    "scheduler_time": 45.889765276000134
}
#Debug simulation 
Total elapsed time: 6.990436868742108. Arrivals time: 0.33778491243720055 Scheduler time: 6.508824745658785 Scheduler overhead time: 0.05276332888752222 Adapter cache time: 0.011769923847168684 Engine time: 0.054710193537175655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.715600338298827,
    "estimated_duration": 3600.035538010746,
    "input_throughput": 6464.3631303871125,
    "output_throughput": 5577.058834006451,
    "total_throughput": 12041.421964393563,
    "itl": 94.0952356177787,
    "ttft": 1758493.360744559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4682644420489668,
    "arrivals": 314533,
    "finished_requests": 93668,
    "scheduler_time": 41.01759964997863
}
#Debug simulation 
Total elapsed time: 6.715719855390489. Arrivals time: 0.37952097225934267 Scheduler time: 6.182829771656543 Scheduler overhead time: 0.056661759968847036 Adapter cache time: 0.013354719616472721 Engine time: 0.05735192680731416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.4642740320414305,
    "estimated_duration": 3600.084450862302,
    "input_throughput": 6156.392524261849,
    "output_throughput": 5306.772732907407,
    "total_throughput": 11463.165257169256,
    "itl": 82.97261772419249,
    "ttft": 1799657.1748857694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4809837616281585,
    "arrivals": 314533,
    "finished_requests": 89113,
    "scheduler_time": 31.484794141589212
}
#Debug simulation 
Total elapsed time: 6.464438824914396. Arrivals time: 0.36927894968539476 Scheduler time: 5.92660646000877 Scheduler overhead time: 0.06187031976878643 Adapter cache time: 0.014682191424071789 Engine time: 0.06326633226126432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.710309169720858,
    "estimated_duration": 3600.0414463071793,
    "input_throughput": 6464.43835358396,
    "output_throughput": 5577.084680676433,
    "total_throughput": 12041.523034260394,
    "itl": 94.09405045699175,
    "ttft": 1758449.7690666297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43911180160939695,
    "arrivals": 314533,
    "finished_requests": 93670,
    "scheduler_time": 41.01812655790507
}
#Debug simulation 
Total elapsed time: 6.710410275962204. Arrivals time: 0.3701121103949845 Scheduler time: 6.187592253088951 Scheduler overhead time: 0.056392901577055454 Adapter cache time: 0.013202126137912273 Engine time: 0.05721639143303037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.446450565010309,
    "estimated_duration": 3600.006001559025,
    "input_throughput": 6156.708902818922,
    "output_throughput": 5306.925041715589,
    "total_throughput": 11463.633944534511,
    "itl": 82.97104234335823,
    "ttft": 1799621.5089523254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47663430091924974,
    "arrivals": 314533,
    "finished_requests": 89114,
    "scheduler_time": 31.482562748013677
}
#Debug simulation 
Total elapsed time: 6.446557614952326. Arrivals time: 0.37491571297869086 Scheduler time: 5.902102410327643 Scheduler overhead time: 0.06225112406536937 Adapter cache time: 0.015035318676382303 Engine time: 0.06342229712754488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.691585578024387,
    "estimated_duration": 3600.0307282770636,
    "input_throughput": 6464.371766942584,
    "output_throughput": 5577.066285100553,
    "total_throughput": 12041.438052043137,
    "itl": 94.09533397304607,
    "ttft": 1758408.4140305943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4085709401965145,
    "arrivals": 314533,
    "finished_requests": 93668,
    "scheduler_time": 41.01915309067151
}
#Debug simulation 
Total elapsed time: 6.691740544047207. Arrivals time: 0.38411950366571546 Scheduler time: 6.154663325753063 Scheduler overhead time: 0.056468251161277294 Adapter cache time: 0.013170322868973017 Engine time: 0.05727264657616615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210517925 . Total output tokens: 185137004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.486138980370015,
    "estimated_duration": 3600.001397856245,
    "input_throughput": 6156.716776054168,
    "output_throughput": 5306.931828242278,
    "total_throughput": 11463.648604296446,
    "itl": 82.97183539613751,
    "ttft": 1799672.926051917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4720777230337263,
    "arrivals": 314533,
    "finished_requests": 89114,
    "scheduler_time": 31.482771643934022
}
#Debug simulation 
Total elapsed time: 6.4862441583536565. Arrivals time: 0.33003891073167324 Scheduler time: 5.986328481230885 Scheduler overhead time: 0.0625098473392427 Adapter cache time: 0.015043180901557207 Engine time: 0.06358769675716758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 282640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.963140548206866,
    "estimated_duration": 3600.048736224488,
    "input_throughput": 6634.931010698,
    "output_throughput": 5733.3848823520275,
    "total_throughput": 12368.315893050027,
    "itl": 100.72660954535104,
    "ttft": 1737441.8088913953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4165817690128457,
    "arrivals": 314275,
    "finished_requests": 96138,
    "scheduler_time": 46.03778365124694
}
#Debug simulation 
Total elapsed time: 6.963257796131074. Arrivals time: 0.38132210075855255 Scheduler time: 6.437406008131802 Scheduler overhead time: 0.053645182866603136 Adapter cache time: 0.011838207487016916 Engine time: 0.05430828034877777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.82427191734314,
    "estimated_duration": 3600.058027747334,
    "input_throughput": 6471.702906016374,
    "output_throughput": 5593.034013565397,
    "total_throughput": 12064.736919581772,
    "itl": 93.87702721972771,
    "ttft": 1758851.7188787377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46049230013508324,
    "arrivals": 314275,
    "finished_requests": 93789,
    "scheduler_time": 41.16041262286074
}
#Debug simulation 
Total elapsed time: 6.824398598168045. Arrivals time: 0.34898917097598314 Scheduler time: 6.321876967325807 Scheduler overhead time: 0.05668036639690399 Adapter cache time: 0.01251192670315504 Engine time: 0.05816745152696967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.442932420875877,
    "estimated_duration": 3600.0557818418947,
    "input_throughput": 6172.1804734457455,
    "output_throughput": 5321.604486416642,
    "total_throughput": 11493.784959862389,
    "itl": 82.82810678778196,
    "ttft": 1801796.4048284364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47290768825449053,
    "arrivals": 314275,
    "finished_requests": 89354,
    "scheduler_time": 31.607628877163624
}
#Debug simulation 
Total elapsed time: 6.443139764945954. Arrivals time: 0.32316251238808036 Scheduler time: 5.950681544374675 Scheduler overhead time: 0.06225193478167057 Adapter cache time: 0.014819413423538208 Engine time: 0.06361054163426161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.810805685818195,
    "estimated_duration": 3600.032900880739,
    "input_throughput": 6471.668910109169,
    "output_throughput": 5593.104717202427,
    "total_throughput": 12064.773627311595,
    "itl": 93.87536427142959,
    "ttft": 1758845.5404356054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43133965969551363,
    "arrivals": 314275,
    "finished_requests": 93789,
    "scheduler_time": 41.159610770213156
}
#Debug simulation 
Total elapsed time: 6.810905892867595. Arrivals time: 0.349162052385509 Scheduler time: 6.308281871955842 Scheduler overhead time: 0.056780118960887194 Adapter cache time: 0.012810572050511837 Engine time: 0.057796863839030266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.501367315184325,
    "estimated_duration": 3600.015624428588,
    "input_throughput": 6171.968490699742,
    "output_throughput": 5321.523015067686,
    "total_throughput": 11493.491505767428,
    "itl": 82.82941916424114,
    "ttft": 1801832.2103943676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4685582275455818,
    "arrivals": 314275,
    "finished_requests": 89352,
    "scheduler_time": 31.608378282444132
}
#Debug simulation 
Total elapsed time: 6.501492340117693. Arrivals time: 0.31436937721446157 Scheduler time: 6.017919775098562 Scheduler overhead time: 0.06221789354458451 Adapter cache time: 0.014587935525923967 Engine time: 0.06357716303318739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 256640,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.871622513048351,
    "estimated_duration": 3600.0639301605556,
    "input_throughput": 6471.951179756103,
    "output_throughput": 5593.410670099391,
    "total_throughput": 12065.361849855493,
    "itl": 93.87445447111224,
    "ttft": 1758823.797630446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40218701925594397,
    "arrivals": 314275,
    "finished_requests": 93793,
    "scheduler_time": 41.160411123923296
}
#Debug simulation 
Total elapsed time: 6.871727728750557. Arrivals time: 0.3892440665513277 Scheduler time: 6.328387920744717 Scheduler overhead time: 0.05703408410772681 Adapter cache time: 0.012915887869894505 Engine time: 0.05798144079744816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 215104,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210374034 . Total output tokens: 185002220
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.6051680888049304,
    "estimated_duration": 3600.044860483601,
    "input_throughput": 6172.19947559629,
    "output_throughput": 5321.672851995081,
    "total_throughput": 11493.87232759137,
    "itl": 82.83054528519925,
    "ttft": 1801750.9740575964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 63,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46420876683667306,
    "arrivals": 314275,
    "finished_requests": 89355,
    "scheduler_time": 31.60952420414829
}
#Debug simulation 
Total elapsed time: 6.605283105745912. Arrivals time: 0.32343699038028717 Scheduler time: 6.1120864767581224 Scheduler overhead time: 0.06221663812175393 Adapter cache time: 0.014652665704488754 Engine time: 0.06406084774062037 
