INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 71.28066300880164,
    "estimated_duration": 3600.0240254015316,
    "input_throughput": 6832.901621331923,
    "output_throughput": 5981.276471508639,
    "total_throughput": 12814.178092840562,
    "itl": 100.48810927215546,
    "ttft": 1907622.508441397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3819934876775353,
    "arrivals": 572657,
    "finished_requests": 99961,
    "scheduler_time": 210.70544789550084
}
#Debug simulation 
Total elapsed time: 71.28088908502832. Arrivals time: 0.5131262168288231 Scheduler time: 70.57415893767029 Scheduler overhead time: 0.07510811788961291 Adapter cache time: 0.014276362955570221 Engine time: 0.07506230287253857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.17736301897094,
    "estimated_duration": 3600.0938952957094,
    "input_throughput": 6757.555693697185,
    "output_throughput": 5921.462778472606,
    "total_throughput": 12679.018472169791,
    "itl": 97.81336493329911,
    "ttft": 1913357.0196642682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5144343499885884,
    "arrivals": 572657,
    "finished_requests": 98874,
    "scheduler_time": 213.0093786592788
}
#Debug simulation 
Total elapsed time: 71.17756273690611. Arrivals time: 0.500220746267587 Scheduler time: 70.47943173861131 Scheduler overhead time: 0.07726640440523624 Adapter cache time: 0.015093509573489428 Engine time: 0.07641688315197825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 66.77414570283145,
    "estimated_duration": 3600.045079448117,
    "input_throughput": 6598.423207423158,
    "output_throughput": 5779.447907132757,
    "total_throughput": 12377.871114555914,
    "itl": 91.59574984283438,
    "ttft": 1932596.87515139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.629941820274111,
    "arrivals": 572657,
    "finished_requests": 96535,
    "scheduler_time": 219.17768538846684
}
#Debug simulation 
Total elapsed time: 66.77431891812012. Arrivals time: 0.49264207668602467 Scheduler time: 66.07910467218608 Scheduler overhead time: 0.0781563138589263 Adapter cache time: 0.01531663304194808 Engine time: 0.0783716943114996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 69.50198439788073,
    "estimated_duration": 3600.0728577838986,
    "input_throughput": 6759.92041310538,
    "output_throughput": 5917.027471803093,
    "total_throughput": 12676.947884908473,
    "itl": 97.8076247919017,
    "ttft": 1913736.0443486783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3711832142993794,
    "arrivals": 572657,
    "finished_requests": 98838,
    "scheduler_time": 213.16877928659991
}
#Debug simulation 
Total elapsed time: 69.50216125976294. Arrivals time: 0.5235779765062034 Scheduler time: 68.78370507899672 Scheduler overhead time: 0.07592033036053181 Adapter cache time: 0.014433992095291615 Engine time: 0.07509756041690707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 62.982647099066526,
    "estimated_duration": 3600.0572651871266,
    "input_throughput": 6577.243431367812,
    "output_throughput": 5758.275625352441,
    "total_throughput": 12335.519056720254,
    "itl": 91.64567684198815,
    "ttft": 1930970.1813141226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7827072903048293,
    "arrivals": 572657,
    "finished_requests": 96170,
    "scheduler_time": 219.02883556621617
}
#Debug simulation 
Total elapsed time: 62.9828173648566. Arrivals time: 0.4892764422111213 Scheduler time: 62.29204717185348 Scheduler overhead time: 0.07743362290784717 Adapter cache time: 0.015303824096918106 Engine time: 0.0781417884863913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 69.75708274263889,
    "estimated_duration": 3600.047151460734,
    "input_throughput": 6758.418702968307,
    "output_throughput": 5918.712478905821,
    "total_throughput": 12677.131181874127,
    "itl": 97.90705927513531,
    "ttft": 1914256.481949735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2831681090546736,
    "arrivals": 572657,
    "finished_requests": 98871,
    "scheduler_time": 213.0113369458395
}
#Debug simulation 
Total elapsed time: 69.75725105591118. Arrivals time: 0.5915325884707272 Scheduler time: 68.96963955601677 Scheduler overhead time: 0.07678686315193772 Adapter cache time: 0.014691302087157965 Engine time: 0.07528965035453439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 67.30281179770827,
    "estimated_duration": 3600.0353462900403,
    "input_throughput": 6566.4813609014645,
    "output_throughput": 5756.562646351963,
    "total_throughput": 12323.044007253427,
    "itl": 91.54678259838853,
    "ttft": 1932032.4416089638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6590296320989768,
    "arrivals": 572657,
    "finished_requests": 96104,
    "scheduler_time": 219.1765241320383
}
#Debug simulation 
Total elapsed time: 67.30299438303337. Arrivals time: 0.5097309025004506 Scheduler time: 66.58980041649193 Scheduler overhead time: 0.07966744899749756 Adapter cache time: 0.01536483271047473 Engine time: 0.07783358450978994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 76.70888881990686,
    "estimated_duration": 3600.0549393110878,
    "input_throughput": 6875.618405072645,
    "output_throughput": 5988.335834709633,
    "total_throughput": 12863.954239782279,
    "itl": 100.45100726793079,
    "ttft": 1892904.867125831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1637839896231876,
    "arrivals": 564989,
    "finished_requests": 100267,
    "scheduler_time": 210.62027335133504
}
#Debug simulation 
Total elapsed time: 76.70906382380053. Arrivals time: 0.5279046003706753 Scheduler time: 75.98348339740187 Scheduler overhead time: 0.07759971730411053 Adapter cache time: 0.014304982032626867 Engine time: 0.07638698816299438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.94077090872452,
    "estimated_duration": 3600.0558898721947,
    "input_throughput": 6786.542139174221,
    "output_throughput": 5920.091979671077,
    "total_throughput": 12706.634118845299,
    "itl": 97.50817824324396,
    "ttft": 1900170.9408929248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4919489612942576,
    "arrivals": 564989,
    "finished_requests": 99029,
    "scheduler_time": 213.41357632230623
}
#Debug simulation 
Total elapsed time: 74.9409478129819. Arrivals time: 0.5321811968460679 Scheduler time: 74.2085035755299 Scheduler overhead time: 0.07845873385667801 Adapter cache time: 0.014921741560101509 Engine time: 0.07695027254521847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 68.12006069114432,
    "estimated_duration": 3600.027646912405,
    "input_throughput": 6621.479426816193,
    "output_throughput": 5768.749308859215,
    "total_throughput": 12390.228735675408,
    "itl": 91.7281697130923,
    "ttft": 1916376.204764468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3910772318532705,
    "arrivals": 564989,
    "finished_requests": 96589,
    "scheduler_time": 218.70126715698777
}
#Debug simulation 
Total elapsed time: 68.12023260490969. Arrivals time: 0.5096583855338395 Scheduler time: 67.40477016987279 Scheduler overhead time: 0.08042743522673845 Adapter cache time: 0.015010516159236431 Engine time: 0.07924577686935663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 76.69489148259163,
    "estimated_duration": 3600.0130679723375,
    "input_throughput": 6789.065355744926,
    "output_throughput": 5921.999336521247,
    "total_throughput": 12711.064692266173,
    "itl": 97.55300436642439,
    "ttft": 1899381.6848325615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.208251564912497,
    "arrivals": 564989,
    "finished_requests": 99084,
    "scheduler_time": 213.23324402024366
}
#Debug simulation 
Total elapsed time: 76.69508780585602. Arrivals time: 0.5356478402391076 Scheduler time: 75.95858633052558 Scheduler overhead time: 0.07905274629592896 Adapter cache time: 0.014563762582838535 Engine time: 0.07745060184970498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 75.79161516716704,
    "estimated_duration": 3600.02079502644,
    "input_throughput": 6475.222040996237,
    "output_throughput": 5656.973711967247,
    "total_throughput": 12132.195752963484,
    "itl": 88.25433329391292,
    "ttft": 1910457.7367472728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3467601421149462,
    "arrivals": 564989,
    "finished_requests": 94500,
    "scheduler_time": 223.0117681620466
}
#Debug simulation 
Total elapsed time: 75.79179329890758. Arrivals time: 0.5306496699340641 Scheduler time: 75.04301697947085 Scheduler overhead time: 0.08506416575983167 Adapter cache time: 0.016129964031279087 Engine time: 0.08430932741612196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 74.15729225194082,
    "estimated_duration": 3600.0537209629083,
    "input_throughput": 6805.557055256073,
    "output_throughput": 5931.256768659759,
    "total_throughput": 12736.813823915832,
    "itl": 97.84508973725426,
    "ttft": 1901204.2892928205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1427218483621226,
    "arrivals": 564989,
    "finished_requests": 99260,
    "scheduler_time": 212.98827469621398
}
#Debug simulation 
Total elapsed time: 74.1574658760801. Arrivals time: 0.5194334383122623 Scheduler time: 73.43776016449556 Scheduler overhead time: 0.07897742791101336 Adapter cache time: 0.014354662038385868 Engine time: 0.07716174935922027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 65.30475089792162,
    "estimated_duration": 3600.0618370100397,
    "input_throughput": 6612.681969866289,
    "output_throughput": 5766.999273891111,
    "total_throughput": 12379.6812437574,
    "itl": 91.72399474028526,
    "ttft": 1917814.064294414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6706229869462588,
    "arrivals": 564989,
    "finished_requests": 96501,
    "scheduler_time": 218.69551422346672
}
#Debug simulation 
Total elapsed time: 65.30493215983734. Arrivals time: 0.5119391162879765 Scheduler time: 64.58988368697464 Scheduler overhead time: 0.07863661274313927 Adapter cache time: 0.015431512612849474 Engine time: 0.07853115908801556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 58.33183076791465,
    "estimated_duration": 3600.0889271828532,
    "input_throughput": 6843.602338256525,
    "output_throughput": 6005.796644839885,
    "total_throughput": 12849.39898309641,
    "itl": 100.67774722679658,
    "ttft": 1897513.250346409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4216679418692348,
    "arrivals": 561196,
    "finished_requests": 100100,
    "scheduler_time": 209.83413001112586
}
#Debug simulation 
Total elapsed time: 58.332004878204316. Arrivals time: 0.6001241006888449 Scheduler time: 57.541995195671916 Scheduler overhead time: 0.07335306238383055 Adapter cache time: 0.014200465753674507 Engine time: 0.07378556113690138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 57.37717281607911,
    "estimated_duration": 3600.0811076796467,
    "input_throughput": 6771.071615025718,
    "output_throughput": 5937.946496371615,
    "total_throughput": 12709.018111397332,
    "itl": 97.86438891518544,
    "ttft": 1903344.9720675768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5532950595580053,
    "arrivals": 561196,
    "finished_requests": 98996,
    "scheduler_time": 212.24746560275977
}
#Debug simulation 
Total elapsed time: 57.377354437019676. Arrivals time: 0.49645478278398514 Scheduler time: 56.6865227567032 Scheduler overhead time: 0.07538803992792964 Adapter cache time: 0.014683478511869907 Engine time: 0.07517085038125515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 56.33761367108673,
    "estimated_duration": 3600.058043048304,
    "input_throughput": 6577.887833149656,
    "output_throughput": 5776.5221425128475,
    "total_throughput": 12354.409975662504,
    "itl": 91.46770951410794,
    "ttft": 1918163.4623353067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7616406898433388,
    "arrivals": 561196,
    "finished_requests": 96181,
    "scheduler_time": 218.73177949666743
}
#Debug simulation 
Total elapsed time: 56.337799676228315. Arrivals time: 0.47932617366313934 Scheduler time: 55.65546520892531 Scheduler overhead time: 0.07806134317070246 Adapter cache time: 0.015417567454278469 Engine time: 0.07820254238322377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 66.19128267280757,
    "estimated_duration": 3600.057473803523,
    "input_throughput": 6734.3360978028495,
    "output_throughput": 5921.958234037774,
    "total_throughput": 12656.294331840623,
    "itl": 97.61390939126666,
    "ttft": 1889089.4571213075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6018448828905805,
    "arrivals": 561196,
    "finished_requests": 98567,
    "scheduler_time": 213.24931022663984
}
#Debug simulation 
Total elapsed time: 66.19146179780364. Arrivals time: 0.5637936484999955 Scheduler time: 65.4301597471349 Scheduler overhead time: 0.0776661871932447 Adapter cache time: 0.014855335466563702 Engine time: 0.07550776470452547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 55.259891028981656,
    "estimated_duration": 3600.0691839207298,
    "input_throughput": 6570.913444012546,
    "output_throughput": 5769.680786350511,
    "total_throughput": 12340.594230363058,
    "itl": 91.50541174793273,
    "ttft": 1917763.1973243353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.587590667824264,
    "arrivals": 561196,
    "finished_requests": 96053,
    "scheduler_time": 218.59038010894807
}
#Debug simulation 
Total elapsed time: 55.260070723015815. Arrivals time: 0.5753293260931969 Scheduler time: 54.48088437039405 Scheduler overhead time: 0.07893646834418178 Adapter cache time: 0.015344011131674051 Engine time: 0.07874544337391853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 57.2215302279219,
    "estimated_duration": 3600.0736495624014,
    "input_throughput": 6775.167225527401,
    "output_throughput": 5943.018694242729,
    "total_throughput": 12718.18591977013,
    "itl": 98.0803267572328,
    "ttft": 1903372.8189589249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5257571047963525,
    "arrivals": 561196,
    "finished_requests": 99037,
    "scheduler_time": 212.06450942760068
}
#Debug simulation 
Total elapsed time: 57.2217191089876. Arrivals time: 0.5471401195973158 Scheduler time: 56.478799434844404 Scheduler overhead time: 0.07572465995326638 Adapter cache time: 0.014967396855354309 Engine time: 0.0756910596974194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 55.64016404887661,
    "estimated_duration": 3600.0718185685832,
    "input_throughput": 6578.3573754971285,
    "output_throughput": 5772.096793408493,
    "total_throughput": 12350.45416890562,
    "itl": 91.59342558332905,
    "ttft": 1917510.5168519102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.645153919029984,
    "arrivals": 561196,
    "finished_requests": 96105,
    "scheduler_time": 218.51525274084867
}
#Debug simulation 
Total elapsed time: 55.64033821225166. Arrivals time: 0.5570571841672063 Scheduler time: 54.880804302636534 Scheduler overhead time: 0.0784789347089827 Adapter cache time: 0.015011847950518131 Engine time: 0.07808651635423303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 71.19236843613908,
    "estimated_duration": 3600.063300758262,
    "input_throughput": 6882.750921291043,
    "output_throughput": 6001.021147447505,
    "total_throughput": 12883.772068738548,
    "itl": 100.7189539301092,
    "ttft": 1898795.2601554652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1571715805912377,
    "arrivals": 559351,
    "finished_requests": 100411,
    "scheduler_time": 209.9924057950935
}
#Debug simulation 
Total elapsed time: 71.19254197599366. Arrivals time: 0.5708245993591845 Scheduler time: 70.42614554008469 Scheduler overhead time: 0.0763100367039442 Adapter cache time: 0.014114176854491234 Engine time: 0.07644077576696873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.12581991078332,
    "estimated_duration": 3600.0531006716487,
    "input_throughput": 6809.769276854896,
    "output_throughput": 5934.028583082404,
    "total_throughput": 12743.7978599373,
    "itl": 97.94097159077094,
    "ttft": 1906716.8046330658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5632864594925209,
    "arrivals": 559351,
    "finished_requests": 99298,
    "scheduler_time": 212.5816275314342
}
#Debug simulation 
Total elapsed time: 75.12599547766149. Arrivals time: 0.5955797415226698 Scheduler time: 74.33090709568933 Scheduler overhead time: 0.07735919440165162 Adapter cache time: 0.015108945779502392 Engine time: 0.07736269943416119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 65.24344043293968,
    "estimated_duration": 3600.024368338152,
    "input_throughput": 6608.347768207489,
    "output_throughput": 5768.239843772485,
    "total_throughput": 12376.587611979974,
    "itl": 91.65974146869516,
    "ttft": 1923092.4889703656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5491810865374314,
    "arrivals": 559351,
    "finished_requests": 96441,
    "scheduler_time": 218.6159543402359
}
#Debug simulation 
Total elapsed time: 65.24361340794712. Arrivals time: 0.4880356299690902 Scheduler time: 64.55452334880829 Scheduler overhead time: 0.0778135871514678 Adapter cache time: 0.014990482944995165 Engine time: 0.07749397493898869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 69.83360844105482,
    "estimated_duration": 3600.10183843282,
    "input_throughput": 6805.043329181134,
    "output_throughput": 5935.867083499664,
    "total_throughput": 12740.910412680798,
    "itl": 98.02327095198343,
    "ttft": 1907098.4330060866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2174119277996933,
    "arrivals": 559351,
    "finished_requests": 99348,
    "scheduler_time": 212.3970202354937
}
#Debug simulation 
Total elapsed time: 69.83378444705158. Arrivals time: 0.5724359764717519 Scheduler time: 69.06509133940563 Scheduler overhead time: 0.07683980045840144 Adapter cache time: 0.014307655394077301 Engine time: 0.07564603444188833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 73.00456386106089,
    "estimated_duration": 3600.0632122142156,
    "input_throughput": 6553.245209683333,
    "output_throughput": 5717.17100137832,
    "total_throughput": 12270.416211061653,
    "itl": 90.07184148681581,
    "ttft": 1918423.7080724842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.548038769662383,
    "arrivals": 559351,
    "finished_requests": 95533,
    "scheduler_time": 220.62111141535024
}
#Debug simulation 
Total elapsed time: 73.00475099822506. Arrivals time: 0.5750022418797016 Scheduler time: 72.22207189816982 Scheduler overhead time: 0.08159052301198244 Adapter cache time: 0.015541150700300932 Engine time: 0.07916500512510538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 75.77318814722821,
    "estimated_duration": 3600.0262717389455,
    "input_throughput": 6809.143919985692,
    "output_throughput": 5931.950321486032,
    "total_throughput": 12741.094241471725,
    "itl": 97.9897068574092,
    "ttft": 1907399.9412689777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1108022436592702,
    "arrivals": 559351,
    "finished_requests": 99336,
    "scheduler_time": 212.49065733379794
}
#Debug simulation 
Total elapsed time: 75.77336089825258. Arrivals time: 0.5767117901705205 Scheduler time: 74.99814179958776 Scheduler overhead time: 0.07752469647675753 Adapter cache time: 0.014175460208207369 Engine time: 0.07669411692768335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.3438765220344,
    "estimated_duration": 3600.0306897670935,
    "input_throughput": 6543.547272238403,
    "output_throughput": 5710.982703130824,
    "total_throughput": 12254.529975369227,
    "itl": 89.89628921089516,
    "ttft": 1918257.543151136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6627540307492048,
    "arrivals": 559351,
    "finished_requests": 95428,
    "scheduler_time": 220.8628279803413
}
#Debug simulation 
Total elapsed time: 74.34404562599957. Arrivals time: 0.509877173230052 Scheduler time: 73.6279127993621 Scheduler overhead time: 0.07973171630874276 Adapter cache time: 0.015470737125724554 Engine time: 0.0799900684505701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 75.75078199524432,
    "estimated_duration": 3600.051473767142,
    "input_throughput": 6874.17754449605,
    "output_throughput": 5991.44544381455,
    "total_throughput": 12865.6229883106,
    "itl": 100.60547599657258,
    "ttft": 1896230.6443257004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2828073521982863,
    "arrivals": 558425,
    "finished_requests": 100121,
    "scheduler_time": 210.26103189830224
}
#Debug simulation 
Total elapsed time: 75.75096264202148. Arrivals time: 0.5086246770806611 Scheduler time: 75.04677260387689 Scheduler overhead time: 0.07646870007738471 Adapter cache time: 0.01465044915676117 Engine time: 0.07548093376681209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 69.02994622522965,
    "estimated_duration": 3600.019027478288,
    "input_throughput": 6801.669883714991,
    "output_throughput": 5933.700304623968,
    "total_throughput": 12735.370188338959,
    "itl": 97.95218509628015,
    "ttft": 1903338.3581190773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.51026968706865,
    "arrivals": 558425,
    "finished_requests": 99084,
    "scheduler_time": 212.60255195620726
}
#Debug simulation 
Total elapsed time: 69.03010893287137. Arrivals time: 0.5096851093694568 Scheduler time: 68.32374612474814 Scheduler overhead time: 0.07635791786015034 Adapter cache time: 0.01482030563056469 Engine time: 0.07615131279453635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.75882460922003,
    "estimated_duration": 3600.030901671497,
    "input_throughput": 6606.769955490327,
    "output_throughput": 5763.371639495245,
    "total_throughput": 12370.141594985573,
    "itl": 91.58972162779666,
    "ttft": 1919187.3707416812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.859794418122624,
    "arrivals": 558425,
    "finished_requests": 96339,
    "scheduler_time": 218.81222292931773
}
#Debug simulation 
Total elapsed time: 71.75900471117347. Arrivals time: 0.49922331888228655 Scheduler time: 71.05640371656045 Scheduler overhead time: 0.07870970852673054 Adapter cache time: 0.015566595830023289 Engine time: 0.07843085937201977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 67.83607765007764,
    "estimated_duration": 3600.1026037532415,
    "input_throughput": 6803.2885991820385,
    "output_throughput": 5932.657857510311,
    "total_throughput": 12735.94645669235,
    "itl": 97.978556041369,
    "ttft": 1902261.1532286014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3059557839203615,
    "arrivals": 558425,
    "finished_requests": 99133,
    "scheduler_time": 212.52404940145655
}
#Debug simulation 
Total elapsed time: 67.83624891377985. Arrivals time: 0.6301945196464658 Scheduler time: 67.00821520760655 Scheduler overhead time: 0.07695937342941761 Adapter cache time: 0.014299243222922087 Engine time: 0.07667187368497252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 70.13850265508518,
    "estimated_duration": 3600.004140476931,
    "input_throughput": 6605.463513952973,
    "output_throughput": 5763.858370799273,
    "total_throughput": 12369.321884752246,
    "itl": 91.63512217246635,
    "ttft": 1918319.489142254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7915581081388554,
    "arrivals": 558425,
    "finished_requests": 96321,
    "scheduler_time": 218.82148774505194
}
#Debug simulation 
Total elapsed time: 70.13866833411157. Arrivals time: 0.49205579049885273 Scheduler time: 69.44340438162908 Scheduler overhead time: 0.07927602902054787 Adapter cache time: 0.01543431170284748 Engine time: 0.07743234653025866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 74.95508533809334,
    "estimated_duration": 3600.051790404084,
    "input_throughput": 6783.530743944904,
    "output_throughput": 5915.457121134821,
    "total_throughput": 12698.987865079725,
    "itl": 97.71942461340701,
    "ttft": 1900794.246070952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2448645834112506,
    "arrivals": 558425,
    "finished_requests": 98893,
    "scheduler_time": 213.09773996731468
}
#Debug simulation 
Total elapsed time: 74.95525250723585. Arrivals time: 0.5620867158286273 Scheduler time: 74.19595283363014 Scheduler overhead time: 0.07731139054521918 Adapter cache time: 0.014249985571950674 Engine time: 0.07586264796555042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.15872871829197,
    "estimated_duration": 3600.052007421354,
    "input_throughput": 6611.172269438372,
    "output_throughput": 5765.773649161221,
    "total_throughput": 12376.945918599593,
    "itl": 91.67202464797151,
    "ttft": 1919139.6619025324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.880441935937856,
    "arrivals": 558425,
    "finished_requests": 96400,
    "scheduler_time": 218.6925038241185
}
#Debug simulation 
Total elapsed time: 71.1588988401927. Arrivals time: 0.5025966875255108 Scheduler time: 70.45312523143366 Scheduler overhead time: 0.07849619118496776 Adapter cache time: 0.015456675086170435 Engine time: 0.07835469488054514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 70.44978201203048,
    "estimated_duration": 3600.0525750719703,
    "input_throughput": 6900.944772869973,
    "output_throughput": 5998.506563357144,
    "total_throughput": 12899.451336227117,
    "itl": 100.61899662754736,
    "ttft": 1895330.2383499676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3026445792941361,
    "arrivals": 557951,
    "finished_requests": 100676,
    "scheduler_time": 209.9630786425551
}
#Debug simulation 
Total elapsed time: 70.44994403095916. Arrivals time: 0.4673265847377479 Scheduler time: 69.7965462198481 Scheduler overhead time: 0.07245779177173972 Adapter cache time: 0.013515241909772158 Engine time: 0.07210581749677658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 69.71071897726506,
    "estimated_duration": 3600.086853919324,
    "input_throughput": 6828.775526133714,
    "output_throughput": 5935.815125331106,
    "total_throughput": 12764.590651464821,
    "itl": 97.95187943810457,
    "ttft": 1901256.969145766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3659326165774848,
    "arrivals": 557951,
    "finished_requests": 99611,
    "scheduler_time": 212.22841896152383
}
#Debug simulation 
Total elapsed time: 69.71087925508618. Arrivals time: 0.487461818382144 Scheduler time: 69.03946070419624 Scheduler overhead time: 0.07185640232637525 Adapter cache time: 0.013371293433010578 Engine time: 0.07046166434884071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 65.25724362535402,
    "estimated_duration": 3600.0963565336797,
    "input_throughput": 6638.46148357291,
    "output_throughput": 5763.85544857454,
    "total_throughput": 12402.31693214745,
    "itl": 91.51369770566257,
    "ttft": 1917671.0267239767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.518585599437359,
    "arrivals": 557951,
    "finished_requests": 96792,
    "scheduler_time": 218.65126829604526
}
#Debug simulation 
Total elapsed time: 65.25741972215474. Arrivals time: 0.701532777864486 Scheduler time: 64.36206869035959 Scheduler overhead time: 0.0754538718611002 Adapter cache time: 0.013991627842187881 Engine time: 0.07444558292627335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 67.24859905987978,
    "estimated_duration": 3600.064163240126,
    "input_throughput": 6818.120146477731,
    "output_throughput": 5937.4744534446945,
    "total_throughput": 12755.594599922426,
    "itl": 97.95889297150427,
    "ttft": 1900440.199267188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3392636095965267,
    "arrivals": 557951,
    "finished_requests": 99502,
    "scheduler_time": 212.26858074754145
}
#Debug simulation 
Total elapsed time: 67.24876211676747. Arrivals time: 0.5099110687151551 Scheduler time: 66.5533059509471 Scheduler overhead time: 0.07236674521118402 Adapter cache time: 0.013263458851724863 Engine time: 0.07154048187658191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 67.34325500996783,
    "estimated_duration": 3600.0929317458986,
    "input_throughput": 6642.539082568293,
    "output_throughput": 5769.874943179976,
    "total_throughput": 12412.41402574827,
    "itl": 91.64795635644819,
    "ttft": 1918796.8835677803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4425837124697878,
    "arrivals": 557951,
    "finished_requests": 96815,
    "scheduler_time": 218.5339425346418
}
#Debug simulation 
Total elapsed time: 67.34341064281762. Arrivals time: 0.5140631343238056 Scheduler time: 66.6380118415691 Scheduler overhead time: 0.07415338885039091 Adapter cache time: 0.0136745092459023 Engine time: 0.07337193144485354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 46.7457080646418,
    "estimated_duration": 3600.088732009147,
    "input_throughput": 6820.726606451213,
    "output_throughput": 5936.096466176129,
    "total_throughput": 12756.823072627343,
    "itl": 97.60618099628182,
    "ttft": 1901743.819978625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.723658653954038,
    "arrivals": 557951,
    "finished_requests": 99507,
    "scheduler_time": 212.6892545882645
}
#Debug simulation 
Total elapsed time: 46.74588317470625. Arrivals time: 0.4811722324229777 Scheduler time: 46.086122108157724 Scheduler overhead time: 0.06861213222146034 Adapter cache time: 0.01363541604951024 Engine time: 0.06867343792691827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 65.52641063788906,
    "estimated_duration": 3600.0223580077777,
    "input_throughput": 6645.134285565544,
    "output_throughput": 5778.719110931437,
    "total_throughput": 12423.853396496981,
    "itl": 91.62972745043149,
    "ttft": 1920375.7043681776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5885695364326267,
    "arrivals": 557951,
    "finished_requests": 96950,
    "scheduler_time": 218.67720800749282
}
#Debug simulation 
Total elapsed time: 65.52656911499798. Arrivals time: 0.5018157251179218 Scheduler time: 64.83202893845737 Scheduler overhead time: 0.0749636935070157 Adapter cache time: 0.01396962534636259 Engine time: 0.07399056339636445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 80.25667353020981,
    "estimated_duration": 3600.094876960509,
    "input_throughput": 6848.575063337271,
    "output_throughput": 6000.714908446836,
    "total_throughput": 12849.289971784106,
    "itl": 100.92007490822817,
    "ttft": 1873367.7758857412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2548314798949343,
    "arrivals": 518627,
    "finished_requests": 100128,
    "scheduler_time": 209.41582630257108
}
#Debug simulation 
Total elapsed time: 80.25683995941654. Arrivals time: 0.44551701191812754 Scheduler time: 79.62290239380673 Scheduler overhead time: 0.07327240379527211 Adapter cache time: 0.015232098288834095 Engine time: 0.07183879055082798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.37445992510766,
    "estimated_duration": 3600.021582809457,
    "input_throughput": 6775.567156729193,
    "output_throughput": 5937.827179168723,
    "total_throughput": 12713.394335897916,
    "itl": 98.30323071381383,
    "ttft": 1881131.2938402258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6469288560282473,
    "arrivals": 518627,
    "finished_requests": 99066,
    "scheduler_time": 211.723885381816
}
#Debug simulation 
Total elapsed time: 77.374615335837. Arrivals time: 0.4766479483805597 Scheduler time: 76.70715115545318 Scheduler overhead time: 0.07386937411502004 Adapter cache time: 0.015602690167725086 Engine time: 0.07265016390010715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 76.29364169435576,
    "estimated_duration": 3600.0371399916744,
    "input_throughput": 6587.398428910332,
    "output_throughput": 5772.640723380999,
    "total_throughput": 12360.03915229133,
    "itl": 91.94708820016334,
    "ttft": 1897387.6242456767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.909662831290644,
    "arrivals": 518627,
    "finished_requests": 96253,
    "scheduler_time": 217.92629897222142
}
#Debug simulation 
Total elapsed time: 76.29378732014447. Arrivals time: 0.6930772042833269 Scheduler time: 75.40335124963894 Scheduler overhead time: 0.07653519744053483 Adapter cache time: 0.01568692270666361 Engine time: 0.07516316790133715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 80.51701415376738,
    "estimated_duration": 3600.081215608416,
    "input_throughput": 6778.217917474404,
    "output_throughput": 5937.970206704696,
    "total_throughput": 12716.1881241791,
    "itl": 98.22497077629966,
    "ttft": 1879839.7091308662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3368363057868518,
    "arrivals": 518627,
    "finished_requests": 99113,
    "scheduler_time": 211.79691696796084
}
#Debug simulation 
Total elapsed time: 80.51716623781249. Arrivals time: 0.4538144487887621 Scheduler time: 79.87077701697126 Scheduler overhead time: 0.07538466760888696 Adapter cache time: 0.015185513533651829 Engine time: 0.07303481921553612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 75.49565308401361,
    "estimated_duration": 3600.0278987062125,
    "input_throughput": 6585.15118966711,
    "output_throughput": 5772.266655896774,
    "total_throughput": 12357.417845563883,
    "itl": 91.97413690890198,
    "ttft": 1897716.8445600122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.882789467312414,
    "arrivals": 518627,
    "finished_requests": 96260,
    "scheduler_time": 217.90633591154707
}
#Debug simulation 
Total elapsed time: 75.49578604102135. Arrivals time: 0.44314196752384305 Scheduler time: 74.85232679732144 Scheduler overhead time: 0.07789193093776703 Adapter cache time: 0.016133280005306005 Engine time: 0.07588172983378172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 78.09590378124267,
    "estimated_duration": 3600.069126377151,
    "input_throughput": 6773.497714622875,
    "output_throughput": 5933.283570445046,
    "total_throughput": 12706.78128506792,
    "itl": 98.08299628418347,
    "ttft": 1880235.215338165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2662919339025303,
    "arrivals": 518627,
    "finished_requests": 98986,
    "scheduler_time": 211.94804234473352
}
#Debug simulation 
Total elapsed time: 78.09604231314734. Arrivals time: 0.5059385215863585 Scheduler time: 77.39904975192621 Scheduler overhead time: 0.07430444844067097 Adapter cache time: 0.014939648564904928 Engine time: 0.07260977057740092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.75284713087603,
    "estimated_duration": 3600.0014135419,
    "input_throughput": 6582.108526642724,
    "output_throughput": 5771.274678350396,
    "total_throughput": 12353.38320499312,
    "itl": 91.92646980540403,
    "ttft": 1897952.7956440109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.875175007730741,
    "arrivals": 518627,
    "finished_requests": 96229,
    "scheduler_time": 217.95289311378065
}
#Debug simulation 
Total elapsed time: 75.7529854811728. Arrivals time: 0.44442454213276505 Scheduler time: 75.10955143207684 Scheduler overhead time: 0.07762468978762627 Adapter cache time: 0.01585365179926157 Engine time: 0.07550042541697621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.07102451799437,
    "estimated_duration": 3600.0657684596526,
    "input_throughput": 6864.999305436157,
    "output_throughput": 5998.546245792572,
    "total_throughput": 12863.545551228728,
    "itl": 100.6626276259524,
    "ttft": 1872493.7058536436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3540176153741865,
    "arrivals": 514902,
    "finished_requests": 100549,
    "scheduler_time": 209.43273327308628
}
#Debug simulation 
Total elapsed time: 79.0711823492311. Arrivals time: 0.5085162380710244 Scheduler time: 78.37365470314398 Scheduler overhead time: 0.07316422928124666 Adapter cache time: 0.015121710021048784 Engine time: 0.07215544674545527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.78435147786513,
    "estimated_duration": 3600.108928864355,
    "input_throughput": 6786.561318772963,
    "output_throughput": 5931.991898571979,
    "total_throughput": 12718.553217344943,
    "itl": 97.8306605326176,
    "ttft": 1879657.233985273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5436717531271307,
    "arrivals": 514902,
    "finished_requests": 99413,
    "scheduler_time": 211.97664376565967
}
#Debug simulation 
Total elapsed time: 77.7845057551749. Arrivals time: 0.4378970651887357 Scheduler time: 77.15606231521815 Scheduler overhead time: 0.07453362783417106 Adapter cache time: 0.015155380591750145 Engine time: 0.07230437733232975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.02159376908094,
    "estimated_duration": 3600.078195750453,
    "input_throughput": 6598.040294802119,
    "output_throughput": 5769.388293986861,
    "total_throughput": 12367.42858878898,
    "itl": 91.66657508696022,
    "ttft": 1898435.3171900187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.100843868665418,
    "arrivals": 514902,
    "finished_requests": 96686,
    "scheduler_time": 217.99200951933258
}
#Debug simulation 
Total elapsed time: 74.0217384439893. Arrivals time: 0.4336890568956733 Scheduler time: 73.38918217085302 Scheduler overhead time: 0.07757601095363498 Adapter cache time: 0.016158179845660925 Engine time: 0.07490550819784403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 77.19801434502006,
    "estimated_duration": 3600.0863426900164,
    "input_throughput": 6787.845255328122,
    "output_throughput": 5930.552483373695,
    "total_throughput": 12718.397738701817,
    "itl": 97.83069622934026,
    "ttft": 1879260.0460017507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2999210011167417,
    "arrivals": 514902,
    "finished_requests": 99382,
    "scheduler_time": 211.9745926581449
}
#Debug simulation 
Total elapsed time: 77.19815818360075. Arrivals time: 0.4450409049168229 Scheduler time: 76.56151581881568 Scheduler overhead time: 0.07482303911820054 Adapter cache time: 0.014821821358054876 Engine time: 0.07320888200774789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 74.89484530314803,
    "estimated_duration": 3600.0540702157205,
    "input_throughput": 6603.264711125725,
    "output_throughput": 5770.201389990581,
    "total_throughput": 12373.466101116306,
    "itl": 91.7673806154585,
    "ttft": 1898076.2041561732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0591647989629047,
    "arrivals": 514902,
    "finished_requests": 96731,
    "scheduler_time": 217.87829592149305
}
#Debug simulation 
Total elapsed time: 74.89498791238293. Arrivals time: 0.4980372008867562 Scheduler time: 74.19638099474832 Scheduler overhead time: 0.07740960363298655 Adapter cache time: 0.01619939599186182 Engine time: 0.076570606790483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 77.63081154087558,
    "estimated_duration": 3600.089324960237,
    "input_throughput": 6793.008948540495,
    "output_throughput": 5934.483028483181,
    "total_throughput": 12727.491977023677,
    "itl": 97.99275624205215,
    "ttft": 1880015.733274301,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3875864317733697,
    "arrivals": 514902,
    "finished_requests": 99470,
    "scheduler_time": 211.81783606870775
}
#Debug simulation 
Total elapsed time: 77.63096579303965. Arrivals time: 0.49081441573798656 Scheduler time: 76.94721873477101 Scheduler overhead time: 0.07525874581187963 Adapter cache time: 0.015524122398346663 Engine time: 0.07345064962282777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.76620858488604,
    "estimated_duration": 3600.0105313708323,
    "input_throughput": 6597.809865560452,
    "output_throughput": 5765.974521217808,
    "total_throughput": 12363.78438677826,
    "itl": 91.58666543121645,
    "ttft": 1897922.5160979393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.989064793139715,
    "arrivals": 514902,
    "finished_requests": 96638,
    "scheduler_time": 218.0749717222547
}
#Debug simulation 
Total elapsed time: 74.76635389681906. Arrivals time: 0.44064308144152164 Scheduler time: 74.12545556947589 Scheduler overhead time: 0.07798683317378163 Adapter cache time: 0.016192985698580742 Engine time: 0.0762151051312685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.68367507401854,
    "estimated_duration": 3600.080661964081,
    "input_throughput": 6901.746192110154,
    "output_throughput": 5994.232081518652,
    "total_throughput": 12895.978273628805,
    "itl": 100.68650413349944,
    "ttft": 1871588.737583049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5193278411729403,
    "arrivals": 512974,
    "finished_requests": 100516,
    "scheduler_time": 209.5291804972028
}
#Debug simulation 
Total elapsed time: 81.68381967814639. Arrivals time: 0.5124982902780175 Scheduler time: 80.97935438202694 Scheduler overhead time: 0.07508799247443676 Adapter cache time: 0.015532022807747126 Engine time: 0.07308830227702856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 80.09649085206911,
    "estimated_duration": 3600.080325112032,
    "input_throughput": 6825.3465981305135,
    "output_throughput": 5927.99245370612,
    "total_throughput": 12753.339051836634,
    "itl": 97.93290878462,
    "ttft": 1878625.8050113164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.867041940758939,
    "arrivals": 512974,
    "finished_requests": 99380,
    "scheduler_time": 211.98035980677986
}
#Debug simulation 
Total elapsed time: 80.09663377795368. Arrivals time: 0.5193398213014007 Scheduler time: 79.38370613520965 Scheduler overhead time: 0.07592191128060222 Adapter cache time: 0.01553725777193904 Engine time: 0.07358791492879391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.42814504913986,
    "estimated_duration": 3600.0265498370277,
    "input_throughput": 6435.060041723508,
    "output_throughput": 5591.801816270302,
    "total_throughput": 12026.86185799381,
    "itl": 86.8719179912481,
    "ttft": 1886351.7375667612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8665039353631765,
    "arrivals": 512974,
    "finished_requests": 93664,
    "scheduler_time": 224.9234334301914
}
#Debug simulation 
Total elapsed time: 75.42828489420936. Arrivals time: 0.4301670757122338 Scheduler time: 74.7903750045225 Scheduler overhead time: 0.08009115839377046 Adapter cache time: 0.017472061794251204 Engine time: 0.07838144758716226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 79.97357935318723,
    "estimated_duration": 3600.0501836262124,
    "input_throughput": 6826.185954787664,
    "output_throughput": 5929.990669879106,
    "total_throughput": 12756.176624666768,
    "itl": 98.03174178834644,
    "ttft": 1878547.3267374828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7101634930912355,
    "arrivals": 512974,
    "finished_requests": 99396,
    "scheduler_time": 211.89742801945354
}
#Debug simulation 
Total elapsed time: 79.97372436476871. Arrivals time: 0.5103844315744936 Scheduler time: 79.26918296003714 Scheduler overhead time: 0.07513357186689973 Adapter cache time: 0.015627040062099695 Engine time: 0.07441736618056893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 77.90419217012823,
    "estimated_duration": 3600.081137563447,
    "input_throughput": 6634.401028018237,
    "output_throughput": 5763.5450999982695,
    "total_throughput": 12397.946128016507,
    "itl": 91.6538314736331,
    "ttft": 1896790.3454471792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1311743835499732,
    "arrivals": 512974,
    "finished_requests": 96598,
    "scheduler_time": 218.14154812335417
}
#Debug simulation 
Total elapsed time: 77.90432894602418. Arrivals time: 0.49612543545663357 Scheduler time: 77.20479922555387 Scheduler overhead time: 0.07961458805948496 Adapter cache time: 0.016448628157377243 Engine time: 0.07648026430979371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.63298861915246,
    "estimated_duration": 3600.0958124707577,
    "input_throughput": 6824.947245817108,
    "output_throughput": 5931.18992167752,
    "total_throughput": 12756.137167494628,
    "itl": 98.02646618262324,
    "ttft": 1878848.8757186558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4258899574167927,
    "arrivals": 512974,
    "finished_requests": 99397,
    "scheduler_time": 211.90883787832857
}
#Debug simulation 
Total elapsed time: 79.63314314978197. Arrivals time: 0.4530536774545908 Scheduler time: 78.98555064108223 Scheduler overhead time: 0.07653999840840697 Adapter cache time: 0.015444742515683174 Engine time: 0.07360540051013231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.72724798321724,
    "estimated_duration": 3600.013129838864,
    "input_throughput": 6630.975260100923,
    "output_throughput": 5762.949537055888,
    "total_throughput": 12393.924797156811,
    "itl": 91.62517762653499,
    "ttft": 1896305.7410646633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.075983821414433,
    "arrivals": 512974,
    "finished_requests": 96559,
    "scheduler_time": 218.1797116041192
}
#Debug simulation 
Total elapsed time: 77.72738347807899. Arrivals time: 0.5116091310046613 Scheduler time: 77.0134880328551 Scheduler overhead time: 0.0789891667664051 Adapter cache time: 0.01625959761440754 Engine time: 0.07688329787924886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 74.80607681674883,
    "estimated_duration": 3600.076867945918,
    "input_throughput": 6814.316443747815,
    "output_throughput": 5997.767767753222,
    "total_throughput": 12812.084211501038,
    "itl": 100.74285274771636,
    "ttft": 1871582.1726980247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4333665237575883,
    "arrivals": 512003,
    "finished_requests": 100040,
    "scheduler_time": 209.499512759675
}
#Debug simulation 
Total elapsed time: 74.8062208481133. Arrivals time: 0.48943406576290727 Scheduler time: 74.12945514172316 Scheduler overhead time: 0.07315776590257883 Adapter cache time: 0.014794700313359499 Engine time: 0.07097933394834399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 73.38347893301398,
    "estimated_duration": 3600.032460882133,
    "input_throughput": 6746.198059016656,
    "output_throughput": 5936.207029300918,
    "total_throughput": 12682.405088317573,
    "itl": 98.12200069689659,
    "ttft": 1878918.5196335113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.678574607609775,
    "arrivals": 512003,
    "finished_requests": 98984,
    "scheduler_time": 211.82164733409206
}
#Debug simulation 
Total elapsed time: 73.38363026781008. Arrivals time: 0.44879093347117305 Scheduler time: 72.7449699533172 Scheduler overhead time: 0.07415647571906447 Adapter cache time: 0.01507488964125514 Engine time: 0.07182816555723548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 72.01298469631001,
    "estimated_duration": 3600.0338240877622,
    "input_throughput": 6557.62977615415,
    "output_throughput": 5768.626633740687,
    "total_throughput": 12326.256409894837,
    "itl": 91.79766160340711,
    "ttft": 1895791.6007797406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 397,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9943091364158514,
    "arrivals": 512003,
    "finished_requests": 96211,
    "scheduler_time": 218.01381064522428
}
#Debug simulation 
Total elapsed time: 72.01312689529732. Arrivals time: 0.49839363200590014 Scheduler time: 71.31738944910467 Scheduler overhead time: 0.076490119099617 Adapter cache time: 0.015593886841088533 Engine time: 0.07498478656634688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 73.2633229601197,
    "estimated_duration": 3600.080571251718,
    "input_throughput": 6745.447086356904,
    "output_throughput": 5936.03881275628,
    "total_throughput": 12681.485899113184,
    "itl": 98.12508984080785,
    "ttft": 1878719.498761768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.566666937330734,
    "arrivals": 512003,
    "finished_requests": 98990,
    "scheduler_time": 211.82608979397133
}
#Debug simulation 
Total elapsed time: 73.26346835074946. Arrivals time: 0.5103702433407307 Scheduler time: 72.56433172198012 Scheduler overhead time: 0.07339457469061017 Adapter cache time: 0.015031722374260426 Engine time: 0.07165127154439688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 71.54931468889117,
    "estimated_duration": 3600.085466657662,
    "input_throughput": 6556.949055411048,
    "output_throughput": 5768.242779880284,
    "total_throughput": 12325.191835291333,
    "itl": 91.74407436117721,
    "ttft": 1895617.2276607011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7426040005451235,
    "arrivals": 512003,
    "finished_requests": 96185,
    "scheduler_time": 218.08254309137575
}
#Debug simulation 
Total elapsed time: 71.54945641709492. Arrivals time: 0.48726704716682434 Scheduler time: 70.86366963433102 Scheduler overhead time: 0.0773520590737462 Adapter cache time: 0.01550299720838666 Engine time: 0.07539916504174471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 73.71437680022791,
    "estimated_duration": 3600.025016697925,
    "input_throughput": 6749.735595528759,
    "output_throughput": 5937.742904799832,
    "total_throughput": 12687.47850032859,
    "itl": 98.16420702515825,
    "ttft": 1879381.4419641346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4258899574167927,
    "arrivals": 512003,
    "finished_requests": 99024,
    "scheduler_time": 211.7832973443979
}
#Debug simulation 
Total elapsed time: 73.71451397705823. Arrivals time: 0.4481757297180593 Scheduler time: 73.07598579814658 Scheduler overhead time: 0.07426717458292842 Adapter cache time: 0.014968124683946371 Engine time: 0.07205397123470902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 71.69073367118835,
    "estimated_duration": 3600.033367572154,
    "input_throughput": 6556.589784032582,
    "output_throughput": 5766.859048309612,
    "total_throughput": 12323.448832342194,
    "itl": 91.69007914511141,
    "ttft": 1895894.7764765203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.816367915701139,
    "arrivals": 512003,
    "finished_requests": 96170,
    "scheduler_time": 218.1276351901276
}
#Debug simulation 
Total elapsed time: 71.69087760616094. Arrivals time: 0.5003399406559765 Scheduler time: 70.99362508254126 Scheduler overhead time: 0.07641794672235847 Adapter cache time: 0.015628620982170105 Engine time: 0.07462705858051777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 77.89936938788742,
    "estimated_duration": 3600.0143825977357,
    "input_throughput": 6817.080820185786,
    "output_throughput": 5991.813283933286,
    "total_throughput": 12808.894104119072,
    "itl": 100.67685855849858,
    "ttft": 1872330.0936068539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.149032935383732,
    "arrivals": 511517,
    "finished_requests": 99871,
    "scheduler_time": 209.62317268071223
}
#Debug simulation 
Total elapsed time: 77.89951095590368. Arrivals time: 0.4485000530257821 Scheduler time: 77.26331751514226 Scheduler overhead time: 0.07355963764712214 Adapter cache time: 0.014491973910480738 Engine time: 0.07136154174804688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.22979231411591,
    "estimated_duration": 3600.070206663943,
    "input_throughput": 6744.239863727316,
    "output_throughput": 5925.723326314947,
    "total_throughput": 12669.963190042263,
    "itl": 97.9829554428205,
    "ttft": 1879750.4398036345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.511752148424278,
    "arrivals": 511517,
    "finished_requests": 98779,
    "scheduler_time": 212.18714073685683
}
#Debug simulation 
Total elapsed time: 77.22993579506874. Arrivals time: 0.44149431120604277 Scheduler time: 76.59783713612705 Scheduler overhead time: 0.07409864058718085 Adapter cache time: 0.014869352336972952 Engine time: 0.0727460253983736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.49936904013157,
    "estimated_duration": 3600.039211865664,
    "input_throughput": 6563.877949472114,
    "output_throughput": 5761.956961921568,
    "total_throughput": 12325.834911393682,
    "itl": 91.75125578986757,
    "ttft": 1897829.5907307537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8247115955688233,
    "arrivals": 511517,
    "finished_requests": 96118,
    "scheduler_time": 218.2446273485485
}
#Debug simulation 
Total elapsed time: 74.49950742721558. Arrivals time: 0.4465055060572922 Scheduler time: 73.85456466721371 Scheduler overhead time: 0.07738454733043909 Adapter cache time: 0.015508436597883701 Engine time: 0.07537600584328175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 76.6229324103333,
    "estimated_duration": 3600.016435813961,
    "input_throughput": 6744.105598648624,
    "output_throughput": 5926.091277740623,
    "total_throughput": 12670.196876389247,
    "itl": 98.02018901671639,
    "ttft": 1880290.0747457922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3845835251221406,
    "arrivals": 511517,
    "finished_requests": 98789,
    "scheduler_time": 212.1619125734869
}
#Debug simulation 
Total elapsed time: 76.62307337811217. Arrivals time: 0.45051468443125486 Scheduler time: 75.98264140728861 Scheduler overhead time: 0.07408884866163135 Adapter cache time: 0.014909554738551378 Engine time: 0.07213589269667864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 74.67982575111091,
    "estimated_duration": 3600.0665043752256,
    "input_throughput": 6561.596840306015,
    "output_throughput": 5760.453584620372,
    "total_throughput": 12322.050424926387,
    "itl": 91.76840771221696,
    "ttft": 1897182.8184757377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7913672266481617,
    "arrivals": 511517,
    "finished_requests": 96086,
    "scheduler_time": 218.23573345759362
}
#Debug simulation 
Total elapsed time: 74.67995873419568. Arrivals time: 0.4678144659847021 Scheduler time: 74.01355324825272 Scheduler overhead time: 0.07748408755287528 Adapter cache time: 0.015554216224700212 Engine time: 0.07540231943130493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 76.89402207080275,
    "estimated_duration": 3600.0543777184134,
    "input_throughput": 6739.557366179809,
    "output_throughput": 5923.338306215977,
    "total_throughput": 12662.895672395785,
    "itl": 97.88888768413887,
    "ttft": 1879559.4090719842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0620064638042743,
    "arrivals": 511517,
    "finished_requests": 98703,
    "scheduler_time": 212.30340199742386
}
#Debug simulation 
Total elapsed time: 76.89416592894122. Arrivals time: 0.4433264941908419 Scheduler time: 76.2585830357857 Scheduler overhead time: 0.0751602970995009 Adapter cache time: 0.014859731309115887 Engine time: 0.0732331839390099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 74.02741566905752,
    "estimated_duration": 3600.084160071397,
    "input_throughput": 6562.264088718269,
    "output_throughput": 5761.681693460364,
    "total_throughput": 12323.945782178633,
    "itl": 91.76900152665331,
    "ttft": 1897417.991112072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8403352943994284,
    "arrivals": 511517,
    "finished_requests": 96073,
    "scheduler_time": 218.23503045857237
}
#Debug simulation 
Total elapsed time: 74.02756168879569. Arrivals time: 0.4357537324540317 Scheduler time: 73.39405768644065 Scheduler overhead time: 0.0769084976054728 Adapter cache time: 0.015587035566568375 Engine time: 0.07515194592997432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 81.1523594497703,
    "estimated_duration": 3600.089172473061,
    "input_throughput": 6889.644898146089,
    "output_throughput": 6003.308241710428,
    "total_throughput": 12892.953139856518,
    "itl": 100.91507637636198,
    "ttft": 1864113.6858687967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9491344282497,
    "arrivals": 507129,
    "finished_requests": 100737,
    "scheduler_time": 209.1887591013682
}
#Debug simulation 
Total elapsed time: 81.15249667270109. Arrivals time: 0.5200856970623136 Scheduler time: 80.43992371531203 Scheduler overhead time: 0.07531448779627681 Adapter cache time: 0.015616093296557665 Engine time: 0.0730710169300437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 80.14931159606203,
    "estimated_duration": 3600.0967342531076,
    "input_throughput": 6815.776300269513,
    "output_throughput": 5933.274735861096,
    "total_throughput": 12749.051036130608,
    "itl": 98.07866230421317,
    "ttft": 1870562.783439699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3203192829061345,
    "arrivals": 507129,
    "finished_requests": 99609,
    "scheduler_time": 211.7008451612015
}
#Debug simulation 
Total elapsed time: 80.14945518504828. Arrivals time: 0.45549471117556095 Scheduler time: 79.5004633241333 Scheduler overhead time: 0.07552987476810813 Adapter cache time: 0.015613646246492863 Engine time: 0.07328910613432527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.92128683114424,
    "estimated_duration": 3600.071119602327,
    "input_throughput": 6629.944855821778,
    "output_throughput": 5771.114322401224,
    "total_throughput": 12401.059178223002,
    "itl": 91.84663261388609,
    "ttft": 1889037.3686571075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7122856520675436,
    "arrivals": 507129,
    "finished_requests": 96896,
    "scheduler_time": 217.75780270905133
}
#Debug simulation 
Total elapsed time: 77.92143322806805. Arrivals time: 0.4442165424115956 Scheduler time: 77.27413899032399 Scheduler overhead time: 0.07895761542022228 Adapter cache time: 0.016869761049747467 Engine time: 0.07707942184060812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 79.80755805410445,
    "estimated_duration": 3600.0162726585872,
    "input_throughput": 6814.107254544186,
    "output_throughput": 5933.354013487739,
    "total_throughput": 12747.461268031926,
    "itl": 98.12226678131107,
    "ttft": 1870058.4671582843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.142060336712743,
    "arrivals": 507129,
    "finished_requests": 99585,
    "scheduler_time": 211.66424414782276
}
#Debug simulation 
Total elapsed time: 79.80769411986694. Arrivals time: 0.4519966011866927 Scheduler time: 79.16093852883205 Scheduler overhead time: 0.07622121274471283 Adapter cache time: 0.01604649191722274 Engine time: 0.0736311818473041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 76.30362976389006,
    "estimated_duration": 3600.0617133053206,
    "input_throughput": 6630.718832340057,
    "output_throughput": 5772.048829941175,
    "total_throughput": 12402.767662281232,
    "itl": 91.89222794039382,
    "ttft": 1889363.20034225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.83481536609124,
    "arrivals": 507129,
    "finished_requests": 96904,
    "scheduler_time": 217.6986213059651
}
#Debug simulation 
Total elapsed time: 76.30376718612388. Arrivals time: 0.41187437484040856 Scheduler time: 75.69479166902602 Scheduler overhead time: 0.07619821280241013 Adapter cache time: 0.016373759135603905 Engine time: 0.07439919980242848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 78.88738436577842,
    "estimated_duration": 3600.055778878636,
    "input_throughput": 6817.892973770907,
    "output_throughput": 5938.282435906862,
    "total_throughput": 12756.175409677768,
    "itl": 98.18887020011208,
    "ttft": 1870809.467372859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.866380502316157,
    "arrivals": 507129,
    "finished_requests": 99662,
    "scheduler_time": 211.60981227877227
}
#Debug simulation 
Total elapsed time: 78.88751437095925. Arrivals time: 0.39216753048822284 Scheduler time: 78.30566739430651 Scheduler overhead time: 0.07368339132517576 Adapter cache time: 0.015621018130332232 Engine time: 0.07192135788500309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.02331246621907,
    "estimated_duration": 3600.0850864606946,
    "input_throughput": 6626.011726698237,
    "output_throughput": 5770.879993401742,
    "total_throughput": 12396.891720099979,
    "itl": 91.7995592282947,
    "ttft": 1888496.333238067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 466,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.438676826693145,
    "arrivals": 507129,
    "finished_requests": 96867,
    "scheduler_time": 217.8223986100555
}
#Debug simulation 
Total elapsed time: 77.02344798110425. Arrivals time: 0.3846939387731254 Scheduler time: 76.43977589625865 Scheduler overhead time: 0.07724192272871733 Adapter cache time: 0.01631626859307289 Engine time: 0.07526058610528708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.01349123660475,
    "estimated_duration": 3600.0863078635834,
    "input_throughput": 6848.236650923707,
    "output_throughput": 5995.873752485029,
    "total_throughput": 12844.110403408737,
    "itl": 100.59799151837095,
    "ttft": 1870022.3603439576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 467,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0879950179206532,
    "arrivals": 505238,
    "finished_requests": 100108,
    "scheduler_time": 209.45066178810671
}
#Debug simulation 
Total elapsed time: 79.01362063689157. Arrivals time: 0.3892524354159832 Scheduler time: 78.43790550110862 Scheduler overhead time: 0.07285792427137494 Adapter cache time: 0.015111506916582584 Engine time: 0.07023932505398989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.02583769895136,
    "estimated_duration": 3600.10209000828,
    "input_throughput": 6777.062258238318,
    "output_throughput": 5934.594204785693,
    "total_throughput": 12711.65646302401,
    "itl": 98.0942885013798,
    "ttft": 1877326.033383545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 493,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6026095029478937,
    "arrivals": 505238,
    "finished_requests": 99059,
    "scheduler_time": 211.6790595141803
}
#Debug simulation 
Total elapsed time: 78.02597832167521. Arrivals time: 0.6785395834594965 Scheduler time: 77.15769284870476 Scheduler overhead time: 0.07429005997255445 Adapter cache time: 0.01562915975227952 Engine time: 0.0714193070307374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 76.0981533061713,
    "estimated_duration": 3600.093459145835,
    "input_throughput": 6583.831855749567,
    "output_throughput": 5769.703546787444,
    "total_throughput": 12353.53540253701,
    "itl": 91.8034434943945,
    "ttft": 1895694.1255863507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 546,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.093406879021821,
    "arrivals": 505238,
    "finished_requests": 96284,
    "scheduler_time": 217.78286611855557
}
#Debug simulation 
Total elapsed time: 76.09828489320353. Arrivals time: 0.6788669978268445 Scheduler time: 75.22286854637787 Scheduler overhead time: 0.0761298113502562 Adapter cache time: 0.01641418831422925 Engine time: 0.07419618591666222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 78.19285960309207,
    "estimated_duration": 3600.010356814733,
    "input_throughput": 6774.720232077136,
    "output_throughput": 5933.833484551651,
    "total_throughput": 12708.553716628787,
    "itl": 97.96105183542738,
    "ttft": 1877611.879541711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.348291211710306,
    "arrivals": 505238,
    "finished_requests": 99038,
    "scheduler_time": 211.79653095125508
}
#Debug simulation 
Total elapsed time: 78.19299006275833. Arrivals time: 0.4446292775683105 Scheduler time: 77.55876119015738 Scheduler overhead time: 0.074013767298311 Adapter cache time: 0.015526182018220425 Engine time: 0.07150028273463249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 75.61996677424759,
    "estimated_duration": 3600.063889506555,
    "input_throughput": 6585.728955840752,
    "output_throughput": 5770.544256326364,
    "total_throughput": 12356.273212167116,
    "itl": 91.79941216657187,
    "ttft": 1895885.6476233765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.017298978562502,
    "arrivals": 505238,
    "finished_requests": 96305,
    "scheduler_time": 217.78754193085993
}
#Debug simulation 
Total elapsed time: 75.6201104032807. Arrivals time: 0.6204766426235437 Scheduler time: 74.80318186385557 Scheduler overhead time: 0.07577438373118639 Adapter cache time: 0.016584487631917 Engine time: 0.07432280760258436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 77.99376791995019,
    "estimated_duration": 3600.042245529808,
    "input_throughput": 6776.976861950548,
    "output_throughput": 5934.192029698256,
    "total_throughput": 12711.168891648804,
    "itl": 97.9929024158578,
    "ttft": 1877505.12685376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 505,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.223880074988105,
    "arrivals": 505238,
    "finished_requests": 99065,
    "scheduler_time": 211.77269577689654
}
#Debug simulation 
Total elapsed time: 77.99389948323369. Arrivals time: 0.4376089796423912 Scheduler time: 77.36735164187849 Scheduler overhead time: 0.0731954057700932 Adapter cache time: 0.01578063564375043 Engine time: 0.07140190759673715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 76.02232720702887,
    "estimated_duration": 3600.059635507943,
    "input_throughput": 6585.910068307725,
    "output_throughput": 5770.60885189166,
    "total_throughput": 12356.518920199385,
    "itl": 91.79912831884025,
    "ttft": 1896099.5495107565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.09954984966669,
    "arrivals": 505238,
    "finished_requests": 96317,
    "scheduler_time": 217.7836009672428
}
#Debug simulation 
Total elapsed time: 76.022479434032. Arrivals time: 0.6610862626694143 Scheduler time: 75.1649183658883 Scheduler overhead time: 0.07636462152004242 Adapter cache time: 0.0164284179918468 Engine time: 0.07330435188487172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.93315035989508,
    "estimated_duration": 3600.0031404617766,
    "input_throughput": 6949.438104320907,
    "output_throughput": 5995.757825153258,
    "total_throughput": 12945.195929474165,
    "itl": 100.64080626546215,
    "ttft": 1864245.3704578814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 483,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1937935624318556,
    "arrivals": 504263,
    "finished_requests": 100990,
    "scheduler_time": 209.41594953906588
}
#Debug simulation 
Total elapsed time: 79.93329680571333. Arrivals time: 0.39473257353529334 Scheduler time: 79.35011450527236 Scheduler overhead time: 0.07340531796216965 Adapter cache time: 0.01563486410304904 Engine time: 0.07098408881574869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 78.81467499397695,
    "estimated_duration": 3600.024086618164,
    "input_throughput": 6871.81402256654,
    "output_throughput": 5933.462523042727,
    "total_throughput": 12805.276545609267,
    "itl": 98.07474342514799,
    "ttft": 1871087.3466256745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.836047613485721,
    "arrivals": 504263,
    "finished_requests": 99888,
    "scheduler_time": 211.69229003154103
}
#Debug simulation 
Total elapsed time: 78.81481164274737. Arrivals time: 0.38603333476930857 Scheduler time: 78.23965622996911 Scheduler overhead time: 0.07361440686509013 Adapter cache time: 0.015647222753614187 Engine time: 0.07106736488640308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.3396102511324,
    "estimated_duration": 3600.017000143971,
    "input_throughput": 6675.332088442623,
    "output_throughput": 5766.841100797508,
    "total_throughput": 12442.173189240131,
    "itl": 91.76003169353054,
    "ttft": 1888102.7096881408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.416286785965819,
    "arrivals": 504263,
    "finished_requests": 96959,
    "scheduler_time": 217.83275073147374
}
#Debug simulation 
Total elapsed time: 77.33974859118462. Arrivals time: 0.4293794478289783 Scheduler time: 76.71157198678702 Scheduler overhead time: 0.07714813994243741 Adapter cache time: 0.01687833620235324 Engine time: 0.07476477278396487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 79.04544341797009,
    "estimated_duration": 3600.095111390047,
    "input_throughput": 6868.450758916903,
    "output_throughput": 5929.849167723025,
    "total_throughput": 12798.299926639927,
    "itl": 97.92081767908661,
    "ttft": 1870593.4851932535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5550792706338625,
    "arrivals": 504263,
    "finished_requests": 99816,
    "scheduler_time": 211.85534672228937
}
#Debug simulation 
Total elapsed time: 79.04558418923989. Arrivals time: 0.4365556538105011 Scheduler time: 78.41880483319983 Scheduler overhead time: 0.07396303489804268 Adapter cache time: 0.01595348957926035 Engine time: 0.07166294800117612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 76.91753173712641,
    "estimated_duration": 3600.0940062900036,
    "input_throughput": 6675.3123551807985,
    "output_throughput": 5766.746913754791,
    "total_throughput": 12442.059268935589,
    "itl": 91.76614693827354,
    "ttft": 1888001.8608079993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.369376421351951,
    "arrivals": 504263,
    "finished_requests": 96963,
    "scheduler_time": 217.83176783142738
}
#Debug simulation 
Total elapsed time: 76.91766596399248. Arrivals time: 0.42545387893915176 Scheduler time: 76.29303291812539 Scheduler overhead time: 0.07735788589343429 Adapter cache time: 0.016754317097365856 Engine time: 0.0750386631116271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.05003990791738,
    "estimated_duration": 3600.0534524760983,
    "input_throughput": 6870.297157112617,
    "output_throughput": 5931.751648107447,
    "total_throughput": 12802.048805220064,
    "itl": 97.96632386954951,
    "ttft": 1870703.382486968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1281212608795474,
    "arrivals": 504263,
    "finished_requests": 99857,
    "scheduler_time": 211.83274324327465
}
#Debug simulation 
Total elapsed time: 79.05017979210243. Arrivals time: 0.3919890411198139 Scheduler time: 78.46824053488672 Scheduler overhead time: 0.07415801472961903 Adapter cache time: 0.015603074803948402 Engine time: 0.07165263779461384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.53531832713634,
    "estimated_duration": 3600.0744302739336,
    "input_throughput": 6672.368159391697,
    "output_throughput": 5763.819165933493,
    "total_throughput": 12436.18732532519,
    "itl": 91.66703225953837,
    "ttft": 1887658.8112979806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.118291140496767,
    "arrivals": 504263,
    "finished_requests": 96937,
    "scheduler_time": 217.96254328249418
}
#Debug simulation 
Total elapsed time: 77.53545790910721. Arrivals time: 0.39489622274413705 Scheduler time: 76.94209997914732 Scheduler overhead time: 0.07689495058730245 Adapter cache time: 0.01670263335108757 Engine time: 0.07475247327238321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 79.29617890994996,
    "estimated_duration": 3600.0273729666146,
    "input_throughput": 6829.940290075683,
    "output_throughput": 5993.257485211929,
    "total_throughput": 12823.197775287612,
    "itl": 100.4442314720701,
    "ttft": 1867740.5643701418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6515760218119433,
    "arrivals": 503772,
    "finished_requests": 99920,
    "scheduler_time": 209.4778763046378
}
#Debug simulation 
Total elapsed time: 79.29631364392117. Arrivals time: 0.3893574355170131 Scheduler time: 78.72168393526226 Scheduler overhead time: 0.07275781687349081 Adapter cache time: 0.014637996908277273 Engine time: 0.06987403705716133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 77.63739993190393,
    "estimated_duration": 3600.0124546713287,
    "input_throughput": 6761.553829741494,
    "output_throughput": 5930.111706224377,
    "total_throughput": 12691.66553596587,
    "itl": 97.87737226388884,
    "ttft": 1875190.5165692393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3636185085168147,
    "arrivals": 503772,
    "finished_requests": 98885,
    "scheduler_time": 211.74206208469369
}
#Debug simulation 
Total elapsed time: 77.63753204001114. Arrivals time: 0.41595806181430817 Scheduler time: 77.03352883039042 Scheduler overhead time: 0.07353188842535019 Adapter cache time: 0.015346935484558344 Engine time: 0.07068363623693585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.0311179580167,
    "estimated_duration": 3600.063498747258,
    "input_throughput": 6565.0175360029825,
    "output_throughput": 5764.0305531335325,
    "total_throughput": 12329.048089136515,
    "itl": 91.63150289759399,
    "ttft": 1891383.9610236043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 542,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.050849747159554,
    "arrivals": 503772,
    "finished_requests": 96024,
    "scheduler_time": 217.86821106278092
}
#Debug simulation 
Total elapsed time: 75.03125987201929. Arrivals time: 0.3903171857818961 Scheduler time: 74.44483936717734 Scheduler overhead time: 0.07613082090392709 Adapter cache time: 0.016173590905964375 Engine time: 0.07353598065674305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 77.99902187800035,
    "estimated_duration": 3600.105626791382,
    "input_throughput": 6761.368838417847,
    "output_throughput": 5929.793793030024,
    "total_throughput": 12691.16263144787,
    "itl": 97.84415672463203,
    "ttft": 1874893.5663007714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 466,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.180363862356166,
    "arrivals": 503772,
    "finished_requests": 98886,
    "scheduler_time": 211.77994501725652
}
#Debug simulation 
Total elapsed time: 77.99915513489395. Arrivals time: 0.6376348263584077 Scheduler time: 77.17255290597677 Scheduler overhead time: 0.07390937767922878 Adapter cache time: 0.015202990733087063 Engine time: 0.07107914332300425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 74.7679112390615,
    "estimated_duration": 3600.00187380573,
    "input_throughput": 6562.604361931762,
    "output_throughput": 5762.8725559712175,
    "total_throughput": 12325.476917902979,
    "itl": 91.59727289129576,
    "ttft": 1891236.1525739364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.047787017170375,
    "arrivals": 503772,
    "finished_requests": 95998,
    "scheduler_time": 217.90676205426723
}
#Debug simulation 
Total elapsed time: 74.76804945804179. Arrivals time: 0.38635531160980463 Scheduler time: 74.1856991937384 Scheduler overhead time: 0.07598488265648484 Adapter cache time: 0.0163576053455472 Engine time: 0.07357110967859626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 78.47775954892859,
    "estimated_duration": 3600.1065585254114,
    "input_throughput": 6759.671027617509,
    "output_throughput": 5931.028610649188,
    "total_throughput": 12690.699638266697,
    "itl": 97.89331373036607,
    "ttft": 1874850.8999151394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.955755395484144,
    "arrivals": 503772,
    "finished_requests": 98885,
    "scheduler_time": 211.7494899975491
}
#Debug simulation 
Total elapsed time: 78.47790415817872. Arrivals time: 0.39763392647728324 Scheduler time: 77.88925110828131 Scheduler overhead time: 0.07436280651018023 Adapter cache time: 0.015518666245043278 Engine time: 0.0721027716062963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_32_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 75.93176855007187,
    "estimated_duration": 3600.0242729715146,
    "input_throughput": 6565.554620688809,
    "output_throughput": 5764.10613555999,
    "total_throughput": 12329.660756248799,
    "itl": 91.62712232297733,
    "ttft": 1891427.5866953258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9845188851468625,
    "arrivals": 503772,
    "finished_requests": 96031,
    "scheduler_time": 217.8695422952187
}
#Debug simulation 
Total elapsed time: 75.93188956426457. Arrivals time: 0.3770537613891065 Scheduler time: 75.35685022966936 Scheduler overhead time: 0.07698827423155308 Adapter cache time: 0.016237206291407347 Engine time: 0.07469250075519085 
