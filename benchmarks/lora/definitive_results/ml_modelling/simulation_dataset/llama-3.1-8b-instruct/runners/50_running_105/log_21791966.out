INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 89.04196958476678,
    "estimated_duration": 3600.099778739839,
    "input_throughput": 7563.158432661774,
    "output_throughput": 6587.339367660834,
    "total_throughput": 14150.497800322608,
    "itl": 90.06203685540812,
    "ttft": 1687122.4797369547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.760427038674242,
    "arrivals": 373254,
    "finished_requests": 110108,
    "scheduler_time": 257.2471437213099
}
#Debug simulation 
Total elapsed time: 89.04222262464464. Arrivals time: 0.6537647624500096 Scheduler time: 88.12678657798097 Scheduler overhead time: 0.10203831689432263 Adapter cache time: 0.018863579723984003 Engine time: 0.10417424095794559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 96.88937022024766,
    "estimated_duration": 3600.0721546941045,
    "input_throughput": 7507.257587812552,
    "output_throughput": 6534.307366402998,
    "total_throughput": 14041.56495421555,
    "itl": 88.80485903137345,
    "ttft": 1684388.6774099946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8502042859978981,
    "arrivals": 373254,
    "finished_requests": 109233,
    "scheduler_time": 259.3717222397377
}
#Debug simulation 
Total elapsed time: 96.88961759023368. Arrivals time: 0.6664437539875507 Scheduler time: 95.94897827506065 Scheduler overhead time: 0.10827918769791722 Adapter cache time: 0.019523724913597107 Engine time: 0.10755836451426148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 114.68630681093782,
    "estimated_duration": 3600.0242219050538,
    "input_throughput": 7498.964822440575,
    "output_throughput": 6540.037663285742,
    "total_throughput": 14039.002485726318,
    "itl": 86.69974449204878,
    "ttft": 1679027.6220855052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9458153765089812,
    "arrivals": 373254,
    "finished_requests": 109236,
    "scheduler_time": 258.9324665429851
}
#Debug simulation 
Total elapsed time: 114.68650048179552. Arrivals time: 0.7281305338256061 Scheduler time: 113.66477735619992 Scheduler overhead time: 0.11717602610588074 Adapter cache time: 0.021133828908205032 Engine time: 0.1159439766779542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 91.88658819021657,
    "estimated_duration": 3600.0592852147606,
    "input_throughput": 7503.003384120284,
    "output_throughput": 6536.649575923911,
    "total_throughput": 14039.652960044194,
    "itl": 88.85789531822384,
    "ttft": 1687562.3229802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8088315099198364,
    "arrivals": 373254,
    "finished_requests": 109229,
    "scheduler_time": 259.24627842854915
}
#Debug simulation 
Total elapsed time: 91.88677257113159. Arrivals time: 0.6171446121297777 Scheduler time: 91.00658781500533 Scheduler overhead time: 0.10434825206175447 Adapter cache time: 0.01889541046693921 Engine time: 0.10253691533580422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 100.65103210369125,
    "estimated_duration": 3600.0475083056594,
    "input_throughput": 7417.964329189799,
    "output_throughput": 6451.307641473518,
    "total_throughput": 13869.271970663318,
    "itl": 86.31442665886138,
    "ttft": 1689567.278720563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8638122984627263,
    "arrivals": 373254,
    "finished_requests": 107849,
    "scheduler_time": 262.84653727730455
}
#Debug simulation 
Total elapsed time: 100.65122984582558. Arrivals time: 0.6522780079394579 Scheduler time: 99.71519026625901 Scheduler overhead time: 0.11320613930001855 Adapter cache time: 0.020119004417210817 Engine time: 0.11139964032918215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 87.50069910520688,
    "estimated_duration": 3600.0930107708105,
    "input_throughput": 7511.772590067012,
    "output_throughput": 6541.502936047128,
    "total_throughput": 14053.275526114141,
    "itl": 88.89819851950249,
    "ttft": 1692069.8149216583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7341509081656107,
    "arrivals": 373254,
    "finished_requests": 109338,
    "scheduler_time": 259.05352090090605
}
#Debug simulation 
Total elapsed time: 87.50089410180226. Arrivals time: 0.62350969389081 Scheduler time: 86.61457709595561 Scheduler overhead time: 0.10428319172933698 Adapter cache time: 0.018483131658285856 Engine time: 0.10320018045604229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 135, 34560, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 135, 270, 270, 270, 135, 135, 34560, 270, 270, 135, 34560, 34560, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 34560, 135, 135, 270, 135, 34560, 135, 135, 34560, 135, 270, 270, 135, 270, 34560, 34560, 34560, 270, 34560, 34560, 135, 135, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 34560, 135, 135, 34560, 135, 135, 270]
Prompts retrieved: 1118880 . Total input tokens: 249198701 . Total output tokens: 219930190
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 94.09789757383987,
    "estimated_duration": 3600.0641282790384,
    "input_throughput": 7434.450900405045,
    "output_throughput": 6465.2481652115575,
    "total_throughput": 13899.699065616604,
    "itl": 86.46859622110128,
    "ttft": 1693159.575459888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8651053739897908,
    "arrivals": 373254,
    "finished_requests": 108140,
    "scheduler_time": 262.3220076451752
}
#Debug simulation 
Total elapsed time: 94.09812358301133. Arrivals time: 0.6461932696402073 Scheduler time: 93.176567485556 Scheduler overhead time: 0.10907063446938992 Adapter cache time: 0.01941093010827899 Engine time: 0.10869531612843275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 124.9965221453458,
    "estimated_duration": 3600.0098874054215,
    "input_throughput": 7659.293685960579,
    "output_throughput": 6649.242293401583,
    "total_throughput": 14308.535979362163,
    "itl": 90.00571701914737,
    "ttft": 1659735.276113007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8199387199617914,
    "arrivals": 372568,
    "finished_requests": 111239,
    "scheduler_time": 254.30321777004875
}
#Debug simulation 
Total elapsed time: 124.99672147119418. Arrivals time: 0.7032305765897036 Scheduler time: 123.99672119924799 Scheduler overhead time: 0.11869921395555139 Adapter cache time: 0.02139353984966874 Engine time: 0.11747054010629654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 93.87312676385045,
    "estimated_duration": 3600.0065202964156,
    "input_throughput": 7685.500524516244,
    "output_throughput": 6675.5529092823035,
    "total_throughput": 14361.053433798548,
    "itl": 89.42422561535098,
    "ttft": 1684678.0278459617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5408010707981898,
    "arrivals": 372568,
    "finished_requests": 111747,
    "scheduler_time": 253.07281460966598
}
#Debug simulation 
Total elapsed time: 93.87329262588173. Arrivals time: 0.6948754279874265 Scheduler time: 92.91037298273295 Scheduler overhead time: 0.10678643127903342 Adapter cache time: 0.020756890531629324 Engine time: 0.10429451428353786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 89.60332822799683,
    "estimated_duration": 3600.0563574823173,
    "input_throughput": 7551.907887079109,
    "output_throughput": 6568.268841362477,
    "total_throughput": 14120.176728441586,
    "itl": 86.9789120490768,
    "ttft": 1689155.053944008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0243993452517322,
    "arrivals": 372568,
    "finished_requests": 109841,
    "scheduler_time": 257.6376568137917
}
#Debug simulation 
Total elapsed time: 89.60353301884606. Arrivals time: 0.6470186822116375 Scheduler time: 88.69158048648387 Scheduler overhead time: 0.10437872540205717 Adapter cache time: 0.0194530775770545 Engine time: 0.1037083650007844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 105.45402491278946,
    "estimated_duration": 3600.0403422409163,
    "input_throughput": 7628.681178307231,
    "output_throughput": 6634.147045456005,
    "total_throughput": 14262.828223763236,
    "itl": 89.15965892720567,
    "ttft": 1674340.6033958315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8640675403643396,
    "arrivals": 372568,
    "finished_requests": 110868,
    "scheduler_time": 254.91603306438842
}
#Debug simulation 
Total elapsed time: 105.45424703788012. Arrivals time: 0.6764846118167043 Scheduler time: 104.48971677199006 Scheduler overhead time: 0.11158259864896536 Adapter cache time: 0.025604935828596354 Engine time: 0.11209329077973962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 92.40320955216885,
    "estimated_duration": 3600.0728217102824,
    "input_throughput": 7576.610349521168,
    "output_throughput": 6589.337542547368,
    "total_throughput": 14165.947892068536,
    "itl": 87.06613375202538,
    "ttft": 1687308.028666515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.183999682110736,
    "arrivals": 372568,
    "finished_requests": 110179,
    "scheduler_time": 256.67045535898654
}
#Debug simulation 
Total elapsed time: 92.40339777385816. Arrivals time: 0.6644682944752276 Scheduler time: 91.47292152279988 Scheduler overhead time: 0.10435677506029606 Adapter cache time: 0.01932332431897521 Engine time: 0.10478619113564491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 103.69244524510577,
    "estimated_duration": 3600.02377126518,
    "input_throughput": 7625.227149639824,
    "output_throughput": 6641.037537259897,
    "total_throughput": 14266.264686899722,
    "itl": 89.27411391359071,
    "ttft": 1675670.0070244966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8490614850958796,
    "arrivals": 372568,
    "finished_requests": 110916,
    "scheduler_time": 254.68175113473453
}
#Debug simulation 
Total elapsed time: 103.69263143884018. Arrivals time: 0.6797634367831051 Scheduler time: 102.72800909774378 Scheduler overhead time: 0.11281431932002306 Adapter cache time: 0.02079877909272909 Engine time: 0.1123928651213646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_96_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 66, 34560, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 66, 270, 270, 270, 66, 66, 34560, 270, 270, 66, 34560, 34560, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 34560, 66, 66, 270, 66, 34560, 66, 66, 34560, 66, 270, 270, 66, 270, 34560, 34560, 34560, 270, 34560, 34560, 66, 66, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 34560, 66, 66, 34560, 66, 66, 270]
Prompts retrieved: 1116672 . Total input tokens: 248713290 . Total output tokens: 219483241
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 89.67965418891981,
    "estimated_duration": 3600.040286756088,
    "input_throughput": 7548.201363181328,
    "output_throughput": 6566.042354292558,
    "total_throughput": 14114.243717473886,
    "itl": 86.9692323870092,
    "ttft": 1689128.728530588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9666211355477573,
    "arrivals": 372568,
    "finished_requests": 109794,
    "scheduler_time": 257.75242304601716
}
#Debug simulation 
Total elapsed time: 89.67985366890207. Arrivals time: 0.6427580118179321 Scheduler time: 88.7730895509012 Scheduler overhead time: 0.10333881014958024 Adapter cache time: 0.01891890401020646 Engine time: 0.10410236567258835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 87.34602563921362,
    "estimated_duration": 3600.010781933182,
    "input_throughput": 7526.709679866431,
    "output_throughput": 6597.047186410566,
    "total_throughput": 14123.756866276997,
    "itl": 90.49208557993012,
    "ttft": 1688194.8687356315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7472022206103421,
    "arrivals": 372217,
    "finished_requests": 109810,
    "scheduler_time": 256.55613422847773
}
#Debug simulation 
Total elapsed time: 87.34621517593041. Arrivals time: 0.6356583824381232 Scheduler time: 86.44818538799882 Scheduler overhead time: 0.10345228016376495 Adapter cache time: 0.01888205762952566 Engine time: 0.1034430917352438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 100.65825857082382,
    "estimated_duration": 3600.0674749873215,
    "input_throughput": 7487.2877209322105,
    "output_throughput": 6560.221763644353,
    "total_throughput": 14047.509484576563,
    "itl": 89.23303741833558,
    "ttft": 1688328.0642873403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8149510554224257,
    "arrivals": 372217,
    "finished_requests": 109254,
    "scheduler_time": 258.1436875504895
}
#Debug simulation 
Total elapsed time: 100.6584683782421. Arrivals time: 0.7158983484841883 Scheduler time: 99.65233084419742 Scheduler overhead time: 0.11531894467771053 Adapter cache time: 0.020841537043452263 Engine time: 0.11520134750753641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 119.77169552119449,
    "estimated_duration": 3600.0412077888395,
    "input_throughput": 7490.8303665122285,
    "output_throughput": 6560.980732358722,
    "total_throughput": 14051.81109887095,
    "itl": 87.06278802778884,
    "ttft": 1684817.1113184788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9037262032460424,
    "arrivals": 372217,
    "finished_requests": 109266,
    "scheduler_time": 258.017959640262
}
#Debug simulation 
Total elapsed time: 119.7718811952509. Arrivals time: 0.7493372391909361 Scheduler time: 118.70488224178553 Scheduler overhead time: 0.1249182834289968 Adapter cache time: 0.023227329831570387 Engine time: 0.1271187043748796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 107.6342407008633,
    "estimated_duration": 3600.0698676892957,
    "input_throughput": 7462.396838771186,
    "output_throughput": 6534.830118477689,
    "total_throughput": 13997.226957248875,
    "itl": 89.0572697640369,
    "ttft": 1682008.8677372006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.740827637594193,
    "arrivals": 372217,
    "finished_requests": 108799,
    "scheduler_time": 259.2640812453355
}
#Debug simulation 
Total elapsed time: 107.6344459541142. Arrivals time: 0.635394467972219 Scheduler time: 106.7263220185414 Scheduler overhead time: 0.1067232508212328 Adapter cache time: 0.01989766489714384 Engine time: 0.10841872310265899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 111.8827504189685,
    "estimated_duration": 3600.07693473666,
    "input_throughput": 7496.22146671542,
    "output_throughput": 6565.941903052434,
    "total_throughput": 14062.163369767855,
    "itl": 87.20218677694227,
    "ttft": 1685318.1998089764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8818266445351767,
    "arrivals": 372217,
    "finished_requests": 109322,
    "scheduler_time": 257.8094073737777
}
#Debug simulation 
Total elapsed time: 111.88294498668984. Arrivals time: 0.680307175964117 Scheduler time: 110.91206016950309 Scheduler overhead time: 0.11491927271708846 Adapter cache time: 0.021189256571233273 Engine time: 0.11509463004767895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 101.45534991566092,
    "estimated_duration": 3600.033318467571,
    "input_throughput": 7479.294667045328,
    "output_throughput": 6562.385653157341,
    "total_throughput": 14041.68032020267,
    "itl": 89.19711210370527,
    "ttft": 1684144.8800766086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7213830662844697,
    "arrivals": 372217,
    "finished_requests": 109145,
    "scheduler_time": 258.0515623289602
}
#Debug simulation 
Total elapsed time: 101.45552740897983. Arrivals time: 0.6827606060542166 Scheduler time: 100.49128414550796 Scheduler overhead time: 0.11116832355037332 Adapter cache time: 0.019666059408336878 Engine time: 0.11166836135089397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 34560, 270, 270, 270, 270, 34560, 270, 34560, 270, 33, 34560, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 33, 270, 270, 270, 33, 33, 34560, 270, 270, 33, 34560, 34560, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 34560, 33, 33, 270, 33, 34560, 33, 33, 34560, 33, 270, 270, 33, 270, 34560, 34560, 34560, 270, 34560, 34560, 33, 33, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 34560, 33, 33, 34560, 33, 33, 270]
Prompts retrieved: 1115616 . Total input tokens: 248477466 . Total output tokens: 219284988
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 101.37268813699484,
    "estimated_duration": 3600.0524451329215,
    "input_throughput": 7501.484051019955,
    "output_throughput": 6568.739028224594,
    "total_throughput": 14070.223079244548,
    "itl": 87.20695150934515,
    "ttft": 1691029.5877979842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9366469999775293,
    "arrivals": 372217,
    "finished_requests": 109413,
    "scheduler_time": 257.71259722535547
}
#Debug simulation 
Total elapsed time: 101.3728788672015. Arrivals time: 0.6500709829851985 Scheduler time: 100.44450999330729 Scheduler overhead time: 0.11013784958049655 Adapter cache time: 0.019976507872343063 Engine time: 0.10973188886418939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 120.65148999588564,
    "estimated_duration": 3600.044796059092,
    "input_throughput": 7567.5613897424455,
    "output_throughput": 6645.365364950118,
    "total_throughput": 14212.926754692564,
    "itl": 90.47367685247806,
    "ttft": 1670343.261331526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7405898115783922,
    "arrivals": 371082,
    "finished_requests": 110900,
    "scheduler_time": 254.28478619108267
}
#Debug simulation 
Total elapsed time: 120.65166794974357. Arrivals time: 0.6927302549593151 Scheduler time: 119.66745203873143 Scheduler overhead time: 0.11769691109657288 Adapter cache time: 0.02080462360754609 Engine time: 0.11394747998565435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 111.26880266517401,
    "estimated_duration": 3600.0666658461228,
    "input_throughput": 7570.107314552954,
    "output_throughput": 6633.839652629587,
    "total_throughput": 14203.94696718254,
    "itl": 89.64711722425596,
    "ttft": 1665175.9452698238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.14109768500086,
    "arrivals": 371082,
    "finished_requests": 110788,
    "scheduler_time": 254.6828045769188
}
#Debug simulation 
Total elapsed time: 111.26903035212308. Arrivals time: 0.7550113089382648 Scheduler time: 110.227676122915 Scheduler overhead time: 0.1135641597211361 Adapter cache time: 0.022157528437674046 Engine time: 0.11186391115188599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.25002886122093,
    "estimated_duration": 3600.0419292778856,
    "input_throughput": 7500.299866067414,
    "output_throughput": 6578.7955988475505,
    "total_throughput": 14079.095464914964,
    "itl": 87.08588391474252,
    "ttft": 1692722.9454678402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0838815130572772,
    "arrivals": 371082,
    "finished_requests": 109769,
    "scheduler_time": 257.3533448155083
}
#Debug simulation 
Total elapsed time: 109.25021610734984. Arrivals time: 0.6921140141785145 Scheduler time: 108.27347949612886 Scheduler overhead time: 0.11209391476586461 Adapter cache time: 0.021540848072618246 Engine time: 0.1121759545058012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 114.08298225188628,
    "estimated_duration": 3600.029265117738,
    "input_throughput": 7595.344644817975,
    "output_throughput": 6654.674791710754,
    "total_throughput": 14250.019436528728,
    "itl": 89.69231183951496,
    "ttft": 1670692.5871114521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0986105410708105,
    "arrivals": 371082,
    "finished_requests": 111095,
    "scheduler_time": 253.80210245416467
}
#Debug simulation 
Total elapsed time: 114.0831459229812. Arrivals time: 0.6967492718249559 Scheduler time: 113.08963610650972 Scheduler overhead time: 0.11904316395521164 Adapter cache time: 0.0221979646012187 Engine time: 0.11552990740165114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 93.11124224727973,
    "estimated_duration": 3600.0208714594482,
    "input_throughput": 7421.789471228339,
    "output_throughput": 6506.579221721007,
    "total_throughput": 13928.368692949345,
    "itl": 87.16475200779944,
    "ttft": 1694271.0207173675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.868056524693966,
    "arrivals": 371082,
    "finished_requests": 108654,
    "scheduler_time": 260.36399882123897
}
#Debug simulation 
Total elapsed time: 93.11141915386543. Arrivals time: 0.6449016947299242 Scheduler time: 92.1998037607409 Scheduler overhead time: 0.10615814849734306 Adapter cache time: 0.01941545633599162 Engine time: 0.1032052394002676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 125.72776765422896,
    "estimated_duration": 3600.065495161917,
    "input_throughput": 7050.076459472433,
    "output_throughput": 6198.6116724791245,
    "total_throughput": 13248.688131951558,
    "itl": 81.88815753289471,
    "ttft": 1688900.8478997124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7149991453438992,
    "arrivals": 371082,
    "finished_requests": 103377,
    "scheduler_time": 274.21750998153
}
#Debug simulation 
Total elapsed time: 125.72793848020956. Arrivals time: 0.6657345267012715 Scheduler time: 124.75328954961151 Scheduler overhead time: 0.12463241722434759 Adapter cache time: 0.021909155882894993 Engine time: 0.11996651068329811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 66, 34560, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 66, 135, 135, 135, 66, 66, 34560, 135, 135, 66, 34560, 34560, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 34560, 66, 66, 135, 66, 34560, 66, 66, 34560, 66, 135, 135, 66, 135, 34560, 34560, 34560, 135, 34560, 34560, 66, 66, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 34560, 66, 66, 34560, 66, 66, 135]
Prompts retrieved: 1112352 . Total input tokens: 247761835 . Total output tokens: 218644987
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 108.73662139195949,
    "estimated_duration": 3600.0598769427756,
    "input_throughput": 7387.155744360617,
    "output_throughput": 6489.780669937278,
    "total_throughput": 13876.936414297894,
    "itl": 87.02994887834974,
    "ttft": 1682747.0375224527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8163899475894869,
    "arrivals": 371082,
    "finished_requests": 108192,
    "scheduler_time": 261.0917875810156
}
#Debug simulation 
Total elapsed time: 108.73676586290821. Arrivals time: 0.6862880839034915 Scheduler time: 107.75771983573213 Scheduler overhead time: 0.11765514500439167 Adapter cache time: 0.021049557719379663 Engine time: 0.11424684477970004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 105.12436822615564,
    "estimated_duration": 3600.010311990116,
    "input_throughput": 7561.352507613244,
    "output_throughput": 6623.219083730632,
    "total_throughput": 14184.571591343874,
    "itl": 90.58620851829039,
    "ttft": 1674588.5938136913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8199387199617914,
    "arrivals": 370740,
    "finished_requests": 110575,
    "scheduler_time": 255.3280490642429
}
#Debug simulation 
Total elapsed time: 105.12455983320251. Arrivals time: 0.6750974645838141 Scheduler time: 104.16520118247718 Scheduler overhead time: 0.11370791820809245 Adapter cache time: 0.02120916498824954 Engine time: 0.11033714516088367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 93.62573650572449,
    "estimated_duration": 3600.0781565824777,
    "input_throughput": 7523.095005723527,
    "output_throughput": 6590.8730221914,
    "total_throughput": 14113.968027914927,
    "itl": 89.44906592766951,
    "ttft": 1681957.9468470337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9268113372847443,
    "arrivals": 370740,
    "finished_requests": 110065,
    "scheduler_time": 256.6400681392178
}
#Debug simulation 
Total elapsed time: 93.62591526797041. Arrivals time: 0.6624415903352201 Scheduler time: 92.6997503824532 Scheduler overhead time: 0.10364901274442673 Adapter cache time: 0.018914532382041216 Engine time: 0.10340249119326472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 112.20667776092887,
    "estimated_duration": 3600.0159967699906,
    "input_throughput": 7420.757025515752,
    "output_throughput": 6510.478292604507,
    "total_throughput": 13931.235318120258,
    "itl": 87.01829922478576,
    "ttft": 1679899.417803305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.927486464767717,
    "arrivals": 370740,
    "finished_requests": 108621,
    "scheduler_time": 260.16561474542493
}
#Debug simulation 
Total elapsed time: 112.20691700326279. Arrivals time: 0.6888739806599915 Scheduler time: 111.21714397892356 Scheduler overhead time: 0.12052715662866831 Adapter cache time: 0.022404986433684826 Engine time: 0.11739558633416891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 97.97900111600757,
    "estimated_duration": 3600.069899193046,
    "input_throughput": 7531.249325485978,
    "output_throughput": 6595.141390260777,
    "total_throughput": 14126.390715746755,
    "itl": 89.49951176526098,
    "ttft": 1683135.011589601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8690632403315972,
    "arrivals": 370740,
    "finished_requests": 110166,
    "scheduler_time": 256.48030350775645
}
#Debug simulation 
Total elapsed time: 97.97915824083611. Arrivals time: 0.6526749460026622 Scheduler time: 97.04847962409258 Scheduler overhead time: 0.11106854723766446 Adapter cache time: 0.020593025255948305 Engine time: 0.10781555669382215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 105.14791713887826,
    "estimated_duration": 3600.035539745692,
    "input_throughput": 7413.891253386185,
    "output_throughput": 6497.530577615457,
    "total_throughput": 13911.421831001642,
    "itl": 86.91298541744146,
    "ttft": 1683576.1123570364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9359752759803089,
    "arrivals": 370740,
    "finished_requests": 108489,
    "scheduler_time": 260.6425791411768
}
#Debug simulation 
Total elapsed time: 105.14808090915903. Arrivals time: 0.6669941456057131 Scheduler time: 104.19696298288181 Scheduler overhead time: 0.11369835352525115 Adapter cache time: 0.021349803544580936 Engine time: 0.10991793870925903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 95.82800955884159,
    "estimated_duration": 3600.0206258728304,
    "input_throughput": 7531.094351279344,
    "output_throughput": 6595.134713775026,
    "total_throughput": 14126.22906505437,
    "itl": 89.49933524403443,
    "ttft": 1682983.4972502815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8107579594524567,
    "arrivals": 370740,
    "finished_requests": 110164,
    "scheduler_time": 256.4831041361377
}
#Debug simulation 
Total elapsed time: 95.82815853971988. Arrivals time: 0.6585268410854042 Scheduler time: 94.89510882645845 Scheduler overhead time: 0.10944999940693378 Adapter cache time: 0.01999605819582939 Engine time: 0.10679807094857097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 34560, 135, 135, 135, 135, 34560, 135, 34560, 135, 33, 34560, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 33, 135, 135, 135, 33, 33, 34560, 135, 135, 33, 34560, 34560, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 34560, 33, 33, 135, 33, 34560, 33, 33, 34560, 33, 135, 135, 33, 135, 34560, 34560, 34560, 135, 34560, 34560, 33, 33, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 34560, 33, 33, 34560, 33, 33, 135]
Prompts retrieved: 1111296 . Total input tokens: 247533445 . Total output tokens: 218431649
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.42405621893704,
    "estimated_duration": 3600.035464134885,
    "input_throughput": 7413.194749294857,
    "output_throughput": 6501.529563577388,
    "total_throughput": 13914.724312872246,
    "itl": 87.0412366330483,
    "ttft": 1680034.326890501,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9115384421683851,
    "arrivals": 370740,
    "finished_requests": 108480,
    "scheduler_time": 260.5191654903405
}
#Debug simulation 
Total elapsed time: 109.42421521386132. Arrivals time: 0.6742993830703199 Scheduler time: 108.45867495238781 Scheduler overhead time: 0.11689913179725409 Adapter cache time: 0.02143967943266034 Engine time: 0.112454685382545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 94.6956240972504,
    "estimated_duration": 3600.083880866294,
    "input_throughput": 7621.481028769129,
    "output_throughput": 6648.686750665455,
    "total_throughput": 14270.167779434583,
    "itl": 90.82089858052855,
    "ttft": 1670187.7824264138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7670394477061919,
    "arrivals": 369999,
    "finished_requests": 111247,
    "scheduler_time": 254.22743570566047
}
#Debug simulation 
Total elapsed time: 94.69585282215849. Arrivals time: 0.6575020970776677 Scheduler time: 93.77163271652535 Scheduler overhead time: 0.10694566881284118 Adapter cache time: 0.018878045491874218 Engine time: 0.10269926581531763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 99.94212038815022,
    "estimated_duration": 3600.0482983177208,
    "input_throughput": 7585.349900100139,
    "output_throughput": 6612.211011481037,
    "total_throughput": 14197.560911581175,
    "itl": 89.64461397898073,
    "ttft": 1674802.5948624609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.846039623077959,
    "arrivals": 369999,
    "finished_requests": 110705,
    "scheduler_time": 255.73366816470124
}
#Debug simulation 
Total elapsed time: 99.94226484093815. Arrivals time: 0.6791024715639651 Scheduler time: 98.98887309152633 Scheduler overhead time: 0.11024265317246318 Adapter cache time: 0.02042287727817893 Engine time: 0.10543121537193656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 98.92299499409273,
    "estimated_duration": 3600.0122921986294,
    "input_throughput": 7575.852743364803,
    "output_throughput": 6607.7907710342615,
    "total_throughput": 14183.643514399064,
    "itl": 87.34244184177612,
    "ttft": 1684621.3490804324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8794979831250386,
    "arrivals": 369999,
    "finished_requests": 110616,
    "scheduler_time": 255.7994208395542
}
#Debug simulation 
Total elapsed time: 98.92317293025553. Arrivals time: 0.6840903116390109 Scheduler time: 97.96587911853567 Scheduler overhead time: 0.10936001921072602 Adapter cache time: 0.01977939484640956 Engine time: 0.10617715585976839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 93.31879099458456,
    "estimated_duration": 3600.0915920161888,
    "input_throughput": 7597.883081824936,
    "output_throughput": 6620.816273913516,
    "total_throughput": 14218.699355738452,
    "itl": 89.77164386540677,
    "ttft": 1680098.0202972624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8626793193910266,
    "arrivals": 369999,
    "finished_requests": 110879,
    "scheduler_time": 255.3953488487592
}
#Debug simulation 
Total elapsed time: 93.31893546972424. Arrivals time: 0.642933995462954 Scheduler time: 92.40757814515382 Scheduler overhead time: 0.10785176511853933 Adapter cache time: 0.019421083852648735 Engine time: 0.10432130796834826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 96.51821741135791,
    "estimated_duration": 3600.0512870578295,
    "input_throughput": 7501.536741180929,
    "output_throughput": 6540.782095147343,
    "total_throughput": 14042.318836328272,
    "itl": 87.36118324910854,
    "ttft": 1684247.7959556223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8877796771610158,
    "arrivals": 369999,
    "finished_requests": 109486,
    "scheduler_time": 258.77978774709686
}
#Debug simulation 
Total elapsed time: 96.51837648404762. Arrivals time: 0.6638497598469257 Scheduler time: 95.58027423964813 Scheduler overhead time: 0.11049522459506989 Adapter cache time: 0.019676143769174814 Engine time: 0.1055692145600915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 94.15098497923464,
    "estimated_duration": 3600.056941742827,
    "input_throughput": 7600.337562092539,
    "output_throughput": 6623.688843225923,
    "total_throughput": 14224.026405318462,
    "itl": 89.77801878082595,
    "ttft": 1680062.8759929244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7979901175713157,
    "arrivals": 369999,
    "finished_requests": 110925,
    "scheduler_time": 255.26977841589533
}
#Debug simulation 
Total elapsed time: 94.15113998018205. Arrivals time: 0.646384259685874 Scheduler time: 93.2382306274958 Scheduler overhead time: 0.10626139072701335 Adapter cache time: 0.019597348291426897 Engine time: 0.10337889147922397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_96_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 34560, 66, 66, 66, 66, 34560, 66, 34560, 66, 33, 34560, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 33, 66, 66, 66, 33, 33, 34560, 66, 66, 33, 34560, 34560, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 34560, 33, 33, 66, 33, 34560, 33, 33, 34560, 33, 66, 66, 33, 66, 34560, 34560, 34560, 66, 34560, 34560, 33, 33, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 34560, 33, 33, 34560, 33, 33, 66]
Prompts retrieved: 1109088 . Total input tokens: 247072596 . Total output tokens: 218002503
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 103.45228881109506,
    "estimated_duration": 3600.0826060800587,
    "input_throughput": 7572.316244621686,
    "output_throughput": 6601.499909991647,
    "total_throughput": 14173.816154613332,
    "itl": 87.2385492641141,
    "ttft": 1674252.3939117014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8497279717028143,
    "arrivals": 369999,
    "finished_requests": 110483,
    "scheduler_time": 256.14328420530865
}
#Debug simulation 
Total elapsed time: 103.45244876667857. Arrivals time: 0.6433106670156121 Scheduler time: 102.54733150359243 Scheduler overhead time: 0.10538788046687841 Adapter cache time: 0.019209980964660645 Engine time: 0.10047672921791673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 122.60041469195858,
    "estimated_duration": 3600.0084622233435,
    "input_throughput": 7347.69522837831,
    "output_throughput": 6392.699695429836,
    "total_throughput": 13740.394923808146,
    "itl": 86.94132433078143,
    "ttft": 1697364.9590962315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 205,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3555438515497356,
    "arrivals": 322758,
    "finished_requests": 106798,
    "scheduler_time": 258.7968443718385
}
#Debug simulation 
Total elapsed time: 122.6005529309623. Arrivals time: 0.7239882564172149 Scheduler time: 121.57997380942106 Scheduler overhead time: 0.119112029671669 Adapter cache time: 0.02183119673281908 Engine time: 0.11540788924321532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 124.98779258783907,
    "estimated_duration": 3600.0628227789607,
    "input_throughput": 7336.288364993793,
    "output_throughput": 6369.657733444488,
    "total_throughput": 13705.94609843828,
    "itl": 85.97054762541875,
    "ttft": 1699392.171694595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4819575613597424,
    "arrivals": 322758,
    "finished_requests": 106448,
    "scheduler_time": 259.9126856524116
}
#Debug simulation 
Total elapsed time: 124.98793444596231. Arrivals time: 0.719858412630856 Scheduler time: 123.97041645552963 Scheduler overhead time: 0.12027547648176551 Adapter cache time: 0.021447885315865278 Engine time: 0.11571173509582877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 121.12437802786008,
    "estimated_duration": 3600.0300374933636,
    "input_throughput": 7269.871842020219,
    "output_throughput": 6317.04340329186,
    "total_throughput": 13586.91524531208,
    "itl": 83.86408189172514,
    "ttft": 1709153.5159554651,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5584980077063695,
    "arrivals": 322758,
    "finished_requests": 105537,
    "scheduler_time": 262.3585699644992
}
#Debug simulation 
Total elapsed time: 121.12456453824416. Arrivals time: 0.7377560948953032 Scheduler time: 120.08584815496579 Scheduler overhead time: 0.12136916117742658 Adapter cache time: 0.02114369533956051 Engine time: 0.1170397731475532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 134.73898423928767,
    "estimated_duration": 3600.072225096744,
    "input_throughput": 7295.457245804064,
    "output_throughput": 6352.511719229586,
    "total_throughput": 13647.96896503365,
    "itl": 85.34517341436263,
    "ttft": 1701443.8621786574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2967954210331651,
    "arrivals": 322758,
    "finished_requests": 106131,
    "scheduler_time": 260.70689834684964
}
#Debug simulation 
Total elapsed time: 134.73913623346016. Arrivals time: 0.7568696406669915 Scheduler time: 133.66301954444498 Scheduler overhead time: 0.1299395076930523 Adapter cache time: 0.023887850809842348 Engine time: 0.12315917015075684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 121.4437478557229,
    "estimated_duration": 3600.0157364731836,
    "input_throughput": 7269.900721500625,
    "output_throughput": 6317.068497672497,
    "total_throughput": 13586.969219173121,
    "itl": 83.8638091582213,
    "ttft": 1709147.5679691967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5448282740497992,
    "arrivals": 322758,
    "finished_requests": 105537,
    "scheduler_time": 262.35835624387494
}
#Debug simulation 
Total elapsed time: 121.4439716679044. Arrivals time: 0.7373214014805853 Scheduler time: 120.3930814769119 Scheduler overhead time: 0.12699083611369133 Adapter cache time: 0.022977286018431187 Engine time: 0.12100387085229158 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 126.44427648978308,
    "estimated_duration": 3600.0324035881254,
    "input_throughput": 7325.575729183692,
    "output_throughput": 6369.755721405318,
    "total_throughput": 13695.33145058901,
    "itl": 85.9540898461442,
    "ttft": 1696910.3272300144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.276784188114103,
    "arrivals": 322758,
    "finished_requests": 106357,
    "scheduler_time": 259.95149518151186
}
#Debug simulation 
Total elapsed time: 126.44449867401272. Arrivals time: 0.7248243200592697 Scheduler time: 125.41611328767613 Scheduler overhead time: 0.1233542743138969 Adapter cache time: 0.022579526528716087 Engine time: 0.11623224057257175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [4320, 4320, 4320, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 17280, 8640, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 4320, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 8640]
Prompts retrieved: 967680 . Total input tokens: 215651961 . Total output tokens: 190166512
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 120.9625241169706,
    "estimated_duration": 3600.03803831875,
    "input_throughput": 7268.803474149034,
    "output_throughput": 6315.691322700095,
    "total_throughput": 13584.494796849129,
    "itl": 83.86846716461778,
    "ttft": 1710020.9134950403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5664643570594519,
    "arrivals": 322758,
    "finished_requests": 105513,
    "scheduler_time": 262.300601333727
}
#Debug simulation 
Total elapsed time: 120.9626866648905. Arrivals time: 0.7088419152423739 Scheduler time: 119.94734677672386 Scheduler overhead time: 0.12284440640360117 Adapter cache time: 0.02212064526975155 Engine time: 0.11950654303655028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 135.5700144949369,
    "estimated_duration": 3600.0535397439953,
    "input_throughput": 7319.496143347308,
    "output_throughput": 6365.160614147286,
    "total_throughput": 13684.656757494595,
    "itl": 85.51445532272807,
    "ttft": 1635530.2102505702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0712102631758886,
    "arrivals": 287932,
    "finished_requests": 106585,
    "scheduler_time": 258.8475131033459
}
#Debug simulation 
Total elapsed time: 135.57015772769228. Arrivals time: 0.7323910011909902 Scheduler time: 134.52478449279442 Scheduler overhead time: 0.12724641943350434 Adapter cache time: 0.023598617874085903 Engine time: 0.12041909946128726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 130.92683438584208,
    "estimated_duration": 3600.0838822642427,
    "input_throughput": 7273.304694092703,
    "output_throughput": 6318.627216456161,
    "total_throughput": 13591.931910548865,
    "itl": 84.29844498841884,
    "ttft": 1645252.5062309254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2740551345841968,
    "arrivals": 287932,
    "finished_requests": 105925,
    "scheduler_time": 260.11547659312663
}
#Debug simulation 
Total elapsed time: 130.9270442002453. Arrivals time: 0.7460773237980902 Scheduler time: 129.8703093510121 Scheduler overhead time: 0.12601268803700805 Adapter cache time: 0.02290788386017084 Engine time: 0.12037884071469307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 131.14062907313928,
    "estimated_duration": 3600.0582911713686,
    "input_throughput": 7202.190048862675,
    "output_throughput": 6263.429693707325,
    "total_throughput": 13465.619742570001,
    "itl": 82.27865096936523,
    "ttft": 1651033.0770832389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3027083833422535,
    "arrivals": 287932,
    "finished_requests": 104946,
    "scheduler_time": 262.86014710677097
}
#Debug simulation 
Total elapsed time: 131.14077995717525. Arrivals time: 0.7214668020606041 Scheduler time: 130.0990371964872 Scheduler overhead time: 0.12950791884213686 Adapter cache time: 0.02284904895350337 Engine time: 0.12454200629144907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 130.92740880092606,
    "estimated_duration": 3600.007242609931,
    "input_throughput": 7273.459533658263,
    "output_throughput": 6318.761732131536,
    "total_throughput": 13592.2212657898,
    "itl": 84.2968279706478,
    "ttft": 1645221.1119553351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1990912020253008,
    "arrivals": 287932,
    "finished_requests": 105925,
    "scheduler_time": 260.11411392453056
}
#Debug simulation 
Total elapsed time: 130.92771477485076. Arrivals time: 0.7450054078362882 Scheduler time: 129.87102918699384 Scheduler overhead time: 0.1265144543722272 Adapter cache time: 0.02270847139880061 Engine time: 0.11985122971236706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 130.21438098698854,
    "estimated_duration": 3600.04634197331,
    "input_throughput": 7202.213954220322,
    "output_throughput": 6263.450483151356,
    "total_throughput": 13465.664437371677,
    "itl": 82.27842155505132,
    "ttft": 1651028.1756292917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2913169386284449,
    "arrivals": 287932,
    "finished_requests": 104946,
    "scheduler_time": 262.8598927470924
}
#Debug simulation 
Total elapsed time: 130.21469522593543. Arrivals time: 0.7330257096327841 Scheduler time: 129.16221226658672 Scheduler overhead time: 0.1289801220409572 Adapter cache time: 0.023199348244816065 Engine time: 0.12385015189647675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 131.72510226396844,
    "estimated_duration": 3600.0561679920424,
    "input_throughput": 7273.771790789982,
    "output_throughput": 6318.942799353092,
    "total_throughput": 13592.714590143074,
    "itl": 84.2954934676464,
    "ttft": 1645234.9243358937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1171861645998407,
    "arrivals": 287932,
    "finished_requests": 105929,
    "scheduler_time": 260.12466241186803
}
#Debug simulation 
Total elapsed time: 131.72525193495676. Arrivals time: 0.7448828713968396 Scheduler time: 130.66263120621443 Scheduler overhead time: 0.1306150951422751 Adapter cache time: 0.02341301552951336 Engine time: 0.12197713647037745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 17280, 8640, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 1080, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 8640]
Prompts retrieved: 864000 . Total input tokens: 192578431 . Total output tokens: 169779390
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 131.30464418698102,
    "estimated_duration": 3600.0338006077304,
    "input_throughput": 7202.239044428688,
    "output_throughput": 6263.472303008238,
    "total_throughput": 13465.711347436925,
    "itl": 82.27817602418057,
    "ttft": 1651023.2565644972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2790970252081777,
    "arrivals": 287932,
    "finished_requests": 104946,
    "scheduler_time": 262.8596724261563
}
#Debug simulation 
Total elapsed time: 131.30489496374503. Arrivals time: 0.7265286408364773 Scheduler time: 130.26187289785594 Scheduler overhead time: 0.1273505981080234 Adapter cache time: 0.023215257562696934 Engine time: 0.12357551418244839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 124.36524579534307,
    "estimated_duration": 3600.0967981519057,
    "input_throughput": 7109.829939333753,
    "output_throughput": 6257.781460644328,
    "total_throughput": 13367.611399978081,
    "itl": 85.80996895158805,
    "ttft": 1669197.815799592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2100708528468371,
    "arrivals": 282072,
    "finished_requests": 103636,
    "scheduler_time": 260.45086848444174
}
#Debug simulation 
Total elapsed time: 124.36542551917955. Arrivals time: 0.7186793969012797 Scheduler time: 123.33839918440208 Scheduler overhead time: 0.12544359359890223 Adapter cache time: 0.02195127308368683 Engine time: 0.1188522893935442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 128.17589299799874,
    "estimated_duration": 3600.078124440938,
    "input_throughput": 7113.609514787113,
    "output_throughput": 6265.146816363215,
    "total_throughput": 13378.756331150327,
    "itl": 84.9059480940195,
    "ttft": 1662751.9013883728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3143040651269273,
    "arrivals": 282072,
    "finished_requests": 103626,
    "scheduler_time": 261.0161355858581
}
#Debug simulation 
Total elapsed time: 128.17603563982993. Arrivals time: 0.7216075719334185 Scheduler time: 127.13526905793697 Scheduler overhead time: 0.12898385571315885 Adapter cache time: 0.02405599970370531 Engine time: 0.12310660723596811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 120.74808160355315,
    "estimated_duration": 3600.063945467699,
    "input_throughput": 7047.519817513653,
    "output_throughput": 6190.329487912691,
    "total_throughput": 13237.849305426344,
    "itl": 82.92083108532569,
    "ttft": 1679649.2322721572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2929235035739848,
    "arrivals": 282072,
    "finished_requests": 102655,
    "scheduler_time": 263.5834946782594
}
#Debug simulation 
Total elapsed time: 120.74822716694325. Arrivals time: 0.7102839564904571 Scheduler time: 119.7270544427447 Scheduler overhead time: 0.12567845126613975 Adapter cache time: 0.023433152120560408 Engine time: 0.11953199375420809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 122.28864985099062,
    "estimated_duration": 3600.0121994148635,
    "input_throughput": 7104.0592596205215,
    "output_throughput": 6251.637981576302,
    "total_throughput": 13355.697241196824,
    "itl": 85.20667271250844,
    "ttft": 1671045.7812121238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2940189790865397,
    "arrivals": 282072,
    "finished_requests": 103603,
    "scheduler_time": 260.6055135782039
}
#Debug simulation 
Total elapsed time: 122.28886142093688. Arrivals time: 0.7494895444251597 Scheduler time: 121.23549546068534 Scheduler overhead time: 0.1221320079639554 Adapter cache time: 0.022870893124490976 Engine time: 0.11753796180710196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 117.73391097225249,
    "estimated_duration": 3600.072839266702,
    "input_throughput": 7034.797941799035,
    "output_throughput": 6193.399965913359,
    "total_throughput": 13228.197907712392,
    "itl": 82.97929888832314,
    "ttft": 1680587.2558897121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3863120402768268,
    "arrivals": 282072,
    "finished_requests": 102617,
    "scheduler_time": 263.33399977615665
}
#Debug simulation 
Total elapsed time: 117.73406625492498. Arrivals time: 0.7052926113829017 Scheduler time: 116.72069879621267 Scheduler overhead time: 0.1249235887080431 Adapter cache time: 0.022324287798255682 Engine time: 0.1189213846810162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 127.08425860060379,
    "estimated_duration": 3600.080689938409,
    "input_throughput": 7074.974200213206,
    "output_throughput": 6231.644769157609,
    "total_throughput": 13306.618969370815,
    "itl": 84.93227591677068,
    "ttft": 1673135.0396436416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1682575321244046,
    "arrivals": 282072,
    "finished_requests": 103186,
    "scheduler_time": 261.6077996081979
}
#Debug simulation 
Total elapsed time: 127.08441440667957. Arrivals time: 0.7294744290411472 Scheduler time: 126.04251192137599 Scheduler overhead time: 0.12718674400821328 Adapter cache time: 0.02417791821062565 Engine time: 0.1194457421079278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 540, 17280, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 540, 8640, 8640, 8640, 540, 540, 17280, 8640, 8640, 540, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 17280, 540, 540, 8640, 540, 17280, 540, 540, 17280, 540, 8640, 8640, 540, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 540, 540, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 17280, 540, 540, 17280, 540, 540, 8640]
Prompts retrieved: 846720 . Total input tokens: 188763411 . Total output tokens: 166351228
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 116.45118868816644,
    "estimated_duration": 3600.0726181710106,
    "input_throughput": 7014.009070969393,
    "output_throughput": 6163.358730044928,
    "total_throughput": 13177.367801014321,
    "itl": 82.70313287921061,
    "ttft": 1681048.0798561429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.345412563327701,
    "arrivals": 282072,
    "finished_requests": 102141,
    "scheduler_time": 264.86114987218355
}
#Debug simulation 
Total elapsed time: 116.45138109475374. Arrivals time: 0.6968134078197181 Scheduler time: 115.45148848183453 Scheduler overhead time: 0.12165748095139861 Adapter cache time: 0.022594738751649857 Engine time: 0.11654184106737375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 130.65434997109696,
    "estimated_duration": 3600.0053190270455,
    "input_throughput": 7208.1390721425905,
    "output_throughput": 6304.66318481278,
    "total_throughput": 13512.80225695537,
    "itl": 85.8361961995881,
    "ttft": 1645091.7782191068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.124109535431488,
    "arrivals": 279133,
    "finished_requests": 104861,
    "scheduler_time": 259.37763219814167
}
#Debug simulation 
Total elapsed time: 130.65450010308996. Arrivals time: 0.7325446824543178 Scheduler time: 129.6031752997078 Scheduler overhead time: 0.12951159104704857 Adapter cache time: 0.0244161537848413 Engine time: 0.12252631317824125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 128.87030734028667,
    "estimated_duration": 3600.0757125694718,
    "input_throughput": 7168.94561686304,
    "output_throughput": 6267.971787708493,
    "total_throughput": 13436.917404571534,
    "itl": 84.70909426619703,
    "ttft": 1644727.139574091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2374136830354119,
    "arrivals": 279133,
    "finished_requests": 104308,
    "scheduler_time": 260.60120073698596
}
#Debug simulation 
Total elapsed time: 128.87047369917855. Arrivals time: 0.7292403830215335 Scheduler time: 127.82415360119194 Scheduler overhead time: 0.1294507421553135 Adapter cache time: 0.02337241219356656 Engine time: 0.12155566643923521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 130.52196772396564,
    "estimated_duration": 3600.0571995087403,
    "input_throughput": 7089.124862650127,
    "output_throughput": 6207.97636300049,
    "total_throughput": 13297.101225650618,
    "itl": 82.49952483072747,
    "ttft": 1660550.2139925717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.226606110190042,
    "arrivals": 279133,
    "finished_requests": 103190,
    "scheduler_time": 263.55588485477637
}
#Debug simulation 
Total elapsed time: 130.52221143292263. Arrivals time: 0.7279867511242628 Scheduler time: 129.473323287908 Scheduler overhead time: 0.13151077600196004 Adapter cache time: 0.02288059936836362 Engine time: 0.12356248823925853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 132.20784308109432,
    "estimated_duration": 3600.066881242374,
    "input_throughput": 7185.61900468715,
    "output_throughput": 6280.573318737117,
    "total_throughput": 13466.192323424266,
    "itl": 84.80739818136396,
    "ttft": 1639358.4554276096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2304536228021594,
    "arrivals": 279133,
    "finished_requests": 104535,
    "scheduler_time": 260.15699636772626
}
#Debug simulation 
Total elapsed time: 132.20800297614187. Arrivals time: 0.7205346133559942 Scheduler time: 131.1721440902911 Scheduler overhead time: 0.12944738799706101 Adapter cache time: 0.023950750939548016 Engine time: 0.11979145044460893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 128.93630814319476,
    "estimated_duration": 3600.046749247634,
    "input_throughput": 7089.145441051184,
    "output_throughput": 6207.99438359257,
    "total_throughput": 13297.139824643753,
    "itl": 82.49933734740337,
    "ttft": 1660545.7442236987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.216250251359307,
    "arrivals": 279133,
    "finished_requests": 103190,
    "scheduler_time": 263.5556893212801
}
#Debug simulation 
Total elapsed time: 128.93645825702697. Arrivals time: 0.741052957251668 Scheduler time: 127.87626803666353 Scheduler overhead time: 0.12963172886520624 Adapter cache time: 0.023047978524118662 Engine time: 0.12264356017112732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 127.52860582899302,
    "estimated_duration": 3600.0057641883877,
    "input_throughput": 7193.760148280914,
    "output_throughput": 6298.038249144739,
    "total_throughput": 13491.798397425653,
    "itl": 84.9603819683341,
    "ttft": 1650518.5984396813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2320967415301096,
    "arrivals": 279133,
    "finished_requests": 104678,
    "scheduler_time": 259.41078250542614
}
#Debug simulation 
Total elapsed time: 127.52884074114263. Arrivals time: 0.7586539909243584 Scheduler time: 126.46023700432852 Scheduler overhead time: 0.12485216185450554 Adapter cache time: 0.023122145794332027 Engine time: 0.12003580294549465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 270, 17280, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 270, 8640, 8640, 8640, 270, 270, 17280, 8640, 8640, 270, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 17280, 270, 270, 8640, 270, 17280, 270, 270, 17280, 270, 8640, 8640, 270, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 270, 270, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 17280, 270, 270, 17280, 270, 270, 8640]
Prompts retrieved: 838080 . Total input tokens: 186844445 . Total output tokens: 164688198
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 130.7361395903863,
    "estimated_duration": 3600.055768152189,
    "input_throughput": 7085.121076635978,
    "output_throughput": 6202.352529516723,
    "total_throughput": 13287.4736061527,
    "itl": 82.47772487975743,
    "ttft": 1660388.6120830835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 163,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.191036817822606,
    "arrivals": 279133,
    "finished_requests": 103126,
    "scheduler_time": 263.868859647037
}
#Debug simulation 
Total elapsed time: 130.73628496238962. Arrivals time: 0.7156067946925759 Scheduler time: 129.7046446017921 Scheduler overhead time: 0.12764378590509295 Adapter cache time: 0.023427855223417282 Engine time: 0.12204779777675867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 124.28285725275055,
    "estimated_duration": 3600.0980545793714,
    "input_throughput": 7224.00018158356,
    "output_throughput": 6303.667471260447,
    "total_throughput": 13527.667652844008,
    "itl": 86.35640696479103,
    "ttft": 1652356.6811147572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.223295670910737,
    "arrivals": 277639,
    "finished_requests": 105027,
    "scheduler_time": 258.31862654193264
}
#Debug simulation 
Total elapsed time: 124.28301458293572. Arrivals time: 0.7366332714445889 Scheduler time: 123.2325911349617 Scheduler overhead time: 0.12732426496222615 Adapter cache time: 0.024029612075537443 Engine time: 0.11996931117027998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 128.23698800383136,
    "estimated_duration": 3600.043581612457,
    "input_throughput": 7146.915701636009,
    "output_throughput": 6236.214226591216,
    "total_throughput": 13383.129928227225,
    "itl": 84.90849175016284,
    "ttft": 1651379.623707352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.368709058524111,
    "arrivals": 277639,
    "finished_requests": 103924,
    "scheduler_time": 261.2755189739549
}
#Debug simulation 
Total elapsed time: 128.2373515269719. Arrivals time: 0.7154459510929883 Scheduler time: 127.2047596774064 Scheduler overhead time: 0.1287213508039713 Adapter cache time: 0.02346986625343561 Engine time: 0.12250999035313725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 124.49639275996014,
    "estimated_duration": 3600.0563974523575,
    "input_throughput": 7135.9632082930375,
    "output_throughput": 6220.893654846244,
    "total_throughput": 13356.856863139283,
    "itl": 83.07315249122796,
    "ttft": 1657138.2727704586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4977749921055543,
    "arrivals": 277639,
    "finished_requests": 103643,
    "scheduler_time": 262.0543066127568
}
#Debug simulation 
Total elapsed time: 124.49651625799015. Arrivals time: 0.7243490936234593 Scheduler time: 123.4577920329757 Scheduler overhead time: 0.1270012790337205 Adapter cache time: 0.02342828642576933 Engine time: 0.12048278655856848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 124.04389488510787,
    "estimated_duration": 3600.006572455152,
    "input_throughput": 7190.297150581791,
    "output_throughput": 6273.9807679285495,
    "total_throughput": 13464.27791851034,
    "itl": 85.32153963278954,
    "ttft": 1654956.6737564115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2840275791520241,
    "arrivals": 277639,
    "finished_requests": 104519,
    "scheduler_time": 259.56199573520814
}
#Debug simulation 
Total elapsed time: 124.04405225813389. Arrivals time: 0.7280949228443205 Scheduler time: 123.00807489641011 Scheduler overhead time: 0.12422525137662888 Adapter cache time: 0.02330759959295392 Engine time: 0.11825739732012153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 125.63925828412175,
    "estimated_duration": 3600.0058584002313,
    "input_throughput": 7093.052346127915,
    "output_throughput": 6192.191312129163,
    "total_throughput": 13285.243658257077,
    "itl": 82.79620877823403,
    "ttft": 1652706.2860422372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4885566018568377,
    "arrivals": 277639,
    "finished_requests": 103111,
    "scheduler_time": 263.26277785422934
}
#Debug simulation 
Total elapsed time: 125.63946829224005. Arrivals time: 0.7119006849825382 Scheduler time: 124.61043838551268 Scheduler overhead time: 0.12927561812102795 Adapter cache time: 0.02312682429328561 Engine time: 0.1215944685973227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 131.14519626786932,
    "estimated_duration": 3600.054950161999,
    "input_throughput": 7166.898382714678,
    "output_throughput": 6268.943755701406,
    "total_throughput": 13435.842138416085,
    "itl": 84.77121276140706,
    "ttft": 1642078.550292005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0022755876695717,
    "arrivals": 277639,
    "finished_requests": 104120,
    "scheduler_time": 260.9549193020704
}
#Debug simulation 
Total elapsed time: 131.14534571394324. Arrivals time: 0.7181819947436452 Scheduler time: 130.10954396799207 Scheduler overhead time: 0.1285079400986433 Adapter cache time: 0.02351598534733057 Engine time: 0.12340260297060013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 135, 17280, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 135, 8640, 8640, 8640, 135, 135, 17280, 8640, 8640, 135, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 17280, 135, 135, 8640, 135, 17280, 135, 135, 17280, 135, 8640, 8640, 135, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 135, 135, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 17280, 135, 135, 17280, 135, 135, 8640]
Prompts retrieved: 833760 . Total input tokens: 185884460 . Total output tokens: 163843506
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 121.81367996288463,
    "estimated_duration": 3600.0127573378263,
    "input_throughput": 7128.037518115929,
    "output_throughput": 6212.870761196897,
    "total_throughput": 13340.908279312825,
    "itl": 83.08187963625745,
    "ttft": 1660334.1604492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.494922731071714,
    "arrivals": 277639,
    "finished_requests": 103574,
    "scheduler_time": 262.2234773599304
}
#Debug simulation 
Total elapsed time: 121.81381885474548. Arrivals time: 0.7158939731307328 Scheduler time: 120.78689949633554 Scheduler overhead time: 0.12616271991282701 Adapter cache time: 0.022569079883396626 Engine time: 0.11927817901596427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 130.53736737789586,
    "estimated_duration": 3600.009761566971,
    "input_throughput": 7323.204865012574,
    "output_throughput": 6439.278372931366,
    "total_throughput": 13762.48323794394,
    "itl": 86.70108286904211,
    "ttft": 1607785.805490799,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.309256988326086,
    "arrivals": 276898,
    "finished_requests": 107352,
    "scheduler_time": 253.85478841253087
}
#Debug simulation 
Total elapsed time: 130.53760460205376. Arrivals time: 0.7588319946080446 Scheduler time: 129.46192941768095 Scheduler overhead time: 0.128980393987149 Adapter cache time: 0.02339213900268078 Engine time: 0.1224608812481165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 124.13788803201169,
    "estimated_duration": 3600.017613349749,
    "input_throughput": 7279.67660569719,
    "output_throughput": 6407.553928197674,
    "total_throughput": 13687.230533894864,
    "itl": 85.71414647729301,
    "ttft": 1618228.4200448638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4833457823330556,
    "arrivals": 276898,
    "finished_requests": 106773,
    "scheduler_time": 255.25773948809044
}
#Debug simulation 
Total elapsed time: 124.13805041462183. Arrivals time: 0.7137933308258653 Scheduler time: 123.12203703215346 Scheduler overhead time: 0.12183512607589364 Adapter cache time: 0.022563804872334003 Engine time: 0.11741768289357424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 121.31353024486452,
    "estimated_duration": 3600.063114077737,
    "input_throughput": 7215.31878105821,
    "output_throughput": 6346.195129374242,
    "total_throughput": 13561.513910432452,
    "itl": 83.62395724549783,
    "ttft": 1625870.9062611791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5215352536272286,
    "arrivals": 276898,
    "finished_requests": 105792,
    "scheduler_time": 257.9916003538415
}
#Debug simulation 
Total elapsed time: 121.313673612196. Arrivals time: 0.7223318098112941 Scheduler time: 120.28556041186675 Scheduler overhead time: 0.12288592336699367 Adapter cache time: 0.023090827744454145 Engine time: 0.11802522139623761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 123.11907111527398,
    "estimated_duration": 3600.02741256658,
    "input_throughput": 7274.886549080031,
    "output_throughput": 6400.275986668996,
    "total_throughput": 13675.162535749027,
    "itl": 85.64467881483365,
    "ttft": 1618823.9343810598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3761789142666374,
    "arrivals": 276898,
    "finished_requests": 106681,
    "scheduler_time": 255.5079742967486
}
#Debug simulation 
Total elapsed time: 123.11927524814382. Arrivals time: 0.7479917276650667 Scheduler time: 122.05959190009162 Scheduler overhead time: 0.12570944568142295 Adapter cache time: 0.02295078756287694 Engine time: 0.12091761641204357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 114.72483400488272,
    "estimated_duration": 3600.003838837088,
    "input_throughput": 7215.291194909044,
    "output_throughput": 6345.763788790727,
    "total_throughput": 13561.05498369977,
    "itl": 83.62004130692243,
    "ttft": 1625941.7877281918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5010840186383603,
    "arrivals": 276898,
    "finished_requests": 105781,
    "scheduler_time": 257.98439873408233
}
#Debug simulation 
Total elapsed time: 114.72502855071798. Arrivals time: 0.6776549154892564 Scheduler time: 113.75174575252458 Scheduler overhead time: 0.1185385650023818 Adapter cache time: 0.022018938791006804 Engine time: 0.11489347368478775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 130.2068508081138,
    "estimated_duration": 3600.0136992747794,
    "input_throughput": 7285.456720701803,
    "output_throughput": 6409.996718803782,
    "total_throughput": 13695.453439505585,
    "itl": 85.68726432643172,
    "ttft": 1611318.2666944677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.264016346232962,
    "arrivals": 276898,
    "finished_requests": 106804,
    "scheduler_time": 255.14401510987247
}
#Debug simulation 
Total elapsed time: 130.20700568519533. Arrivals time: 0.7387176002375782 Scheduler time: 129.14517166418955 Scheduler overhead time: 0.1316756447777152 Adapter cache time: 0.02409119764342904 Engine time: 0.1244021886959672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 66, 17280, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 66, 8640, 8640, 8640, 66, 66, 17280, 8640, 8640, 66, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 17280, 66, 66, 8640, 66, 17280, 66, 66, 17280, 66, 8640, 8640, 66, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 66, 66, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 17280, 66, 66, 17280, 66, 66, 8640]
Prompts retrieved: 831552 . Total input tokens: 185392347 . Total output tokens: 163412615
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 121.51174298813567,
    "estimated_duration": 3600.059135465045,
    "input_throughput": 7215.343977021794,
    "output_throughput": 6345.777427639096,
    "total_throughput": 13561.12140466089,
    "itl": 83.61900819831278,
    "ttft": 1625842.7893510095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.48741428498179,
    "arrivals": 276898,
    "finished_requests": 105784,
    "scheduler_time": 257.9825551601308
}
#Debug simulation 
Total elapsed time: 121.51194838201627. Arrivals time: 0.7130016181617975 Scheduler time: 120.49377414537594 Scheduler overhead time: 0.12428205367177725 Adapter cache time: 0.02219699462875724 Engine time: 0.11694431724026799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 127.55160562274978,
    "estimated_duration": 3600.0977736217974,
    "input_throughput": 7348.414866351124,
    "output_throughput": 6388.417883679433,
    "total_throughput": 13736.832750030557,
    "itl": 86.7354724886176,
    "ttft": 1624379.2919645594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 188,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2431328980065868,
    "arrivals": 276581,
    "finished_requests": 106894,
    "scheduler_time": 254.7517082219693
}
#Debug simulation 
Total elapsed time: 127.55175533797592. Arrivals time: 0.7344923266209662 Scheduler time: 126.50940213631839 Scheduler overhead time: 0.12490486586466432 Adapter cache time: 0.023183444049209356 Engine time: 0.11820674315094948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 126.36073093954474,
    "estimated_duration": 3600.098226434762,
    "input_throughput": 7247.45891887481,
    "output_throughput": 6305.764890887866,
    "total_throughput": 13553.223809762676,
    "itl": 85.36170076186461,
    "ttft": 1643921.1314154246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1899497945047928,
    "arrivals": 276581,
    "finished_requests": 105389,
    "scheduler_time": 258.41262104568386
}
#Debug simulation 
Total elapsed time: 126.3608790775761. Arrivals time: 0.7220807890407741 Scheduler time: 125.32875591889024 Scheduler overhead time: 0.12625060137361288 Adapter cache time: 0.022870985325425863 Engine time: 0.1182389329187572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 131.339823869057,
    "estimated_duration": 3600.039856429492,
    "input_throughput": 7184.591291067716,
    "output_throughput": 6253.097714958838,
    "total_throughput": 13437.689006026554,
    "itl": 83.25931035651718,
    "ttft": 1639307.2767528724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2895058907847867,
    "arrivals": 276581,
    "finished_requests": 104585,
    "scheduler_time": 260.4909595781113
}
#Debug simulation 
Total elapsed time: 131.3400158379227. Arrivals time: 0.7354689086787403 Scheduler time: 130.28775056730956 Scheduler overhead time: 0.12831942597404122 Adapter cache time: 0.023199635557830334 Engine time: 0.12194943148642778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 128.50708779506385,
    "estimated_duration": 3600.0858402744143,
    "input_throughput": 7268.705014546365,
    "output_throughput": 6311.333675940371,
    "total_throughput": 13580.038690486737,
    "itl": 85.009198738579,
    "ttft": 1634626.983732042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0999987620441238,
    "arrivals": 276581,
    "finished_requests": 105625,
    "scheduler_time": 258.24160008257167
}
#Debug simulation 
Total elapsed time: 128.50723165133968. Arrivals time: 0.7191903088241816 Scheduler time: 127.47601708257571 Scheduler overhead time: 0.1262106280773878 Adapter cache time: 0.02328233327716589 Engine time: 0.12074217014014721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 129.2475235243328,
    "estimated_duration": 3600.0599343969216,
    "input_throughput": 7182.98930329667,
    "output_throughput": 6249.706785442466,
    "total_throughput": 13432.696088739136,
    "itl": 83.33194710661515,
    "ttft": 1645976.5407167748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2604606101056572,
    "arrivals": 276581,
    "finished_requests": 104547,
    "scheduler_time": 260.81611127932206
}
#Debug simulation 
Total elapsed time: 129.24771338701248. Arrivals time: 0.733865883667022 Scheduler time: 128.1961181522347 Scheduler overhead time: 0.1298997076228261 Adapter cache time: 0.023461632430553436 Engine time: 0.12202105112373829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 123.6915333471261,
    "estimated_duration": 3600.055509279974,
    "input_throughput": 7267.266555351704,
    "output_throughput": 6321.790856094842,
    "total_throughput": 13589.057411446545,
    "itl": 85.68683135302683,
    "ttft": 1639184.0734800668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0724987180158472,
    "arrivals": 276581,
    "finished_requests": 105801,
    "scheduler_time": 257.54761516903363
}
#Debug simulation 
Total elapsed time: 123.69175515929237. Arrivals time: 0.7229879968799651 Scheduler time: 122.65832811687142 Scheduler overhead time: 0.1266607721336186 Adapter cache time: 0.023291146382689476 Engine time: 0.11899399943649769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 8640, 33, 17280, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 33, 8640, 8640, 8640, 33, 33, 17280, 8640, 8640, 33, 17280, 17280, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 17280, 33, 33, 8640, 33, 17280, 33, 33, 17280, 33, 8640, 8640, 33, 8640, 17280, 17280, 17280, 8640, 17280, 17280, 33, 33, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 17280, 33, 33, 17280, 33, 33, 8640]
Prompts retrieved: 830496 . Total input tokens: 185149554 . Total output tokens: 163203095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 124.85725809307769,
    "estimated_duration": 3600.0902634311387,
    "input_throughput": 7192.599380916964,
    "output_throughput": 6253.923749828961,
    "total_throughput": 13446.523130745925,
    "itl": 83.25493790231404,
    "ttft": 1649406.0353511435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3233073839545273,
    "arrivals": 276581,
    "finished_requests": 104599,
    "scheduler_time": 260.6266974418388
}
#Debug simulation 
Total elapsed time: 124.85739782406017. Arrivals time: 0.7223288249224424 Scheduler time: 123.82706611882895 Scheduler overhead time: 0.12594109028577805 Adapter cache time: 0.022623553406447172 Engine time: 0.11847338080406189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 144.30541168292984,
    "estimated_duration": 3600.0336294593467,
    "input_throughput": 7251.488926763315,
    "output_throughput": 6342.980746940787,
    "total_throughput": 13594.469673704103,
    "itl": 84.52493303288823,
    "ttft": 1539001.1075696715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1637839896231876,
    "arrivals": 241359,
    "finished_requests": 105782,
    "scheduler_time": 253.76659763432772
}
#Debug simulation 
Total elapsed time: 144.305563762784. Arrivals time: 0.7373027177527547 Scheduler time: 143.24044459639117 Scheduler overhead time: 0.13285047747194767 Adapter cache time: 0.0249031875282526 Engine time: 0.1270256182178855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 137.41048886394128,
    "estimated_duration": 3600.0470808112445,
    "input_throughput": 7215.733965941989,
    "output_throughput": 6309.349430752882,
    "total_throughput": 13525.08339669487,
    "itl": 83.4672505272068,
    "ttft": 1547336.7248446718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1757937316503382,
    "arrivals": 241359,
    "finished_requests": 105281,
    "scheduler_time": 255.31807764032862
}
#Debug simulation 
Total elapsed time: 137.41072846390307. Arrivals time: 0.7501935609616339 Scheduler time: 136.33075656881556 Scheduler overhead time: 0.13472471199929714 Adapter cache time: 0.02429417008534074 Engine time: 0.12758463667705655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 128.71951757511124,
    "estimated_duration": 3600.066617532112,
    "input_throughput": 7177.662733839538,
    "output_throughput": 6285.028418588195,
    "total_throughput": 13462.691152427733,
    "itl": 82.20264751921681,
    "ttft": 1557167.9005896903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.347279252195732,
    "arrivals": 241359,
    "finished_requests": 104734,
    "scheduler_time": 256.42019187740794
}
#Debug simulation 
Total elapsed time: 128.71966806892306. Arrivals time: 0.7307320651598275 Scheduler time: 127.67189044272527 Scheduler overhead time: 0.1287344992160797 Adapter cache time: 0.0229320484213531 Engine time: 0.1229637828655541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 144.03122062794864,
    "estimated_duration": 3600.0570657187304,
    "input_throughput": 7168.703586882625,
    "output_throughput": 6292.929969285662,
    "total_throughput": 13461.633556168286,
    "itl": 82.98901044444202,
    "ttft": 1534162.748818663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9781470802472901,
    "arrivals": 241359,
    "finished_requests": 104683,
    "scheduler_time": 257.10698751788766
}
#Debug simulation 
Total elapsed time: 144.03136746725067. Arrivals time: 0.7444088109768927 Scheduler time: 142.95833900244907 Scheduler overhead time: 0.1339558376930654 Adapter cache time: 0.02425932791084051 Engine time: 0.1270998241379857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_96_slots_16_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 127.66079260082915,
    "estimated_duration": 3600.054553974176,
    "input_throughput": 7177.686785739013,
    "output_throughput": 6285.049479325836,
    "total_throughput": 13462.736265064848,
    "itl": 82.20233431868292,
    "ttft": 1557162.9678751775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3356806903053087,
    "arrivals": 241359,
    "finished_requests": 104734,
    "scheduler_time": 256.42003027502693
}
#Debug simulation 
Total elapsed time: 127.66102518280968. Arrivals time: 0.7390414201654494 Scheduler time: 126.59869554685429 Scheduler overhead time: 0.13095615338534117 Adapter cache time: 0.024392565246671438 Engine time: 0.12466837465763092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_96_slots_16_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_96_slots_16_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 139.9535249308683,
    "estimated_duration": 3600.0948560613897,
    "input_throughput": 7192.614093598885,
    "output_throughput": 6291.800329056037,
    "total_throughput": 13484.414422654922,
    "itl": 83.21855440879163,
    "ttft": 1546221.8228790767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0597308761347062,
    "arrivals": 241359,
    "finished_requests": 105023,
    "scheduler_time": 256.1541680329353
}
#Debug simulation 
Total elapsed time: 139.95370902772993. Arrivals time: 0.7507604528218508 Scheduler time: 138.87816435704008 Scheduler overhead time: 0.13247784646227956 Adapter cache time: 0.023886225651949644 Engine time: 0.1251575700007379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_96_slots_16_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_96_slots_16_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 17280, 4320, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 1080, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 4320]
Prompts retrieved: 725760 . Total input tokens: 161845506 . Total output tokens: 142589050
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 130.5739180445671,
    "estimated_duration": 3600.053741716937,
    "input_throughput": 7181.2908514166165,
    "output_throughput": 6287.353640783432,
    "total_throughput": 13468.644492200048,
    "itl": 82.23205493311993,
    "ttft": 1555796.3206905555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2903298699483292,
    "arrivals": 241359,
    "finished_requests": 104878,
    "scheduler_time": 256.32243949792576
}
#Debug simulation 
Total elapsed time: 130.57406128989533. Arrivals time: 0.7191830757074058 Scheduler time: 129.53668168839067 Scheduler overhead time: 0.13009432144463062 Adapter cache time: 0.022930340841412544 Engine time: 0.12230841489508748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 144.38890744326636,
    "estimated_duration": 3600.0589756888126,
    "input_throughput": 7297.484618282789,
    "output_throughput": 6371.965891367352,
    "total_throughput": 13669.45050965014,
    "itl": 84.69151705787495,
    "ttft": 1512843.8004349938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8860628102812906,
    "arrivals": 235577,
    "finished_requests": 106184,
    "scheduler_time": 252.7267800232801
}
#Debug simulation 
Total elapsed time: 144.38914163503796. Arrivals time: 0.7621048069559038 Scheduler time: 143.3018603711389 Scheduler overhead time: 0.13238131068646908 Adapter cache time: 0.023763916920870543 Engine time: 0.12625082954764366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 139.59753035428002,
    "estimated_duration": 3600.0538752439593,
    "input_throughput": 7221.770812593237,
    "output_throughput": 6308.786420164591,
    "total_throughput": 13530.557232757828,
    "itl": 83.35465249117844,
    "ttft": 1537171.2985430034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0853044706303634,
    "arrivals": 235577,
    "finished_requests": 105126,
    "scheduler_time": 255.6085994150034
}
#Debug simulation 
Total elapsed time: 139.59767329506576. Arrivals time: 0.7341134250164032 Scheduler time: 138.53667064756155 Scheduler overhead time: 0.13282157760113478 Adapter cache time: 0.024476552847772837 Engine time: 0.1268907911144197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 143.27765834704041,
    "estimated_duration": 3600.065830851661,
    "input_throughput": 7156.6024652124215,
    "output_throughput": 6258.169449826467,
    "total_throughput": 13414.771915038888,
    "itl": 81.36856461821057,
    "ttft": 1527950.517996416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0851223608525475,
    "arrivals": 235577,
    "finished_requests": 104221,
    "scheduler_time": 257.7284254891214
}
#Debug simulation 
Total elapsed time: 143.2778378110379. Arrivals time: 0.7396997977048159 Scheduler time: 142.2070239163004 Scheduler overhead time: 0.13434027694165707 Adapter cache time: 0.023846940603107214 Engine time: 0.12950289249420166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 144.8747324598953,
    "estimated_duration": 3600.008403999582,
    "input_throughput": 7219.999811979055,
    "output_throughput": 6303.949728224756,
    "total_throughput": 13523.949540203812,
    "itl": 83.3212407231906,
    "ttft": 1524355.7314828688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0100666849501427,
    "arrivals": 235577,
    "finished_requests": 105061,
    "scheduler_time": 255.70649502397117
}
#Debug simulation 
Total elapsed time: 144.8749402021058. Arrivals time: 0.7322850236669183 Scheduler time: 143.81572481431067 Scheduler overhead time: 0.13396137859672308 Adapter cache time: 0.023174862377345562 Engine time: 0.12678053136914968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_96_slots_16_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 140.224330924917,
    "estimated_duration": 3600.0802964912496,
    "input_throughput": 7155.6878953804235,
    "output_throughput": 6258.266800870719,
    "total_throughput": 13413.954696251141,
    "itl": 81.36852380224762,
    "ttft": 1527463.6134448058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0760092050815007,
    "arrivals": 235577,
    "finished_requests": 104211,
    "scheduler_time": 257.73620974437824
}
#Debug simulation 
Total elapsed time: 140.22447687806562. Arrivals time: 0.7114411974325776 Scheduler time: 139.18748195655644 Scheduler overhead time: 0.1336942184716463 Adapter cache time: 0.02302997326478362 Engine time: 0.1255201930180192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_96_slots_16_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_96_slots_16_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 141.8521460997872,
    "estimated_duration": 3600.046385817617,
    "input_throughput": 7231.622098693404,
    "output_throughput": 6316.611110785849,
    "total_throughput": 13548.233209479253,
    "itl": 83.35424248997279,
    "ttft": 1531494.4289070226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9575881410855781,
    "arrivals": 235577,
    "finished_requests": 105218,
    "scheduler_time": 255.4657968651823
}
#Debug simulation 
Total elapsed time: 141.85228834999725. Arrivals time: 0.7342438260093331 Scheduler time: 140.7917689755559 Scheduler overhead time: 0.13270197017118335 Adapter cache time: 0.02357128309085965 Engine time: 0.12621050886809826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_96_slots_16_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_96_slots_16_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 4320, 540, 17280, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 17280, 4320, 4320, 540, 17280, 17280, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 17280, 540, 540, 4320, 540, 17280, 540, 540, 17280, 540, 4320, 4320, 540, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 540, 540, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 17280, 540, 540, 17280, 540, 540, 4320]
Prompts retrieved: 708480 . Total input tokens: 157976578 . Total output tokens: 139181202
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 142.60962938610464,
    "estimated_duration": 3600.04546623111,
    "input_throughput": 7156.642948449371,
    "output_throughput": 6258.204850836644,
    "total_throughput": 13414.847799286015,
    "itl": 81.3681617334624,
    "ttft": 1527941.8024114142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0662746977806097,
    "arrivals": 235577,
    "finished_requests": 104221,
    "scheduler_time": 257.7279198438595
}
#Debug simulation 
Total elapsed time: 142.6098386598751. Arrivals time: 0.7368022976443172 Scheduler time: 141.54622351331636 Scheduler overhead time: 0.13148787384852767 Adapter cache time: 0.023636549711227417 Engine time: 0.1269029201939702 
