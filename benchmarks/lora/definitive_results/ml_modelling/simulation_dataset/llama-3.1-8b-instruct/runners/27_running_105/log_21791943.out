INFO 05-31 19:30:53 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:54 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106303446 . Total output tokens: 93582864
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 161.09533473290503,
    "estimated_duration": 3600.033812323336,
    "input_throughput": 7080.982659867991,
    "output_throughput": 6228.035115461925,
    "total_throughput": 13309.017775329916,
    "itl": 78.70841096639613,
    "ttft": 1265850.0158328954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5818919948115939,
    "arrivals": 158690,
    "finished_requests": 103744,
    "scheduler_time": 218.97592532552898
}
#Debug simulation 
Total elapsed time: 161.09562955703586. Arrivals time: 0.7535263220779598 Scheduler time: 159.9584620478563 Scheduler overhead time: 0.1526973843574524 Adapter cache time: 0.027317821979522705 Engine time: 0.156422458589077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106303446 . Total output tokens: 93582864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 155.6697870180942,
    "estimated_duration": 3600.0828369337846,
    "input_throughput": 7053.6629711631995,
    "output_throughput": 6211.305409584742,
    "total_throughput": 13264.968380747941,
    "itl": 78.05695014111228,
    "ttft": 1270446.3028184052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6656182849640028,
    "arrivals": 158690,
    "finished_requests": 103431,
    "scheduler_time": 219.67185973122378
}
#Debug simulation 
Total elapsed time: 155.67001524893567. Arrivals time: 0.752780731767416 Scheduler time: 154.54367335420102 Scheduler overhead time: 0.15067847818136215 Adapter cache time: 0.02639417489990592 Engine time: 0.150252191349864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106303446 . Total output tokens: 93582864
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 155.99304596334696,
    "estimated_duration": 3600.037876960637,
    "input_throughput": 7006.706835344723,
    "output_throughput": 6170.025638383824,
    "total_throughput": 13176.732473728547,
    "itl": 76.50403168677751,
    "ttft": 1277290.663000155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6917345585394652,
    "arrivals": 158690,
    "finished_requests": 102740,
    "scheduler_time": 221.25152900671875
}
#Debug simulation 
Total elapsed time: 155.99322164338082. Arrivals time: 0.7384802210144699 Scheduler time: 154.87464551255107 Scheduler overhead time: 0.15312339179217815 Adapter cache time: 0.026491865515708923 Engine time: 0.15216671396046877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106303446 . Total output tokens: 93582864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 161.92049001483247,
    "estimated_duration": 3600.0398152897665,
    "input_throughput": 7051.3442357461145,
    "output_throughput": 6197.2506262973775,
    "total_throughput": 13248.594862043492,
    "itl": 77.73009109002163,
    "ttft": 1268393.8858219727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6153684768034143,
    "arrivals": 158690,
    "finished_requests": 103217,
    "scheduler_time": 220.16291819684557
}
#Debug simulation 
Total elapsed time: 161.92066445481032. Arrivals time: 0.7656751833856106 Scheduler time: 160.7721007289365 Scheduler overhead time: 0.15540623618289828 Adapter cache time: 0.026448737364262342 Engine time: 0.1537166521884501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106303446 . Total output tokens: 93582864
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 153.51096987025812,
    "estimated_duration": 3600.0352019304564,
    "input_throughput": 7006.712041724994,
    "output_throughput": 6170.030223062548,
    "total_throughput": 13176.742264787543,
    "itl": 76.50477980262632,
    "ttft": 1277289.6993023944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6859352775942535,
    "arrivals": 158690,
    "finished_requests": 102740,
    "scheduler_time": 221.25135652888625
}
#Debug simulation 
Total elapsed time: 153.5111525412649. Arrivals time: 0.7538649179041386 Scheduler time: 152.3758833152242 Scheduler overhead time: 0.15112207736819983 Adapter cache time: 0.027254515327513218 Engine time: 0.15513396775349975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106303446 . Total output tokens: 93582864
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 173.09589580027387,
    "estimated_duration": 3600.0101386605306,
    "input_throughput": 7049.148758631585,
    "output_throughput": 6206.771686569133,
    "total_throughput": 13255.920445200718,
    "itl": 78.13605210002028,
    "ttft": 1247497.9286019593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5490172008890662,
    "arrivals": 158690,
    "finished_requests": 103360,
    "scheduler_time": 219.75181708658153
}
#Debug simulation 
Total elapsed time: 173.09611737914383. Arrivals time: 0.7733125486411154 Scheduler time: 171.9152102516964 Scheduler overhead time: 0.16396754514425993 Adapter cache time: 0.02925115218386054 Engine time: 0.16572096850723028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106303446 . Total output tokens: 93582864
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 160.88394928816706,
    "estimated_duration": 3600.0042726788342,
    "input_throughput": 7006.772239531266,
    "output_throughput": 6170.08323255999,
    "total_throughput": 13176.855472091256,
    "itl": 76.50539777410678,
    "ttft": 1277279.3784647756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6788932935893537,
    "arrivals": 158690,
    "finished_requests": 102740,
    "scheduler_time": 221.24746113234113
}
#Debug simulation 
Total elapsed time: 160.88414202211425. Arrivals time: 0.7323952149599791 Scheduler time: 159.76528946915641 Scheduler overhead time: 0.1540047312155366 Adapter cache time: 0.02781062200665474 Engine time: 0.15659014182165265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105660551 . Total output tokens: 93034366
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 158.3583493521437,
    "estimated_duration": 3600.080282307261,
    "input_throughput": 7181.004581217714,
    "output_throughput": 6244.987121673627,
    "total_throughput": 13425.99170289134,
    "itl": 79.6268528422815,
    "ttft": 1254331.4198823525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6480160851310932,
    "arrivals": 157801,
    "finished_requests": 104358,
    "scheduler_time": 218.3091011188429
}
#Debug simulation 
Total elapsed time: 158.35859475191683. Arrivals time: 0.7448714761994779 Scheduler time: 157.23628507414833 Scheduler overhead time: 0.15235446114093065 Adapter cache time: 0.02646260941401124 Engine time: 0.15250198682770133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105660551 . Total output tokens: 93034366
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 158.0627965000458,
    "estimated_duration": 3600.0306122611883,
    "input_throughput": 7157.9785772472815,
    "output_throughput": 6223.902908960706,
    "total_throughput": 13381.881486207987,
    "itl": 78.80164118516,
    "ttft": 1257870.092709846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7624914669245486,
    "arrivals": 157801,
    "finished_requests": 104026,
    "scheduler_time": 219.20794872693293
}
#Debug simulation 
Total elapsed time: 158.06298467796296. Arrivals time: 0.7499575265683234 Scheduler time: 156.93483053334057 Scheduler overhead time: 0.1525638489983976 Adapter cache time: 0.0264949225820601 Engine time: 0.15198577474802732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105660551 . Total output tokens: 93034366
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 166.24288509087637,
    "estimated_duration": 3600.0172357059964,
    "input_throughput": 7095.836027293454,
    "output_throughput": 6171.431008619322,
    "total_throughput": 13267.267035912775,
    "itl": 76.93126927128402,
    "ttft": 1252801.1525219854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7269885062240066,
    "arrivals": 157801,
    "finished_requests": 103072,
    "scheduler_time": 221.06536641909014
}
#Debug simulation 
Total elapsed time: 166.24324701307341. Arrivals time: 0.783990444149822 Scheduler time: 165.0597270326689 Scheduler overhead time: 0.16053852625191212 Adapter cache time: 0.029392775613814592 Engine time: 0.1612626276910305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105660551 . Total output tokens: 93034366
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 168.15141669102013,
    "estimated_duration": 3600.0028360266683,
    "input_throughput": 7140.422985991663,
    "output_throughput": 6209.790663574465,
    "total_throughput": 13350.213649566129,
    "itl": 78.44972963560407,
    "ttft": 1246092.0347005657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6600559233874084,
    "arrivals": 157801,
    "finished_requests": 103725,
    "scheduler_time": 219.57440938139987
}
#Debug simulation 
Total elapsed time: 168.15162215428427. Arrivals time: 0.7803570837713778 Scheduler time: 166.9862056230195 Scheduler overhead time: 0.1547880880534649 Adapter cache time: 0.02636330248787999 Engine time: 0.15621074382215738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105660551 . Total output tokens: 93034366
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 162.17457883339375,
    "estimated_duration": 3600.0452841553515,
    "input_throughput": 7095.750743033615,
    "output_throughput": 6171.382370600554,
    "total_throughput": 13267.13311363417,
    "itl": 76.93279679897363,
    "ttft": 1252783.5208334744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7207749909255655,
    "arrivals": 157801,
    "finished_requests": 103072,
    "scheduler_time": 221.06593206987816
}
#Debug simulation 
Total elapsed time: 162.17477677110583. Arrivals time: 0.796299209818244 Scheduler time: 160.99005352379754 Scheduler overhead time: 0.15656442614272237 Adapter cache time: 0.027866993099451065 Engine time: 0.15691162133589387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105660551 . Total output tokens: 93034366
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 137.29199885297567,
    "estimated_duration": 3600.0402367957436,
    "input_throughput": 7153.267548731873,
    "output_throughput": 6219.355209187442,
    "total_throughput": 13372.622757919315,
    "itl": 78.68352718865371,
    "ttft": 1258693.3760726098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6958473825221877,
    "arrivals": 157801,
    "finished_requests": 103955,
    "scheduler_time": 219.3206120224823
}
#Debug simulation 
Total elapsed time: 137.29218417499214. Arrivals time: 0.6472947378642857 Scheduler time: 136.33697047736496 Scheduler overhead time: 0.12107506627216935 Adapter cache time: 0.02222503488883376 Engine time: 0.12270085280761123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105660551 . Total output tokens: 93034366
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 156.72566100023687,
    "estimated_duration": 3600.096883085276,
    "input_throughput": 7106.554026422288,
    "output_throughput": 6180.715609222934,
    "total_throughput": 13287.269635645223,
    "itl": 77.09403900644176,
    "ttft": 1265931.4340812573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8306261707656087,
    "arrivals": 157801,
    "finished_requests": 103301,
    "scheduler_time": 220.859922422117
}
#Debug simulation 
Total elapsed time: 156.72584587009624. Arrivals time: 0.7648559836670756 Scheduler time: 155.57958478666842 Scheduler overhead time: 0.15363516053184867 Adapter cache time: 0.026704907417297363 Engine time: 0.15404346259310842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105339642 . Total output tokens: 92744421
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 155.68515330180526,
    "estimated_duration": 3600.000035273912,
    "input_throughput": 7120.199930234082,
    "output_throughput": 6174.753550608968,
    "total_throughput": 13294.95348084305,
    "itl": 79.1390378393848,
    "ttft": 1277706.7584690358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8463883560895911,
    "arrivals": 157261,
    "finished_requests": 102912,
    "scheduler_time": 220.46360309841168
}
#Debug simulation 
Total elapsed time: 155.68539061304182. Arrivals time: 0.7686725230887532 Scheduler time: 154.53607962932438 Scheduler overhead time: 0.1534407427534461 Adapter cache time: 0.027636876329779625 Engine time: 0.15097432862967253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105339642 . Total output tokens: 92744421
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 154.71948858490214,
    "estimated_duration": 3600.025563320625,
    "input_throughput": 7092.949077963483,
    "output_throughput": 6151.9407599905235,
    "total_throughput": 13244.889837954008,
    "itl": 78.32803725949304,
    "ttft": 1282749.8718788133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9676174517534679,
    "arrivals": 157261,
    "finished_requests": 102533,
    "scheduler_time": 221.2292312231045
}
#Debug simulation 
Total elapsed time: 154.71970134600997. Arrivals time: 0.7625306313857436 Scheduler time: 153.57188534131274 Scheduler overhead time: 0.15472066355869174 Adapter cache time: 0.027387948241084814 Engine time: 0.15510438149794936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105339642 . Total output tokens: 92744421
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 157.48071113601327,
    "estimated_duration": 3600.0282479112634,
    "input_throughput": 7030.5467782606875,
    "output_throughput": 6090.504154438638,
    "total_throughput": 13121.050932699325,
    "itl": 76.41986287128346,
    "ttft": 1300195.6442097635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.032943377224729,
    "arrivals": 157261,
    "finished_requests": 101536,
    "scheduler_time": 223.47177660627693
}
#Debug simulation 
Total elapsed time: 157.48090003384277. Arrivals time: 0.7424671505577862 Scheduler time: 156.357732555829 Scheduler overhead time: 0.1515728640370071 Adapter cache time: 0.027384190820157528 Engine time: 0.1529713161289692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105339642 . Total output tokens: 92744421
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 157.40341343497857,
    "estimated_duration": 3600.0096144503605,
    "input_throughput": 7092.403280677978,
    "output_throughput": 6151.756070624064,
    "total_throughput": 13244.159351302042,
    "itl": 78.33014139325161,
    "ttft": 1282833.6960500265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9134768337942661,
    "arrivals": 157261,
    "finished_requests": 102524,
    "scheduler_time": 221.2270563239431
}
#Debug simulation 
Total elapsed time: 157.40362457511947. Arrivals time: 0.7670504725538194 Scheduler time: 156.2439670995809 Scheduler overhead time: 0.158996791113168 Adapter cache time: 0.02783568063750863 Engine time: 0.15773249557241797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105339642 . Total output tokens: 92744421
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 154.98815746605396,
    "estimated_duration": 3600.065065214122,
    "input_throughput": 7035.852002995248,
    "output_throughput": 6094.696790902303,
    "total_throughput": 13130.548793897551,
    "itl": 76.44245348452733,
    "ttft": 1299904.3374290988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9929757481953131,
    "arrivals": 157261,
    "finished_requests": 101602,
    "scheduler_time": 223.3726355877263
}
#Debug simulation 
Total elapsed time: 154.98834564583376. Arrivals time: 0.7580073960125446 Scheduler time: 153.8468447024934 Scheduler overhead time: 0.1539224456064403 Adapter cache time: 0.026761917863041162 Engine time: 0.15420825220644474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105339642 . Total output tokens: 92744421
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 154.15868145180866,
    "estimated_duration": 3600.0286358351987,
    "input_throughput": 7092.167752736953,
    "output_throughput": 6152.00217563321,
    "total_throughput": 13244.169928370164,
    "itl": 78.3246399864176,
    "ttft": 1282705.138184141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8426775641553091,
    "arrivals": 157261,
    "finished_requests": 102526,
    "scheduler_time": 221.2407122298297
}
#Debug simulation 
Total elapsed time: 154.1589331892319. Arrivals time: 0.7513292524963617 Scheduler time: 153.0280536157079 Scheduler overhead time: 0.15235808119177818 Adapter cache time: 0.026326391845941544 Engine time: 0.1531776781193912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105339642 . Total output tokens: 92744421
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 152.55110152810812,
    "estimated_duration": 3600.09250482113,
    "input_throughput": 7042.18321225044,
    "output_throughput": 6105.919214732002,
    "total_throughput": 13148.102426982441,
    "itl": 76.6467746435859,
    "ttft": 1293720.0443620004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9992381394468249,
    "arrivals": 157261,
    "finished_requests": 101765,
    "scheduler_time": 222.94921420088545
}
#Debug simulation 
Total elapsed time: 152.55129162687808. Arrivals time: 0.7526871827431023 Scheduler time: 151.4210802926682 Scheduler overhead time: 0.151717574801296 Adapter cache time: 0.027311372105032206 Engine time: 0.1506090434268117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105185848 . Total output tokens: 92612733
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 144.966411779169,
    "estimated_duration": 3600.01265914759,
    "input_throughput": 7245.790631796385,
    "output_throughput": 6365.672893335371,
    "total_throughput": 13611.463525131756,
    "itl": 82.55285168607493,
    "ttft": 1206357.670397208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7273649935144924,
    "arrivals": 157020,
    "finished_requests": 105514,
    "scheduler_time": 212.78275410319222
}
#Debug simulation 
Total elapsed time: 144.96667663007975. Arrivals time: 0.7425057594664395 Scheduler time: 143.87731340201572 Scheduler overhead time: 0.13967465795576572 Adapter cache time: 0.02592831151559949 Engine time: 0.1371671063825488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105185848 . Total output tokens: 92612733
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 131.00311392825097,
    "estimated_duration": 3600.084737114139,
    "input_throughput": 7190.459639222456,
    "output_throughput": 6333.180651263414,
    "total_throughput": 13523.64029048587,
    "itl": 81.43048990444919,
    "ttft": 1225165.0741289484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 142,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0397859869990502,
    "arrivals": 157020,
    "finished_requests": 104748,
    "scheduler_time": 213.86547635887123
}
#Debug simulation 
Total elapsed time: 131.00343836285174. Arrivals time: 0.717790539842099 Scheduler time: 129.95293842535466 Scheduler overhead time: 0.13243374694138765 Adapter cache time: 0.02416642103344202 Engine time: 0.13233577134087682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105185848 . Total output tokens: 92612733
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 134.13366502616554,
    "estimated_duration": 3600.024771002875,
    "input_throughput": 7132.017870212454,
    "output_throughput": 6273.097391412912,
    "total_throughput": 13405.115261625366,
    "itl": 79.66985964996408,
    "ttft": 1234634.3844783343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1582749798148877,
    "arrivals": 157020,
    "finished_requests": 103900,
    "scheduler_time": 216.18796317118378
}
#Debug simulation 
Total elapsed time: 134.13388430606574. Arrivals time: 0.7226013257168233 Scheduler time: 133.06752761639655 Scheduler overhead time: 0.13657758943736553 Adapter cache time: 0.024886372033506632 Engine time: 0.13753106584772468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105185848 . Total output tokens: 92612733
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 121.3032572180964,
    "estimated_duration": 3600.0665019980493,
    "input_throughput": 7256.725114800159,
    "output_throughput": 6383.248472561814,
    "total_throughput": 13639.973587361972,
    "itl": 82.17469080514387,
    "ttft": 1202079.729874166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1627330812811847,
    "arrivals": 157020,
    "finished_requests": 105838,
    "scheduler_time": 212.43402355315516
}
#Debug simulation 
Total elapsed time: 121.30345360422507. Arrivals time: 0.7181945694610476 Scheduler time: 120.26158497715369 Scheduler overhead time: 0.12942671356722713 Adapter cache time: 0.023533130530267954 Engine time: 0.1281689996831119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105185848 . Total output tokens: 92612733
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 123.15944033581764,
    "estimated_duration": 3600.0770883812706,
    "input_throughput": 7161.677477187806,
    "output_throughput": 6307.145497878647,
    "total_throughput": 13468.822975066452,
    "itl": 80.07851702121312,
    "ttft": 1214541.558783547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2305420540459464,
    "arrivals": 157020,
    "finished_requests": 104458,
    "scheduler_time": 215.40969876819267
}
#Debug simulation 
Total elapsed time: 123.15961657976732. Arrivals time: 0.7342169401235878 Scheduler time: 122.09280814928934 Scheduler overhead time: 0.13159117940813303 Adapter cache time: 0.023523719515651464 Engine time: 0.13459662487730384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105185848 . Total output tokens: 92612733
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 146.39835510915145,
    "estimated_duration": 3600.068928534586,
    "input_throughput": 7217.870411880465,
    "output_throughput": 6346.201823780028,
    "total_throughput": 13564.072235660493,
    "itl": 81.74997512299903,
    "ttft": 1185546.3713987616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7979901175713157,
    "arrivals": 157020,
    "finished_requests": 105225,
    "scheduler_time": 213.3511418789778
}
#Debug simulation 
Total elapsed time: 146.39854680607095. Arrivals time: 0.7576247975230217 Scheduler time: 145.2881381958723 Scheduler overhead time: 0.13943867944180965 Adapter cache time: 0.025321288499981165 Engine time: 0.14290358452126384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105185848 . Total output tokens: 92612733
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 113.91122375195846,
    "estimated_duration": 3600.0963697957473,
    "input_throughput": 7117.931957319758,
    "output_throughput": 6261.394886292699,
    "total_throughput": 13379.326843612456,
    "itl": 79.34873566475405,
    "ttft": 1218536.3282885402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0007398286648097,
    "arrivals": 157020,
    "finished_requests": 103781,
    "scheduler_time": 216.63506606752702
}
#Debug simulation 
Total elapsed time: 113.91141469310969. Arrivals time: 0.6257531237788498 Scheduler time: 112.99064257135615 Scheduler overhead time: 0.11679289815947413 Adapter cache time: 0.020114666782319546 Engine time: 0.1167978155426681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92398094 . Total output tokens: 81346516
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 209.29440800426528,
    "estimated_duration": 3600.0851923765326,
    "input_throughput": 6892.849383827753,
    "output_throughput": 5978.580464033883,
    "total_throughput": 12871.429847861635,
    "itl": 72.21778362968162,
    "ttft": 1189982.0818508978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6215664490032935,
    "arrivals": 137849,
    "finished_requests": 99980,
    "scheduler_time": 210.3189859516536
}
#Debug simulation 
Total elapsed time: 209.29459200706333. Arrivals time: 0.776605995837599 Scheduler time: 208.08868089318275 Scheduler overhead time: 0.1705990443006158 Adapter cache time: 0.030368478503078222 Engine time: 0.17753818910568953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92398094 . Total output tokens: 81346516
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 210.87980188522488,
    "estimated_duration": 3600.0368966172487,
    "input_throughput": 6878.503668467472,
    "output_throughput": 5959.381421940177,
    "total_throughput": 12837.88509040765,
    "itl": 71.65626078176176,
    "ttft": 1188767.9566069888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6670065059373156,
    "arrivals": 137849,
    "finished_requests": 99751,
    "scheduler_time": 210.9036638905343
}
#Debug simulation 
Total elapsed time: 210.87997705303133. Arrivals time: 0.7775785652920604 Scheduler time: 209.6577628790401 Scheduler overhead time: 0.17820762982591987 Adapter cache time: 0.03125789947807789 Engine time: 0.18041111109778285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92398094 . Total output tokens: 81346516
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 206.99784866720438,
    "estimated_duration": 3600.0851416264163,
    "input_throughput": 6871.518596591871,
    "output_throughput": 5956.938837927466,
    "total_throughput": 12828.457434519336,
    "itl": 70.98383559736742,
    "ttft": 1196781.4421137513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7367733859922738,
    "arrivals": 137849,
    "finished_requests": 99649,
    "scheduler_time": 211.144343293852
}
#Debug simulation 
Total elapsed time: 206.99803278921172. Arrivals time: 0.7507237792015076 Scheduler time: 205.8139321054332 Scheduler overhead time: 0.17336747655645013 Adapter cache time: 0.03129847254604101 Engine time: 0.17590058548375964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92398094 . Total output tokens: 81346516
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 208.40588258719072,
    "estimated_duration": 3600.045868706136,
    "input_throughput": 6877.617925712348,
    "output_throughput": 5960.264336218519,
    "total_throughput": 12837.882261930867,
    "itl": 71.69460229724953,
    "ttft": 1192206.0550007157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6345202396251262,
    "arrivals": 137849,
    "finished_requests": 99717,
    "scheduler_time": 210.9628478944453
}
#Debug simulation 
Total elapsed time: 208.40607752418146. Arrivals time: 0.769260186702013 Scheduler time: 207.20122335711494 Scheduler overhead time: 0.17368234414607286 Adapter cache time: 0.03059800574555993 Engine time: 0.17715391656383872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92398094 . Total output tokens: 81346516
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 203.99742702301592,
    "estimated_duration": 3600.00036717769,
    "input_throughput": 6864.234299890641,
    "output_throughput": 5950.522448639143,
    "total_throughput": 12814.756748529784,
    "itl": 70.94919809311953,
    "ttft": 1195249.5072507327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7224837973201649,
    "arrivals": 137849,
    "finished_requests": 99582,
    "scheduler_time": 211.35427767048589
}
#Debug simulation 
Total elapsed time: 203.99764263583347. Arrivals time: 0.7696759384125471 Scheduler time: 202.79153904272243 Scheduler overhead time: 0.17533409176394343 Adapter cache time: 0.030744721181690693 Engine time: 0.17526448844000697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92398094 . Total output tokens: 81346516
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 211.16062856232747,
    "estimated_duration": 3600.0669106991204,
    "input_throughput": 6869.2681590180655,
    "output_throughput": 5955.57819669422,
    "total_throughput": 12824.846355712285,
    "itl": 71.38849567339705,
    "ttft": 1190439.1576123422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5681689637107777,
    "arrivals": 137849,
    "finished_requests": 99611,
    "scheduler_time": 211.21925723355827
}
#Debug simulation 
Total elapsed time: 211.1608582213521. Arrivals time: 0.761539027094841 Scheduler time: 209.9689180003479 Scheduler overhead time: 0.17361085442826152 Adapter cache time: 0.029425143264234066 Engine time: 0.17446741834282875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92398094 . Total output tokens: 81346516
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 204.71505173295736,
    "estimated_duration": 3600.060062946397,
    "input_throughput": 6871.533409849205,
    "output_throughput": 5952.234580903729,
    "total_throughput": 12823.767990752935,
    "itl": 70.88664917705208,
    "ttft": 1188711.7937849993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6931295167654754,
    "arrivals": 137849,
    "finished_requests": 99666,
    "scheduler_time": 211.2900162020895
}
#Debug simulation 
Total elapsed time: 204.7152279317379. Arrivals time: 0.6564893629401922 Scheduler time: 203.66197669459507 Scheduler overhead time: 0.16062975395470858 Adapter cache time: 0.028001945931464434 Engine time: 0.15720017068088055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91140276 . Total output tokens: 80211485
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 217.25762155093253,
    "estimated_duration": 3600.088777449772,
    "input_throughput": 6867.392036235678,
    "output_throughput": 5953.721789934122,
    "total_throughput": 12821.1138261698,
    "itl": 71.37158117990677,
    "ttft": 1170532.2600165782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6149540399713436,
    "arrivals": 135943,
    "finished_requests": 99568,
    "scheduler_time": 208.5378913673714
}
#Debug simulation 
Total elapsed time: 217.25788121391088. Arrivals time: 0.6852635047398508 Scheduler time: 216.15305606648326 Scheduler overhead time: 0.1683148192241788 Adapter cache time: 0.02884818334132433 Engine time: 0.17044992186129093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91140276 . Total output tokens: 80211485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 217.17494034394622,
    "estimated_duration": 3600.0563424472552,
    "input_throughput": 6846.29257308939,
    "output_throughput": 5935.789045310777,
    "total_throughput": 12782.081618400167,
    "itl": 70.81527085613665,
    "ttft": 1171908.1598969626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6769979058718307,
    "arrivals": 135943,
    "finished_requests": 99260,
    "scheduler_time": 208.90295646879787
}
#Debug simulation 
Total elapsed time: 217.1751294559799. Arrivals time: 0.6903024241328239 Scheduler time: 216.0657340893522 Scheduler overhead time: 0.16934211598709226 Adapter cache time: 0.029857128858566284 Engine time: 0.1676024398766458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91140276 . Total output tokens: 80211485
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 215.52392248809338,
    "estimated_duration": 3600.078362824843,
    "input_throughput": 6828.574970437684,
    "output_throughput": 5919.606978576444,
    "total_throughput": 12748.181949014128,
    "itl": 69.94279217788062,
    "ttft": 1174263.3867953229,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.71549482006114,
    "arrivals": 135943,
    "finished_requests": 98977,
    "scheduler_time": 209.57453503973355
}
#Debug simulation 
Total elapsed time: 215.52412550291047. Arrivals time: 0.6830322174355388 Scheduler time: 214.42274686088786 Scheduler overhead time: 0.17027632845565677 Adapter cache time: 0.029932114761322737 Engine time: 0.16615875530987978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91140276 . Total output tokens: 80211485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 219.63377382885665,
    "estimated_duration": 3600.095926305481,
    "input_throughput": 6846.410346986002,
    "output_throughput": 5934.644919844027,
    "total_throughput": 12781.05526683003,
    "itl": 70.82629198535085,
    "ttft": 1171386.206011884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6367394976457579,
    "arrivals": 135943,
    "finished_requests": 99241,
    "scheduler_time": 208.9276773530259
}
#Debug simulation 
Total elapsed time: 219.63398401113227. Arrivals time: 0.6797104962170124 Scheduler time: 218.53222268354148 Scheduler overhead time: 0.16944999806582928 Adapter cache time: 0.029999001882970333 Engine time: 0.16984062362462282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91140276 . Total output tokens: 80211485
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 212.675383600872,
    "estimated_duration": 3600.07318265758,
    "input_throughput": 6828.584796115863,
    "output_throughput": 5919.615496335035,
    "total_throughput": 12748.200292450898,
    "itl": 69.94268344927332,
    "ttft": 1174262.3915046405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7094884219393136,
    "arrivals": 135943,
    "finished_requests": 98977,
    "scheduler_time": 209.57455222081833
}
#Debug simulation 
Total elapsed time: 212.67552446573973. Arrivals time: 0.6728282901458442 Scheduler time: 211.58565556444228 Scheduler overhead time: 0.16813441971316934 Adapter cache time: 0.02976520825177431 Engine time: 0.16776632936671376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91140276 . Total output tokens: 80211485
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 186.23524953005835,
    "estimated_duration": 3600.073716831278,
    "input_throughput": 6853.2052231741245,
    "output_throughput": 5940.653631621822,
    "total_throughput": 12793.858854795946,
    "itl": 70.82647012299007,
    "ttft": 1171675.2954897156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5937046474730597,
    "arrivals": 135943,
    "finished_requests": 99356,
    "scheduler_time": 209.0000501840783
}
#Debug simulation 
Total elapsed time: 186.23543577687815. Arrivals time: 0.5765600600279868 Scheduler time: 185.33007314708084 Scheduler overhead time: 0.13303809612989426 Adapter cache time: 0.022594320587813854 Engine time: 0.1279014595784247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91140276 . Total output tokens: 80211485
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 215.46271791309118,
    "estimated_duration": 3600.067761371236,
    "input_throughput": 6828.595079176061,
    "output_throughput": 5919.624410592427,
    "total_throughput": 12748.219489768488,
    "itl": 69.94260412053222,
    "ttft": 1174261.4843130424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7028606722876432,
    "arrivals": 135943,
    "finished_requests": 98977,
    "scheduler_time": 209.57444397824472
}
#Debug simulation 
Total elapsed time: 215.46285388432443. Arrivals time: 0.687342053744942 Scheduler time: 214.3528039031662 Scheduler overhead time: 0.17223278107121587 Adapter cache time: 0.030128713697195053 Engine time: 0.16847844514995813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90522113 . Total output tokens: 79647408
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 219.88487762212753,
    "estimated_duration": 3600.0225176875683,
    "input_throughput": 6751.339715400451,
    "output_throughput": 5941.054505886336,
    "total_throughput": 12692.394221286786,
    "itl": 71.44278604963925,
    "ttft": 1180955.3843561648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5818919948115939,
    "arrivals": 135044,
    "finished_requests": 98751,
    "scheduler_time": 208.84972208635597
}
#Debug simulation 
Total elapsed time: 219.88500448083505. Arrivals time: 0.6692298385314643 Scheduler time: 218.79505072021857 Scheduler overhead time: 0.17097730468958616 Adapter cache time: 0.02866914728656411 Engine time: 0.1692998562939465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90522113 . Total output tokens: 79647408
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 220.81513116369024,
    "estimated_duration": 3600.068647507822,
    "input_throughput": 6728.954742785184,
    "output_throughput": 5918.764358771496,
    "total_throughput": 12647.71910155668,
    "itl": 70.66821718994589,
    "ttft": 1179958.8842374526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6189854334807023,
    "arrivals": 135044,
    "finished_requests": 98368,
    "scheduler_time": 209.71753524651743
}
#Debug simulation 
Total elapsed time: 220.8152785920538. Arrivals time: 0.686274784617126 Scheduler time: 219.70011638104916 Scheduler overhead time: 0.17416479950770736 Adapter cache time: 0.03184921573847532 Engine time: 0.16970858816057444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90522113 . Total output tokens: 79647408
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 222.0952317621559,
    "estimated_duration": 3600.004949920241,
    "input_throughput": 6683.036644306012,
    "output_throughput": 5880.69635861724,
    "total_throughput": 12563.733002923253,
    "itl": 69.42712701223891,
    "ttft": 1180240.1238185738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6721647990029307,
    "arrivals": 135044,
    "finished_requests": 97678,
    "scheduler_time": 211.20456711429816
}
#Debug simulation 
Total elapsed time: 222.09539571916685. Arrivals time: 0.6851795958355069 Scheduler time: 220.98446420021355 Scheduler overhead time: 0.17350710649043322 Adapter cache time: 0.031162769068032503 Engine time: 0.16801795549690723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90522113 . Total output tokens: 79647408
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 217.42603745963424,
    "estimated_duration": 3600.044301519349,
    "input_throughput": 6737.527921465673,
    "output_throughput": 5928.623153607402,
    "total_throughput": 12666.151075073074,
    "itl": 70.8831889175586,
    "ttft": 1182391.7411015627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6020434509962798,
    "arrivals": 135044,
    "finished_requests": 98525,
    "scheduler_time": 209.36241397146995
}
#Debug simulation 
Total elapsed time: 217.42617085203528. Arrivals time: 0.6921414681710303 Scheduler time: 216.31496875407174 Scheduler overhead time: 0.1698707677423954 Adapter cache time: 0.029303960967808962 Engine time: 0.16711547784507275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90522113 . Total output tokens: 79647408
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 219.15982276992872,
    "estimated_duration": 3600.0944848158842,
    "input_throughput": 6682.870436171461,
    "output_throughput": 5880.550104807236,
    "total_throughput": 12563.420540978697,
    "itl": 69.42700224267766,
    "ttft": 1180219.3748328532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6665726352343336,
    "arrivals": 135044,
    "finished_requests": 97678,
    "scheduler_time": 211.2108319406289
}
#Debug simulation 
Total elapsed time: 219.15998903894797. Arrivals time: 0.6844840445555747 Scheduler time: 218.04988927952945 Scheduler overhead time: 0.1725422153249383 Adapter cache time: 0.030193593353033066 Engine time: 0.1685707252472639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90522113 . Total output tokens: 79647408
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 217.7486263117753,
    "estimated_duration": 3600.002888028096,
    "input_throughput": 6737.605428223951,
    "output_throughput": 5928.691354936887,
    "total_throughput": 12666.296783160838,
    "itl": 70.88265782475587,
    "ttft": 1182384.2352448981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5617850427702072,
    "arrivals": 135044,
    "finished_requests": 98525,
    "scheduler_time": 209.3608984341225
}
#Debug simulation 
Total elapsed time: 217.74876076774672. Arrivals time: 0.6884705894626677 Scheduler time: 216.63542955880985 Scheduler overhead time: 0.17108805803582072 Adapter cache time: 0.0309203346259892 Engine time: 0.16897593345493078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90522113 . Total output tokens: 79647408
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 222.42524347407743,
    "estimated_duration": 3600.0895083730006,
    "input_throughput": 6682.879673975951,
    "output_throughput": 5880.5582335556055,
    "total_throughput": 12563.437907531556,
    "itl": 69.42695463961282,
    "ttft": 1180218.8315272157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6601520027592779,
    "arrivals": 135044,
    "finished_requests": 97678,
    "scheduler_time": 211.21076290372733
}
#Debug simulation 
Total elapsed time: 222.42544727306813. Arrivals time: 0.6924057104624808 Scheduler time: 221.30066243559122 Scheduler overhead time: 0.1777147240936756 Adapter cache time: 0.02976486971601844 Engine time: 0.17106448812410235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90195380 . Total output tokens: 79368359
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 217.9233005279675,
    "estimated_duration": 3600.046156429336,
    "input_throughput": 6768.4223871639915,
    "output_throughput": 5958.16360901178,
    "total_throughput": 12726.585996175772,
    "itl": 71.57766055717562,
    "ttft": 1169732.692200776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5686671767476941,
    "arrivals": 134574,
    "finished_requests": 98690,
    "scheduler_time": 208.6734980110217
}
#Debug simulation 
Total elapsed time: 217.9234464080073. Arrivals time: 0.7008653748780489 Scheduler time: 216.7957669799216 Scheduler overhead time: 0.17297688405960798 Adapter cache time: 0.03108552983030677 Engine time: 0.17019720701500773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90195380 . Total output tokens: 79368359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 163.8232063408941,
    "estimated_duration": 3600.0160269393164,
    "input_throughput": 6775.412891907983,
    "output_throughput": 5963.859838216745,
    "total_throughput": 12739.272730124727,
    "itl": 71.17375609238925,
    "ttft": 1171154.403527348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6359179382817819,
    "arrivals": 134574,
    "finished_requests": 98768,
    "scheduler_time": 208.57636681848274
}
#Debug simulation 
Total elapsed time: 163.82333734817803. Arrivals time: 0.5143230990506709 Scheduler time: 163.0174166802317 Scheduler overhead time: 0.11687659798189998 Adapter cache time: 0.020073664840310812 Engine time: 0.11148764519020915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90195380 . Total output tokens: 79368359
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 217.7513397601433,
    "estimated_duration": 3600.0726033089813,
    "input_throughput": 6738.65076434901,
    "output_throughput": 5936.654438678743,
    "total_throughput": 12675.305203027754,
    "itl": 70.05596582699992,
    "ttft": 1173086.9974743426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6594302650447936,
    "arrivals": 134574,
    "finished_requests": 98256,
    "scheduler_time": 209.77399405095912
}
#Debug simulation 
Total elapsed time: 217.7516048992984. Arrivals time: 0.6917549539357424 Scheduler time: 216.63847202435136 Scheduler overhead time: 0.17165580531582236 Adapter cache time: 0.030001427046954632 Engine time: 0.1677920501679182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90195380 . Total output tokens: 79368359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 215.37910135602579,
    "estimated_duration": 3600.0813875496624,
    "input_throughput": 6753.23454744108,
    "output_throughput": 5945.583084322626,
    "total_throughput": 12698.817631763706,
    "itl": 71.04467542726849,
    "ttft": 1170813.1015335605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587887388141826,
    "arrivals": 134574,
    "finished_requests": 98467,
    "scheduler_time": 209.1682430469705
}
#Debug simulation 
Total elapsed time: 215.37924584001303. Arrivals time: 0.6936871092766523 Scheduler time: 214.26896983291954 Scheduler overhead time: 0.16799032129347324 Adapter cache time: 0.029880318325012922 Engine time: 0.16554565215483308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90195380 . Total output tokens: 79368359
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 213.48949751630425,
    "estimated_duration": 3600.065543894179,
    "input_throughput": 6738.663978255917,
    "output_throughput": 5936.666079940745,
    "total_throughput": 12675.330058196661,
    "itl": 70.05582689803086,
    "ttft": 1173085.3920018438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6534238669229673,
    "arrivals": 134574,
    "finished_requests": 98256,
    "scheduler_time": 209.7737360729049
}
#Debug simulation 
Total elapsed time: 213.48964752722532. Arrivals time: 0.68807314010337 Scheduler time: 212.37565687671304 Scheduler overhead time: 0.1714019188657403 Adapter cache time: 0.030731995590031147 Engine time: 0.170207807328552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90195380 . Total output tokens: 79368359
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 218.00490263616666,
    "estimated_duration": 3600.066287729716,
    "input_throughput": 6754.538682490405,
    "output_throughput": 5946.023014342651,
    "total_throughput": 12700.561696833056,
    "itl": 71.05656396434935,
    "ttft": 1170876.1265985619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5490172008890662,
    "arrivals": 134574,
    "finished_requests": 98479,
    "scheduler_time": 209.16169635393783
}
#Debug simulation 
Total elapsed time: 218.00506187416613. Arrivals time: 0.683947435580194 Scheduler time: 216.89732929272577 Scheduler overhead time: 0.17293645394966006 Adapter cache time: 0.028884997125715017 Engine time: 0.16979994578287005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90195380 . Total output tokens: 79368359
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 216.68442484736443,
    "estimated_duration": 3600.043465584266,
    "input_throughput": 6738.657805639253,
    "output_throughput": 5936.615267096904,
    "total_throughput": 12675.273072736156,
    "itl": 70.05363392822771,
    "ttft": 1173065.9353153836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6474174688011409,
    "arrivals": 134574,
    "finished_requests": 98255,
    "scheduler_time": 209.7761188468582
}
#Debug simulation 
Total elapsed time: 216.6845723344013. Arrivals time: 0.6809689877554774 Scheduler time: 215.58167423540726 Scheduler overhead time: 0.1696230759844184 Adapter cache time: 0.03036171104758978 Engine time: 0.16862657479941845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90037262 . Total output tokens: 79231279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 221.40806071273983,
    "estimated_duration": 3600.020160299302,
    "input_throughput": 6841.244466239989,
    "output_throughput": 5959.9527348802885,
    "total_throughput": 12801.197201120278,
    "itl": 71.17553126584464,
    "ttft": 1158530.029845099,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5818919948115939,
    "arrivals": 134344,
    "finished_requests": 99433,
    "scheduler_time": 207.26141098045395
}
#Debug simulation 
Total elapsed time: 221.40820628078654. Arrivals time: 0.7057030582800508 Scheduler time: 220.28094968898222 Scheduler overhead time: 0.17067266814410686 Adapter cache time: 0.030269909650087357 Engine time: 0.1682759877294302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90037262 . Total output tokens: 79231279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 223.35071254521608,
    "estimated_duration": 3600.059106382804,
    "input_throughput": 6809.7340281260385,
    "output_throughput": 5936.528642573757,
    "total_throughput": 12746.262670699794,
    "itl": 70.3838861831478,
    "ttft": 1158647.711860604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436900801956653,
    "arrivals": 134344,
    "finished_requests": 99026,
    "scheduler_time": 208.15719839564952
}
#Debug simulation 
Total elapsed time: 223.35084518231452. Arrivals time: 0.6914927228353918 Scheduler time: 222.23599301790819 Scheduler overhead time: 0.17201792867854238 Adapter cache time: 0.03035720670595765 Engine time: 0.16783074289560318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90037262 . Total output tokens: 79231279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 223.17836153507233,
    "estimated_duration": 3600.0243847813686,
    "input_throughput": 6785.604870696882,
    "output_throughput": 5916.994643160908,
    "total_throughput": 12702.59951385779,
    "itl": 69.48829649489494,
    "ttft": 1161649.5615944527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6611390714393931,
    "arrivals": 134344,
    "finished_requests": 98670,
    "scheduler_time": 209.00776787417993
}
#Debug simulation 
Total elapsed time: 223.17854230012745. Arrivals time: 0.7086398550309241 Scheduler time: 222.04108330328017 Scheduler overhead time: 0.17346343770623207 Adapter cache time: 0.030500032007694244 Engine time: 0.1721046702004969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90037262 . Total output tokens: 79231279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 222.81364298425615,
    "estimated_duration": 3600.031743070842,
    "input_throughput": 6826.062866611904,
    "output_throughput": 5952.211682923023,
    "total_throughput": 12778.274549534926,
    "itl": 70.66901333257783,
    "ttft": 1160638.342271622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6098155929101633,
    "arrivals": 134344,
    "finished_requests": 99261,
    "scheduler_time": 207.58013140366867
}
#Debug simulation 
Total elapsed time: 222.81377261132002. Arrivals time: 0.694713824428618 Scheduler time: 221.69094752287492 Scheduler overhead time: 0.1739814653992653 Adapter cache time: 0.030756095424294472 Engine time: 0.170553142670542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90037262 . Total output tokens: 79231279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 221.79398440103978,
    "estimated_duration": 3600.0187784839823,
    "input_throughput": 6785.615437896997,
    "output_throughput": 5917.003857677177,
    "total_throughput": 12702.619295574174,
    "itl": 69.4882535107427,
    "ttft": 1161648.4332958113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6549255561409519,
    "arrivals": 134344,
    "finished_requests": 98670,
    "scheduler_time": 209.00756604232643
}
#Debug simulation 
Total elapsed time: 221.79411032190546. Arrivals time: 0.6924161636270583 Scheduler time: 220.67006669240072 Scheduler overhead time: 0.17566928546875715 Adapter cache time: 0.03037530230358243 Engine time: 0.17168347723782063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90037262 . Total output tokens: 79231279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 221.85975842596963,
    "estimated_duration": 3600.0430084251243,
    "input_throughput": 6821.361562217224,
    "output_throughput": 5948.8286528467515,
    "total_throughput": 12770.190215063976,
    "itl": 70.57891424762603,
    "ttft": 1160354.4890863274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6000885684136302,
    "arrivals": 134344,
    "finished_requests": 99172,
    "scheduler_time": 207.75129798472196
}
#Debug simulation 
Total elapsed time: 221.85996752604842. Arrivals time: 0.7071909932419658 Scheduler time: 220.7298375912942 Scheduler overhead time: 0.17262030020356178 Adapter cache time: 0.028975713532418013 Engine time: 0.16815090412274003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90037262 . Total output tokens: 79231279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 224.15181826101616,
    "estimated_duration": 3600.0356908734175,
    "input_throughput": 6772.063416428286,
    "output_throughput": 5905.947836545377,
    "total_throughput": 12678.011252973662,
    "itl": 69.32831059302133,
    "ttft": 1160991.7985514777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 94,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6961328952014447,
    "arrivals": 134344,
    "finished_requests": 98494,
    "scheduler_time": 209.34772020774707
}
#Debug simulation 
Total elapsed time: 224.15195434121415. Arrivals time: 0.7064106054604053 Scheduler time: 223.01910072332248 Scheduler overhead time: 0.17358235828578472 Adapter cache time: 0.03176066931337118 Engine time: 0.16839081794023514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88626587 . Total output tokens: 77965993
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 228.95510446513072,
    "estimated_duration": 3600.07366349068,
    "input_throughput": 6729.3359148962645,
    "output_throughput": 5929.5483913242615,
    "total_throughput": 12658.884306220525,
    "itl": 70.35663173474842,
    "ttft": 1149212.5610303865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8067139018978915,
    "arrivals": 132130,
    "finished_requests": 98330,
    "scheduler_time": 204.83307634155142
}
#Debug simulation 
Total elapsed time: 228.95525733288378. Arrivals time: 0.6985624488443136 Scheduler time: 227.81565145403147 Scheduler overhead time: 0.17806291673332453 Adapter cache time: 0.032207305543124676 Engine time: 0.17603372503072023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88626587 . Total output tokens: 77965993
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 228.46254342608154,
    "estimated_duration": 3600.027114473343,
    "input_throughput": 6735.81287832813,
    "output_throughput": 5934.855577643513,
    "total_throughput": 12670.668455971643,
    "itl": 70.09794661916051,
    "ttft": 1151678.6250933786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9159889003029098,
    "arrivals": 132130,
    "finished_requests": 98401,
    "scheduler_time": 204.6666324539671
}
#Debug simulation 
Total elapsed time: 228.46268173214048. Arrivals time: 0.7155307177454233 Scheduler time: 227.30601391475648 Scheduler overhead time: 0.18108824733644724 Adapter cache time: 0.031086169183254242 Engine time: 0.17443138547241688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88626587 . Total output tokens: 77965993
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 229.00501528568566,
    "estimated_duration": 3600.0619237034107,
    "input_throughput": 6705.692988515616,
    "output_throughput": 5907.894767021296,
    "total_throughput": 12613.587755536912,
    "itl": 69.0548595038154,
    "ttft": 1153174.1479043905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0231584974564616,
    "arrivals": 132130,
    "finished_requests": 97949,
    "scheduler_time": 205.68577257203592
}
#Debug simulation 
Total elapsed time: 229.00514978775755. Arrivals time: 0.6954505098983645 Scheduler time: 227.87068253429607 Scheduler overhead time: 0.17856986029073596 Adapter cache time: 0.032863442320376635 Engine time: 0.17467466183006763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88626587 . Total output tokens: 77965993
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 228.22497202223167,
    "estimated_duration": 3600.0738273837846,
    "input_throughput": 6735.81130907594,
    "output_throughput": 5935.046897504144,
    "total_throughput": 12670.858206580084,
    "itl": 70.09714407146826,
    "ttft": 1151631.3283676032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8604600613703947,
    "arrivals": 132130,
    "finished_requests": 98403,
    "scheduler_time": 204.67274948097696
}
#Debug simulation 
Total elapsed time: 228.2250993810594. Arrivals time: 0.6960120932199061 Scheduler time: 227.0902335010469 Scheduler overhead time: 0.17921999422833323 Adapter cache time: 0.032148264814168215 Engine time: 0.174424153752625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88626587 . Total output tokens: 77965993
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 227.59256549505517,
    "estimated_duration": 3600.0516109684863,
    "input_throughput": 6705.712197694191,
    "output_throughput": 5907.911690821084,
    "total_throughput": 12613.623888515276,
    "itl": 69.05455320527409,
    "ttft": 1153172.2534967451,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0140453416854145,
    "arrivals": 132130,
    "finished_requests": 97949,
    "scheduler_time": 205.6855787477538
}
#Debug simulation 
Total elapsed time: 227.59271784499288. Arrivals time: 0.6989657040685415 Scheduler time: 226.44961887365207 Scheduler overhead time: 0.18150370614603162 Adapter cache time: 0.03239521011710167 Engine time: 0.17611996224150062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88626587 . Total output tokens: 77965993
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 203.47147004818544,
    "estimated_duration": 3600.034387977807,
    "input_throughput": 6735.885101814613,
    "output_throughput": 5935.111917639749,
    "total_throughput": 12670.997019454362,
    "itl": 70.09579253743152,
    "ttft": 1151627.0736925085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7979901175713157,
    "arrivals": 132130,
    "finished_requests": 98403,
    "scheduler_time": 204.6741030785187
}
#Debug simulation 
Total elapsed time: 203.47159266704693. Arrivals time: 0.6220902604982257 Scheduler time: 202.46161739947274 Scheduler overhead time: 0.15563613455742598 Adapter cache time: 0.027072440832853317 Engine time: 0.15513841295614839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88626587 . Total output tokens: 77965993
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 238.565048977267,
    "estimated_duration": 3600.08073715176,
    "input_throughput": 6709.656189297219,
    "output_throughput": 5909.245528985261,
    "total_throughput": 12618.90171828248,
    "itl": 69.07336804039262,
    "ttft": 1153040.0851545725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9441554460674525,
    "arrivals": 132130,
    "finished_requests": 97990,
    "scheduler_time": 205.65989494804126
}
#Debug simulation 
Total elapsed time: 238.56518271332607. Arrivals time: 0.7676585740409791 Scheduler time: 237.32510138396174 Scheduler overhead time: 0.19199190568178892 Adapter cache time: 0.0335837178863585 Engine time: 0.19084144989028573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88002976 . Total output tokens: 77408960
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 228.74647265393287,
    "estimated_duration": 3600.0215977802363,
    "input_throughput": 6818.8496466621855,
    "output_throughput": 5873.903093536626,
    "total_throughput": 12692.752740198812,
    "itl": 69.2796440305197,
    "ttft": 1143848.8942032568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 131,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8662255831854409,
    "arrivals": 131205,
    "finished_requests": 98737,
    "scheduler_time": 203.83308004681356
}
#Debug simulation 
Total elapsed time: 228.7466057818383. Arrivals time: 0.6978923436254263 Scheduler time: 227.6055831857957 Scheduler overhead time: 0.1817209548316896 Adapter cache time: 0.031610059551894665 Engine time: 0.17601640056818724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88002976 . Total output tokens: 77408960
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 229.68690052395687,
    "estimated_duration": 3600.019334702103,
    "input_throughput": 6830.115817151484,
    "output_throughput": 5878.647871692953,
    "total_throughput": 12708.763688844438,
    "itl": 69.14165945207522,
    "ttft": 1158249.5023501012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8049596554879103,
    "arrivals": 131205,
    "finished_requests": 98887,
    "scheduler_time": 203.86171329946313
}
#Debug simulation 
Total elapsed time: 229.68702414212748. Arrivals time: 0.7057220060378313 Scheduler time: 228.53733680862933 Scheduler overhead time: 0.18010663660243154 Adapter cache time: 0.032266873866319656 Engine time: 0.17518294230103493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88002976 . Total output tokens: 77408960
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 225.24692062614486,
    "estimated_duration": 3600.023145663264,
    "input_throughput": 6780.901125429423,
    "output_throughput": 5842.5310474288235,
    "total_throughput": 12623.432172858247,
    "itl": 68.02410966603443,
    "ttft": 1164871.7778776803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8510792610188948,
    "arrivals": 131205,
    "finished_requests": 98198,
    "scheduler_time": 205.08588803199845
}
#Debug simulation 
Total elapsed time: 225.24704413115978. Arrivals time: 0.6936168009415269 Scheduler time: 224.11337979417294 Scheduler overhead time: 0.1790987909771502 Adapter cache time: 0.03186015225946903 Engine time: 0.17447860212996602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88002976 . Total output tokens: 77408960
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 231.04320132220164,
    "estimated_duration": 3600.059615454718,
    "input_throughput": 6809.45251427555,
    "output_throughput": 5865.168984801104,
    "total_throughput": 12674.621499076655,
    "itl": 68.88743961856147,
    "ttft": 1144651.657909013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9051475079543884,
    "arrivals": 131205,
    "finished_requests": 98592,
    "scheduler_time": 204.1983914563404
}
#Debug simulation 
Total elapsed time: 231.04333595512435. Arrivals time: 0.7024512360803783 Scheduler time: 229.898936658632 Scheduler overhead time: 0.17880614334717393 Adapter cache time: 0.03313035471364856 Engine time: 0.17454196605831385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88002976 . Total output tokens: 77408960
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 221.52103034686297,
    "estimated_duration": 3600.0164915994,
    "input_throughput": 6780.913658857881,
    "output_throughput": 5842.541846427886,
    "total_throughput": 12623.455505285767,
    "itl": 68.0239597799529,
    "ttft": 1164870.7652643889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8434159254841507,
    "arrivals": 131205,
    "finished_requests": 98198,
    "scheduler_time": 205.0858859914533
}
#Debug simulation 
Total elapsed time: 221.52123320382088. Arrivals time: 0.7099526030942798 Scheduler time: 220.3707353794016 Scheduler overhead time: 0.17789515992626548 Adapter cache time: 0.031437032390385866 Engine time: 0.17598102055490017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88002976 . Total output tokens: 77408960
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 230.2346111452207,
    "estimated_duration": 3600.0962149430034,
    "input_throughput": 6802.23014550396,
    "output_throughput": 5862.293877702414,
    "total_throughput": 12664.524023206373,
    "itl": 68.82613452661226,
    "ttft": 1144975.743172686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8554454060364501,
    "arrivals": 131205,
    "finished_requests": 98526,
    "scheduler_time": 204.31398399727172
}
#Debug simulation 
Total elapsed time: 230.23475879617035. Arrivals time: 0.6927807182073593 Scheduler time: 229.09627906093374 Scheduler overhead time: 0.18088809121400118 Adapter cache time: 0.03198382444679737 Engine time: 0.17891995050013065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88002976 . Total output tokens: 77408960
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 226.85283387219533,
    "estimated_duration": 3600.0488001050826,
    "input_throughput": 6781.3880743192685,
    "output_throughput": 5842.76107573487,
    "total_throughput": 12624.149150054138,
    "itl": 68.02440038141917,
    "ttft": 1164669.4427449876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8351312384195627,
    "arrivals": 131205,
    "finished_requests": 98203,
    "scheduler_time": 205.09048936991215
}
#Debug simulation 
Total elapsed time: 226.8529797079973. Arrivals time: 0.7053134301677346 Scheduler time: 225.707852920983 Scheduler overhead time: 0.17792941723018885 Adapter cache time: 0.03176570078358054 Engine time: 0.17635288508608937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87691996 . Total output tokens: 77111729
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 239.28343334700912,
    "estimated_duration": 3600.0069434126035,
    "input_throughput": 6844.964575718526,
    "output_throughput": 5869.904511897508,
    "total_throughput": 12714.869087616034,
    "itl": 69.06036133628082,
    "ttft": 1132809.5360235798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7538146296422921,
    "arrivals": 130771,
    "finished_requests": 98831,
    "scheduler_time": 203.38265255893472
}
#Debug simulation 
Total elapsed time: 239.28355790581554. Arrivals time: 0.7061776756308973 Scheduler time: 238.1342971995473 Scheduler overhead time: 0.18076236033812165 Adapter cache time: 0.03356315195560455 Engine time: 0.1732160155661404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87691996 . Total output tokens: 77111729
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 237.5366369727999,
    "estimated_duration": 3600.063721185463,
    "input_throughput": 6828.961625130491,
    "output_throughput": 5857.372155917369,
    "total_throughput": 12686.33378104786,
    "itl": 68.58141262704952,
    "ttft": 1133821.1588267733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8191157183423641,
    "arrivals": 130771,
    "finished_requests": 98617,
    "scheduler_time": 203.84598786543913
}
#Debug simulation 
Total elapsed time: 237.53677821578458. Arrivals time: 0.7160833426751196 Scheduler time: 236.37614864110947 Scheduler overhead time: 0.1802515210583806 Adapter cache time: 0.03293251432478428 Engine time: 0.17733388720080256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87691996 . Total output tokens: 77111729
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 237.76709995884448,
    "estimated_duration": 3600.0942083907903,
    "input_throughput": 6794.550526757981,
    "output_throughput": 5827.035567876686,
    "total_throughput": 12621.586094634667,
    "itl": 67.4875219540046,
    "ttft": 1135750.5758154336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9347896489454435,
    "arrivals": 130771,
    "finished_requests": 98101,
    "scheduler_time": 204.99492318208476
}
#Debug simulation 
Total elapsed time: 237.76723516359925. Arrivals time: 0.6954726525582373 Scheduler time: 236.62702253600582 Scheduler overhead time: 0.17933023162186146 Adapter cache time: 0.033581117168068886 Engine time: 0.17603695951402187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87691996 . Total output tokens: 77111729
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 241.02957412227988,
    "estimated_duration": 3600.0720180039534,
    "input_throughput": 6818.6994252438435,
    "output_throughput": 5846.985531048031,
    "total_throughput": 12665.684956291874,
    "itl": 68.43374776515982,
    "ttft": 1133672.6527552628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8038358099525793,
    "arrivals": 130771,
    "finished_requests": 98455,
    "scheduler_time": 204.21598150044468
}
#Debug simulation 
Total elapsed time: 241.0297054639086. Arrivals time: 0.6977910553105175 Scheduler time: 239.8772893724963 Scheduler overhead time: 0.18396036652848125 Adapter cache time: 0.03232734138146043 Engine time: 0.18102713767439127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87691996 . Total output tokens: 77111729
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 236.22894044592977,
    "estimated_duration": 3600.085924701942,
    "input_throughput": 6794.56616081328,
    "output_throughput": 5827.048975709322,
    "total_throughput": 12621.615136522601,
    "itl": 67.4873560656873,
    "ttft": 1135749.4721567477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.926090727527626,
    "arrivals": 130771,
    "finished_requests": 98101,
    "scheduler_time": 204.99483275854712
}
#Debug simulation 
Total elapsed time: 236.22908666497096. Arrivals time: 0.7031285734847188 Scheduler time: 235.06841318588704 Scheduler overhead time: 0.18633720837533474 Adapter cache time: 0.03329085744917393 Engine time: 0.18132994696497917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87691996 . Total output tokens: 77111729
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 235.7590730949305,
    "estimated_duration": 3600.0693470478495,
    "input_throughput": 6832.926987968115,
    "output_throughput": 5859.215466862405,
    "total_throughput": 12692.14245483052,
    "itl": 68.58434344675815,
    "ttft": 1133585.665056948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7086152244033287,
    "arrivals": 130771,
    "finished_requests": 98653,
    "scheduler_time": 203.80324719383214
}
#Debug simulation 
Total elapsed time: 235.7592283426784. Arrivals time: 0.7178674130700529 Scheduler time: 234.58841876033694 Scheduler overhead time: 0.18452043505385518 Adapter cache time: 0.03249081410467625 Engine time: 0.17861456982791424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87691996 . Total output tokens: 77111729
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 237.16029487876222,
    "estimated_duration": 3600.0011649928133,
    "input_throughput": 6794.318079074518,
    "output_throughput": 5826.955058788676,
    "total_throughput": 12621.273137863194,
    "itl": 67.4863941221409,
    "ttft": 1135847.7764464086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9093157327361406,
    "arrivals": 130771,
    "finished_requests": 98093,
    "scheduler_time": 204.98669654711554
}
#Debug simulation 
Total elapsed time: 237.160425926093. Arrivals time: 0.7040111212991178 Scheduler time: 236.0077307438478 Scheduler overhead time: 0.18034562980756164 Adapter cache time: 0.03185657924041152 Engine time: 0.18048467300832272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87528464 . Total output tokens: 76979800
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 195.84573412314057,
    "estimated_duration": 3600.092701986144,
    "input_throughput": 6738.998689288022,
    "output_throughput": 5868.261944572923,
    "total_throughput": 12607.260633860946,
    "itl": 69.31445649845122,
    "ttft": 1141046.6733095755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6810781302908429,
    "arrivals": 130530,
    "finished_requests": 98150,
    "scheduler_time": 203.97852716183095
}
#Debug simulation 
Total elapsed time: 195.84586526593193. Arrivals time: 0.5963279325515032 Scheduler time: 194.87232626136392 Scheduler overhead time: 0.15224224422127008 Adapter cache time: 0.025402838829904795 Engine time: 0.15088111255317926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87528464 . Total output tokens: 76979800
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 239.62240652181208,
    "estimated_duration": 3600.0514992810718,
    "input_throughput": 6721.682732825463,
    "output_throughput": 5856.629829937293,
    "total_throughput": 12578.312562762756,
    "itl": 68.80072921968606,
    "ttft": 1141934.8930910642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7866389297135175,
    "arrivals": 130530,
    "finished_requests": 97949,
    "scheduler_time": 204.36528102500964
}
#Debug simulation 
Total elapsed time: 239.62254560319707. Arrivals time: 0.7564104190096259 Scheduler time: 238.392054237891 Scheduler overhead time: 0.19528042478486896 Adapter cache time: 0.034458347130566835 Engine time: 0.18733478663489223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87528464 . Total output tokens: 76979800
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 228.78603375330567,
    "estimated_duration": 3600.030893646332,
    "input_throughput": 6709.257146272937,
    "output_throughput": 5845.288449368316,
    "total_throughput": 12554.545595641252,
    "itl": 68.04628248936493,
    "ttft": 1146195.5591804283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8540289152087646,
    "arrivals": 130530,
    "finished_requests": 97782,
    "scheduler_time": 204.75564524415842
}
#Debug simulation 
Total elapsed time: 228.78616176033393. Arrivals time: 0.6969384727999568 Scheduler time: 227.64742391975597 Scheduler overhead time: 0.17781943222507834 Adapter cache time: 0.03247889969497919 Engine time: 0.17588151199743152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87528464 . Total output tokens: 76979800
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 230.18732477584854,
    "estimated_duration": 3600.0038477045296,
    "input_throughput": 6725.814755847805,
    "output_throughput": 5859.5115150919455,
    "total_throughput": 12585.32627093975,
    "itl": 68.83998141724983,
    "ttft": 1142013.6008324076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7366629746742541,
    "arrivals": 130530,
    "finished_requests": 98003,
    "scheduler_time": 204.33072985340624
}
#Debug simulation 
Total elapsed time: 230.18754818476737. Arrivals time: 0.707056560087949 Scheduler time: 229.0319403996691 Scheduler overhead time: 0.1771919927559793 Adapter cache time: 0.03157856268808246 Engine time: 0.18442167714238167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87528464 . Total output tokens: 76979800
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 227.00878697400913,
    "estimated_duration": 3600.023501334556,
    "input_throughput": 6709.270923105391,
    "output_throughput": 5845.3004521218045,
    "total_throughput": 12554.571375227195,
    "itl": 68.04610149434718,
    "ttft": 1146194.3818453667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8461584624974059,
    "arrivals": 130530,
    "finished_requests": 97782,
    "scheduler_time": 204.7556113262611
}
#Debug simulation 
Total elapsed time: 227.00890971021727. Arrivals time: 0.6970512443222106 Scheduler time: 225.86922578606755 Scheduler overhead time: 0.1766218082047999 Adapter cache time: 0.031912936829030514 Engine time: 0.17909290315583348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87528464 . Total output tokens: 76979800
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 231.23402694612741,
    "estimated_duration": 3600.0188674485944,
    "input_throughput": 6724.521423732699,
    "output_throughput": 5857.932354378454,
    "total_throughput": 12582.453778111152,
    "itl": 68.86717755030215,
    "ttft": 1142422.1342903872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7022313034627582,
    "arrivals": 130530,
    "finished_requests": 97979,
    "scheduler_time": 204.33359511757848
}
#Debug simulation 
Total elapsed time: 231.2341565489769. Arrivals time: 0.7193689970299602 Scheduler time: 230.06277977349237 Scheduler overhead time: 0.18468132289126515 Adapter cache time: 0.033273091074079275 Engine time: 0.17800821270793676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87528464 . Total output tokens: 76979800
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 228.89211927214637,
    "estimated_duration": 3600.016263560441,
    "input_throughput": 6709.284411985403,
    "output_throughput": 5845.312204003242,
    "total_throughput": 12554.596615988645,
    "itl": 68.0460315361816,
    "ttft": 1146193.2090149904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8384951269626619,
    "arrivals": 130530,
    "finished_requests": 97782,
    "scheduler_time": 204.7554258734518
}
#Debug simulation 
Total elapsed time: 228.89231113204733. Arrivals time: 0.6884368881583214 Scheduler time: 227.77113317511976 Scheduler overhead time: 0.17503317072987556 Adapter cache time: 0.03256548894569278 Engine time: 0.1722385692410171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86768647 . Total output tokens: 76278364
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 227.86229697195813,
    "estimated_duration": 3600.074141559194,
    "input_throughput": 6783.515016561071,
    "output_throughput": 5863.477853502679,
    "total_throughput": 12646.99287006375,
    "itl": 68.46124452431076,
    "ttft": 1136416.1290668475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0712102631758886,
    "arrivals": 129375,
    "finished_requests": 98001,
    "scheduler_time": 201.7443360121613
}
#Debug simulation 
Total elapsed time: 227.86244001006708. Arrivals time: 0.6459725671447814 Scheduler time: 226.78589618764818 Scheduler overhead time: 0.1730009694583714 Adapter cache time: 0.03269225871190429 Engine time: 0.17104159761220217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86768647 . Total output tokens: 76278364
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 225.81329482235014,
    "estimated_duration": 3600.0924307141795,
    "input_throughput": 6761.440843109442,
    "output_throughput": 5847.286814195487,
    "total_throughput": 12608.727657304928,
    "itl": 67.85698993458986,
    "ttft": 1140395.8857583285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2682283975696202,
    "arrivals": 129375,
    "finished_requests": 97706,
    "scheduler_time": 202.2720749017332
}
#Debug simulation 
Total elapsed time: 225.81348819937557. Arrivals time: 0.6758703961968422 Scheduler time: 224.6916862600483 Scheduler overhead time: 0.18151993304491043 Adapter cache time: 0.03220054227858782 Engine time: 0.17711078794673085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86768647 . Total output tokens: 76278364
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 221.04408157011494,
    "estimated_duration": 3600.0538027831867,
    "input_throughput": 6727.985559903233,
    "output_throughput": 5823.292414072448,
    "total_throughput": 12551.27797397568,
    "itl": 66.88764251351427,
    "ttft": 1154076.3393777155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4059885308053366,
    "arrivals": 129375,
    "finished_requests": 97232,
    "scheduler_time": 203.19087500472943
}
#Debug simulation 
Total elapsed time: 221.04421203304082. Arrivals time: 0.655582694336772 Scheduler time: 219.95887305028737 Scheduler overhead time: 0.17172596976161003 Adapter cache time: 0.03176363417878747 Engine time: 0.17375624598935246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86768647 . Total output tokens: 76278364
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 225.57658431772143,
    "estimated_duration": 3600.0888796816025,
    "input_throughput": 6759.0453495003885,
    "output_throughput": 5849.435306681219,
    "total_throughput": 12608.480656181608,
    "itl": 67.93426534272479,
    "ttft": 1143927.2742455176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2593229324370612,
    "arrivals": 129375,
    "finished_requests": 97732,
    "scheduler_time": 202.19806722337682
}
#Debug simulation 
Total elapsed time: 225.57672250363976. Arrivals time: 0.6676325877197087 Scheduler time: 224.46786285890266 Scheduler overhead time: 0.17754734959453344 Adapter cache time: 0.033243277575820684 Engine time: 0.17591433692723513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86768647 . Total output tokens: 76278364
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 221.57694593304768,
    "estimated_duration": 3600.0443239762044,
    "input_throughput": 6730.162692341381,
    "output_throughput": 5825.869659525511,
    "total_throughput": 12556.032351866892,
    "itl": 66.9342336391979,
    "ttft": 1152471.396796664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4322849296638769,
    "arrivals": 129375,
    "finished_requests": 97301,
    "scheduler_time": 203.14417346665817
}
#Debug simulation 
Total elapsed time: 221.5770883159712. Arrivals time: 0.6575924204662442 Scheduler time: 220.48029657825828 Scheduler overhead time: 0.17738058231770992 Adapter cache time: 0.0324469143524766 Engine time: 0.17437259946018457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86768647 . Total output tokens: 76278364
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 167.09815972624347,
    "estimated_duration": 3600.099148408006,
    "input_throughput": 6761.605443773524,
    "output_throughput": 5851.0216334777315,
    "total_throughput": 12612.627077251254,
    "itl": 68.02997525257791,
    "ttft": 1143401.2491322956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1682575321244046,
    "arrivals": 129375,
    "finished_requests": 97766,
    "scheduler_time": 202.11308766976785
}
#Debug simulation 
Total elapsed time: 167.0983734112233. Arrivals time: 0.46326801739633083 Scheduler time: 166.33809033036232 Scheduler overhead time: 0.1192312971688807 Adapter cache time: 0.020402309019118547 Engine time: 0.11306022806093097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86768647 . Total output tokens: 76278364
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 222.88237191597,
    "estimated_duration": 3600.0308902730885,
    "input_throughput": 6730.187806294646,
    "output_throughput": 5825.891399062139,
    "total_throughput": 12556.079205356786,
    "itl": 66.93395693999057,
    "ttft": 1152469.2860418323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4192365475371505,
    "arrivals": 129375,
    "finished_requests": 97301,
    "scheduler_time": 203.14388625569632
}
#Debug simulation 
Total elapsed time: 222.8826251118444. Arrivals time: 0.6587934130802751 Scheduler time: 221.78338806517422 Scheduler overhead time: 0.17588662821799517 Adapter cache time: 0.03240008046850562 Engine time: 0.1764659732580185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86455484 . Total output tokens: 75990362
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 231.2890373840928,
    "estimated_duration": 3600.09738835148,
    "input_throughput": 6704.35668715403,
    "output_throughput": 5870.763959993335,
    "total_throughput": 12575.120647147365,
    "itl": 68.53814403480939,
    "ttft": 1146453.1447152463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0579854451119888,
    "arrivals": 128882,
    "finished_requests": 97642,
    "scheduler_time": 201.09669312646693
}
#Debug simulation 
Total elapsed time: 231.28917068103328. Arrivals time: 0.665803465526551 Scheduler time: 230.1844566152431 Scheduler overhead time: 0.17855379357933998 Adapter cache time: 0.03274854086339474 Engine time: 0.17428873805329204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86455484 . Total output tokens: 75990362
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 230.56497692409903,
    "estimated_duration": 3600.0144064213987,
    "input_throughput": 6683.572142678684,
    "output_throughput": 5856.587952090556,
    "total_throughput": 12540.16009476924,
    "itl": 68.01069799103936,
    "ttft": 1151673.705006167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 174,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.271835876563565,
    "arrivals": 128882,
    "finished_requests": 97366,
    "scheduler_time": 201.5527142087265
}
#Debug simulation 
Total elapsed time: 230.56510797236115. Arrivals time: 0.6657230504788458 Scheduler time: 229.4581940905191 Scheduler overhead time: 0.17846035957336426 Adapter cache time: 0.03210645169019699 Engine time: 0.1756989797577262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86455484 . Total output tokens: 75990362
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 230.17195223923773,
    "estimated_duration": 3600.0622492880025,
    "input_throughput": 6657.252386327472,
    "output_throughput": 5836.843516846359,
    "total_throughput": 12494.095903173831,
    "itl": 67.13948591334166,
    "ttft": 1157368.0462692005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3384302896261235,
    "arrivals": 128882,
    "finished_requests": 96978,
    "scheduler_time": 202.3770421622875
}
#Debug simulation 
Total elapsed time: 230.1721317009069. Arrivals time: 0.6554426900111139 Scheduler time: 229.08407162455842 Scheduler overhead time: 0.17363805789500475 Adapter cache time: 0.03017806028947234 Engine time: 0.1745633934624493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86455484 . Total output tokens: 75990362
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 228.89539064886048,
    "estimated_duration": 3600.022431244932,
    "input_throughput": 6688.543324346238,
    "output_throughput": 5859.505712220052,
    "total_throughput": 12548.04903656629,
    "itl": 68.10540217860041,
    "ttft": 1152112.1891063638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2076943809865033,
    "arrivals": 128882,
    "finished_requests": 97449,
    "scheduler_time": 201.46797156527074
}
#Debug simulation 
Total elapsed time: 228.89553033187985. Arrivals time: 0.6754222316667438 Scheduler time: 227.78343593003228 Scheduler overhead time: 0.17779261013492942 Adapter cache time: 0.03221068624407053 Engine time: 0.17251750361174345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86455484 . Total output tokens: 75990362
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 226.4012345680967,
    "estimated_duration": 3600.049649368081,
    "input_throughput": 6657.275686241399,
    "output_throughput": 5836.863945387093,
    "total_throughput": 12494.139631628492,
    "itl": 67.13920129412928,
    "ttft": 1157366.1464474807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3251747903227828,
    "arrivals": 128882,
    "finished_requests": 96978,
    "scheduler_time": 202.37698982312736
}
#Debug simulation 
Total elapsed time: 226.401363782119. Arrivals time: 0.6704613184556365 Scheduler time: 225.2912852577865 Scheduler overhead time: 0.1784505140967667 Adapter cache time: 0.032506420742720366 Engine time: 0.1750427703373134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86455484 . Total output tokens: 75990362
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 233.34158089198172,
    "estimated_duration": 3600.0125596586117,
    "input_throughput": 6691.809431432791,
    "output_throughput": 5862.172325865796,
    "total_throughput": 12553.981757298587,
    "itl": 68.14915575365238,
    "ttft": 1145689.1526606702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0469630342535652,
    "arrivals": 128882,
    "finished_requests": 97472,
    "scheduler_time": 201.4409081702358
}
#Debug simulation 
Total elapsed time: 233.34169905539602. Arrivals time: 0.6798571427352726 Scheduler time: 232.22260856395587 Scheduler overhead time: 0.17654662672430277 Adapter cache time: 0.032082696445286274 Engine time: 0.17440306022763252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86455484 . Total output tokens: 75990362
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 227.1260013221763,
    "estimated_duration": 3600.0810057135814,
    "input_throughput": 6659.908752594197,
    "output_throughput": 5836.531446557036,
    "total_throughput": 12496.440199151233,
    "itl": 67.16369110112333,
    "ttft": 1156161.7775280748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3435503640025876,
    "arrivals": 128882,
    "finished_requests": 97038,
    "scheduler_time": 202.36064605216887
}
#Debug simulation 
Total elapsed time: 227.12612752011046. Arrivals time: 0.6601908160373569 Scheduler time: 226.02710808254778 Scheduler overhead time: 0.17670977395027876 Adapter cache time: 0.03186564426869154 Engine time: 0.1749864793382585 
