INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 8640, 8640, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 34560, 34560, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 656640 . Total input tokens: 146660716 . Total output tokens: 128957705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 65.26080511277542,
    "estimated_duration": 3600.052526683824,
    "input_throughput": 7896.048401878371,
    "output_throughput": 6886.901459417921,
    "total_throughput": 14782.94986129629,
    "itl": 88.58687269822079,
    "ttft": 1340751.084637778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 48,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3173956335335968,
    "arrivals": 218245,
    "finished_requests": 115122,
    "scheduler_time": 170.00282280575775
}
#Debug simulation 
Total elapsed time: 65.26100892899558. Arrivals time: 0.43096676375716925 Scheduler time: 64.63648251071572 Scheduler overhead time: 0.07410100847482681 Adapter cache time: 0.01311240578070283 Engine time: 0.07542984699830413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 8640, 8640, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 34560, 34560, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 656640 . Total input tokens: 146660716 . Total output tokens: 128957705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 65.92190144630149,
    "estimated_duration": 3600.016652446187,
    "input_throughput": 7861.119192537688,
    "output_throughput": 6858.467997147432,
    "total_throughput": 14719.587189685119,
    "itl": 87.43027626180057,
    "ttft": 1343721.9433499326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 47,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3444673553528264,
    "arrivals": 218245,
    "finished_requests": 114642,
    "scheduler_time": 170.40465811308962
}
#Debug simulation 
Total elapsed time: 65.92209278931841. Arrivals time: 0.4461059817112982 Scheduler time: 65.27858710940927 Scheduler overhead time: 0.076752420514822 Adapter cache time: 0.01338263088837266 Engine time: 0.07526505785062909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 8640, 8640, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 34560, 34560, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 656640 . Total input tokens: 146660716 . Total output tokens: 128957705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 66.65835272567347,
    "estimated_duration": 3600.019340448695,
    "input_throughput": 7775.55433813618,
    "output_throughput": 6790.249631534826,
    "total_throughput": 14565.803969671008,
    "itl": 85.13468348564116,
    "ttft": 1351748.200965232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34586727926973254,
    "arrivals": 218245,
    "finished_requests": 113426,
    "scheduler_time": 171.39836958419346
}
#Debug simulation 
Total elapsed time: 66.65852681780234. Arrivals time: 0.4528252468444407 Scheduler time: 66.00256166234612 Scheduler overhead time: 0.0788441514596343 Adapter cache time: 0.013767654541879892 Engine time: 0.07791393855586648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 8640, 8640, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 34560, 34560, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 656640 . Total input tokens: 146660716 . Total output tokens: 128957705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 65.2413430409506,
    "estimated_duration": 3600.0931663265233,
    "input_throughput": 7866.565305835583,
    "output_throughput": 6862.643231316639,
    "total_throughput": 14729.208537152223,
    "itl": 87.44859290384757,
    "ttft": 1344141.352739681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 48,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3300279616937043,
    "arrivals": 218245,
    "finished_requests": 114712,
    "scheduler_time": 170.48042197644241
}
#Debug simulation 
Total elapsed time: 65.2414977052249. Arrivals time: 0.4366988209076226 Scheduler time: 64.60930976783857 Scheduler overhead time: 0.07585335429757833 Adapter cache time: 0.012920310720801353 Engine time: 0.07531466009095311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 8640, 8640, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 34560, 34560, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 656640 . Total input tokens: 146660716 . Total output tokens: 128957705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 66.57913749618456,
    "estimated_duration": 3600.029800237973,
    "input_throughput": 7772.780658135187,
    "output_throughput": 6788.075198262142,
    "total_throughput": 14560.85585639733,
    "itl": 85.06002489600768,
    "ttft": 1351913.5668857857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.342760521620512,
    "arrivals": 218245,
    "finished_requests": 113391,
    "scheduler_time": 171.43544637816925
}
#Debug simulation 
Total elapsed time: 66.57930440502241. Arrivals time: 0.44898707140237093 Scheduler time: 65.92939201137051 Scheduler overhead time: 0.0776988104917109 Adapter cache time: 0.013592418283224106 Engine time: 0.07740550069138408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 8640, 8640, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 34560, 34560, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 656640 . Total input tokens: 146660716 . Total output tokens: 128957705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 64.71615729108453,
    "estimated_duration": 3600.064395111971,
    "input_throughput": 7858.54526336051,
    "output_throughput": 6856.730405577161,
    "total_throughput": 14715.275668937671,
    "itl": 87.4528515761341,
    "ttft": 1344933.7211261145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 48,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30642820514738567,
    "arrivals": 218245,
    "finished_requests": 114616,
    "scheduler_time": 170.40152760815286
}
#Debug simulation 
Total elapsed time: 64.71631428273395. Arrivals time: 0.4225198649801314 Scheduler time: 64.09866617806256 Scheduler overhead time: 0.07536568818613887 Adapter cache time: 0.01290270034223795 Engine time: 0.0752916750498116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 8640, 8640, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 34560, 34560, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 656640 . Total input tokens: 146660716 . Total output tokens: 128957705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 66.30942167015746,
    "estimated_duration": 3600.076071844083,
    "input_throughput": 7776.9578867978325,
    "output_throughput": 6790.170683109583,
    "total_throughput": 14567.128569907416,
    "itl": 85.14968879782332,
    "ttft": 1351999.176839131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3394466467946768,
    "arrivals": 218245,
    "finished_requests": 113422,
    "scheduler_time": 171.39711855143318
}
#Debug simulation 
Total elapsed time: 66.30969225522131. Arrivals time: 0.41604590183123946 Scheduler time: 65.69502625241876 Scheduler overhead time: 0.07678955141454935 Adapter cache time: 0.013566362671554089 Engine time: 0.07615507068112493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 4320, 4320, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 613440 . Total input tokens: 136961208 . Total output tokens: 120482857
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 68.79352816520259,
    "estimated_duration": 3600.08890620243,
    "input_throughput": 7888.542127687978,
    "output_throughput": 6894.29140409245,
    "total_throughput": 14782.833531780427,
    "itl": 88.27887194288881,
    "ttft": 1293483.038699023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3438452696613965,
    "arrivals": 203816,
    "finished_requests": 114779,
    "scheduler_time": 168.4778469818259
}
#Debug simulation 
Total elapsed time: 68.79369192523882. Arrivals time: 0.4077669084072113 Scheduler time: 68.19064225954935 Scheduler overhead time: 0.07578158797696233 Adapter cache time: 0.012923740316182375 Engine time: 0.07477893028408289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 4320, 4320, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 613440 . Total input tokens: 136961208 . Total output tokens: 120482857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 68.28965539112687,
    "estimated_duration": 3600.0616183877605,
    "input_throughput": 7849.033987550059,
    "output_throughput": 6861.424502801221,
    "total_throughput": 14710.45849035128,
    "itl": 87.13639634484335,
    "ttft": 1299647.130590247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38055162297561757,
    "arrivals": 203816,
    "finished_requests": 114222,
    "scheduler_time": 168.94254298781735
}
#Debug simulation 
Total elapsed time: 68.2898311978206. Arrivals time: 0.408403932582587 Scheduler time: 67.68378977151588 Scheduler overhead time: 0.0760410176590085 Adapter cache time: 0.013427200727164745 Engine time: 0.0765958852134645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 4320, 4320, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 613440 . Total input tokens: 136961208 . Total output tokens: 120482857
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 66.93884633807465,
    "estimated_duration": 3600.0866283211235,
    "input_throughput": 7789.507557788303,
    "output_throughput": 6808.374778312049,
    "total_throughput": 14597.88233610035,
    "itl": 84.94233304503885,
    "ttft": 1308809.5198453206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39090610672254117,
    "arrivals": 203816,
    "finished_requests": 113364,
    "scheduler_time": 169.73531236395579
}
#Debug simulation 
Total elapsed time: 66.93902149889618. Arrivals time: 0.4189369613304734 Scheduler time: 66.32000638404861 Scheduler overhead time: 0.07764836167916656 Adapter cache time: 0.013384237419813871 Engine time: 0.07674062717705965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 4320, 4320, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 613440 . Total input tokens: 136961208 . Total output tokens: 120482857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 68.1217468669638,
    "estimated_duration": 3600.0420096506245,
    "input_throughput": 7849.927285360365,
    "output_throughput": 6862.355198571013,
    "total_throughput": 14712.282483931378,
    "itl": 87.13178887193763,
    "ttft": 1299344.0620150573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3555636454559864,
    "arrivals": 203816,
    "finished_requests": 114236,
    "scheduler_time": 168.9499474164145
}
#Debug simulation 
Total elapsed time: 68.12191596580669. Arrivals time: 0.42059726314619184 Scheduler time: 67.5043804673478 Scheduler overhead time: 0.07613081112504005 Adapter cache time: 0.012923084665089846 Engine time: 0.07595826033502817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 4320, 4320, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 613440 . Total input tokens: 136961208 . Total output tokens: 120482857
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 66.66517574712634,
    "estimated_duration": 3600.0361450008395,
    "input_throughput": 7789.450125088525,
    "output_throughput": 6808.044423119059,
    "total_throughput": 14597.494548207584,
    "itl": 84.94336296229659,
    "ttft": 1308848.1361613204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38738511472009124,
    "arrivals": 203816,
    "finished_requests": 113359,
    "scheduler_time": 169.73380261991423
}
#Debug simulation 
Total elapsed time: 66.66534436773509. Arrivals time: 0.4222911410033703 Scheduler time: 66.04185705026612 Scheduler overhead time: 0.07752757053822279 Adapter cache time: 0.013247730676084757 Engine time: 0.07810298027470708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 4320, 4320, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 613440 . Total input tokens: 136961208 . Total output tokens: 120482857
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 68.72751021618024,
    "estimated_duration": 3600.0364538843764,
    "input_throughput": 7849.761901579794,
    "output_throughput": 6862.218290413299,
    "total_throughput": 14711.980191993092,
    "itl": 87.13252473704121,
    "ttft": 1299396.942382768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3319638889096679,
    "arrivals": 203816,
    "finished_requests": 114233,
    "scheduler_time": 168.9507455543144
}
#Debug simulation 
Total elapsed time: 68.7276773978956. Arrivals time: 0.4182459735311568 Scheduler time: 68.11344587430358 Scheduler overhead time: 0.07547072786837816 Adapter cache time: 0.012939055915921926 Engine time: 0.07573812827467918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 4320, 4320, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 613440 . Total input tokens: 136961208 . Total output tokens: 120482857
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 67.51296034594998,
    "estimated_duration": 3600.034468709173,
    "input_throughput": 7789.5379179674765,
    "output_throughput": 6807.988149287898,
    "total_throughput": 14597.526067255374,
    "itl": 84.94206016085356,
    "ttft": 1308882.9322240714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 52,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3836570055410266,
    "arrivals": 203816,
    "finished_requests": 113358,
    "scheduler_time": 169.7327418033828
}
#Debug simulation 
Total elapsed time: 67.51324180234224. Arrivals time: 0.4142916910350323 Scheduler time: 66.89832867868245 Scheduler overhead time: 0.07752462942153215 Adapter cache time: 0.01316311489790678 Engine time: 0.07725662412121892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 1080, 1080, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 581040 . Total input tokens: 129766786 . Total output tokens: 114164316
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 98.86777964700013,
    "estimated_duration": 3600.048523236741,
    "input_throughput": 7984.685154786648,
    "output_throughput": 6913.32515085751,
    "total_throughput": 14898.010305644159,
    "itl": 88.54856588654827,
    "ttft": 1229052.2177016519,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26449636127799736,
    "arrivals": 193289,
    "finished_requests": 116027,
    "scheduler_time": 166.03227692255507
}
#Debug simulation 
Total elapsed time: 98.867937436793. Arrivals time: 0.44239392317831516 Scheduler time: 98.2159084258601 Scheduler overhead time: 0.08115993300452828 Adapter cache time: 0.014164887834340334 Engine time: 0.08202172629535198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 1080, 1080, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 581040 . Total input tokens: 129766786 . Total output tokens: 114164316
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 97.42235867399722,
    "estimated_duration": 3600.096698860071,
    "input_throughput": 7949.300919906302,
    "output_throughput": 6880.155471335781,
    "total_throughput": 14829.456391242084,
    "itl": 87.42367470278738,
    "ttft": 1235666.3991617686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27229882010724393,
    "arrivals": 193289,
    "finished_requests": 115481,
    "scheduler_time": 166.50067026275553
}
#Debug simulation 
Total elapsed time: 97.4225041451864. Arrivals time: 0.43330081226304173 Scheduler time: 96.7766535365954 Scheduler overhead time: 0.08260784950107336 Adapter cache time: 0.014112722594290972 Engine time: 0.08258187444880605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 1080, 1080, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 581040 . Total input tokens: 129766786 . Total output tokens: 114164316
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 96.08962029311806,
    "estimated_duration": 3600.034918291887,
    "input_throughput": 7869.513113901133,
    "output_throughput": 6811.299766957391,
    "total_throughput": 14680.812880858524,
    "itl": 85.05710179080462,
    "ttft": 1249808.5325637725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2413462840113789,
    "arrivals": 193289,
    "finished_requests": 114310,
    "scheduler_time": 167.53826816300744
}
#Debug simulation 
Total elapsed time: 96.08977059088647. Arrivals time: 0.4357963213697076 Scheduler time: 95.43905911175534 Scheduler overhead time: 0.08382884133607149 Adapter cache time: 0.014217840041965246 Engine time: 0.0830045840702951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 1080, 1080, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 581040 . Total input tokens: 129766786 . Total output tokens: 114164316
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 96.96017938386649,
    "estimated_duration": 3600.0291149231557,
    "input_throughput": 7945.90407100377,
    "output_throughput": 6878.798812305882,
    "total_throughput": 14824.702883309652,
    "itl": 87.37753928464839,
    "ttft": 1235650.6492373964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2542519474541768,
    "arrivals": 193289,
    "finished_requests": 115448,
    "scheduler_time": 166.5206349869644
}
#Debug simulation 
Total elapsed time: 96.96033228188753. Arrivals time: 0.41520278388634324 Scheduler time: 96.33806525636464 Scheduler overhead time: 0.08087330311536789 Adapter cache time: 0.013265580404549837 Engine time: 0.08060823893174529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 1080, 1080, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 581040 . Total input tokens: 129766786 . Total output tokens: 114164316
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 94.14089608285576,
    "estimated_duration": 3600.075120471688,
    "input_throughput": 7869.078575307131,
    "output_throughput": 6810.8067691619235,
    "total_throughput": 14679.885344469054,
    "itl": 85.04690708729136,
    "ttft": 1249853.9539843234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861717,
    "arrivals": 193289,
    "finished_requests": 114306,
    "scheduler_time": 167.54632071384498
}
#Debug simulation 
Total elapsed time: 94.14104706095532. Arrivals time: 0.3901533046737313 Scheduler time: 93.54761314066127 Scheduler overhead time: 0.07917752442881465 Adapter cache time: 0.01323798717930913 Engine time: 0.07873766543343663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 1080, 1080, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 581040 . Total input tokens: 129766786 . Total output tokens: 114164316
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 96.39751555398107,
    "estimated_duration": 3600.0445060196116,
    "input_throughput": 7944.399007339438,
    "output_throughput": 6878.375519690062,
    "total_throughput": 14822.7745270295,
    "itl": 87.35175050056759,
    "ttft": 1235564.3183990233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23620507480110967,
    "arrivals": 193289,
    "finished_requests": 115434,
    "scheduler_time": 166.53473778213862
}
#Debug simulation 
Total elapsed time: 96.39766870904714. Arrivals time: 0.4090644409880042 Scheduler time: 95.78652209648862 Scheduler overhead time: 0.07891089143231511 Adapter cache time: 0.013255447149276733 Engine time: 0.07814762275665998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 1080, 1080, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 581040 . Total input tokens: 129766786 . Total output tokens: 114164316
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 95.57889947807416,
    "estimated_duration": 3600.0895748386706,
    "input_throughput": 7869.666409972482,
    "output_throughput": 6811.40885254192,
    "total_throughput": 14681.075262514403,
    "itl": 85.05844382651216,
    "ttft": 1249868.6702742823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585545,
    "arrivals": 193289,
    "finished_requests": 114315,
    "scheduler_time": 167.54188588207586
}
#Debug simulation 
Total elapsed time: 95.5791099620983. Arrivals time: 0.4097430305555463 Scheduler time: 94.9606287041679 Scheduler overhead time: 0.08011190081015229 Adapter cache time: 0.013137094676494598 Engine time: 0.08203258411958814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 540, 540, 17280, 540, 34560, 17280, 540, 540, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 34560, 34560, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 575640 . Total input tokens: 128559279 . Total output tokens: 113095279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 86.40089791081846,
    "estimated_duration": 3600.0317629658193,
    "input_throughput": 7984.336498275754,
    "output_throughput": 6914.73843538873,
    "total_throughput": 14899.074933664484,
    "itl": 88.54613394943584,
    "ttft": 1223447.9327275231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4562562232045453,
    "arrivals": 191429,
    "finished_requests": 115967,
    "scheduler_time": 166.06191980595824
}
#Debug simulation 
Total elapsed time: 86.40105779375881. Arrivals time: 0.4018575041554868 Scheduler time: 85.79807780124247 Scheduler overhead time: 0.07731568207964301 Adapter cache time: 0.013434856198728085 Engine time: 0.0784473866224289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 540, 540, 17280, 540, 34560, 17280, 540, 540, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 34560, 34560, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 575640 . Total input tokens: 128559279 . Total output tokens: 113095279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 85.58615083200857,
    "estimated_duration": 3600.035075637719,
    "input_throughput": 7942.374004490775,
    "output_throughput": 6879.912689631934,
    "total_throughput": 14822.28669412271,
    "itl": 87.27005207398648,
    "ttft": 1231340.481592803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5229432885674761,
    "arrivals": 191429,
    "finished_requests": 115332,
    "scheduler_time": 166.60936663351907
}
#Debug simulation 
Total elapsed time: 85.58631074801087. Arrivals time: 0.40323117794469 Scheduler time: 84.98048758832738 Scheduler overhead time: 0.07952779065817595 Adapter cache time: 0.013594766613095999 Engine time: 0.07726972410455346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 540, 540, 17280, 540, 34560, 17280, 540, 540, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 34560, 34560, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 575640 . Total input tokens: 128559279 . Total output tokens: 113095279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 84.96964148897678,
    "estimated_duration": 3600.007170862381,
    "input_throughput": 7865.913220727442,
    "output_throughput": 6813.666427815482,
    "total_throughput": 14679.579648542925,
    "itl": 85.06230050844819,
    "ttft": 1244481.2132674751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.56204942596145,
    "arrivals": 191429,
    "finished_requests": 114227,
    "scheduler_time": 167.5713401338457
}
#Debug simulation 
Total elapsed time: 84.96979260304943. Arrivals time: 0.39728707587346435 Scheduler time: 84.3677817103453 Scheduler overhead time: 0.0792192192748189 Adapter cache time: 0.014063108712434769 Engine time: 0.07854552892968059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 540, 540, 17280, 540, 34560, 17280, 540, 540, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 34560, 34560, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 575640 . Total input tokens: 128559279 . Total output tokens: 113095279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 85.87587316799909,
    "estimated_duration": 3600.0028284624295,
    "input_throughput": 7946.499034340565,
    "output_throughput": 6883.821535935948,
    "total_throughput": 14830.320570276514,
    "itl": 87.41235449962299,
    "ttft": 1230846.3237624366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49379064812790635,
    "arrivals": 191429,
    "finished_requests": 115400,
    "scheduler_time": 166.53624278475377
}
#Debug simulation 
Total elapsed time: 85.87602489022538. Arrivals time: 0.39912835182622075 Scheduler time: 85.27604541555047 Scheduler overhead time: 0.07840855559334159 Adapter cache time: 0.013222536072134972 Engine time: 0.0766056040301919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 540, 540, 17280, 540, 34560, 17280, 540, 540, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 34560, 34560, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 575640 . Total input tokens: 128559279 . Total output tokens: 113095279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 84.23138406174257,
    "estimated_duration": 3600.0248969975314,
    "input_throughput": 7862.635901103717,
    "output_throughput": 6811.056784760012,
    "total_throughput": 14673.692685863729,
    "itl": 84.99305470427515,
    "ttft": 1244735.644280873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5494167747022587,
    "arrivals": 191429,
    "finished_requests": 114182,
    "scheduler_time": 167.60855443499577
}
#Debug simulation 
Total elapsed time: 84.23153401864693. Arrivals time: 0.4010811601765454 Scheduler time: 83.62484938604757 Scheduler overhead time: 0.08000315399840474 Adapter cache time: 0.013974409084767103 Engine time: 0.07854689611122012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 540, 540, 17280, 540, 34560, 17280, 540, 540, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 34560, 34560, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 575640 . Total input tokens: 128559279 . Total output tokens: 113095279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 86.537255610805,
    "estimated_duration": 3600.020147019518,
    "input_throughput": 7954.682982457415,
    "output_throughput": 6890.420327379771,
    "total_throughput": 14845.103309837186,
    "itl": 87.38191842524608,
    "ttft": 1229871.7967539653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2745086004445329,
    "arrivals": 191429,
    "finished_requests": 115513,
    "scheduler_time": 166.66909481657137
}
#Debug simulation 
Total elapsed time: 86.53740926412866. Arrivals time: 0.4137346390634775 Scheduler time: 85.92389414831996 Scheduler overhead time: 0.0773576651699841 Adapter cache time: 0.013614749535918236 Engine time: 0.07658475963398814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 540, 540, 17280, 540, 34560, 17280, 540, 540, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 34560, 34560, 540, 34560, 34560, 34560, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 575640 . Total input tokens: 128559279 . Total output tokens: 113095279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 83.30590601498261,
    "estimated_duration": 3600.041211729277,
    "input_throughput": 7863.243039487949,
    "output_throughput": 6806.968186963855,
    "total_throughput": 14670.211226451805,
    "itl": 84.87510728837525,
    "ttft": 1245486.3388651274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5533505045436325,
    "arrivals": 191429,
    "finished_requests": 114138,
    "scheduler_time": 167.66781563595572
}
#Debug simulation 
Total elapsed time: 83.30606988631189. Arrivals time: 0.40111061930656433 Scheduler time: 82.7022248650901 Scheduler overhead time: 0.07858403539285064 Adapter cache time: 0.013682621531188488 Engine time: 0.07812296040356159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 270, 270, 17280, 270, 34560, 17280, 270, 270, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 34560, 34560, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 572940 . Total input tokens: 127961315 . Total output tokens: 112566747
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 102.70299019804224,
    "estimated_duration": 3600.0215735729994,
    "input_throughput": 7936.782437567973,
    "output_throughput": 6937.825368421655,
    "total_throughput": 14874.607805989628,
    "itl": 88.32322904506188,
    "ttft": 1225278.880626484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22482190708629776,
    "arrivals": 190474,
    "finished_requests": 115845,
    "scheduler_time": 166.32193257906027
}
#Debug simulation 
Total elapsed time: 102.70310573326424. Arrivals time: 0.4140634648501873 Scheduler time: 102.08414240460843 Scheduler overhead time: 0.0806273533962667 Adapter cache time: 0.014029468409717083 Engine time: 0.07813129900023341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 270, 270, 17280, 270, 34560, 17280, 270, 270, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 34560, 34560, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 572940 . Total input tokens: 127961315 . Total output tokens: 112566747
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 102.17810403183103,
    "estimated_duration": 3600.0025379594586,
    "input_throughput": 7883.987219655573,
    "output_throughput": 6894.424583952755,
    "total_throughput": 14778.411803608327,
    "itl": 87.27734130388114,
    "ttft": 1230463.4693279904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49324294188525536,
    "arrivals": 190474,
    "finished_requests": 115080,
    "scheduler_time": 166.55540645909934
}
#Debug simulation 
Total elapsed time: 102.1782132438384. Arrivals time: 0.4166138684377074 Scheduler time: 101.55687128659338 Scheduler overhead time: 0.07976934174075723 Adapter cache time: 0.014246858190745115 Engine time: 0.07835039729252458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 270, 270, 17280, 270, 34560, 17280, 270, 270, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 34560, 34560, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 572940 . Total input tokens: 127961315 . Total output tokens: 112566747
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 101.43811456207186,
    "estimated_duration": 3600.0218391367325,
    "input_throughput": 7817.357298795851,
    "output_throughput": 6833.627433187808,
    "total_throughput": 14650.984731983659,
    "itl": 84.84962465575717,
    "ttft": 1241963.2347321978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25578962436411534,
    "arrivals": 190474,
    "finished_requests": 114063,
    "scheduler_time": 167.82670748328528
}
#Debug simulation 
Total elapsed time: 101.43822672823444. Arrivals time: 0.4186863242648542 Scheduler time: 100.80764169432223 Scheduler overhead time: 0.08317692205309868 Adapter cache time: 0.014363500289618969 Engine time: 0.08114885864779353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 270, 270, 17280, 270, 34560, 17280, 270, 270, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 34560, 34560, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 572940 . Total input tokens: 127961315 . Total output tokens: 112566747
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 102.37888099718839,
    "estimated_duration": 3600.020917083949,
    "input_throughput": 7885.568071364074,
    "output_throughput": 6895.171325867371,
    "total_throughput": 14780.739397231444,
    "itl": 87.30364327158023,
    "ttft": 1230300.5057006294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4690860014129434,
    "arrivals": 190474,
    "finished_requests": 115101,
    "scheduler_time": 166.54236637788287
}
#Debug simulation 
Total elapsed time: 102.37899367138743. Arrivals time: 0.4135814602486789 Scheduler time: 101.75737272622064 Scheduler overhead time: 0.08156625786796212 Adapter cache time: 0.014132981188595295 Engine time: 0.08014473412185907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 270, 270, 17280, 270, 34560, 17280, 270, 270, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 34560, 34560, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 572940 . Total input tokens: 127961315 . Total output tokens: 112566747
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 100.8112603072077,
    "estimated_duration": 3600.018270678846,
    "input_throughput": 7815.657834062707,
    "output_throughput": 6832.499212666991,
    "total_throughput": 14648.157046729699,
    "itl": 84.81556280052386,
    "ttft": 1241722.6554251576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2533042182447389,
    "arrivals": 190474,
    "finished_requests": 114041,
    "scheduler_time": 167.84886289511317
}
#Debug simulation 
Total elapsed time: 100.81137124681845. Arrivals time: 0.42008921038359404 Scheduler time: 100.18122419482097 Scheduler overhead time: 0.08288953267037868 Adapter cache time: 0.014059037435799837 Engine time: 0.07999032689258456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 270, 270, 17280, 270, 34560, 17280, 270, 270, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 34560, 34560, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 572940 . Total input tokens: 127961315 . Total output tokens: 112566747
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 102.70998700289056,
    "estimated_duration": 3600.0622933896293,
    "input_throughput": 7896.673358180478,
    "output_throughput": 6903.868870724024,
    "total_throughput": 14800.542228904504,
    "itl": 87.16693977626949,
    "ttft": 1230281.3816799654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2170533119793981,
    "arrivals": 190474,
    "finished_requests": 115253,
    "scheduler_time": 166.81276420323601
}
#Debug simulation 
Total elapsed time: 102.71010176697746. Arrivals time: 0.42882751673460007 Scheduler time: 102.07310840953141 Scheduler overhead time: 0.08183196699246764 Adapter cache time: 0.014126569498330355 Engine time: 0.07916789688169956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 270, 270, 17280, 270, 34560, 17280, 270, 270, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 34560, 34560, 270, 34560, 34560, 34560, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 572940 . Total input tokens: 127961315 . Total output tokens: 112566747
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 101.4372296538204,
    "estimated_duration": 3600.001709163437,
    "input_throughput": 7817.112399799251,
    "output_throughput": 6833.379255732784,
    "total_throughput": 14650.491655532034,
    "itl": 84.84877083699247,
    "ttft": 1241984.5006841256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2510259293019772,
    "arrivals": 190474,
    "finished_requests": 114060,
    "scheduler_time": 167.82609384704406
}
#Debug simulation 
Total elapsed time: 101.4373908797279. Arrivals time: 0.4210148514248431 Scheduler time: 100.80368909845129 Scheduler overhead time: 0.0847000116482377 Adapter cache time: 0.01439108932390809 Engine time: 0.08091229619458318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 135, 135, 17280, 135, 34560, 17280, 135, 135, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 34560, 34560, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 571590 . Total input tokens: 127634182 . Total output tokens: 112310632
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 86.89517941279337,
    "estimated_duration": 3600.042050058053,
    "input_throughput": 8001.148208681481,
    "output_throughput": 6961.209522426518,
    "total_throughput": 14962.357731107999,
    "itl": 87.76973838742954,
    "ttft": 1220708.2422892388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30417081546969693,
    "arrivals": 190041,
    "finished_requests": 116225,
    "scheduler_time": 167.06872721452226
}
#Debug simulation 
Total elapsed time: 86.89530682004988. Arrivals time: 0.40943952230736613 Scheduler time: 86.28513380885124 Scheduler overhead time: 0.07863247534260154 Adapter cache time: 0.013781722635030746 Engine time: 0.076631935313344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 135, 135, 17280, 135, 34560, 17280, 135, 135, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 34560, 34560, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 571590 . Total input tokens: 127634182 . Total output tokens: 112310632
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 86.20809220802039,
    "estimated_duration": 3600.0413306147316,
    "input_throughput": 7963.02330093107,
    "output_throughput": 6931.648197421916,
    "total_throughput": 14894.671498352986,
    "itl": 86.64566576415184,
    "ttft": 1227930.4187456656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33947165538556856,
    "arrivals": 190041,
    "finished_requests": 115707,
    "scheduler_time": 167.56291505192814
}
#Debug simulation 
Total elapsed time: 86.20820979680866. Arrivals time: 0.4070929712615907 Scheduler time: 85.59822155022994 Scheduler overhead time: 0.07906450098380446 Adapter cache time: 0.014075956773012877 Engine time: 0.07748704683035612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 135, 135, 17280, 135, 34560, 17280, 135, 135, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 34560, 34560, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 571590 . Total input tokens: 127634182 . Total output tokens: 112310632
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 84.69449855666608,
    "estimated_duration": 3600.027959489608,
    "input_throughput": 7886.9137460880775,
    "output_throughput": 6862.984476238016,
    "total_throughput": 14749.898222326094,
    "itl": 84.34489536573824,
    "ttft": 1241610.7566540998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3492848920589314,
    "arrivals": 190041,
    "finished_requests": 114566,
    "scheduler_time": 168.61281608931657
}
#Debug simulation 
Total elapsed time: 84.69461140502244. Arrivals time: 0.40327319595962763 Scheduler time: 84.08708978118375 Scheduler overhead time: 0.07942911610007286 Adapter cache time: 0.013671757187694311 Engine time: 0.07847841223701835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 135, 135, 17280, 135, 34560, 17280, 135, 135, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 34560, 34560, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 571590 . Total input tokens: 127634182 . Total output tokens: 112310632
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 86.07832227880135,
    "estimated_duration": 3600.0419441145923,
    "input_throughput": 7964.543315078477,
    "output_throughput": 6932.91339030175,
    "total_throughput": 14897.456705380228,
    "itl": 86.69095015636046,
    "ttft": 1227753.520599005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31726011981256297,
    "arrivals": 190041,
    "finished_requests": 115726,
    "scheduler_time": 167.538320632996
}
#Debug simulation 
Total elapsed time: 86.07844341406599. Arrivals time: 0.41102193016558886 Scheduler time: 85.46605589846149 Scheduler overhead time: 0.07867783308029175 Adapter cache time: 0.013756784610450268 Engine time: 0.07733773393556476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 135, 135, 17280, 135, 34560, 17280, 135, 135, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 34560, 34560, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 571590 . Total input tokens: 127634182 . Total output tokens: 112310632
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 84.56580952601507,
    "estimated_duration": 3600.0812333079343,
    "input_throughput": 7887.628406070227,
    "output_throughput": 6863.891506496562,
    "total_throughput": 14751.51991256679,
    "itl": 84.36775607744775,
    "ttft": 1241391.7401748614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34597101723309615,
    "arrivals": 190041,
    "finished_requests": 114580,
    "scheduler_time": 168.60312189075154
}
#Debug simulation 
Total elapsed time: 84.56592523725703. Arrivals time: 0.40374406427145004 Scheduler time: 83.95504290657118 Scheduler overhead time: 0.0809118296019733 Adapter cache time: 0.014051119796931744 Engine time: 0.07926734676584601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 135, 135, 17280, 135, 34560, 17280, 135, 135, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 34560, 34560, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 571590 . Total input tokens: 127634182 . Total output tokens: 112310632
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 86.10100008407608,
    "estimated_duration": 3600.0597203833327,
    "input_throughput": 7964.750095019771,
    "output_throughput": 6932.928045244311,
    "total_throughput": 14897.678140264083,
    "itl": 86.69087208787568,
    "ttft": 1227827.914281672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29366036326624456,
    "arrivals": 190041,
    "finished_requests": 115727,
    "scheduler_time": 167.53761013456173
}
#Debug simulation 
Total elapsed time: 86.10113001102582. Arrivals time: 0.4083643080666661 Scheduler time: 85.4899386158213 Scheduler overhead time: 0.07996378792449832 Adapter cache time: 0.013513337355107069 Engine time: 0.07719691889360547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 135, 135, 17280, 135, 34560, 17280, 135, 135, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 34560, 34560, 135, 34560, 34560, 34560, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 571590 . Total input tokens: 127634182 . Total output tokens: 112310632
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 85.10368350800127,
    "estimated_duration": 3600.083408226283,
    "input_throughput": 7887.312537014207,
    "output_throughput": 6863.635976749258,
    "total_throughput": 14750.948513763464,
    "itl": 84.36080770570395,
    "ttft": 1241415.3998414178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 46,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3424500252306462,
    "arrivals": 190041,
    "finished_requests": 114577,
    "scheduler_time": 168.6085750966299
}
#Debug simulation 
Total elapsed time: 85.10384583100677. Arrivals time: 0.40432677464559674 Scheduler time: 84.49120915168896 Scheduler overhead time: 0.08176963636651635 Adapter cache time: 0.014085704926401377 Engine time: 0.07987935841083527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 66, 66, 17280, 66, 34560, 17280, 66, 66, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 34560, 34560, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570900 . Total input tokens: 127485006 . Total output tokens: 112169169
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 92.03201491571963,
    "estimated_duration": 3600.032724615608,
    "input_throughput": 8078.9546164765725,
    "output_throughput": 6976.934911801914,
    "total_throughput": 15055.889528278487,
    "itl": 87.34963287665856,
    "ttft": 1215956.8537828927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3636824967572463,
    "arrivals": 189807,
    "finished_requests": 116911,
    "scheduler_time": 167.5635283527787
}
#Debug simulation 
Total elapsed time: 92.03214129805565. Arrivals time: 0.41772616608068347 Scheduler time: 91.40986592229456 Scheduler overhead time: 0.08060826687142253 Adapter cache time: 0.013858533930033445 Engine time: 0.07799312612041831 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 66, 66, 17280, 66, 34560, 17280, 66, 66, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 34560, 34560, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570900 . Total input tokens: 127485006 . Total output tokens: 112169169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 106.78145625302568,
    "estimated_duration": 3600.080210127503,
    "input_throughput": 8052.655026531551,
    "output_throughput": 6953.155079595699,
    "total_throughput": 15005.81010612725,
    "itl": 86.1901745545324,
    "ttft": 1225723.0935508008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2736870410805568,
    "arrivals": 189807,
    "finished_requests": 116570,
    "scheduler_time": 168.07964639357425
}
#Debug simulation 
Total elapsed time: 106.78157630003989. Arrivals time: 0.42851767921820283 Scheduler time: 106.13955376856029 Scheduler overhead time: 0.08438131213188171 Adapter cache time: 0.01487481128424406 Engine time: 0.08137544710189104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 66, 66, 17280, 66, 34560, 17280, 66, 66, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 34560, 34560, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570900 . Total input tokens: 127485006 . Total output tokens: 112169169
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 105.84435091307387,
    "estimated_duration": 3600.0004809953766,
    "input_throughput": 7963.555602657381,
    "output_throughput": 6879.161858654153,
    "total_throughput": 14842.717461311535,
    "itl": 83.97973067399583,
    "ttft": 1237836.665545291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2817266508797184,
    "arrivals": 189807,
    "finished_requests": 115312,
    "scheduler_time": 168.99954121840716
}
#Debug simulation 
Total elapsed time: 105.84446380380541. Arrivals time: 0.43005800852552056 Scheduler time: 105.19883466744795 Scheduler overhead time: 0.08493906445801258 Adapter cache time: 0.014363707043230534 Engine time: 0.08274550456553698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 66, 66, 17280, 66, 34560, 17280, 66, 66, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 34560, 34560, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570900 . Total input tokens: 127485006 . Total output tokens: 112169169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 86.43467583600432,
    "estimated_duration": 3600.019081046556,
    "input_throughput": 8054.575919517192,
    "output_throughput": 6953.462866847587,
    "total_throughput": 15008.03878636478,
    "itl": 85.99144958956678,
    "ttft": 1223357.1725297023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 51,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3519561664620414,
    "arrivals": 189807,
    "finished_requests": 116532,
    "scheduler_time": 168.50048104172728
}
#Debug simulation 
Total elapsed time: 86.4347976166755. Arrivals time: 0.4113616654649377 Scheduler time: 85.81825594743714 Scheduler overhead time: 0.08117190981283784 Adapter cache time: 0.013442725874483585 Engine time: 0.07833301881328225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 66, 66, 17280, 66, 34560, 17280, 66, 66, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 34560, 34560, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570900 . Total input tokens: 127485006 . Total output tokens: 112169169
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 105.30570994503796,
    "estimated_duration": 3600.007077796143,
    "input_throughput": 7963.759342815233,
    "output_throughput": 6879.583974363078,
    "total_throughput": 14843.34331717831,
    "itl": 83.98209426039013,
    "ttft": 1237791.6241038158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27861989323049785,
    "arrivals": 189807,
    "finished_requests": 115316,
    "scheduler_time": 168.99695419072492
}
#Debug simulation 
Total elapsed time: 105.30582609213889. Arrivals time: 0.42947016144171357 Scheduler time: 104.65770359104499 Scheduler overhead time: 0.08762118080630898 Adapter cache time: 0.014923141803592443 Engine time: 0.08228865265846252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 66, 66, 17280, 66, 34560, 17280, 66, 66, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 34560, 34560, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570900 . Total input tokens: 127485006 . Total output tokens: 112169169
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 91.34488164121285,
    "estimated_duration": 3600.010154986334,
    "input_throughput": 8039.72787685369,
    "output_throughput": 6945.777073813538,
    "total_throughput": 14985.504950667228,
    "itl": 86.19476785692137,
    "ttft": 1222797.1633117443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 55,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35111565173137954,
    "arrivals": 189807,
    "finished_requests": 116354,
    "scheduler_time": 168.07756432831076
}
#Debug simulation 
Total elapsed time: 91.34502255218104. Arrivals time: 0.41409260919317603 Scheduler time: 90.72434464283288 Scheduler overhead time: 0.08093080017715693 Adapter cache time: 0.013835918623954058 Engine time: 0.07919169496744871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 66, 66, 17280, 66, 34560, 17280, 66, 66, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 34560, 34560, 66, 34560, 34560, 34560, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570900 . Total input tokens: 127485006 . Total output tokens: 112169169
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 105.93108086520806,
    "estimated_duration": 3600.082958158636,
    "input_throughput": 7962.302072800428,
    "output_throughput": 6878.530102724599,
    "total_throughput": 14840.832175525027,
    "itl": 83.94746393444764,
    "ttft": 1237722.1422113392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27613448711112143,
    "arrivals": 189807,
    "finished_requests": 115299,
    "scheduler_time": 169.02679339185968
}
#Debug simulation 
Total elapsed time: 105.93124773632735. Arrivals time: 0.4358525173738599 Scheduler time: 105.27686699200422 Scheduler overhead time: 0.08624629722908139 Adapter cache time: 0.014879304450005293 Engine time: 0.08349591819569468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 33, 33, 17280, 33, 34560, 17280, 33, 33, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 34560, 34560, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570570 . Total input tokens: 127408721 . Total output tokens: 112106331
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 105.11247957870364,
    "estimated_duration": 3600.0606586611702,
    "input_throughput": 8004.468461016604,
    "output_throughput": 6991.186645549469,
    "total_throughput": 14995.655106566073,
    "itl": 87.56666983154733,
    "ttft": 1219598.802788686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24465913418214755,
    "arrivals": 189688,
    "finished_requests": 116484,
    "scheduler_time": 167.59019612063858
}
#Debug simulation 
Total elapsed time: 105.11259680101648. Arrivals time: 0.4318864718079567 Scheduler time: 104.47077813697979 Scheduler overhead time: 0.08330192090943456 Adapter cache time: 0.014063711743801832 Engine time: 0.07999305706471205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 33, 33, 17280, 33, 34560, 17280, 33, 33, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 34560, 34560, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570570 . Total input tokens: 127408721 . Total output tokens: 112106331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 104.64440775383264,
    "estimated_duration": 3600.016799082619,
    "input_throughput": 7950.571788246572,
    "output_throughput": 6945.25981277961,
    "total_throughput": 14895.831601026182,
    "itl": 86.52346832387003,
    "ttft": 1224660.9203976095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2956152458488942,
    "arrivals": 189688,
    "finished_requests": 115713,
    "scheduler_time": 167.77535410761618
}
#Debug simulation 
Total elapsed time: 104.64452273910865. Arrivals time: 0.4280727584846318 Scheduler time: 104.0062361098826 Scheduler overhead time: 0.08357494557276368 Adapter cache time: 0.013932017609477043 Engine time: 0.0797137999907136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 33, 33, 17280, 33, 34560, 17280, 33, 33, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 34560, 34560, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570570 . Total input tokens: 127408721 . Total output tokens: 112106331
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 102.61739639006555,
    "estimated_duration": 3600.0016081124136,
    "input_throughput": 7876.602314871678,
    "output_throughput": 6881.333870567106,
    "total_throughput": 14757.936185438784,
    "itl": 84.18678281771814,
    "ttft": 1235341.6918312926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2817266508797184,
    "arrivals": 189688,
    "finished_requests": 114649,
    "scheduler_time": 168.956236932452
}
#Debug simulation 
Total elapsed time: 102.61751083517447. Arrivals time: 0.42466361448168755 Scheduler time: 101.97899623867124 Scheduler overhead time: 0.08496228186413646 Adapter cache time: 0.014056019484996796 Engine time: 0.08138784719631076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 33, 33, 17280, 33, 34560, 17280, 33, 33, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 34560, 34560, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570570 . Total input tokens: 127408721 . Total output tokens: 112106331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 104.51661636307836,
    "estimated_duration": 3600.084366029766,
    "input_throughput": 7963.698092891569,
    "output_throughput": 6956.158926802479,
    "total_throughput": 14919.857019694047,
    "itl": 86.44246177641679,
    "ttft": 1224676.8118937956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2542519474541768,
    "arrivals": 189688,
    "finished_requests": 115904,
    "scheduler_time": 168.03389474501597
}
#Debug simulation 
Total elapsed time: 104.51672925893217. Arrivals time: 0.433202073443681 Scheduler time: 103.87259542336687 Scheduler overhead time: 0.08264410821720958 Adapter cache time: 0.01421812316402793 Engine time: 0.08073610300198197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 33, 33, 17280, 33, 34560, 17280, 33, 33, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 34560, 34560, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570570 . Total input tokens: 127408721 . Total output tokens: 112106331
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 103.17353245522827,
    "estimated_duration": 3600.051700777969,
    "input_throughput": 7876.412995366815,
    "output_throughput": 6881.232843030175,
    "total_throughput": 14757.64583839699,
    "itl": 84.17304898791068,
    "ttft": 1235254.1340246466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27882701040711255,
    "arrivals": 189688,
    "finished_requests": 114647,
    "scheduler_time": 168.96307053266284
}
#Debug simulation 
Total elapsed time: 103.17364635784179. Arrivals time: 0.4368032431229949 Scheduler time: 102.52044647093862 Scheduler overhead time: 0.0851960419677198 Adapter cache time: 0.014354840386658907 Engine time: 0.08310785796493292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 33, 33, 17280, 33, 34560, 17280, 33, 33, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 34560, 34560, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570570 . Total input tokens: 127408721 . Total output tokens: 112106331
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 106.1701549673453,
    "estimated_duration": 3600.038688089622,
    "input_throughput": 7962.929702628336,
    "output_throughput": 6955.2496985212,
    "total_throughput": 14918.179401149537,
    "itl": 86.40928071829725,
    "ttft": 1224494.6781252217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23620507480110967,
    "arrivals": 189688,
    "finished_requests": 115886,
    "scheduler_time": 168.05025363514412
}
#Debug simulation 
Total elapsed time: 106.17026845505461. Arrivals time: 0.4375871713273227 Scheduler time: 105.51738976314664 Scheduler overhead time: 0.08535619778558612 Adapter cache time: 0.014368493109941483 Engine time: 0.08194272359833121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 17280, 33, 33, 17280, 33, 34560, 17280, 33, 33, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 34560, 34560, 33, 34560, 34560, 34560, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560]
Prompts retrieved: 570570 . Total input tokens: 127408721 . Total output tokens: 112106331
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 102.88637503096834,
    "estimated_duration": 3600.0792252183173,
    "input_throughput": 7875.270855540878,
    "output_throughput": 6880.705242951375,
    "total_throughput": 14755.976098492254,
    "itl": 84.15637469098766,
    "ttft": 1235363.4067076806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 37,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2761344871111215,
    "arrivals": 189688,
    "finished_requests": 114632,
    "scheduler_time": 168.97856084084952
}
#Debug simulation 
Total elapsed time: 102.886544871144. Arrivals time: 0.43212085822597146 Scheduler time: 102.23826065473258 Scheduler overhead time: 0.08545069769024849 Adapter cache time: 0.014264336321502924 Engine time: 0.08239508094266057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 4320, 4320, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 518400 . Total input tokens: 115643427 . Total output tokens: 101855273
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 130.64584921533242,
    "estimated_duration": 3600.023414586471,
    "input_throughput": 7910.202718298458,
    "output_throughput": 6929.732984213283,
    "total_throughput": 14839.93570251174,
    "itl": 88.58401217103881,
    "ttft": 961838.62593124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21820949805434783,
    "arrivals": 172540,
    "finished_requests": 115300,
    "scheduler_time": 160.63865660128218
}
#Debug simulation 
Total elapsed time: 130.64596207020804. Arrivals time: 0.4589145160280168 Scheduler time: 129.96635372238234 Scheduler overhead time: 0.08736403938382864 Adapter cache time: 0.014427293092012405 Engine time: 0.0850998256355524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 4320, 4320, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 518400 . Total input tokens: 115643427 . Total output tokens: 101855273
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 129.6007332028821,
    "estimated_duration": 3600.082945939013,
    "input_throughput": 7866.067650453443,
    "output_throughput": 6897.20525134274,
    "total_throughput": 14763.272901796183,
    "itl": 87.43819888372968,
    "ttft": 958016.4133695055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24121025245171038,
    "arrivals": 172540,
    "finished_requests": 114703,
    "scheduler_time": 161.14001060880494
}
#Debug simulation 
Total elapsed time: 129.60084185982123. Arrivals time: 0.4618016737513244 Scheduler time: 128.91400099545717 Scheduler overhead time: 0.08852727897465229 Adapter cache time: 0.015425833407789469 Engine time: 0.08625497575849295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 4320, 4320, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 518400 . Total input tokens: 115643427 . Total output tokens: 101855273
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 124.4713324457407,
    "estimated_duration": 3600.0900192303125,
    "input_throughput": 7812.094655903319,
    "output_throughput": 6834.053556599701,
    "total_throughput": 14646.14821250302,
    "itl": 85.13521264510602,
    "ttft": 949159.538369707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 19,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14490136212669313,
    "arrivals": 172540,
    "finished_requests": 113822,
    "scheduler_time": 162.08081565472256
}
#Debug simulation 
Total elapsed time: 124.47147224191576. Arrivals time: 0.45281186047941446 Scheduler time: 123.79269601078704 Scheduler overhead time: 0.09006590116769075 Adapter cache time: 0.014798284973949194 Engine time: 0.08677781652659178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 4320, 4320, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 518400 . Total input tokens: 115643427 . Total output tokens: 101855273
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 129.9470003680326,
    "estimated_duration": 3600.0899572205317,
    "input_throughput": 7865.770671425849,
    "output_throughput": 6897.315704625023,
    "total_throughput": 14763.086376050873,
    "itl": 87.4354387931779,
    "ttft": 957980.0012329173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22593982174526897,
    "arrivals": 172540,
    "finished_requests": 114700,
    "scheduler_time": 161.14242653178567
}
#Debug simulation 
Total elapsed time: 129.94710300676525. Arrivals time: 0.4046676610596478 Scheduler time: 129.3206416075118 Scheduler overhead time: 0.08749506156891584 Adapter cache time: 0.014786286279559135 Engine time: 0.08562047826126218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 4320, 4320, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 518400 . Total input tokens: 115643427 . Total output tokens: 101855273
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 127.84564197529107,
    "estimated_duration": 3600.0261475865022,
    "input_throughput": 7789.830365204635,
    "output_throughput": 6825.1845938598435,
    "total_throughput": 14615.01495906448,
    "itl": 85.08393042497357,
    "ttft": 953032.1670108009,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24522814487107103,
    "arrivals": 172540,
    "finished_requests": 113544,
    "scheduler_time": 162.20363608691642
}
#Debug simulation 
Total elapsed time: 127.84574628900737. Arrivals time: 0.3995397980324924 Scheduler time: 127.22252023080364 Scheduler overhead time: 0.08839262463152409 Adapter cache time: 0.015231650788336992 Engine time: 0.08586782775819302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 4320, 4320, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 518400 . Total input tokens: 115643427 . Total output tokens: 101855273
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 130.39811437670141,
    "estimated_duration": 3600.004260578547,
    "input_throughput": 7865.815413076554,
    "output_throughput": 6897.150726152106,
    "total_throughput": 14762.96613922866,
    "itl": 87.43619788906312,
    "ttft": 957855.429538336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21066939103882756,
    "arrivals": 172540,
    "finished_requests": 114697,
    "scheduler_time": 161.1371445558687
}
#Debug simulation 
Total elapsed time: 130.3982128277421. Arrivals time: 0.4039108967408538 Scheduler time: 129.7704772283323 Scheduler overhead time: 0.08872602507472038 Adapter cache time: 0.015134793240576982 Engine time: 0.08537283726036549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 4320, 4320, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 34560, 34560, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 518400 . Total input tokens: 115643427 . Total output tokens: 101855273
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 124.17703783744946,
    "estimated_duration": 3600.0218494390856,
    "input_throughput": 7811.835365494177,
    "output_throughput": 6834.032411173645,
    "total_throughput": 14645.867776667821,
    "itl": 85.13355053300484,
    "ttft": 949149.9879976579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 19,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1420017216540873,
    "arrivals": 172540,
    "finished_requests": 113819,
    "scheduler_time": 162.0773564844898
}
#Debug simulation 
Total elapsed time: 124.17721657734364. Arrivals time: 0.3957883692346513 Scheduler time: 123.55822373367846 Scheduler overhead time: 0.08889015018939972 Adapter cache time: 0.014871257357299328 Engine time: 0.08507985109463334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 1080, 1080, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 486000 . Total input tokens: 108409730 . Total output tokens: 95518396
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 90.90728137688711,
    "estimated_duration": 3600.003058047786,
    "input_throughput": 7979.388221847087,
    "output_throughput": 6939.872160427584,
    "total_throughput": 14919.260382274671,
    "itl": 88.23775462550662,
    "ttft": 906981.7586086211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11241095354314896,
    "arrivals": 161835,
    "finished_requests": 115718,
    "scheduler_time": 155.71346645615438
}
#Debug simulation 
Total elapsed time: 90.9074059650302. Arrivals time: 0.39529224624857306 Scheduler time: 90.29634243389592 Scheduler overhead time: 0.08527016546577215 Adapter cache time: 0.014294263906776905 Engine time: 0.08289876161143184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 1080, 1080, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 486000 . Total input tokens: 108409730 . Total output tokens: 95518396
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 89.21239138673991,
    "estimated_duration": 3600.0385428879904,
    "input_throughput": 7935.933090616608,
    "output_throughput": 6905.798286275047,
    "total_throughput": 14841.731376891656,
    "itl": 87.08481319338443,
    "ttft": 914442.8738718787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.127961749616079,
    "arrivals": 161835,
    "finished_requests": 115129,
    "scheduler_time": 156.27005352193194
}
#Debug simulation 
Total elapsed time: 89.2125067547895. Arrivals time: 0.3912049578502774 Scheduler time: 88.6036019497551 Scheduler overhead time: 0.0860924101434648 Adapter cache time: 0.014188981615006924 Engine time: 0.08424126589670777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 1080, 1080, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 486000 . Total input tokens: 108409730 . Total output tokens: 95518396
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 87.26223003305495,
    "estimated_duration": 3600.093126685469,
    "input_throughput": 7855.094855847786,
    "output_throughput": 6837.013692104541,
    "total_throughput": 14692.108547952326,
    "itl": 84.74657252096696,
    "ttft": 932239.3142586761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13216682816855607,
    "arrivals": 161835,
    "finished_requests": 113994,
    "scheduler_time": 157.42952247568894
}
#Debug simulation 
Total elapsed time: 87.26234334986657. Arrivals time: 0.3901925338432193 Scheduler time: 86.65142221888527 Scheduler overhead time: 0.08696985431015491 Adapter cache time: 0.014443796128034592 Engine time: 0.08507896959781647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 1080, 1080, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 486000 . Total input tokens: 108409730 . Total output tokens: 95518396
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 89.39426802797243,
    "estimated_duration": 3600.064587119859,
    "input_throughput": 7935.796791594846,
    "output_throughput": 6905.5288866050305,
    "total_throughput": 14841.325678199877,
    "itl": 87.08554810845212,
    "ttft": 914471.5974978529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963242377620191,
    "arrivals": 161835,
    "finished_requests": 115128,
    "scheduler_time": 156.2721354907759
}
#Debug simulation 
Total elapsed time: 89.39438852202147. Arrivals time: 0.39913389133289456 Scheduler time: 88.77639624709263 Scheduler overhead time: 0.08685626648366451 Adapter cache time: 0.01454918785020709 Engine time: 0.08400913467630744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 1080, 1080, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 486000 . Total input tokens: 108409730 . Total output tokens: 95518396
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 87.24376753298566,
    "estimated_duration": 3600.0892985116425,
    "input_throughput": 7854.962100993159,
    "output_throughput": 6836.7893013586045,
    "total_throughput": 14691.751402351763,
    "itl": 84.74478038090749,
    "ttft": 932260.2194492264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13092412510886786,
    "arrivals": 161835,
    "finished_requests": 113992,
    "scheduler_time": 157.43141381730928
}
#Debug simulation 
Total elapsed time: 87.24388570105657. Arrivals time: 0.39162374660372734 Scheduler time: 86.6300974697806 Scheduler overhead time: 0.08725854335352778 Adapter cache time: 0.0148597895167768 Engine time: 0.0855541443452239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 1080, 1080, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 486000 . Total input tokens: 108409730 . Total output tokens: 95518396
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 89.22500050673261,
    "estimated_duration": 3600.0532634416663,
    "input_throughput": 7935.67258854611,
    "output_throughput": 6905.550051843788,
    "total_throughput": 14841.222640389899,
    "itl": 87.08490932708462,
    "ttft": 914358.894547855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10852665598969911,
    "arrivals": 161835,
    "finished_requests": 115127,
    "scheduler_time": 156.27027736260635
}
#Debug simulation 
Total elapsed time: 89.22512431489304. Arrivals time: 0.39340709475800395 Scheduler time: 88.61434086691588 Scheduler overhead time: 0.08599678054451942 Adapter cache time: 0.014508028514683247 Engine time: 0.08331989170983434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 1080, 1080, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 34560, 34560, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 486000 . Total input tokens: 108409730 . Total output tokens: 95518396
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 87.38732863496989,
    "estimated_duration": 3600.049073225204,
    "input_throughput": 7855.296254243853,
    "output_throughput": 6836.942080333773,
    "total_throughput": 14692.238334577625,
    "itl": 84.74580691593688,
    "ttft": 932363.3257990271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12926718769595025,
    "arrivals": 161835,
    "finished_requests": 113993,
    "scheduler_time": 157.42866694407425
}
#Debug simulation 
Total elapsed time: 87.38749642390758. Arrivals time: 0.3874813565053046 Scheduler time: 86.77997476933524 Scheduler overhead time: 0.08702457696199417 Adapter cache time: 0.01450229762122035 Engine time: 0.0845225527882576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 540, 540, 8640, 540, 34560, 8640, 540, 540, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 34560, 34560, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 480600 . Total input tokens: 107207044 . Total output tokens: 94440961
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 123.20660434802994,
    "estimated_duration": 3600.004083505379,
    "input_throughput": 8002.290645167529,
    "output_throughput": 6941.1060155434025,
    "total_throughput": 14943.39666071093,
    "itl": 88.25062120700754,
    "ttft": 820831.1845633395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 19,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12563577160704884,
    "arrivals": 160041,
    "finished_requests": 116113,
    "scheduler_time": 154.29740362262422
}
#Debug simulation 
Total elapsed time: 123.20672331284732. Arrivals time: 0.3983513140119612 Scheduler time: 122.58545663859695 Scheduler overhead time: 0.08802265767008066 Adapter cache time: 0.01472564646974206 Engine time: 0.08541745878756046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 540, 540, 8640, 540, 34560, 8640, 540, 540, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 34560, 34560, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 480600 . Total input tokens: 107207044 . Total output tokens: 94440961
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 122.10760188801214,
    "estimated_duration": 3600.011550351599,
    "input_throughput": 7965.519443180489,
    "output_throughput": 6908.335335082755,
    "total_throughput": 14873.854778263245,
    "itl": 87.0877134637963,
    "ttft": 829511.3551526548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1512781753577292,
    "arrivals": 160041,
    "finished_requests": 115563,
    "scheduler_time": 154.86951791774325
}
#Debug simulation 
Total elapsed time: 122.10772277368233. Arrivals time: 0.3986202534288168 Scheduler time: 121.48403144674376 Scheduler overhead time: 0.08981427969411016 Adapter cache time: 0.01447954773902893 Engine time: 0.08655966632068157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 540, 540, 8640, 540, 34560, 8640, 540, 540, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 34560, 34560, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 480600 . Total input tokens: 107207044 . Total output tokens: 94440961
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 117.90792676387355,
    "estimated_duration": 3600.032535648481,
    "input_throughput": 7883.430418744187,
    "output_throughput": 6837.672369970928,
    "total_throughput": 14721.102788715116,
    "itl": 84.76722685630287,
    "ttft": 847261.8275694113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 19,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14831897491589188,
    "arrivals": 160041,
    "finished_requests": 114379,
    "scheduler_time": 156.0591414564045
}
#Debug simulation 
Total elapsed time: 117.90804266976193. Arrivals time: 0.3888840237632394 Scheduler time: 117.29503054125234 Scheduler overhead time: 0.08976088277995586 Adapter cache time: 0.014456945937126875 Engine time: 0.0860908292233944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 540, 540, 8640, 540, 34560, 8640, 540, 540, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 34560, 34560, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 480600 . Total input tokens: 107207044 . Total output tokens: 94440961
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 121.78481852822006,
    "estimated_duration": 3600.0815826802336,
    "input_throughput": 7965.500875858095,
    "output_throughput": 6908.292611936911,
    "total_throughput": 14873.793487795007,
    "itl": 87.09087127910095,
    "ttft": 829376.4228250462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 19,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13378848663065582,
    "arrivals": 160041,
    "finished_requests": 115565,
    "scheduler_time": 154.87027852127693
}
#Debug simulation 
Total elapsed time: 121.78493634238839. Arrivals time: 0.39543734211474657 Scheduler time: 121.16449942765757 Scheduler overhead time: 0.08877155091613531 Adapter cache time: 0.014880502596497536 Engine time: 0.08690025703981519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 540, 540, 8640, 540, 34560, 8640, 540, 540, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 34560, 34560, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 480600 . Total input tokens: 107207044 . Total output tokens: 94440961
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 118.18634281912819,
    "estimated_duration": 3600.057135807136,
    "input_throughput": 7883.500991613274,
    "output_throughput": 6837.654256973642,
    "total_throughput": 14721.155248586916,
    "itl": 84.76815648379207,
    "ttft": 847302.1759518647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 19,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14686915467958894,
    "arrivals": 160041,
    "finished_requests": 114380,
    "scheduler_time": 156.06154146123617
}
#Debug simulation 
Total elapsed time: 118.18645703885704. Arrivals time: 0.3943875855766237 Scheduler time: 117.56551409931853 Scheduler overhead time: 0.08974880585446954 Adapter cache time: 0.01483100838959217 Engine time: 0.08721367083489895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 540, 540, 8640, 540, 34560, 8640, 540, 540, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 34560, 34560, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 480600 . Total input tokens: 107207044 . Total output tokens: 94440961
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 122.25876429816708,
    "estimated_duration": 3600.0460295601915,
    "input_throughput": 7965.54259710488,
    "output_throughput": 6908.566944917215,
    "total_throughput": 14874.109542022094,
    "itl": 87.08643743651827,
    "ttft": 829416.3845248213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 19,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12129449787084019,
    "arrivals": 160041,
    "finished_requests": 115567,
    "scheduler_time": 154.8699959323834
}
#Debug simulation 
Total elapsed time: 122.2588817840442. Arrivals time: 0.3980556670576334 Scheduler time: 121.63653999427333 Scheduler overhead time: 0.08943581767380238 Adapter cache time: 0.014816954266279936 Engine time: 0.08615577686578035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 540, 540, 8640, 540, 34560, 8640, 540, 540, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 34560, 34560, 540, 34560, 34560, 34560, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 480600 . Total input tokens: 107207044 . Total output tokens: 94440961
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 118.97475280659273,
    "estimated_duration": 3600.0108280300033,
    "input_throughput": 7883.369344066727,
    "output_throughput": 6837.686100369376,
    "total_throughput": 14721.055444436102,
    "itl": 84.76841649906869,
    "ttft": 847236.2381106694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.15287405628710982,
    "arrivals": 160041,
    "finished_requests": 114378,
    "scheduler_time": 156.0587315265175
}
#Debug simulation 
Total elapsed time: 118.97493060864508. Arrivals time: 0.397758720908314 Scheduler time: 118.35181011166424 Scheduler overhead time: 0.08979349955916405 Adapter cache time: 0.014669147320091724 Engine time: 0.08633282268419862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 270, 270, 8640, 270, 34560, 8640, 270, 270, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 34560, 34560, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 477900 . Total input tokens: 106609668 . Total output tokens: 93900804
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 79.6868957998231,
    "estimated_duration": 3600.0227000723057,
    "input_throughput": 7906.489033924235,
    "output_throughput": 6945.675370185246,
    "total_throughput": 14852.16440410948,
    "itl": 88.53007389254978,
    "ttft": 913488.1821185391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 159140,
    "finished_requests": 115217,
    "scheduler_time": 154.50895643433506
}
#Debug simulation 
Total elapsed time: 79.68701891368255. Arrivals time: 0.38706362387165427 Scheduler time: 79.08490730309859 Scheduler overhead time: 0.08542888518422842 Adapter cache time: 0.013935055118054152 Engine time: 0.08235453441739082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 270, 270, 8640, 270, 34560, 8640, 270, 270, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 34560, 34560, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 477900 . Total input tokens: 106609668 . Total output tokens: 93900804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 79.19129249919206,
    "estimated_duration": 3600.072458258061,
    "input_throughput": 7866.783607378685,
    "output_throughput": 6913.051414537949,
    "total_throughput": 14779.835021916633,
    "itl": 87.37744100493073,
    "ttft": 921530.2012294477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12018960770219561,
    "arrivals": 159140,
    "finished_requests": 114643,
    "scheduler_time": 155.0879256435767
}
#Debug simulation 
Total elapsed time: 79.19141099695116. Arrivals time: 0.38908373285084963 Scheduler time: 78.58684558235109 Scheduler overhead time: 0.08542147930711508 Adapter cache time: 0.013575389981269836 Engine time: 0.08284926321357489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 270, 270, 8640, 270, 34560, 8640, 270, 270, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 34560, 34560, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 477900 . Total input tokens: 106609668 . Total output tokens: 93900804
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 77.07311662472785,
    "estimated_duration": 3600.03720900954,
    "input_throughput": 7785.882026400389,
    "output_throughput": 6840.866238373021,
    "total_throughput": 14626.748264773409,
    "itl": 85.03232536027691,
    "ttft": 938871.5600727441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12409075479488818,
    "arrivals": 159140,
    "finished_requests": 113470,
    "scheduler_time": 156.28309503853404
}
#Debug simulation 
Total elapsed time: 77.0732380989939. Arrivals time: 0.3940258827060461 Scheduler time: 76.46300783101469 Scheduler overhead time: 0.08510460937395692 Adapter cache time: 0.014007666148245335 Engine time: 0.08339335396885872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 270, 270, 8640, 270, 34560, 8640, 270, 270, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 34560, 34560, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 477900 . Total input tokens: 106609668 . Total output tokens: 93900804
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 78.5583375217393,
    "estimated_duration": 3600.004598467143,
    "input_throughput": 7866.42217403225,
    "output_throughput": 6912.964503044409,
    "total_throughput": 14779.38667707666,
    "itl": 87.37921579325021,
    "ttft": 921547.9905250538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11186028186231851,
    "arrivals": 159140,
    "finished_requests": 114640,
    "scheduler_time": 155.08345253133288
}
#Debug simulation 
Total elapsed time: 78.55846617510542. Arrivals time: 0.39382855081930757 Scheduler time: 77.95015717903152 Scheduler overhead time: 0.08503382047638297 Adapter cache time: 0.014071139506995678 Engine time: 0.08218297827988863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 270, 270, 8640, 270, 34560, 8640, 270, 270, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 34560, 34560, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 477900 . Total input tokens: 106609668 . Total output tokens: 93900804
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 76.64502007700503,
    "estimated_duration": 3600.091133643173,
    "input_throughput": 7785.872068086987,
    "output_throughput": 6840.823769668767,
    "total_throughput": 14626.695837755755,
    "itl": 85.0312771183023,
    "ttft": 938872.4537656027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12284805173519997,
    "arrivals": 159140,
    "finished_requests": 113471,
    "scheduler_time": 156.28652411466368
}
#Debug simulation 
Total elapsed time: 76.64513764204457. Arrivals time: 0.39020046638324857 Scheduler time: 76.0369757800363 Scheduler overhead time: 0.08619869919493794 Adapter cache time: 0.014122391119599342 Engine time: 0.08405612548813224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 270, 270, 8640, 270, 34560, 8640, 270, 270, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 34560, 34560, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 477900 . Total input tokens: 106609668 . Total output tokens: 93900804
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 78.95743092382327,
    "estimated_duration": 3600.0862669938424,
    "input_throughput": 7866.892040797989,
    "output_throughput": 6913.252392916219,
    "total_throughput": 14780.144433714207,
    "itl": 87.37775266441328,
    "ttft": 921497.1801550227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 159140,
    "finished_requests": 114648,
    "scheduler_time": 155.0863980910602
}
#Debug simulation 
Total elapsed time: 78.9575550002046. Arrivals time: 0.3918522740714252 Scheduler time: 78.35272793890908 Scheduler overhead time: 0.08467844361439347 Adapter cache time: 0.013970090076327324 Engine time: 0.0814480516128242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 270, 270, 8640, 270, 34560, 8640, 270, 270, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 34560, 34560, 270, 34560, 34560, 34560, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 477900 . Total input tokens: 106609668 . Total output tokens: 93900804
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 76.83175484603271,
    "estimated_duration": 3600.0160441938096,
    "input_throughput": 7785.888633803827,
    "output_throughput": 6840.964233956663,
    "total_throughput": 14626.85286776049,
    "itl": 85.03181409687856,
    "ttft": 938753.280351189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12139823149889706,
    "arrivals": 159140,
    "finished_requests": 113471,
    "scheduler_time": 156.28162160547012
}
#Debug simulation 
Total elapsed time: 76.8319400739856. Arrivals time: 0.3896663263440132 Scheduler time: 76.22656389139593 Scheduler overhead time: 0.0850227726623416 Adapter cache time: 0.01414869213476777 Engine time: 0.08304049912840128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 135, 135, 8640, 135, 34560, 8640, 135, 135, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 34560, 34560, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 476550 . Total input tokens: 106305212 . Total output tokens: 93638864
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 120.13442170992494,
    "estimated_duration": 3600.0675165984158,
    "input_throughput": 8005.69791180807,
    "output_throughput": 6947.627199956777,
    "total_throughput": 14953.325111764847,
    "itl": 88.29523493741227,
    "ttft": 815273.067326949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1190233625750989,
    "arrivals": 158673,
    "finished_requests": 116316,
    "scheduler_time": 153.3309033804265
}
#Debug simulation 
Total elapsed time: 120.13453887496144. Arrivals time: 0.4015612378716469 Scheduler time: 119.5116039304994 Scheduler overhead time: 0.0873154359869659 Adapter cache time: 0.014559971634298563 Engine time: 0.08513535698875785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 135, 135, 8640, 135, 34560, 8640, 135, 135, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 34560, 34560, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 476550 . Total input tokens: 106305212 . Total output tokens: 93638864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 118.04321241006255,
    "estimated_duration": 3600.02484614401,
    "input_throughput": 7965.495302265725,
    "output_throughput": 6913.419785604557,
    "total_throughput": 14878.915087870282,
    "itl": 87.14742615842894,
    "ttft": 823509.2572070197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13434567055664956,
    "arrivals": 158673,
    "finished_requests": 115747,
    "scheduler_time": 153.91400470633874
}
#Debug simulation 
Total elapsed time: 118.0433306847699. Arrivals time: 0.3992599658668041 Scheduler time: 117.42347064940259 Scheduler overhead time: 0.08661158615723252 Adapter cache time: 0.014939629938453436 Engine time: 0.08490096451714635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 135, 135, 8640, 135, 34560, 8640, 135, 135, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 34560, 34560, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 476550 . Total input tokens: 106305212 . Total output tokens: 93638864
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 114.37845450267196,
    "estimated_duration": 3600.0859255636215,
    "input_throughput": 7887.974505929096,
    "output_throughput": 6844.116643172207,
    "total_throughput": 14732.091149101303,
    "itl": 84.82478413671303,
    "ttft": 841490.2439018454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1385340951476246,
    "arrivals": 158673,
    "finished_requests": 114626,
    "scheduler_time": 155.14392023902602
}
#Debug simulation 
Total elapsed time: 114.37857867963612. Arrivals time: 0.3922957135364413 Scheduler time: 113.76319983834401 Scheduler overhead time: 0.08900164067745209 Adapter cache time: 0.014810615219175816 Engine time: 0.08483960200101137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 135, 135, 8640, 135, 34560, 8640, 135, 135, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 34560, 34560, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 476550 . Total input tokens: 106305212 . Total output tokens: 93638864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 117.7340666600503,
    "estimated_duration": 3600.014320524462,
    "input_throughput": 7965.518591554488,
    "output_throughput": 6913.439998864827,
    "total_throughput": 14878.958590419315,
    "itl": 87.14753812684819,
    "ttft": 823479.0068551957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12462812374345959,
    "arrivals": 158673,
    "finished_requests": 115747,
    "scheduler_time": 153.91216997974936
}
#Debug simulation 
Total elapsed time: 117.73418460227549. Arrivals time: 0.39567538583651185 Scheduler time: 117.11724369507283 Scheduler overhead time: 0.08768984023481607 Adapter cache time: 0.014538605697453022 Engine time: 0.08472677506506443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 135, 135, 8640, 135, 34560, 8640, 135, 135, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 34560, 34560, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 476550 . Total input tokens: 106305212 . Total output tokens: 93638864
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 113.68945354502648,
    "estimated_duration": 3600.0017518993227,
    "input_throughput": 7888.158383539075,
    "output_throughput": 6844.157780479061,
    "total_throughput": 14732.316164018137,
    "itl": 84.82461288811375,
    "ttft": 841415.0509848514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1370842749113217,
    "arrivals": 158673,
    "finished_requests": 114624,
    "scheduler_time": 155.13953083642255
}
#Debug simulation 
Total elapsed time: 113.68956501921639. Arrivals time: 0.3964343019761145 Scheduler time: 113.0698573263362 Scheduler overhead time: 0.089141427539289 Adapter cache time: 0.014757667202502489 Engine time: 0.08511755662038922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 135, 135, 8640, 135, 34560, 8640, 135, 135, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 34560, 34560, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 476550 . Total input tokens: 106305212 . Total output tokens: 93638864
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 118.17353880079463,
    "estimated_duration": 3600.045838665538,
    "input_throughput": 7965.448854015033,
    "output_throughput": 6913.379472197399,
    "total_throughput": 14878.828326212431,
    "itl": 87.14746691395,
    "ttft": 823574.8067090544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11491057693026965,
    "arrivals": 158673,
    "finished_requests": 115747,
    "scheduler_time": 153.91336295212406
}
#Debug simulation 
Total elapsed time: 118.17366175586358. Arrivals time: 0.39457635255530477 Scheduler time: 117.55779192876071 Scheduler overhead time: 0.08782603405416012 Adapter cache time: 0.014703882858157158 Engine time: 0.08501141611486673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 135, 135, 8640, 135, 34560, 8640, 135, 135, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 34560, 34560, 135, 34560, 34560, 34560, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 476550 . Total input tokens: 106305212 . Total output tokens: 93638864
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 114.15685014985502,
    "estimated_duration": 3600.0749216627896,
    "input_throughput": 7888.377497122557,
    "output_throughput": 6844.417834676388,
    "total_throughput": 14732.795331798945,
    "itl": 84.8244960316516,
    "ttft": 841444.1710083716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13563445467501878,
    "arrivals": 158673,
    "finished_requests": 114629,
    "scheduler_time": 155.14367716189878
}
#Debug simulation 
Total elapsed time: 114.15703379921615. Arrivals time: 0.39398740604519844 Scheduler time: 113.53854837315157 Scheduler overhead time: 0.08896061824634671 Adapter cache time: 0.014961677137762308 Engine time: 0.08621393935754895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 66, 66, 8640, 66, 34560, 8640, 66, 66, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 34560, 34560, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475860 . Total input tokens: 106146780 . Total output tokens: 93512102
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 78.5302523477003,
    "estimated_duration": 3600.067508899451,
    "input_throughput": 7960.860436409238,
    "output_throughput": 6956.442049514762,
    "total_throughput": 14917.302485924,
    "itl": 88.69024902383048,
    "ttft": 897270.6874808342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11241095354314896,
    "arrivals": 158470,
    "finished_requests": 116193,
    "scheduler_time": 153.23001557222784
}
#Debug simulation 
Total elapsed time: 78.530372004956. Arrivals time: 0.39036695798859 Scheduler time: 77.9289164361544 Scheduler overhead time: 0.08333741268143058 Adapter cache time: 0.013460097834467888 Engine time: 0.08179371897131205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 66, 66, 8640, 66, 34560, 8640, 66, 66, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 34560, 34560, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475860 . Total input tokens: 106146780 . Total output tokens: 93512102
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 77.55837643984705,
    "estimated_duration": 3600.0622178355593,
    "input_throughput": 7922.990846849542,
    "output_throughput": 6923.017295791199,
    "total_throughput": 14846.008142640741,
    "itl": 87.51769762241204,
    "ttft": 905330.5571356127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12796174961607898,
    "arrivals": 158470,
    "finished_requests": 115640,
    "scheduler_time": 153.820602835444
}
#Debug simulation 
Total elapsed time: 77.55850099073723. Arrivals time: 0.38830998446792364 Scheduler time: 76.95713408617303 Scheduler overhead time: 0.08373521221801639 Adapter cache time: 0.014281864743679762 Engine time: 0.08248015586286783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 66, 66, 8640, 66, 34560, 8640, 66, 66, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 34560, 34560, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475860 . Total input tokens: 106146780 . Total output tokens: 93512102
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 75.66021513519809,
    "estimated_duration": 3600.080186520187,
    "input_throughput": 7842.461983406625,
    "output_throughput": 6852.992633991059,
    "total_throughput": 14695.454617397685,
    "itl": 85.171985134723,
    "ttft": 922148.4907863998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13216682816855607,
    "arrivals": 158470,
    "finished_requests": 114449,
    "scheduler_time": 155.04809539358595
}
#Debug simulation 
Total elapsed time: 75.66033426532522. Arrivals time: 0.38611512910574675 Scheduler time: 75.0573264202103 Scheduler overhead time: 0.08564866241067648 Adapter cache time: 0.01430390402674675 Engine time: 0.08285108068957925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 66, 66, 8640, 66, 34560, 8640, 66, 66, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 34560, 34560, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475860 . Total input tokens: 106146780 . Total output tokens: 93512102
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 77.82887262990698,
    "estimated_duration": 3600.0709117481583,
    "input_throughput": 7922.986713100428,
    "output_throughput": 6922.922245411448,
    "total_throughput": 14845.908958511876,
    "itl": 87.51842282473325,
    "ttft": 905348.08256505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12102064474951474,
    "arrivals": 158470,
    "finished_requests": 115639,
    "scheduler_time": 153.81858879635087
}
#Debug simulation 
Total elapsed time: 77.82898942194879. Arrivals time: 0.38477007346227765 Scheduler time: 77.22870851727203 Scheduler overhead time: 0.0852675000205636 Adapter cache time: 0.013716331217437983 Engine time: 0.08315565157681704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 66, 66, 8640, 66, 34560, 8640, 66, 66, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 34560, 34560, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475860 . Total input tokens: 106146780 . Total output tokens: 93512102
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 75.31775631289929,
    "estimated_duration": 3600.0259572730142,
    "input_throughput": 7842.663729399005,
    "output_throughput": 6852.910032539818,
    "total_throughput": 14695.573761938822,
    "itl": 85.17030462031484,
    "ttft": 922125.9025227681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13113124228548256,
    "arrivals": 158470,
    "finished_requests": 114448,
    "scheduler_time": 155.04718657395614
}
#Debug simulation 
Total elapsed time: 75.31788421375677. Arrivals time: 0.3839999446645379 Scheduler time: 74.71472340729088 Scheduler overhead time: 0.0875111292116344 Adapter cache time: 0.014227233361452818 Engine time: 0.08351923851296306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 66, 66, 8640, 66, 34560, 8640, 66, 66, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 34560, 34560, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475860 . Total input tokens: 106146780 . Total output tokens: 93512102
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 77.68520459579304,
    "estimated_duration": 3600.0648271444497,
    "input_throughput": 7923.006492809337,
    "output_throughput": 6922.917001960972,
    "total_throughput": 14845.92349477031,
    "itl": 87.51978543427036,
    "ttft": 905416.2536089357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10852665598969911,
    "arrivals": 158470,
    "finished_requests": 115640,
    "scheduler_time": 153.81915127972988
}
#Debug simulation 
Total elapsed time: 77.68533424008638. Arrivals time: 0.38953507086262107 Scheduler time: 77.0813052658923 Scheduler overhead time: 0.08490671124309301 Adapter cache time: 0.014019664376974106 Engine time: 0.08257006760686636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 66, 66, 8640, 66, 34560, 8640, 66, 66, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 34560, 34560, 66, 34560, 34560, 34560, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475860 . Total input tokens: 106146780 . Total output tokens: 93512102
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 75.50602495204657,
    "estimated_duration": 3600.0482827836495,
    "input_throughput": 7842.679814885351,
    "output_throughput": 6852.925311580504,
    "total_throughput": 14695.605126465856,
    "itl": 85.17087204354162,
    "ttft": 922110.1648814167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 17,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12926718769595025,
    "arrivals": 158470,
    "finished_requests": 114448,
    "scheduler_time": 155.0482170894329
}
#Debug simulation 
Total elapsed time: 75.50620336877182. Arrivals time: 0.38765607960522175 Scheduler time: 74.90127877891064 Scheduler overhead time: 0.08522594906389713 Adapter cache time: 0.014208854641765356 Engine time: 0.08385945297777653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 33, 33, 8640, 33, 34560, 8640, 33, 33, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 34560, 34560, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475530 . Total input tokens: 106074198 . Total output tokens: 93448812
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 76.54931624326855,
    "estimated_duration": 3600.083076202673,
    "input_throughput": 7967.931126260797,
    "output_throughput": 6953.043990974503,
    "total_throughput": 14920.9751172353,
    "itl": 88.506605141474,
    "ttft": 892379.2266160544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13224818063899876,
    "arrivals": 158357,
    "finished_requests": 116365,
    "scheduler_time": 152.71199390959146
}
#Debug simulation 
Total elapsed time: 76.54942812304944. Arrivals time: 0.3881182889454067 Scheduler time: 75.94877653755248 Scheduler overhead time: 0.08414330752566457 Adapter cache time: 0.013757660519331694 Engine time: 0.08175211446359754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 33, 33, 8640, 33, 34560, 8640, 33, 33, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 34560, 34560, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475530 . Total input tokens: 106074198 . Total output tokens: 93448812
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 75.43886823393404,
    "estimated_duration": 3600.0486529535247,
    "input_throughput": 7926.63926266343,
    "output_throughput": 6920.1131433491255,
    "total_throughput": 14846.752406012554,
    "itl": 87.35430711993848,
    "ttft": 899859.5864281014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14988995438441632,
    "arrivals": 158357,
    "finished_requests": 115811,
    "scheduler_time": 153.3043167439584
}
#Debug simulation 
Total elapsed time: 75.43897975003347. Arrivals time: 0.38724925462156534 Scheduler time: 74.83708193525672 Scheduler overhead time: 0.08518968755379319 Adapter cache time: 0.014351473189890385 Engine time: 0.08261608332395554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 33, 33, 8640, 33, 34560, 8640, 33, 33, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 34560, 34560, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475530 . Total input tokens: 106074198 . Total output tokens: 93448812
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 73.56183391809464,
    "estimated_duration": 3600.000451519251,
    "input_throughput": 7846.537904760301,
    "output_throughput": 6849.931363090034,
    "total_throughput": 14696.469267850334,
    "itl": 85.0111193857933,
    "ttft": 917702.5114751036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.15468624189496039,
    "arrivals": 158357,
    "finished_requests": 114636,
    "scheduler_time": 154.544690198796
}
#Debug simulation 
Total elapsed time: 73.56194406189024. Arrivals time: 0.3824368161149323 Scheduler time: 72.9639148125425 Scheduler overhead time: 0.08583127474412322 Adapter cache time: 0.013981939759105444 Engine time: 0.08238084428012371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 33, 33, 8640, 33, 34560, 8640, 33, 33, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 34560, 34560, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475530 . Total input tokens: 106074198 . Total output tokens: 93448812
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 75.10330589814112,
    "estimated_duration": 3600.085573383437,
    "input_throughput": 7926.705190282601,
    "output_throughput": 6920.1293947538525,
    "total_throughput": 14846.834585036453,
    "itl": 87.35612391061898,
    "ttft": 899831.5545818724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14017240757122632,
    "arrivals": 158357,
    "finished_requests": 115812,
    "scheduler_time": 153.30698026742255
}
#Debug simulation 
Total elapsed time: 75.10342021239921. Arrivals time: 0.38800505781546235 Scheduler time: 74.5012830751948 Scheduler overhead time: 0.08408560464158654 Adapter cache time: 0.014153488911688328 Engine time: 0.08134571369737387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 33, 33, 8640, 33, 34560, 8640, 33, 33, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 34560, 34560, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475530 . Total input tokens: 106074198 . Total output tokens: 93448812
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 72.91839319001883,
    "estimated_duration": 3600.0890041220596,
    "input_throughput": 7846.820722391846,
    "output_throughput": 6849.908702747145,
    "total_throughput": 14696.72942513899,
    "itl": 85.01131690507145,
    "ttft": 917630.1976336945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.15323642165865747,
    "arrivals": 158357,
    "finished_requests": 114639,
    "scheduler_time": 154.54957224396904
}
#Debug simulation 
Total elapsed time: 72.91850489610806. Arrivals time: 0.3847057344391942 Scheduler time: 72.31658295122907 Scheduler overhead time: 0.0855208714492619 Adapter cache time: 0.014725282788276672 Engine time: 0.08321520080789924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 33, 33, 8640, 33, 34560, 8640, 33, 33, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 34560, 34560, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475530 . Total input tokens: 106074198 . Total output tokens: 93448812
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 75.55084096873179,
    "estimated_duration": 3600.0895514274034,
    "input_throughput": 7926.815872867728,
    "output_throughput": 6920.183968791028,
    "total_throughput": 14846.999841658755,
    "itl": 87.35523848662487,
    "ttft": 899804.9037333729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12767841881141073,
    "arrivals": 158357,
    "finished_requests": 115814,
    "scheduler_time": 153.30740647308247
}
#Debug simulation 
Total elapsed time: 75.55095423897728. Arrivals time: 0.3897050367668271 Scheduler time: 74.94745337450877 Scheduler overhead time: 0.08392765279859304 Adapter cache time: 0.014064548071473837 Engine time: 0.08250015741214156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_32_slots_16_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [10 11 11]
Adapter prompts. [34560, 8640, 33, 33, 8640, 33, 34560, 8640, 33, 33, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 34560, 34560, 33, 34560, 34560, 34560, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560]
Prompts retrieved: 475530 . Total input tokens: 106074198 . Total output tokens: 93448812
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 72.97791174892336,
    "estimated_duration": 3600.0379150862263,
    "input_throughput": 7846.495416513808,
    "output_throughput": 6849.850357592515,
    "total_throughput": 14696.345774106323,
    "itl": 85.012578127856,
    "ttft": 917643.8044516171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 20,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.15137236706912516,
    "arrivals": 158357,
    "finished_requests": 114635,
    "scheduler_time": 154.54630232053626
}
#Debug simulation 
Total elapsed time: 72.97808142798021. Arrivals time: 0.3828085334971547 Scheduler time: 72.38214883347973 Scheduler overhead time: 0.08265743078663945 Adapter cache time: 0.01405812380835414 Engine time: 0.08291139593347907 
