INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 4320, 4320, 270, 4320, 270, 540, 270, 4320, 540, 4320, 270, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 4320, 270, 270, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 4320, 540]
Prompts retrieved: 112050 . Total input tokens: 24972636 . Total output tokens: 21965048
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.3538457727991045,
    "estimated_duration": 3600.009414273116,
    "input_throughput": 2601.737640703132,
    "output_throughput": 2227.8350073757733,
    "total_throughput": 4829.572648078905,
    "itl": 33.95826011163797,
    "ttft": 10573.52832494958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.18564723084856,
    "arrivals": 37601,
    "finished_requests": 37494,
    "scheduler_time": 13.579813011391922
}
#Debug simulation 
Total elapsed time: 3.3539600488729775. Arrivals time: 0.10664484789595008 Scheduler time: 2.9439563946798444 Scheduler overhead time: 0.10472737113013864 Adapter cache time: 0.045241554733365774 Engine time: 0.10352516174316406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 4320, 4320, 270, 4320, 270, 540, 270, 4320, 540, 4320, 270, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 4320, 270, 270, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 4320, 540]
Prompts retrieved: 112050 . Total input tokens: 24972636 . Total output tokens: 21965048
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.370815952308476,
    "estimated_duration": 3600.0074446121002,
    "input_throughput": 2601.739064183856,
    "output_throughput": 2227.836226284298,
    "total_throughput": 4829.575290468154,
    "itl": 33.99961144941707,
    "ttft": 10577.964997968285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.41500502100106,
    "arrivals": 37601,
    "finished_requests": 37494,
    "scheduler_time": 13.605012768666668
}
#Debug simulation 
Total elapsed time: 3.3709245370700955. Arrivals time: 0.10552390711382031 Scheduler time: 2.9663924891501665 Scheduler overhead time: 0.10473103169351816 Adapter cache time: 0.045661693438887596 Engine time: 0.09895320516079664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 4320, 4320, 270, 4320, 270, 540, 270, 4320, 540, 4320, 270, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 4320, 270, 270, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 4320, 540]
Prompts retrieved: 112050 . Total input tokens: 24972636 . Total output tokens: 21965048
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.3802199666388333,
    "estimated_duration": 3600.0105190124364,
    "input_throughput": 2601.7368423049443,
    "output_throughput": 2227.834323717512,
    "total_throughput": 4829.571166022456,
    "itl": 34.012630118313524,
    "ttft": 10578.861083720309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.33561788860343,
    "arrivals": 37601,
    "finished_requests": 37494,
    "scheduler_time": 13.612152859585887
}
#Debug simulation 
Total elapsed time: 3.3803084236569703. Arrivals time: 0.10551102459430695 Scheduler time: 2.9731485825031996 Scheduler overhead time: 0.10440547112375498 Adapter cache time: 0.045313647482544184 Engine time: 0.10216635139659047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 4320, 4320, 270, 4320, 270, 540, 270, 4320, 540, 4320, 270, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 4320, 270, 270, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 4320, 540]
Prompts retrieved: 112050 . Total input tokens: 24972636 . Total output tokens: 21965048
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.387234279885888,
    "estimated_duration": 3600.0153988680668,
    "input_throughput": 2601.7333156255354,
    "output_throughput": 2227.831303866577,
    "total_throughput": 4829.564619492113,
    "itl": 33.970649659730356,
    "ttft": 10669.72724807205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.07306676134111,
    "arrivals": 37601,
    "finished_requests": 37494,
    "scheduler_time": 13.586783194942306
}
#Debug simulation 
Total elapsed time: 3.387326906900853. Arrivals time: 0.1067718742415309 Scheduler time: 2.9750858303159475 Scheduler overhead time: 0.1054936433210969 Adapter cache time: 0.04572487948462367 Engine time: 0.10450882464647293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 4320, 4320, 270, 4320, 270, 540, 270, 4320, 540, 4320, 270, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 4320, 270, 270, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 4320, 540]
Prompts retrieved: 112050 . Total input tokens: 24972636 . Total output tokens: 21965048
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.356444546021521,
    "estimated_duration": 3600.0008092076696,
    "input_throughput": 2601.7438596246984,
    "output_throughput": 2227.8403325595878,
    "total_throughput": 4829.584192184287,
    "itl": 34.010440784266216,
    "ttft": 10578.422130314078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.99759966333972,
    "arrivals": 37601,
    "finished_requests": 37494,
    "scheduler_time": 13.609446231128663
}
#Debug simulation 
Total elapsed time: 3.356539888307452. Arrivals time: 0.10558015014976263 Scheduler time: 2.952309307642281 Scheduler overhead time: 0.10418303683400154 Adapter cache time: 0.0455547864548862 Engine time: 0.09946045046672225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 4320, 4320, 270, 4320, 270, 540, 270, 4320, 540, 4320, 270, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 4320, 270, 270, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 4320, 540]
Prompts retrieved: 112050 . Total input tokens: 24972636 . Total output tokens: 21965048
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.357208180706948,
    "estimated_duration": 3600.0024554832094,
    "input_throughput": 2601.7426698512663,
    "output_throughput": 2227.839313771659,
    "total_throughput": 4829.581983622926,
    "itl": 33.94457772528522,
    "ttft": 10573.010997389752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.16175085652898,
    "arrivals": 37601,
    "finished_requests": 37494,
    "scheduler_time": 13.572112043379887
}
#Debug simulation 
Total elapsed time: 3.357301549986005. Arrivals time: 0.10624514752998948 Scheduler time: 2.9481037613004446 Scheduler overhead time: 0.10560729820281267 Adapter cache time: 0.045417880173772573 Engine time: 0.10237933555617929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 4320, 4320, 270, 4320, 270, 540, 270, 4320, 540, 4320, 270, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 4320, 270, 270, 270, 4320, 540, 270, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 270, 540, 540, 4320, 4320, 540, 4320, 270, 4320, 540, 540, 540, 4320, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 4320, 540]
Prompts retrieved: 112050 . Total input tokens: 24972636 . Total output tokens: 21965048
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.3605421888642013,
    "estimated_duration": 3600.014932011773,
    "input_throughput": 2601.7336530229063,
    "output_throughput": 2227.8315927756744,
    "total_throughput": 4829.565245798581,
    "itl": 34.00544297072376,
    "ttft": 10578.566326079464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 33.72150947388377,
    "arrivals": 37601,
    "finished_requests": 37494,
    "scheduler_time": 13.607499561839132
}
#Debug simulation 
Total elapsed time: 3.360635139979422. Arrivals time: 0.10613790387287736 Scheduler time: 2.953018859960139 Scheduler overhead time: 0.10423977486789227 Adapter cache time: 0.045620331075042486 Engine time: 0.10232897428795695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 4320, 4320, 135, 4320, 135, 540, 135, 4320, 540, 4320, 135, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 4320, 135, 135, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 4320, 540]
Prompts retrieved: 109215 . Total input tokens: 24344640 . Total output tokens: 21401087
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.251169225666672,
    "estimated_duration": 3599.8649477531894,
    "input_throughput": 2506.3690252133865,
    "output_throughput": 2202.3562314325923,
    "total_throughput": 4708.725256645979,
    "itl": 33.3521578298518,
    "ttft": 8888.559020149945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.09409227624831,
    "arrivals": 36626,
    "finished_requests": 36537,
    "scheduler_time": 12.78807650979527
}
#Debug simulation 
Total elapsed time: 3.251258160918951. Arrivals time: 0.10109324753284454 Scheduler time: 2.8541488852351904 Scheduler overhead time: 0.10388578567653894 Adapter cache time: 0.04296058090403676 Engine time: 0.0994813977740705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 4320, 4320, 135, 4320, 135, 540, 135, 4320, 540, 4320, 135, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 4320, 135, 135, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 4320, 540]
Prompts retrieved: 109215 . Total input tokens: 24344640 . Total output tokens: 21401087
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.3720307499170303,
    "estimated_duration": 3599.8705522272894,
    "input_throughput": 2506.054278651295,
    "output_throughput": 2202.083904126864,
    "total_throughput": 4708.13818277816,
    "itl": 33.59518257438377,
    "ttft": 9238.103298163123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3763,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.540446445740887,
    "arrivals": 36626,
    "finished_requests": 36534,
    "scheduler_time": 12.937460722773446
}
#Debug simulation 
Total elapsed time: 3.3721265909262. Arrivals time: 0.10490842536091805 Scheduler time: 2.9666100991889834 Scheduler overhead time: 0.10440719034522772 Adapter cache time: 0.04265520488843322 Engine time: 0.10385595262050629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 4320, 4320, 135, 4320, 135, 540, 135, 4320, 540, 4320, 135, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 4320, 135, 135, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 4320, 540]
Prompts retrieved: 109215 . Total input tokens: 24344640 . Total output tokens: 21401087
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.371662843041122,
    "estimated_duration": 3599.83997099716,
    "input_throughput": 2505.9505624360204,
    "output_throughput": 2202.1020556100307,
    "total_throughput": 4708.052618046051,
    "itl": 33.60404207262135,
    "ttft": 9336.441187201897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3763,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.291849852545287,
    "arrivals": 36626,
    "finished_requests": 36533,
    "scheduler_time": 12.94322950872794
}
#Debug simulation 
Total elapsed time: 3.3718425570987165. Arrivals time: 0.10509477695450187 Scheduler time: 2.9621168030425906 Scheduler overhead time: 0.10732561256736517 Adapter cache time: 0.04273344995453954 Engine time: 0.103923873975873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 4320, 4320, 135, 4320, 135, 540, 135, 4320, 540, 4320, 135, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 4320, 135, 135, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 4320, 540]
Prompts retrieved: 109215 . Total input tokens: 24344640 . Total output tokens: 21401087
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.3159729121252894,
    "estimated_duration": 3599.866822392124,
    "input_throughput": 2506.367720015947,
    "output_throughput": 2202.355084550515,
    "total_throughput": 4708.722804566462,
    "itl": 33.36004300291434,
    "ttft": 8888.807683929219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.595208665226746,
    "arrivals": 36626,
    "finished_requests": 36537,
    "scheduler_time": 12.79218933819615
}
#Debug simulation 
Total elapsed time: 3.316095044836402. Arrivals time: 0.1036269273608923 Scheduler time: 2.9142306186258793 Scheduler overhead time: 0.10414521442726254 Adapter cache time: 0.0437217210419476 Engine time: 0.10067070927470922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 4320, 4320, 135, 4320, 135, 540, 135, 4320, 540, 4320, 135, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 4320, 135, 135, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 4320, 540]
Prompts retrieved: 109215 . Total input tokens: 24344640 . Total output tokens: 21401087
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.3873880486935377,
    "estimated_duration": 3599.860813456022,
    "input_throughput": 2506.061058327141,
    "output_throughput": 2202.0898614659295,
    "total_throughput": 4708.150919793071,
    "itl": 33.5870293598751,
    "ttft": 9233.608445665563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.182596473028987,
    "arrivals": 36626,
    "finished_requests": 36534,
    "scheduler_time": 12.93347482195128
}
#Debug simulation 
Total elapsed time: 3.387510060798377. Arrivals time: 0.10594890220090747 Scheduler time: 2.979933417402208 Scheduler overhead time: 0.10469755809754133 Adapter cache time: 0.04319544741883874 Engine time: 0.10362024325877428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 4320, 4320, 135, 4320, 135, 540, 135, 4320, 540, 4320, 135, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 4320, 135, 135, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 4320, 540]
Prompts retrieved: 109215 . Total input tokens: 24344640 . Total output tokens: 21401087
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.330314958933741,
    "estimated_duration": 3599.866539039594,
    "input_throughput": 2506.3679172970483,
    "output_throughput": 2202.3552579021875,
    "total_throughput": 4708.723175199236,
    "itl": 33.340778696273865,
    "ttft": 8888.297097265755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.23336389040787,
    "arrivals": 36626,
    "finished_requests": 36537,
    "scheduler_time": 12.78123893357188
}
#Debug simulation 
Total elapsed time: 3.330409314017743. Arrivals time: 0.10591622767969966 Scheduler time: 2.9225112134590745 Scheduler overhead time: 0.10415463289245963 Adapter cache time: 0.043761979788541794 Engine time: 0.10446717124432325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 4320, 4320, 135, 4320, 135, 540, 135, 4320, 540, 4320, 135, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 4320, 135, 135, 135, 4320, 540, 135, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 135, 540, 540, 4320, 4320, 540, 4320, 135, 4320, 540, 540, 540, 4320, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 4320, 540]
Prompts retrieved: 109215 . Total input tokens: 24344640 . Total output tokens: 21401087
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.3420236869715154,
    "estimated_duration": 3599.8501865560124,
    "input_throughput": 2506.0684563184195,
    "output_throughput": 2202.0963621222227,
    "total_throughput": 4708.164818440642,
    "itl": 33.59742975303236,
    "ttft": 9237.977187795575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3762,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.75743716441193,
    "arrivals": 36626,
    "finished_requests": 36534,
    "scheduler_time": 12.939216225927003
}
#Debug simulation 
Total elapsed time: 3.3421189789660275. Arrivals time: 0.10456033004447818 Scheduler time: 2.9413198539987206 Scheduler overhead time: 0.10436780517920852 Adapter cache time: 0.04235510202124715 Engine time: 0.10017651692032814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 4320, 4320, 66, 4320, 66, 540, 66, 4320, 540, 4320, 66, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 4320, 66, 66, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 4320, 540]
Prompts retrieved: 107766 . Total input tokens: 24033613 . Total output tokens: 21116913
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.284214103128761,
    "estimated_duration": 3599.9525346932023,
    "input_throughput": 2467.116417343793,
    "output_throughput": 2164.153256166184,
    "total_throughput": 4631.269673509976,
    "itl": 32.86183437711645,
    "ttft": 7656.45653834625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.02746072160006,
    "arrivals": 36150,
    "finished_requests": 36074,
    "scheduler_time": 11.90088859244518
}
#Debug simulation 
Total elapsed time: 3.2843391122296453. Arrivals time: 0.10279496945440769 Scheduler time: 2.879205761477351 Scheduler overhead time: 0.1060285335406661 Adapter cache time: 0.04181910865008831 Engine time: 0.10401131631806493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 4320, 4320, 66, 4320, 66, 540, 66, 4320, 540, 4320, 66, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 4320, 66, 66, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 4320, 540]
Prompts retrieved: 107766 . Total input tokens: 24033613 . Total output tokens: 21116913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.2701161936856806,
    "estimated_duration": 3599.9306045591297,
    "input_throughput": 2467.131446576228,
    "output_throughput": 2164.1664398011685,
    "total_throughput": 4631.297886377396,
    "itl": 32.892918109581224,
    "ttft": 7656.893422028391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.288036794778318,
    "arrivals": 36150,
    "finished_requests": 36074,
    "scheduler_time": 11.91903072707003
}
#Debug simulation 
Total elapsed time: 3.270208336878568. Arrivals time: 0.10321652656421065 Scheduler time: 2.8654283257201314 Scheduler overhead time: 0.10469272639602423 Adapter cache time: 0.04188177967444062 Engine time: 0.10463699931278825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 4320, 4320, 66, 4320, 66, 540, 66, 4320, 540, 4320, 66, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 4320, 66, 66, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 4320, 540]
Prompts retrieved: 107766 . Total input tokens: 24033613 . Total output tokens: 21116913
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.2696961611509323,
    "estimated_duration": 3599.9578199967127,
    "input_throughput": 2467.1127952293923,
    "output_throughput": 2164.150078849289,
    "total_throughput": 4631.262874078681,
    "itl": 32.901753420036414,
    "ttft": 7657.193555257382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.920438866029166,
    "arrivals": 36150,
    "finished_requests": 36074,
    "scheduler_time": 11.924156122465288
}
#Debug simulation 
Total elapsed time: 3.2698238352313638. Arrivals time: 0.10226462781429291 Scheduler time: 2.8707161326892674 Scheduler overhead time: 0.10475525399670005 Adapter cache time: 0.04189033294096589 Engine time: 0.09995979070663452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 4320, 4320, 66, 4320, 66, 540, 66, 4320, 540, 4320, 66, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 4320, 66, 66, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 4320, 540]
Prompts retrieved: 107766 . Total input tokens: 24033613 . Total output tokens: 21116913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.297491007950157,
    "estimated_duration": 3599.9378593091683,
    "input_throughput": 2467.1264747065297,
    "output_throughput": 2164.162078479619,
    "total_throughput": 4631.288553186149,
    "itl": 32.86762329476346,
    "ttft": 7656.523417450073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.322873080494805,
    "arrivals": 36150,
    "finished_requests": 36074,
    "scheduler_time": 11.903253109146132
}
#Debug simulation 
Total elapsed time: 3.297584041953087. Arrivals time: 0.10272595658898354 Scheduler time: 2.8934426843188703 Scheduler overhead time: 0.10540936747565866 Adapter cache time: 0.041913426481187344 Engine time: 0.10387923195958138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 4320, 4320, 66, 4320, 66, 540, 66, 4320, 540, 4320, 66, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 4320, 66, 66, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 4320, 540]
Prompts retrieved: 107766 . Total input tokens: 24033613 . Total output tokens: 21116913
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.2803664677776396,
    "estimated_duration": 3599.9339228325325,
    "input_throughput": 2467.1291724743037,
    "output_throughput": 2164.1644449601267,
    "total_throughput": 4631.29361743443,
    "itl": 32.8972627937302,
    "ttft": 7657.183250032808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.63414956245164,
    "arrivals": 36150,
    "finished_requests": 36074,
    "scheduler_time": 11.921779782486068
}
#Debug simulation 
Total elapsed time: 3.280482050962746. Arrivals time: 0.10353441582992673 Scheduler time: 2.8763639512471855 Scheduler overhead time: 0.10488439025357366 Adapter cache time: 0.041898234747350216 Engine time: 0.10341217182576656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 4320, 4320, 66, 4320, 66, 540, 66, 4320, 540, 4320, 66, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 4320, 66, 66, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 4320, 540]
Prompts retrieved: 107766 . Total input tokens: 24033613 . Total output tokens: 21116913
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.2988694859668612,
    "estimated_duration": 3599.9486325012094,
    "input_throughput": 2467.119091593598,
    "output_throughput": 2164.1556020167413,
    "total_throughput": 4631.274693610339,
    "itl": 32.85953618654947,
    "ttft": 7656.576084344724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.211493697847896,
    "arrivals": 36150,
    "finished_requests": 36074,
    "scheduler_time": 11.90095165843576
}
#Debug simulation 
Total elapsed time: 3.2989615867845714. Arrivals time: 0.10340125299990177 Scheduler time: 2.891422361135483 Scheduler overhead time: 0.10697895335033536 Adapter cache time: 0.041721757501363754 Engine time: 0.10475350683555007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 4320, 4320, 66, 4320, 66, 540, 66, 4320, 540, 4320, 66, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 4320, 66, 66, 66, 4320, 540, 66, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 66, 540, 540, 4320, 4320, 540, 4320, 66, 4320, 540, 540, 540, 4320, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 4320, 540]
Prompts retrieved: 107766 . Total input tokens: 24033613 . Total output tokens: 21116913
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.3201333512552083,
    "estimated_duration": 3599.9507481371497,
    "input_throughput": 2467.117641705479,
    "output_throughput": 2164.154330175627,
    "total_throughput": 4631.271971881106,
    "itl": 32.8953733815672,
    "ttft": 7657.117353565728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.483989609405967,
    "arrivals": 36150,
    "finished_requests": 36074,
    "scheduler_time": 11.920785754526083
}
#Debug simulation 
Total elapsed time: 3.3202237272635102. Arrivals time: 0.10675664572045207 Scheduler time: 2.9089444978162646 Scheduler overhead time: 0.10704735713079572 Adapter cache time: 0.04199829651042819 Engine time: 0.10310821048915386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 4320, 4320, 33, 4320, 33, 540, 33, 4320, 540, 4320, 33, 540, 4320, 4320, 4320, 33, 4320, 540, 540, 33, 33, 33, 4320, 33, 33, 33, 4320, 540, 33, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 33, 540, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 4320, 540]
Prompts retrieved: 107073 . Total input tokens: 23864987 . Total output tokens: 20980197
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.235857330262661,
    "estimated_duration": 3599.927020150182,
    "input_throughput": 2458.9104030310914,
    "output_throughput": 2139.3339245190336,
    "total_throughput": 4598.244327550125,
    "itl": 32.563132517647276,
    "ttft": 8892.547883478936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2892,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.12308692039893,
    "arrivals": 35929,
    "finished_requests": 35841,
    "scheduler_time": 11.397201000953535
}
#Debug simulation 
Total elapsed time: 3.235979671124369. Arrivals time: 0.10238815099000931 Scheduler time: 2.8346316888928413 Scheduler overhead time: 0.10508379945531487 Adapter cache time: 0.04036157904192805 Engine time: 0.10312279127538204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 4320, 4320, 33, 4320, 33, 540, 33, 4320, 540, 4320, 33, 540, 4320, 4320, 4320, 33, 4320, 540, 540, 33, 33, 33, 4320, 33, 33, 33, 4320, 540, 33, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 33, 540, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 4320, 540]
Prompts retrieved: 107073 . Total input tokens: 23864987 . Total output tokens: 20980197
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.239686978980899,
    "estimated_duration": 3599.925680375547,
    "input_throughput": 2458.911318157147,
    "output_throughput": 2139.334720709173,
    "total_throughput": 4598.246038866319,
    "itl": 32.5827722288746,
    "ttft": 8892.263459297088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2910,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.250923531660327,
    "arrivals": 35929,
    "finished_requests": 35841,
    "scheduler_time": 11.407010758082064
}
#Debug simulation 
Total elapsed time: 3.239832559134811. Arrivals time: 0.1017238674685359 Scheduler time: 2.835910706780851 Scheduler overhead time: 0.10644563101232052 Adapter cache time: 0.040901947766542435 Engine time: 0.10401331400498748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 4320, 4320, 33, 4320, 33, 540, 33, 4320, 540, 4320, 33, 540, 4320, 4320, 4320, 33, 4320, 540, 540, 33, 33, 33, 4320, 33, 33, 33, 4320, 540, 33, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 33, 540, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 4320, 540]
Prompts retrieved: 107073 . Total input tokens: 23864987 . Total output tokens: 20980197
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.266923490911722,
    "estimated_duration": 3599.9329826986486,
    "input_throughput": 2458.9063303518155,
    "output_throughput": 2139.3303811524565,
    "total_throughput": 4598.2367115042725,
    "itl": 32.59812090509719,
    "ttft": 8892.738446723937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2894,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.69069085110942,
    "arrivals": 35929,
    "finished_requests": 35841,
    "scheduler_time": 11.41772820030061
}
#Debug simulation 
Total elapsed time: 3.2670287331566215. Arrivals time: 0.10449065221473575 Scheduler time: 2.8592478618957102 Scheduler overhead time: 0.10739630088210106 Adapter cache time: 0.04066991712898016 Engine time: 0.10446352511644363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 4320, 4320, 33, 4320, 33, 540, 33, 4320, 540, 4320, 33, 540, 4320, 4320, 4320, 33, 4320, 540, 540, 33, 33, 33, 4320, 33, 33, 33, 4320, 540, 33, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 33, 540, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 4320, 540]
Prompts retrieved: 107073 . Total input tokens: 23864987 . Total output tokens: 20980197
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.243512735236436,
    "estimated_duration": 3599.938769606221,
    "input_throughput": 2458.902377655791,
    "output_throughput": 2139.3269421753034,
    "total_throughput": 4598.2293198310945,
    "itl": 32.56805475832905,
    "ttft": 8892.628161139295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2895,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.312995485966926,
    "arrivals": 35929,
    "finished_requests": 35841,
    "scheduler_time": 11.39859698652711
}
#Debug simulation 
Total elapsed time: 3.243604344315827. Arrivals time: 0.10243568941950798 Scheduler time: 2.8406330100260675 Scheduler overhead time: 0.10609733872115612 Adapter cache time: 0.04054633853957057 Engine time: 0.10326985130086541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 4320, 4320, 33, 4320, 33, 540, 33, 4320, 540, 4320, 33, 540, 4320, 4320, 4320, 33, 4320, 540, 540, 33, 33, 33, 4320, 33, 33, 33, 4320, 540, 33, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 33, 540, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 4320, 540]
Prompts retrieved: 107073 . Total input tokens: 23864987 . Total output tokens: 20980197
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.2390023809857666,
    "estimated_duration": 3599.9442634350007,
    "input_throughput": 2458.898625156402,
    "output_throughput": 2139.3236773758886,
    "total_throughput": 4598.222302532291,
    "itl": 32.59714733911725,
    "ttft": 8892.78349111393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2895,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.427236305941186,
    "arrivals": 35929,
    "finished_requests": 35841,
    "scheduler_time": 11.415687162275992
}
#Debug simulation 
Total elapsed time: 3.239093618001789. Arrivals time: 0.10100142797455192 Scheduler time: 2.840838660951704 Scheduler overhead time: 0.10499121667817235 Adapter cache time: 0.04040440730750561 Engine time: 0.10131980432197452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 4320, 4320, 33, 4320, 33, 540, 33, 4320, 540, 4320, 33, 540, 4320, 4320, 4320, 33, 4320, 540, 540, 33, 33, 33, 4320, 33, 33, 33, 4320, 540, 33, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 33, 540, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 4320, 540]
Prompts retrieved: 107073 . Total input tokens: 23864987 . Total output tokens: 20980197
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.2437529116868973,
    "estimated_duration": 3599.93064626979,
    "input_throughput": 2458.90792623248,
    "output_throughput": 2139.3317696217723,
    "total_throughput": 4598.239695854252,
    "itl": 32.54718959689208,
    "ttft": 8891.918860442664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2906,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.551674253299336,
    "arrivals": 35929,
    "finished_requests": 35841,
    "scheduler_time": 11.385433013628054
}
#Debug simulation 
Total elapsed time: 3.2438459238037467. Arrivals time: 0.101565255317837 Scheduler time: 2.8378651938401163 Scheduler overhead time: 0.10678299842402339 Adapter cache time: 0.04088446404784918 Engine time: 0.10506592923775315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 4320, 4320, 33, 4320, 33, 540, 33, 4320, 540, 4320, 33, 540, 4320, 4320, 4320, 33, 4320, 540, 540, 33, 33, 33, 4320, 33, 33, 33, 4320, 540, 33, 4320, 4320, 4320, 4320, 540, 540, 540, 4320, 33, 540, 540, 4320, 4320, 540, 4320, 33, 4320, 540, 540, 540, 4320, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 4320, 540]
Prompts retrieved: 107073 . Total input tokens: 23864987 . Total output tokens: 20980197
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.2518228758126497,
    "estimated_duration": 3599.956255217781,
    "input_throughput": 2458.8904343407085,
    "output_throughput": 2139.316551093507,
    "total_throughput": 4598.206985434215,
    "itl": 32.58318149619413,
    "ttft": 8892.272542877437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2905,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.381655561942893,
    "arrivals": 35929,
    "finished_requests": 35841,
    "scheduler_time": 11.408254619989107
}
#Debug simulation 
Total elapsed time: 3.2519126958213747. Arrivals time: 0.10190309723839164 Scheduler time: 2.8484413577243686 Scheduler overhead time: 0.10565512627363205 Adapter cache time: 0.04061486246064305 Engine time: 0.10474398685619235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 4320, 4320, 135, 4320, 135, 270, 135, 4320, 270, 4320, 135, 270, 4320, 4320, 4320, 135, 4320, 270, 270, 135, 135, 135, 4320, 135, 135, 135, 4320, 270, 135, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 135, 270, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 4320, 270]
Prompts retrieved: 103545 . Total input tokens: 23105715 . Total output tokens: 20279491
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.141209878027439,
    "estimated_duration": 3599.8887289888476,
    "input_throughput": 2379.486602188958,
    "output_throughput": 2057.4747048030067,
    "total_throughput": 4436.961306991965,
    "itl": 31.443136753527604,
    "ttft": 5137.665626191768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.572223270242537,
    "arrivals": 34705,
    "finished_requests": 34656,
    "scheduler_time": 9.460170479363956
}
#Debug simulation 
Total elapsed time: 3.1413030959665775. Arrivals time: 0.09873989690095186 Scheduler time: 2.738167897798121 Scheduler overhead time: 0.10843155719339848 Adapter cache time: 0.03854520572349429 Engine time: 0.10552850738167763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 4320, 4320, 135, 4320, 135, 270, 135, 4320, 270, 4320, 135, 270, 4320, 4320, 4320, 135, 4320, 270, 270, 135, 135, 135, 4320, 135, 135, 135, 4320, 270, 135, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 135, 270, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 4320, 270]
Prompts retrieved: 103545 . Total input tokens: 23105715 . Total output tokens: 20279491
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.1252199946902692,
    "estimated_duration": 3599.905311769724,
    "input_throughput": 2379.4756412048473,
    "output_throughput": 2057.465227150337,
    "total_throughput": 4436.940868355185,
    "itl": 31.466857675023967,
    "ttft": 5137.851595811558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.226418062708902,
    "arrivals": 34705,
    "finished_requests": 34656,
    "scheduler_time": 9.47336315716579
}
#Debug simulation 
Total elapsed time: 3.1253159940242767. Arrivals time: 0.097994321025908 Scheduler time: 2.723006660118699 Scheduler overhead time: 0.1080850912258029 Adapter cache time: 0.03852442651987076 Engine time: 0.10585410101339221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 4320, 4320, 135, 4320, 135, 270, 135, 4320, 270, 4320, 135, 270, 4320, 4320, 4320, 135, 4320, 270, 270, 135, 135, 135, 4320, 135, 135, 135, 4320, 270, 135, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 135, 270, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 4320, 270]
Prompts retrieved: 103545 . Total input tokens: 23105715 . Total output tokens: 20279491
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.130157950799912,
    "estimated_duration": 3599.895835663258,
    "input_throughput": 2379.481904765111,
    "output_throughput": 2057.4706430735837,
    "total_throughput": 4436.952547838695,
    "itl": 31.473004039710567,
    "ttft": 5137.921210115773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.72216874148682,
    "arrivals": 34705,
    "finished_requests": 34656,
    "scheduler_time": 9.477432478620301
}
#Debug simulation 
Total elapsed time: 3.1302789198234677. Arrivals time: 0.09868867322802544 Scheduler time: 2.727325107436627 Scheduler overhead time: 0.10870068613439798 Adapter cache time: 0.03838493488729 Engine time: 0.1048379153944552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 4320, 4320, 135, 4320, 135, 270, 135, 4320, 270, 4320, 135, 270, 4320, 4320, 4320, 135, 4320, 270, 270, 135, 135, 135, 4320, 135, 135, 135, 4320, 270, 135, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 135, 270, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 4320, 270]
Prompts retrieved: 103545 . Total input tokens: 23105715 . Total output tokens: 20279491
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.0991893308237195,
    "estimated_duration": 3599.885176571803,
    "input_throughput": 2379.4889502996193,
    "output_throughput": 2057.4767351478235,
    "total_throughput": 4436.965685447443,
    "itl": 31.433797486922607,
    "ttft": 5137.628136318598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.984498520154004,
    "arrivals": 34705,
    "finished_requests": 34656,
    "scheduler_time": 9.451721981829893
}
#Debug simulation 
Total elapsed time: 3.099282608833164. Arrivals time: 0.09839705750346184 Scheduler time: 2.694400272797793 Scheduler overhead time: 0.10813793679699302 Adapter cache time: 0.03885931335389614 Engine time: 0.10735306376591325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 4320, 4320, 135, 4320, 135, 270, 135, 4320, 270, 4320, 135, 270, 4320, 4320, 4320, 135, 4320, 270, 270, 135, 135, 135, 4320, 135, 135, 135, 4320, 270, 135, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 135, 270, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 4320, 270]
Prompts retrieved: 103545 . Total input tokens: 23105715 . Total output tokens: 20279491
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.1319535663351417,
    "estimated_duration": 3599.907597801802,
    "input_throughput": 2379.474130177829,
    "output_throughput": 2057.463920608049,
    "total_throughput": 4436.938050785879,
    "itl": 31.46913424122424,
    "ttft": 5137.829056914919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.518107950053928,
    "arrivals": 34705,
    "finished_requests": 34656,
    "scheduler_time": 9.475765029448452
}
#Debug simulation 
Total elapsed time: 3.132045621983707. Arrivals time: 0.09822555724531412 Scheduler time: 2.728767146822065 Scheduler overhead time: 0.10834716958925128 Adapter cache time: 0.03827460156753659 Engine time: 0.10627009114250541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 4320, 4320, 135, 4320, 135, 270, 135, 4320, 270, 4320, 135, 270, 4320, 4320, 4320, 135, 4320, 270, 270, 135, 135, 135, 4320, 135, 135, 135, 4320, 270, 135, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 135, 270, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 4320, 270]
Prompts retrieved: 103545 . Total input tokens: 23105715 . Total output tokens: 20279491
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.1653927783481777,
    "estimated_duration": 3599.9072717663357,
    "input_throughput": 2379.4743456814235,
    "output_throughput": 2057.464106947907,
    "total_throughput": 4436.938452629331,
    "itl": 31.421660139637368,
    "ttft": 5137.621176199989,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.06605341974736,
    "arrivals": 34705,
    "finished_requests": 34656,
    "scheduler_time": 9.444285965196558
}
#Debug simulation 
Total elapsed time: 3.1655201143585145. Arrivals time: 0.10984771605581045 Scheduler time: 2.7498546079732478 Scheduler overhead time: 0.10789134539663792 Adapter cache time: 0.03869223315268755 Engine time: 0.10722440155223012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 4320, 4320, 135, 4320, 135, 270, 135, 4320, 270, 4320, 135, 270, 4320, 4320, 4320, 135, 4320, 270, 270, 135, 135, 135, 4320, 135, 135, 135, 4320, 270, 135, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 135, 270, 270, 4320, 4320, 270, 4320, 135, 4320, 270, 270, 270, 4320, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 4320, 270]
Prompts retrieved: 103545 . Total input tokens: 23105715 . Total output tokens: 20279491
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.096767348702997,
    "estimated_duration": 3599.9107186839283,
    "input_throughput": 2379.472067332702,
    "output_throughput": 2057.4621369242645,
    "total_throughput": 4436.934204256967,
    "itl": 31.46679086212695,
    "ttft": 5137.773756255338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.374370125997615,
    "arrivals": 34705,
    "finished_requests": 34656,
    "scheduler_time": 9.474601265905221
}
#Debug simulation 
Total elapsed time: 3.0968607887625694. Arrivals time: 0.09803233295679092 Scheduler time: 2.695452290587127 Scheduler overhead time: 0.10777905583381653 Adapter cache time: 0.03847973793745041 Engine time: 0.10536479530856013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 4320, 4320, 66, 4320, 66, 270, 66, 4320, 270, 4320, 66, 270, 4320, 4320, 4320, 66, 4320, 270, 270, 66, 66, 66, 4320, 66, 66, 66, 4320, 270, 66, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 66, 270, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 4320, 270]
Prompts retrieved: 102096 . Total input tokens: 22777251 . Total output tokens: 19994543
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.1560203973203897,
    "estimated_duration": 3599.7570258728424,
    "input_throughput": 2344.3971188454616,
    "output_throughput": 2061.5552512743793,
    "total_throughput": 4405.952370119841,
    "itl": 31.100311318615905,
    "ttft": 9202.284130859167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.32705667173054,
    "arrivals": 34232,
    "finished_requests": 34145,
    "scheduler_time": 9.275357873225744
}
#Debug simulation 
Total elapsed time: 3.1561955232173204. Arrivals time: 0.09695162996649742 Scheduler time: 2.753567522391677 Scheduler overhead time: 0.10982367489486933 Adapter cache time: 0.03604996297508478 Engine time: 0.10711757745593786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 4320, 4320, 66, 4320, 66, 270, 66, 4320, 270, 4320, 66, 270, 4320, 4320, 4320, 66, 4320, 270, 270, 66, 66, 66, 4320, 66, 66, 66, 4320, 270, 66, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 66, 270, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 4320, 270]
Prompts retrieved: 102096 . Total input tokens: 22777251 . Total output tokens: 19994543
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.097325613256544,
    "estimated_duration": 3599.76157381814,
    "input_throughput": 2344.3941569298922,
    "output_throughput": 2061.552646701738,
    "total_throughput": 4405.94680363163,
    "itl": 31.11576839680666,
    "ttft": 9202.370959723641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.577364798700255,
    "arrivals": 34232,
    "finished_requests": 34145,
    "scheduler_time": 9.285408715041282
}
#Debug simulation 
Total elapsed time: 3.0974209853447974. Arrivals time: 0.0949884639121592 Scheduler time: 2.7002370092086494 Scheduler overhead time: 0.10831469111144543 Adapter cache time: 0.036090804263949394 Engine time: 0.10603955062106252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 4320, 4320, 66, 4320, 66, 270, 66, 4320, 270, 4320, 66, 270, 4320, 4320, 4320, 66, 4320, 270, 270, 66, 66, 66, 4320, 66, 66, 66, 4320, 270, 66, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 66, 270, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 4320, 270]
Prompts retrieved: 102096 . Total input tokens: 22777251 . Total output tokens: 19994543
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.119542900007218,
    "estimated_duration": 3599.7374307551586,
    "input_throughput": 2344.37654477194,
    "output_throughput": 2061.4942458298256,
    "total_throughput": 4405.870790601766,
    "itl": 31.121145578855458,
    "ttft": 9412.635629395816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.920102267982228,
    "arrivals": 34232,
    "finished_requests": 34143,
    "scheduler_time": 9.2880590775194
}
#Debug simulation 
Total elapsed time: 3.1196517632342875. Arrivals time: 0.09755801502615213 Scheduler time: 2.717658349312842 Scheduler overhead time: 0.10846953932195902 Adapter cache time: 0.036269139498472214 Engine time: 0.10728470794856548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 4320, 4320, 66, 4320, 66, 270, 66, 4320, 270, 4320, 66, 270, 4320, 4320, 4320, 66, 4320, 270, 270, 66, 66, 66, 4320, 66, 66, 66, 4320, 270, 66, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 66, 270, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 4320, 270]
Prompts retrieved: 102096 . Total input tokens: 22777251 . Total output tokens: 19994543
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.127933682873845,
    "estimated_duration": 3599.7479110114473,
    "input_throughput": 2344.3697193864873,
    "output_throughput": 2061.4882440239858,
    "total_throughput": 4405.857963410473,
    "itl": 31.10343890488066,
    "ttft": 9412.672580876573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.513987533143005,
    "arrivals": 34232,
    "finished_requests": 34143,
    "scheduler_time": 9.276674934957656
}
#Debug simulation 
Total elapsed time: 3.128040671814233. Arrivals time: 0.09581547882407904 Scheduler time: 2.724963551852852 Scheduler overhead time: 0.10996457654982805 Adapter cache time: 0.0361377471126616 Engine time: 0.10800212621688843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 4320, 4320, 66, 4320, 66, 270, 66, 4320, 270, 4320, 66, 270, 4320, 4320, 4320, 66, 4320, 270, 270, 66, 66, 66, 4320, 66, 66, 66, 4320, 270, 66, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 66, 270, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 4320, 270]
Prompts retrieved: 102096 . Total input tokens: 22777251 . Total output tokens: 19994543
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.074609551113099,
    "estimated_duration": 3599.736998726503,
    "input_throughput": 2344.376826136342,
    "output_throughput": 2061.4944932436197,
    "total_throughput": 4405.871319379961,
    "itl": 31.12003019204617,
    "ttft": 9412.68576099466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.759741704300774,
    "arrivals": 34232,
    "finished_requests": 34143,
    "scheduler_time": 9.286722565951193
}
#Debug simulation 
Total elapsed time: 3.0747024877928197. Arrivals time: 0.09469408122822642 Scheduler time: 2.6757673588581383 Scheduler overhead time: 0.10951716871932149 Adapter cache time: 0.03627649508416653 Engine time: 0.10619299439713359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 4320, 4320, 66, 4320, 66, 270, 66, 4320, 270, 4320, 66, 270, 4320, 4320, 4320, 66, 4320, 270, 270, 66, 66, 66, 4320, 66, 66, 66, 4320, 270, 66, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 66, 270, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 4320, 270]
Prompts retrieved: 102096 . Total input tokens: 22777251 . Total output tokens: 19994543
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.083297995850444,
    "estimated_duration": 3599.752175536518,
    "input_throughput": 2344.3999999089347,
    "output_throughput": 2061.5574734374422,
    "total_throughput": 4405.957473346377,
    "itl": 31.095991836416644,
    "ttft": 9307.376406182908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.942040492138245,
    "arrivals": 34232,
    "finished_requests": 34144,
    "scheduler_time": 9.27218927766989
}
#Debug simulation 
Total elapsed time: 3.083395184017718. Arrivals time: 0.09488218789920211 Scheduler time: 2.6872928142547607 Scheduler overhead time: 0.10830890294164419 Adapter cache time: 0.03600990492850542 Engine time: 0.10507221473380923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 4320, 4320, 66, 4320, 66, 270, 66, 4320, 270, 4320, 66, 270, 4320, 4320, 4320, 66, 4320, 270, 270, 66, 66, 66, 4320, 66, 66, 66, 4320, 270, 66, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 66, 270, 270, 4320, 4320, 270, 4320, 66, 4320, 270, 270, 270, 4320, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 4320, 270]
Prompts retrieved: 102096 . Total input tokens: 22777251 . Total output tokens: 19994543
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.109121993649751,
    "estimated_duration": 3599.7557408806083,
    "input_throughput": 2344.364897918194,
    "output_throughput": 2061.4843156509946,
    "total_throughput": 4405.849213569189,
    "itl": 31.117833791154684,
    "ttft": 9307.484755383965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.676118233930119,
    "arrivals": 34232,
    "finished_requests": 34144,
    "scheduler_time": 9.286128411477739
}
#Debug simulation 
Total elapsed time: 3.1092182309366763. Arrivals time: 0.09607520140707493 Scheduler time: 2.705816843546927 Scheduler overhead time: 0.10929599590599537 Adapter cache time: 0.03611221257597208 Engine time: 0.10959759959951043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 4320, 4320, 33, 4320, 33, 270, 33, 4320, 270, 4320, 33, 270, 4320, 4320, 4320, 33, 4320, 270, 270, 33, 33, 33, 4320, 33, 33, 33, 4320, 270, 33, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 33, 270, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 4320, 270]
Prompts retrieved: 101403 . Total input tokens: 22610988 . Total output tokens: 19863471
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.0298169320449233,
    "estimated_duration": 3600.0222419473694,
    "input_throughput": 2345.65023004751,
    "output_throughput": 2019.1600249858316,
    "total_throughput": 4364.8102550333415,
    "itl": 30.709754838087168,
    "ttft": 9581.385596366952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.819427412445906,
    "arrivals": 33997,
    "finished_requests": 33907,
    "scheduler_time": 8.467096055531592
}
#Debug simulation 
Total elapsed time: 3.0299152210354805. Arrivals time: 0.09578016214072704 Scheduler time: 2.6275722812861204 Scheduler overhead time: 0.11072365613654256 Adapter cache time: 0.03518343064934015 Engine time: 0.1076287585310638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 4320, 4320, 33, 4320, 33, 270, 33, 4320, 270, 4320, 33, 270, 4320, 4320, 4320, 33, 4320, 270, 270, 33, 33, 33, 4320, 33, 33, 33, 4320, 270, 33, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 33, 270, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 4320, 270]
Prompts retrieved: 101403 . Total input tokens: 22610988 . Total output tokens: 19863471
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.0387816173024476,
    "estimated_duration": 3600.023763875527,
    "input_throughput": 2345.6492384120747,
    "output_throughput": 2019.1591713757728,
    "total_throughput": 4364.808409787847,
    "itl": 30.72097686721749,
    "ttft": 9581.427453369668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.857511655394811,
    "arrivals": 33997,
    "finished_requests": 33907,
    "scheduler_time": 8.475228923160106
}
#Debug simulation 
Total elapsed time: 3.0388734359294176. Arrivals time: 0.09533311426639557 Scheduler time: 2.639017848763615 Scheduler overhead time: 0.11068913387134671 Adapter cache time: 0.03518778085708618 Engine time: 0.10594044718891382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 4320, 4320, 33, 4320, 33, 270, 33, 4320, 270, 4320, 33, 270, 4320, 4320, 4320, 33, 4320, 270, 270, 33, 33, 33, 4320, 33, 33, 33, 4320, 270, 33, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 33, 270, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 4320, 270]
Prompts retrieved: 101403 . Total input tokens: 22610988 . Total output tokens: 19863471
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.0469222250394523,
    "estimated_duration": 3600.032738538959,
    "input_throughput": 2345.6433908506847,
    "output_throughput": 2019.1541377343326,
    "total_throughput": 4364.797528585017,
    "itl": 30.725842108403153,
    "ttft": 9581.324406008895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.154249950745008,
    "arrivals": 33997,
    "finished_requests": 33907,
    "scheduler_time": 8.477618538736067
}
#Debug simulation 
Total elapsed time: 3.0470144422724843. Arrivals time: 0.09603736875578761 Scheduler time: 2.6444345247000456 Scheduler overhead time: 0.11025890940800309 Adapter cache time: 0.03536599641665816 Engine time: 0.10780694242566824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 4320, 4320, 33, 4320, 33, 270, 33, 4320, 270, 4320, 33, 270, 4320, 4320, 4320, 33, 4320, 270, 270, 33, 33, 33, 4320, 33, 33, 33, 4320, 270, 33, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 33, 270, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 4320, 270]
Prompts retrieved: 101403 . Total input tokens: 22610988 . Total output tokens: 19863471
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.0531507381238043,
    "estimated_duration": 3600.028898801477,
    "input_throughput": 2345.6458926791697,
    "output_throughput": 2019.1562913342182,
    "total_throughput": 4364.802184013388,
    "itl": 30.709699040677002,
    "ttft": 9581.405647755062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.903803846729112,
    "arrivals": 33997,
    "finished_requests": 33907,
    "scheduler_time": 8.467686940881274
}
#Debug simulation 
Total elapsed time: 3.0532421520911157. Arrivals time: 0.09673523157835007 Scheduler time: 2.6501699038781226 Scheduler overhead time: 0.10968982335180044 Adapter cache time: 0.0353602534160018 Engine time: 0.10877524595707655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 4320, 4320, 33, 4320, 33, 270, 33, 4320, 270, 4320, 33, 270, 4320, 4320, 4320, 33, 4320, 270, 270, 33, 33, 33, 4320, 33, 33, 33, 4320, 270, 33, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 33, 270, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 4320, 270]
Prompts retrieved: 101403 . Total input tokens: 22610988 . Total output tokens: 19863471
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.0261870343238115,
    "estimated_duration": 3600.015152671584,
    "input_throughput": 2345.654849184006,
    "output_throughput": 2019.1640011863933,
    "total_throughput": 4364.818850370399,
    "itl": 30.723370435347128,
    "ttft": 9581.551250315068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.005593183431643,
    "arrivals": 33997,
    "finished_requests": 33907,
    "scheduler_time": 8.476347541893501
}
#Debug simulation 
Total elapsed time: 3.0262822983786464. Arrivals time: 0.09508314682170749 Scheduler time: 2.6276334482245147 Scheduler overhead time: 0.11005362216383219 Adapter cache time: 0.03523067990317941 Engine time: 0.10554431239143014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 4320, 4320, 33, 4320, 33, 270, 33, 4320, 270, 4320, 33, 270, 4320, 4320, 4320, 33, 4320, 270, 270, 33, 33, 33, 4320, 33, 33, 33, 4320, 270, 33, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 33, 270, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 4320, 270]
Prompts retrieved: 101403 . Total input tokens: 22610988 . Total output tokens: 19863471
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.0240271538496017,
    "estimated_duration": 3600.0086743148463,
    "input_throughput": 2345.6590702818617,
    "output_throughput": 2019.167634751169,
    "total_throughput": 4364.8267050330305,
    "itl": 30.70536480880357,
    "ttft": 9475.81011571454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.48650651768797,
    "arrivals": 33997,
    "finished_requests": 33907,
    "scheduler_time": 8.464241723538086
}
#Debug simulation 
Total elapsed time: 3.0241276347078383. Arrivals time: 0.094986901152879 Scheduler time: 2.6234554778784513 Scheduler overhead time: 0.1097448505461216 Adapter cache time: 0.0351811689324677 Engine time: 0.10774293728172779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 4320, 4320, 33, 4320, 33, 270, 33, 4320, 270, 4320, 33, 270, 4320, 4320, 4320, 33, 4320, 270, 270, 33, 33, 33, 4320, 33, 33, 33, 4320, 270, 33, 4320, 4320, 4320, 4320, 270, 270, 270, 4320, 33, 270, 270, 4320, 4320, 270, 4320, 33, 4320, 270, 270, 270, 4320, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 4320, 270]
Prompts retrieved: 101403 . Total input tokens: 22610988 . Total output tokens: 19863471
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.027557768393308,
    "estimated_duration": 3600.000669724186,
    "input_throughput": 2345.6642858477485,
    "output_throughput": 2019.1721243643312,
    "total_throughput": 4364.8364102120795,
    "itl": 30.723513337438906,
    "ttft": 9475.831685009054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.94428649915369,
    "arrivals": 33997,
    "finished_requests": 33907,
    "scheduler_time": 8.475779944171338
}
#Debug simulation 
Total elapsed time: 3.027701855171472. Arrivals time: 0.09553300915285945 Scheduler time: 2.6258653826080263 Scheduler overhead time: 0.10999212833121419 Adapter cache time: 0.03511349391192198 Engine time: 0.10824638744816184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 4320, 4320, 66, 4320, 66, 135, 66, 4320, 135, 4320, 66, 135, 4320, 4320, 4320, 66, 4320, 135, 135, 66, 66, 66, 4320, 66, 66, 66, 4320, 135, 66, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 66, 135, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 4320, 135]
Prompts retrieved: 99261 . Total input tokens: 22131967 . Total output tokens: 19445509
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.017577812075615,
    "estimated_duration": 3599.915324572507,
    "input_throughput": 2274.6087787418664,
    "output_throughput": 2003.0951702610698,
    "total_throughput": 4277.7039490029365,
    "itl": 30.206813471493017,
    "ttft": 6213.485670872347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1094,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.233975480953397,
    "arrivals": 33294,
    "finished_requests": 33237,
    "scheduler_time": 7.83165605880308
}
#Debug simulation 
Total elapsed time: 3.0177002009004354. Arrivals time: 0.09374952036887407 Scheduler time: 2.6161716720089316 Scheduler overhead time: 0.1135831749998033 Adapter cache time: 0.03250705450773239 Engine time: 0.10777558712288737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 4320, 4320, 66, 4320, 66, 135, 66, 4320, 135, 4320, 66, 135, 4320, 4320, 4320, 66, 4320, 135, 135, 66, 66, 66, 4320, 66, 66, 66, 4320, 135, 66, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 66, 135, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 4320, 135]
Prompts retrieved: 99261 . Total input tokens: 22131967 . Total output tokens: 19445509
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 2.993430753238499,
    "estimated_duration": 3599.9312918481846,
    "input_throughput": 2274.5986898533615,
    "output_throughput": 2003.086285654615,
    "total_throughput": 4277.684975507977,
    "itl": 30.215244277424972,
    "ttft": 6213.467089241693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1095,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.014900508229632,
    "arrivals": 33294,
    "finished_requests": 33237,
    "scheduler_time": 7.837813882745877
}
#Debug simulation 
Total elapsed time: 2.993528353050351. Arrivals time: 0.09390854369848967 Scheduler time: 2.5911831869743764 Scheduler overhead time: 0.11203984962776303 Adapter cache time: 0.03253187984228134 Engine time: 0.10969103593379259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 4320, 4320, 66, 4320, 66, 135, 66, 4320, 135, 4320, 66, 135, 4320, 4320, 4320, 66, 4320, 135, 135, 66, 66, 66, 4320, 66, 66, 66, 4320, 135, 66, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 66, 135, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 4320, 135]
Prompts retrieved: 99261 . Total input tokens: 22131967 . Total output tokens: 19445509
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.0088075175881386,
    "estimated_duration": 3599.9331727744816,
    "input_throughput": 2274.597501400053,
    "output_throughput": 2003.085239063612,
    "total_throughput": 4277.682740463665,
    "itl": 30.218137130313476,
    "ttft": 6213.349350767734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1095,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.23325646129437,
    "arrivals": 33294,
    "finished_requests": 33237,
    "scheduler_time": 7.839537804957464
}
#Debug simulation 
Total elapsed time: 3.008899458684027. Arrivals time: 0.09337748028337955 Scheduler time: 2.6112985191866755 Scheduler overhead time: 0.11135397711768746 Adapter cache time: 0.0324938022531569 Engine time: 0.10684079118072987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 4320, 4320, 66, 4320, 66, 135, 66, 4320, 135, 4320, 66, 135, 4320, 4320, 4320, 66, 4320, 135, 135, 66, 66, 66, 4320, 66, 66, 66, 4320, 135, 66, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 66, 135, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 4320, 135]
Prompts retrieved: 99261 . Total input tokens: 22131967 . Total output tokens: 19445509
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 3.0207534008659422,
    "estimated_duration": 3599.9340000217185,
    "input_throughput": 2274.596978708665,
    "output_throughput": 2003.0847787644152,
    "total_throughput": 4277.68175747308,
    "itl": 30.20885305229308,
    "ttft": 6213.357030940915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1094,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.383817149298248,
    "arrivals": 33294,
    "finished_requests": 33237,
    "scheduler_time": 7.83295465056032
}
#Debug simulation 
Total elapsed time: 3.0208826628513634. Arrivals time: 0.0934138661250472 Scheduler time: 2.620631381403655 Scheduler overhead time: 0.11138872290030122 Adapter cache time: 0.032767226453870535 Engine time: 0.10917045595124364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 4320, 4320, 66, 4320, 66, 135, 66, 4320, 135, 4320, 66, 135, 4320, 4320, 4320, 66, 4320, 135, 135, 66, 66, 66, 4320, 66, 66, 66, 4320, 135, 66, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 66, 135, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 4320, 135]
Prompts retrieved: 99261 . Total input tokens: 22131967 . Total output tokens: 19445509
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 3.035368016920984,
    "estimated_duration": 3599.9108543994316,
    "input_throughput": 2274.611603227064,
    "output_throughput": 2003.0976575954676,
    "total_throughput": 4277.709260822531,
    "itl": 30.216271241069734,
    "ttft": 6213.38712332163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1095,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.140053731817757,
    "arrivals": 33294,
    "finished_requests": 33237,
    "scheduler_time": 7.838766205729585
}
#Debug simulation 
Total elapsed time: 3.03546192497015. Arrivals time: 0.09427451062947512 Scheduler time: 2.632576354313642 Scheduler overhead time: 0.11316721187904477 Adapter cache time: 0.032671034801751375 Engine time: 0.10884086648002267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 4320, 4320, 66, 4320, 66, 135, 66, 4320, 135, 4320, 66, 135, 4320, 4320, 4320, 66, 4320, 135, 135, 66, 66, 66, 4320, 66, 66, 66, 4320, 135, 66, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 66, 135, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 4320, 135]
Prompts retrieved: 99261 . Total input tokens: 22131967 . Total output tokens: 19445509
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 3.013728516176343,
    "estimated_duration": 3599.9107724144756,
    "input_throughput": 2274.61165502944,
    "output_throughput": 2003.097703214341,
    "total_throughput": 4277.709358243781,
    "itl": 30.20295372425751,
    "ttft": 6213.393832425515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1095,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.990393429924699,
    "arrivals": 33294,
    "finished_requests": 33237,
    "scheduler_time": 7.829728103095462
}
#Debug simulation 
Total elapsed time: 3.0138338482938707. Arrivals time: 0.0978325423784554 Scheduler time: 2.608326209243387 Scheduler overhead time: 0.11139806546270847 Adapter cache time: 0.03262964868918061 Engine time: 0.10991649841889739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 4320, 4320, 66, 4320, 66, 135, 66, 4320, 135, 4320, 66, 135, 4320, 4320, 4320, 66, 4320, 135, 135, 66, 66, 66, 4320, 66, 66, 66, 4320, 135, 66, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 66, 135, 135, 4320, 4320, 135, 4320, 66, 4320, 135, 135, 135, 4320, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 4320, 135]
Prompts retrieved: 99261 . Total input tokens: 22131967 . Total output tokens: 19445509
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 2.995616147760302,
    "estimated_duration": 3599.9085035559187,
    "input_throughput": 2274.6130886136857,
    "output_throughput": 2003.0989656756951,
    "total_throughput": 4277.712054289381,
    "itl": 30.215934982428728,
    "ttft": 6213.500421285045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1095,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.080403984952728,
    "arrivals": 33294,
    "finished_requests": 33237,
    "scheduler_time": 7.838240964426248
}
#Debug simulation 
Total elapsed time: 2.9957060730084777. Arrivals time: 0.09278794191777706 Scheduler time: 2.5998860583640635 Scheduler overhead time: 0.11110647954046726 Adapter cache time: 0.03248098352923989 Engine time: 0.10614652978256345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 4320, 4320, 33, 4320, 33, 135, 33, 4320, 135, 4320, 33, 135, 4320, 4320, 4320, 33, 4320, 135, 135, 33, 33, 33, 4320, 33, 33, 33, 4320, 135, 33, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 33, 135, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 4320, 135]
Prompts retrieved: 98568 . Total input tokens: 21971033 . Total output tokens: 19315384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 2.950363175943494,
    "estimated_duration": 3599.6923068011533,
    "input_throughput": 2255.836696002519,
    "output_throughput": 1977.1345419032634,
    "total_throughput": 4232.971237905783,
    "itl": 29.897366973673165,
    "ttft": 7457.540228641392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.547811177806109,
    "arrivals": 33045,
    "finished_requests": 32977,
    "scheduler_time": 7.298647055136655
}
#Debug simulation 
Total elapsed time: 2.9504556423053145. Arrivals time: 0.09224206674844027 Scheduler time: 2.553165369667113 Scheduler overhead time: 0.11159761715680361 Adapter cache time: 0.031027864199131727 Engine time: 0.1088785664178431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 4320, 4320, 33, 4320, 33, 135, 33, 4320, 135, 4320, 33, 135, 4320, 4320, 4320, 33, 4320, 135, 135, 33, 33, 33, 4320, 33, 33, 33, 4320, 135, 33, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 33, 135, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 4320, 135]
Prompts retrieved: 98568 . Total input tokens: 21971033 . Total output tokens: 19315384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.0027463887818158,
    "estimated_duration": 3599.6865954968303,
    "input_throughput": 2255.840275139072,
    "output_throughput": 1977.1376788477603,
    "total_throughput": 4232.977953986832,
    "itl": 29.903390312057567,
    "ttft": 7457.675084509369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 838,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.146564586879695,
    "arrivals": 33045,
    "finished_requests": 32977,
    "scheduler_time": 7.303205887297164
}
#Debug simulation 
Total elapsed time: 3.002847578842193. Arrivals time: 0.09375878004357219 Scheduler time: 2.6023379396647215 Scheduler overhead time: 0.1122465138323605 Adapter cache time: 0.03137970482930541 Engine time: 0.10951777175068855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 4320, 4320, 33, 4320, 33, 135, 33, 4320, 135, 4320, 33, 135, 4320, 4320, 4320, 33, 4320, 135, 135, 33, 33, 33, 4320, 33, 33, 33, 4320, 135, 33, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 33, 135, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 4320, 135]
Prompts retrieved: 98568 . Total input tokens: 21971033 . Total output tokens: 19315384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 2.970105384942144,
    "estimated_duration": 3599.694786794201,
    "input_throughput": 2255.835141854278,
    "output_throughput": 1977.13317976558,
    "total_throughput": 4232.968321619858,
    "itl": 29.906251396540586,
    "ttft": 7457.731335355504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 838,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.316624598959487,
    "arrivals": 33045,
    "finished_requests": 32977,
    "scheduler_time": 7.304528289282617
}
#Debug simulation 
Total elapsed time: 2.970218155067414. Arrivals time: 0.09278863854706287 Scheduler time: 2.569490731228143 Scheduler overhead time: 0.11190861742943525 Adapter cache time: 0.031232934445142746 Engine time: 0.1106175547465682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 4320, 4320, 33, 4320, 33, 135, 33, 4320, 135, 4320, 33, 135, 4320, 4320, 4320, 33, 4320, 135, 135, 33, 33, 33, 4320, 33, 33, 33, 4320, 135, 33, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 33, 135, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 4320, 135]
Prompts retrieved: 98568 . Total input tokens: 21971033 . Total output tokens: 19315384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 2.9823984187096357,
    "estimated_duration": 3599.688436026525,
    "input_throughput": 2255.839121722301,
    "output_throughput": 1977.1366679323235,
    "total_throughput": 4232.975789654624,
    "itl": 29.898146936493376,
    "ttft": 7457.673478567339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 838,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.6370874896738545,
    "arrivals": 33045,
    "finished_requests": 32977,
    "scheduler_time": 7.2993003958233205
}
#Debug simulation 
Total elapsed time: 2.9825156396254897. Arrivals time: 0.09201131714507937 Scheduler time: 2.585846634581685 Scheduler overhead time: 0.11217282526195049 Adapter cache time: 0.031075242441147566 Engine time: 0.10725162830203772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 4320, 4320, 33, 4320, 33, 135, 33, 4320, 135, 4320, 33, 135, 4320, 4320, 4320, 33, 4320, 135, 135, 33, 33, 33, 4320, 33, 33, 33, 4320, 135, 33, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 33, 135, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 4320, 135]
Prompts retrieved: 98568 . Total input tokens: 21971033 . Total output tokens: 19315384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 2.9614099552854896,
    "estimated_duration": 3599.7001144963992,
    "input_throughput": 2255.8318031267554,
    "output_throughput": 1977.1302535282678,
    "total_throughput": 4232.962056655023,
    "itl": 29.905173011724077,
    "ttft": 7457.629360437577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.24868866851556,
    "arrivals": 33045,
    "finished_requests": 32977,
    "scheduler_time": 7.304112633162364
}
#Debug simulation 
Total elapsed time: 2.961502328980714. Arrivals time: 0.09223659848794341 Scheduler time: 2.564647324383259 Scheduler overhead time: 0.11150309117510915 Adapter cache time: 0.031161085702478886 Engine time: 0.10815225820988417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 4320, 4320, 33, 4320, 33, 135, 33, 4320, 135, 4320, 33, 135, 4320, 4320, 4320, 33, 4320, 135, 135, 33, 33, 33, 4320, 33, 33, 33, 4320, 135, 33, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 33, 135, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 4320, 135]
Prompts retrieved: 98568 . Total input tokens: 21971033 . Total output tokens: 19315384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 2.9595374930649996,
    "estimated_duration": 3599.6895254852543,
    "input_throughput": 2255.8384389846356,
    "output_throughput": 1977.1360695449384,
    "total_throughput": 4232.974508529574,
    "itl": 29.894753645351354,
    "ttft": 7457.542609897681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.356109669138651,
    "arrivals": 33045,
    "finished_requests": 32977,
    "scheduler_time": 7.297240170802304
}
#Debug simulation 
Total elapsed time: 2.9596873219124973. Arrivals time: 0.09271177183836699 Scheduler time: 2.559436538256705 Scheduler overhead time: 0.1120301503688097 Adapter cache time: 0.031261245254427195 Engine time: 0.11038495786488056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 4320, 4320, 33, 4320, 33, 135, 33, 4320, 135, 4320, 33, 135, 4320, 4320, 4320, 33, 4320, 135, 135, 33, 33, 33, 4320, 33, 33, 33, 4320, 135, 33, 4320, 4320, 4320, 4320, 135, 135, 135, 4320, 33, 135, 135, 4320, 4320, 135, 4320, 33, 4320, 135, 135, 135, 4320, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 4320, 135]
Prompts retrieved: 98568 . Total input tokens: 21971033 . Total output tokens: 19315384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 3.008991653099656,
    "estimated_duration": 3599.6729888046257,
    "input_throughput": 2255.848802170384,
    "output_throughput": 1977.1451523888086,
    "total_throughput": 4232.993954559192,
    "itl": 29.90483532909841,
    "ttft": 7457.748563804844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 838,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.1977393395826486,
    "arrivals": 33045,
    "finished_requests": 32977,
    "scheduler_time": 7.303700771606194
}
#Debug simulation 
Total elapsed time: 3.009111935738474. Arrivals time: 0.09477111883461475 Scheduler time: 2.604962387587875 Scheduler overhead time: 0.11229190276935697 Adapter cache time: 0.03129933122545481 Engine time: 0.11163890501484275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 4320, 4320, 33, 4320, 33, 66, 33, 4320, 66, 4320, 33, 66, 4320, 4320, 4320, 33, 4320, 66, 66, 33, 33, 33, 4320, 33, 33, 33, 4320, 66, 33, 4320, 4320, 4320, 4320, 66, 66, 66, 4320, 33, 66, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 4320, 66]
Prompts retrieved: 97119 . Total input tokens: 21650533 . Total output tokens: 19030301
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 2.9338575890287757,
    "estimated_duration": 3599.932277266739,
    "input_throughput": 2246.733376368099,
    "output_throughput": 1954.9012197927386,
    "total_throughput": 4201.634596160838,
    "itl": 29.445355817063934,
    "ttft": 5689.681004052704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.438452696614011,
    "arrivals": 32548,
    "finished_requests": 32497,
    "scheduler_time": 6.68331074035585
}
#Debug simulation 
Total elapsed time: 2.933949591126293. Arrivals time: 0.0907388417981565 Scheduler time: 2.538607364986092 Scheduler overhead time: 0.1131354016251862 Adapter cache time: 0.02881269995123148 Engine time: 0.108434799592942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 4320, 4320, 33, 4320, 33, 66, 33, 4320, 66, 4320, 33, 66, 4320, 4320, 4320, 33, 4320, 66, 66, 33, 33, 33, 4320, 33, 33, 33, 4320, 66, 33, 4320, 4320, 4320, 4320, 66, 66, 66, 4320, 33, 66, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 4320, 66]
Prompts retrieved: 97119 . Total input tokens: 21650533 . Total output tokens: 19030301
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 2.9374144482426345,
    "estimated_duration": 3599.9348292884924,
    "input_throughput": 2246.7315058585564,
    "output_throughput": 1954.7017748070693,
    "total_throughput": 4201.433280665626,
    "itl": 29.460269600674444,
    "ttft": 5800.452915527115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.801908750762237,
    "arrivals": 32548,
    "finished_requests": 32496,
    "scheduler_time": 6.6951491427606005
}
#Debug simulation 
Total elapsed time: 2.9375174790620804. Arrivals time: 0.09352174866944551 Scheduler time: 2.5343552618287504 Scheduler overhead time: 0.11355100525543094 Adapter cache time: 0.028633964713662863 Engine time: 0.11312730191275477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 4320, 4320, 33, 4320, 33, 66, 33, 4320, 66, 4320, 33, 66, 4320, 4320, 4320, 33, 4320, 66, 66, 33, 33, 33, 4320, 33, 33, 33, 4320, 66, 33, 4320, 4320, 4320, 4320, 66, 66, 66, 4320, 33, 66, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 4320, 66]
Prompts retrieved: 97119 . Total input tokens: 21650533 . Total output tokens: 19030301
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 2.98147951439023,
    "estimated_duration": 3599.9367821341516,
    "input_throughput": 2246.730287081635,
    "output_throughput": 1954.7007144465388,
    "total_throughput": 4201.4310015281735,
    "itl": 29.461813899084742,
    "ttft": 5800.45408176366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9061114130355774,
    "arrivals": 32548,
    "finished_requests": 32496,
    "scheduler_time": 6.69595702013881
}
#Debug simulation 
Total elapsed time: 2.9815703574568033. Arrivals time: 0.0918775568716228 Scheduler time: 2.5797893069684505 Scheduler overhead time: 0.11446149460971355 Adapter cache time: 0.02871373062953353 Engine time: 0.11166926380246878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 4320, 4320, 33, 4320, 33, 66, 33, 4320, 66, 4320, 33, 66, 4320, 4320, 4320, 33, 4320, 66, 66, 33, 33, 33, 4320, 33, 33, 33, 4320, 66, 33, 4320, 4320, 4320, 4320, 66, 66, 66, 4320, 33, 66, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 4320, 66]
Prompts retrieved: 97119 . Total input tokens: 21650533 . Total output tokens: 19030301
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 2.9376096851192415,
    "estimated_duration": 3599.9568109580255,
    "input_throughput": 2246.7180648891135,
    "output_throughput": 1954.8878971487347,
    "total_throughput": 4201.605962037848,
    "itl": 29.45730175448835,
    "ttft": 5689.780197245028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5145470092864635,
    "arrivals": 32548,
    "finished_requests": 32497,
    "scheduler_time": 6.692943054406124
}
#Debug simulation 
Total elapsed time: 2.9377012699842453. Arrivals time: 0.08961278479546309 Scheduler time: 2.541100548580289 Scheduler overhead time: 0.11310246028006077 Adapter cache time: 0.0285087781958282 Engine time: 0.11099675949662924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 4320, 4320, 33, 4320, 33, 66, 33, 4320, 66, 4320, 33, 66, 4320, 4320, 4320, 33, 4320, 66, 66, 33, 33, 33, 4320, 33, 33, 33, 4320, 66, 33, 4320, 4320, 4320, 4320, 66, 66, 66, 4320, 33, 66, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 4320, 66]
Prompts retrieved: 97119 . Total input tokens: 21650533 . Total output tokens: 19030301
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 2.948619387112558,
    "estimated_duration": 3599.936922883601,
    "input_throughput": 2246.7301992395264,
    "output_throughput": 1954.7006380221305,
    "total_throughput": 4201.430837261657,
    "itl": 29.461615652920674,
    "ttft": 5800.323005181981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.863238157476334,
    "arrivals": 32548,
    "finished_requests": 32496,
    "scheduler_time": 6.695555625034745
}
#Debug simulation 
Total elapsed time: 2.948714063037187. Arrivals time: 0.09383771847933531 Scheduler time: 2.546487310901284 Scheduler overhead time: 0.1135943210683763 Adapter cache time: 0.028654619585722685 Engine time: 0.11158871278166771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 4320, 4320, 33, 4320, 33, 66, 33, 4320, 66, 4320, 33, 66, 4320, 4320, 4320, 33, 4320, 66, 66, 33, 33, 33, 4320, 33, 33, 33, 4320, 66, 33, 4320, 4320, 4320, 4320, 66, 66, 66, 4320, 33, 66, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 4320, 66]
Prompts retrieved: 97119 . Total input tokens: 21650533 . Total output tokens: 19030301
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 2.954595232848078,
    "estimated_duration": 3599.932604048183,
    "input_throughput": 2246.7331724223986,
    "output_throughput": 1954.9010423379048,
    "total_throughput": 4201.634214760303,
    "itl": 29.4443084312751,
    "ttft": 5689.688888236614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3196388890966624,
    "arrivals": 32548,
    "finished_requests": 32497,
    "scheduler_time": 6.682454783901138
}
#Debug simulation 
Total elapsed time: 2.954691657796502. Arrivals time: 0.09172653034329414 Scheduler time: 2.5523459003306925 Scheduler overhead time: 0.11415718216449022 Adapter cache time: 0.02902435977011919 Engine time: 0.11280983220785856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.4-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 4320, 4320, 33, 4320, 33, 66, 33, 4320, 66, 4320, 33, 66, 4320, 4320, 4320, 33, 4320, 66, 66, 33, 33, 33, 4320, 33, 33, 33, 4320, 66, 33, 4320, 4320, 4320, 4320, 66, 66, 66, 4320, 33, 66, 66, 4320, 4320, 66, 4320, 33, 4320, 66, 66, 66, 4320, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 4320, 66]
Prompts retrieved: 97119 . Total input tokens: 21650533 . Total output tokens: 19030301
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 2.9372546640224755,
    "estimated_duration": 3599.928134580841,
    "input_throughput": 2246.7356840560205,
    "output_throughput": 1954.7054099232269,
    "total_throughput": 4201.441093979248,
    "itl": 29.46102967064393,
    "ttft": 5800.378863831468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8332061668672024,
    "arrivals": 32548,
    "finished_requests": 32496,
    "scheduler_time": 6.695389204163053
}
#Debug simulation 
Total elapsed time: 2.937377983238548. Arrivals time: 0.09121871693059802 Scheduler time: 2.5391603247262537 Scheduler overhead time: 0.11287562269717455 Adapter cache time: 0.028651597909629345 Engine time: 0.11100310645997524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 1080, 1080, 270, 1080, 270, 540, 270, 1080, 540, 1080, 270, 540, 1080, 1080, 1080, 270, 1080, 540, 540, 270, 270, 270, 1080, 270, 270, 270, 1080, 540, 270, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 270, 540, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 1080, 540]
Prompts retrieved: 40770 . Total input tokens: 9042678 . Total output tokens: 8015864
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.572656482923776,
    "estimated_duration": 3599.616577965385,
    "input_throughput": 939.564791623349,
    "output_throughput": 834.6502842528278,
    "total_throughput": 1774.2150758761768,
    "itl": 25.278599693324757,
    "ttft": 6046.030136947607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.738544581511945,
    "arrivals": 13796,
    "finished_requests": 13773,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5727783129550517. Arrivals time: 0.04869924811646342 Scheduler time: 1.1591168567538261 Scheduler overhead time: 0.12388373631983995 Adapter cache time: 0.06104733608663082 Engine time: 0.11912970105186105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 1080, 1080, 270, 1080, 270, 540, 270, 1080, 540, 1080, 270, 540, 1080, 1080, 1080, 270, 1080, 540, 540, 270, 270, 270, 1080, 270, 270, 270, 1080, 540, 270, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 270, 540, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 1080, 540]
Prompts retrieved: 40770 . Total input tokens: 9042678 . Total output tokens: 8015864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.5819390751421452,
    "estimated_duration": 3599.6266615631007,
    "input_throughput": 939.5621596300127,
    "output_throughput": 834.6479461554386,
    "total_throughput": 1774.2101057854513,
    "itl": 25.313095413909224,
    "ttft": 6046.442602386982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 40.543295734071826,
    "arrivals": 13796,
    "finished_requests": 13773,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5820271652191877. Arrivals time: 0.04847741639241576 Scheduler time: 1.1647347412072122 Scheduler overhead time: 0.12486971681937575 Adapter cache time: 0.061494764406234026 Engine time: 0.12005165265873075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 1080, 1080, 270, 1080, 270, 540, 270, 1080, 540, 1080, 270, 540, 1080, 1080, 1080, 270, 1080, 540, 540, 270, 270, 270, 1080, 270, 270, 270, 1080, 540, 270, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 270, 540, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 1080, 540]
Prompts retrieved: 40770 . Total input tokens: 9042678 . Total output tokens: 8015864
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.5854159849695861,
    "estimated_duration": 3599.61266941273,
    "input_throughput": 939.5658118271316,
    "output_throughput": 834.6511905377213,
    "total_throughput": 1774.2170023648528,
    "itl": 25.323779004064296,
    "ttft": 6046.490992656927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.61089204980476,
    "arrivals": 13796,
    "finished_requests": 13773,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5855055111460388. Arrivals time: 0.048549255821853876 Scheduler time: 1.1692630341276526 Scheduler overhead time: 0.12388033187016845 Adapter cache time: 0.06120936060324311 Engine time: 0.12130834674462676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 1080, 1080, 270, 1080, 270, 540, 270, 1080, 540, 1080, 270, 540, 1080, 1080, 1080, 270, 1080, 540, 540, 270, 270, 270, 1080, 270, 270, 270, 1080, 540, 270, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 270, 540, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 1080, 540]
Prompts retrieved: 40770 . Total input tokens: 9042678 . Total output tokens: 8015864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 1.5865071476437151,
    "estimated_duration": 3599.624999783426,
    "input_throughput": 939.5625933822231,
    "output_throughput": 834.6483314736294,
    "total_throughput": 1774.2109248558525,
    "itl": 25.289581443313818,
    "ttft": 6046.144244706842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.03504481073595,
    "arrivals": 13796,
    "finished_requests": 13773,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5866006650030613. Arrivals time: 0.048746553249657154 Scheduler time: 1.1697957352735102 Scheduler overhead time: 0.12419172702357173 Adapter cache time: 0.061508893966674805 Engine time: 0.12125623831525445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 1080, 1080, 270, 1080, 270, 540, 270, 1080, 540, 1080, 270, 540, 1080, 1080, 1080, 270, 1080, 540, 540, 270, 270, 270, 1080, 270, 270, 270, 1080, 540, 270, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 270, 540, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 1080, 540]
Prompts retrieved: 40770 . Total input tokens: 9042678 . Total output tokens: 8015864
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 1.600623263977468,
    "estimated_duration": 3599.6171059959233,
    "input_throughput": 939.5646537978838,
    "output_throughput": 834.6501618173504,
    "total_throughput": 1774.214815615234,
    "itl": 25.31906808939644,
    "ttft": 6046.548530138893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.228501872792414,
    "arrivals": 13796,
    "finished_requests": 13773,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6007933970540762. Arrivals time: 0.04899833770468831 Scheduler time: 1.18449553148821 Scheduler overhead time: 0.12477587116882205 Adapter cache time: 0.06112172966822982 Engine time: 0.11943396693095565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 1080, 1080, 270, 1080, 270, 540, 270, 1080, 540, 1080, 270, 540, 1080, 1080, 1080, 270, 1080, 540, 540, 270, 270, 270, 1080, 270, 270, 270, 1080, 540, 270, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 270, 540, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 1080, 540]
Prompts retrieved: 40770 . Total input tokens: 9042678 . Total output tokens: 8015864
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.5705135301686823,
    "estimated_duration": 3599.6236444218725,
    "input_throughput": 939.5629471544898,
    "output_throughput": 834.6486457426672,
    "total_throughput": 1774.211592897157,
    "itl": 25.263957429243273,
    "ttft": 6045.988709206652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.48821650863522,
    "arrivals": 13796,
    "finished_requests": 13773,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5705909323878586. Arrivals time: 0.04884866997599602 Scheduler time: 1.1566887004300952 Scheduler overhead time: 0.12368264142423868 Adapter cache time: 0.06078628543764353 Engine time: 0.11969364993274212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 1080, 1080, 270, 1080, 270, 540, 270, 1080, 540, 1080, 270, 540, 1080, 1080, 1080, 270, 1080, 540, 540, 270, 270, 270, 1080, 270, 270, 270, 1080, 540, 270, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 270, 540, 540, 1080, 1080, 540, 1080, 270, 1080, 540, 540, 540, 1080, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 1080, 540]
Prompts retrieved: 40770 . Total input tokens: 9042678 . Total output tokens: 8015864
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.5754064861685038,
    "estimated_duration": 3599.6259218552395,
    "input_throughput": 939.5623527060519,
    "output_throughput": 834.6481176720519,
    "total_throughput": 1774.2104703781038,
    "itl": 25.316606969942335,
    "ttft": 6046.420873647129,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 40.87500023174996,
    "arrivals": 13796,
    "finished_requests": 13773,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.57550231507048. Arrivals time: 0.04885499970987439 Scheduler time: 1.158624482806772 Scheduler overhead time: 0.12396399676799774 Adapter cache time: 0.06107830163091421 Engine time: 0.12178336549550295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 1080, 1080, 135, 1080, 135, 540, 135, 1080, 540, 1080, 135, 540, 1080, 1080, 1080, 135, 1080, 540, 540, 135, 135, 135, 1080, 135, 135, 135, 1080, 540, 135, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 135, 540, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 1080, 540]
Prompts retrieved: 37935 . Total input tokens: 8403910 . Total output tokens: 7439174
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.4905065009370446,
    "estimated_duration": 3599.535053202045,
    "input_throughput": 870.057369552225,
    "output_throughput": 759.8939750751379,
    "total_throughput": 1629.951344627363,
    "itl": 24.476616709375193,
    "ttft": 3983.175375048577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.578831836085104,
    "arrivals": 12790,
    "finished_requests": 12776,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4905860829167068. Arrivals time: 0.04581582034006715 Scheduler time: 1.0743257170543075 Scheduler overhead time: 0.12727705854922533 Adapter cache time: 0.05689077824354172 Engine time: 0.12293900968506932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 1080, 1080, 135, 1080, 135, 540, 135, 1080, 540, 1080, 135, 540, 1080, 1080, 1080, 135, 1080, 540, 540, 135, 135, 135, 1080, 135, 135, 135, 1080, 540, 135, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 135, 540, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 1080, 540]
Prompts retrieved: 37935 . Total input tokens: 8403910 . Total output tokens: 7439174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.488928349222988,
    "estimated_duration": 3599.535514104753,
    "input_throughput": 870.057258145685,
    "output_throughput": 759.8938777744752,
    "total_throughput": 1629.9511359201601,
    "itl": 24.50303146959128,
    "ttft": 3983.339246970933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.59687151939518,
    "arrivals": 12790,
    "finished_requests": 12776,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4890357162803411. Arrivals time: 0.04705977253615856 Scheduler time: 1.0698005645535886 Scheduler overhead time: 0.12527220556512475 Adapter cache time: 0.05657254857942462 Engine time: 0.12430187314748764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 1080, 1080, 135, 1080, 135, 540, 135, 1080, 540, 1080, 135, 540, 1080, 1080, 1080, 135, 1080, 540, 540, 135, 135, 135, 1080, 135, 135, 135, 1080, 540, 135, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 135, 540, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 1080, 540]
Prompts retrieved: 37935 . Total input tokens: 8403910 . Total output tokens: 7439174
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4902386358007789,
    "estimated_duration": 3599.5400785896727,
    "input_throughput": 870.0561548482783,
    "output_throughput": 759.89291417244,
    "total_throughput": 1629.9490690207183,
    "itl": 24.50881181922146,
    "ttft": 3983.4003647673044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 32.40698213068905,
    "arrivals": 12790,
    "finished_requests": 12776,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4903239477425814. Arrivals time: 0.04554136376827955 Scheduler time: 1.071579609066248 Scheduler overhead time: 0.12986368034034967 Adapter cache time: 0.056750175543129444 Engine time: 0.12311311718076468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 1080, 1080, 135, 1080, 135, 540, 135, 1080, 540, 1080, 135, 540, 1080, 1080, 1080, 135, 1080, 540, 540, 135, 135, 135, 1080, 135, 135, 135, 1080, 540, 135, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 135, 540, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 1080, 540]
Prompts retrieved: 37935 . Total input tokens: 8403910 . Total output tokens: 7439174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4754006257280707,
    "estimated_duration": 3599.5262290918295,
    "input_throughput": 870.0595024668462,
    "output_throughput": 759.895837928125,
    "total_throughput": 1629.9553403949712,
    "itl": 24.484257856982108,
    "ttft": 3983.0866144651604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.45957788210458,
    "arrivals": 12790,
    "finished_requests": 12776,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4754977077245712. Arrivals time: 0.04599162004888058 Scheduler time: 1.0588900418952107 Scheduler overhead time: 0.12641917448490858 Adapter cache time: 0.0572325736284256 Engine time: 0.12386645562946796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 1080, 1080, 135, 1080, 135, 540, 135, 1080, 540, 1080, 135, 540, 1080, 1080, 1080, 135, 1080, 540, 540, 135, 135, 135, 1080, 135, 135, 135, 1080, 540, 135, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 135, 540, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 1080, 540]
Prompts retrieved: 37935 . Total input tokens: 8403910 . Total output tokens: 7439174
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4805806400254369,
    "estimated_duration": 3599.5337042014194,
    "input_throughput": 870.0576956244423,
    "output_throughput": 759.8942598613163,
    "total_throughput": 1629.9519554857586,
    "itl": 24.506572821094615,
    "ttft": 3983.468002319336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 32.132699637298046,
    "arrivals": 12790,
    "finished_requests": 12776,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4806731841526926. Arrivals time: 0.04584199422970414 Scheduler time: 1.0677629401907325 Scheduler overhead time: 0.12573085771873593 Adapter cache time: 0.05634203227236867 Engine time: 0.12176478235051036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 1080, 1080, 135, 1080, 135, 540, 135, 1080, 540, 1080, 135, 540, 1080, 1080, 1080, 135, 1080, 540, 540, 135, 135, 135, 1080, 135, 135, 135, 1080, 540, 135, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 135, 540, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 1080, 540]
Prompts retrieved: 37935 . Total input tokens: 8403910 . Total output tokens: 7439174
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.4678937164135277,
    "estimated_duration": 3599.544951912592,
    "input_throughput": 870.0549769036611,
    "output_throughput": 759.8918853747435,
    "total_throughput": 1629.9468622784048,
    "itl": 24.46910570125834,
    "ttft": 3982.9816314803056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.59769022608899,
    "arrivals": 12790,
    "finished_requests": 12776,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4679867550730705. Arrivals time: 0.04560839757323265 Scheduler time: 1.0555696790106595 Scheduler overhead time: 0.12651521293446422 Adapter cache time: 0.057031274773180485 Engine time: 0.12031597457826138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 1080, 1080, 135, 1080, 135, 540, 135, 1080, 540, 1080, 135, 540, 1080, 1080, 1080, 135, 1080, 540, 540, 135, 135, 135, 1080, 135, 135, 135, 1080, 540, 135, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 135, 540, 540, 1080, 1080, 540, 1080, 135, 1080, 540, 540, 540, 1080, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 1080, 540]
Prompts retrieved: 37935 . Total input tokens: 8403910 . Total output tokens: 7439174
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.479574945755303,
    "estimated_duration": 3599.535720895714,
    "input_throughput": 870.0572081614674,
    "output_throughput": 759.8938341190715,
    "total_throughput": 1629.951042280539,
    "itl": 24.505382821516253,
    "ttft": 3983.3705037724785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.8216674930617,
    "arrivals": 12790,
    "finished_requests": 12776,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4796952297911048. Arrivals time: 0.04575082240626216 Scheduler time: 1.0629831841215491 Scheduler overhead time: 0.12803369387984276 Adapter cache time: 0.05658521689474583 Engine time: 0.12304799864068627 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 1080, 1080, 66, 1080, 66, 540, 66, 1080, 540, 1080, 66, 540, 1080, 1080, 1080, 66, 1080, 540, 540, 66, 66, 66, 1080, 66, 66, 66, 1080, 540, 66, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 66, 540, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 1080, 540]
Prompts retrieved: 36486 . Total input tokens: 8090413 . Total output tokens: 7145923
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.4511911598965526,
    "estimated_duration": 3599.5542773016564,
    "input_throughput": 842.1250984086682,
    "output_throughput": 730.5382270749486,
    "total_throughput": 1572.6633254836167,
    "itl": 24.343952540148283,
    "ttft": 4423.143440420161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.771610469858636,
    "arrivals": 12326,
    "finished_requests": 12311,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4515039180405438. Arrivals time: 0.045129896607249975 Scheduler time: 1.0395643808878958 Scheduler overhead time: 0.1260583084076643 Adapter cache time: 0.05401673913002014 Engine time: 0.12388386856764555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 1080, 1080, 66, 1080, 66, 540, 66, 1080, 540, 1080, 66, 540, 1080, 1080, 1080, 66, 1080, 540, 540, 66, 66, 66, 1080, 66, 66, 66, 1080, 540, 66, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 66, 540, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 1080, 540]
Prompts retrieved: 36486 . Total input tokens: 8090413 . Total output tokens: 7145923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.436424792278558,
    "estimated_duration": 3599.5566400333664,
    "input_throughput": 842.1245456417936,
    "output_throughput": 730.537747553161,
    "total_throughput": 1572.6622931949546,
    "itl": 24.36625479122843,
    "ttft": 4423.142357801998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.16531555554304,
    "arrivals": 12326,
    "finished_requests": 12311,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4365032790228724. Arrivals time: 0.044695292599499226 Scheduler time: 1.0260725994594395 Scheduler overhead time: 0.1266064140945673 Adapter cache time: 0.05368898157030344 Engine time: 0.12245184928178787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 1080, 1080, 66, 1080, 66, 540, 66, 1080, 540, 1080, 66, 540, 1080, 1080, 1080, 66, 1080, 540, 540, 66, 66, 66, 1080, 66, 66, 66, 1080, 540, 66, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 66, 540, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 1080, 540]
Prompts retrieved: 36486 . Total input tokens: 8090413 . Total output tokens: 7145923
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4443059689365327,
    "estimated_duration": 3599.5507108461325,
    "input_throughput": 842.1259327910537,
    "output_throughput": 730.5389508964209,
    "total_throughput": 1572.6648836874747,
    "itl": 24.371339061262514,
    "ttft": 4423.319978952636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.833477059291653,
    "arrivals": 12326,
    "finished_requests": 12311,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.444393530022353. Arrivals time: 0.04454465676099062 Scheduler time: 1.0329845007508993 Scheduler overhead time: 0.1277023800648749 Adapter cache time: 0.053883729968219995 Engine time: 0.12185665080323815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 1080, 1080, 66, 1080, 66, 540, 66, 1080, 540, 1080, 66, 540, 1080, 1080, 1080, 66, 1080, 540, 540, 66, 66, 66, 1080, 66, 66, 66, 1080, 540, 66, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 66, 540, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 1080, 540]
Prompts retrieved: 36486 . Total input tokens: 8090413 . Total output tokens: 7145923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4374486957676709,
    "estimated_duration": 3599.550185377032,
    "input_throughput": 842.12605572618,
    "output_throughput": 730.5390575418699,
    "total_throughput": 1572.66511326805,
    "itl": 24.347942755439863,
    "ttft": 4423.049181805254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3596,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.443364364710526,
    "arrivals": 12326,
    "finished_requests": 12311,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.43753368454054. Arrivals time: 0.04549404373392463 Scheduler time: 1.0264195180498064 Scheduler overhead time: 0.12599134165793657 Adapter cache time: 0.0534249828197062 Engine time: 0.12317596236243844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 1080, 1080, 66, 1080, 66, 540, 66, 1080, 540, 1080, 66, 540, 1080, 1080, 1080, 66, 1080, 540, 540, 66, 66, 66, 1080, 66, 66, 66, 1080, 540, 66, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 66, 540, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 1080, 540]
Prompts retrieved: 36486 . Total input tokens: 8090413 . Total output tokens: 7145923
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4271619846113026,
    "estimated_duration": 3599.542497795738,
    "input_throughput": 842.1278542637766,
    "output_throughput": 730.5406177619246,
    "total_throughput": 1572.6684720257012,
    "itl": 24.37069632876674,
    "ttft": 4423.280224799963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3596,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.59321628612555,
    "arrivals": 12326,
    "finished_requests": 12311,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4272558139637113. Arrivals time: 0.04723231168463826 Scheduler time: 1.0168283893726766 Scheduler overhead time: 0.12606755830347538 Adapter cache time: 0.05367835843935609 Engine time: 0.12082704342901707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 1080, 1080, 66, 1080, 66, 540, 66, 1080, 540, 1080, 66, 540, 1080, 1080, 1080, 66, 1080, 540, 540, 66, 66, 66, 1080, 66, 66, 66, 1080, 540, 66, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 66, 540, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 1080, 540]
Prompts retrieved: 36486 . Total input tokens: 8090413 . Total output tokens: 7145923
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.458055763039738,
    "estimated_duration": 3599.555192350023,
    "input_throughput": 842.124884330774,
    "output_throughput": 730.5380413637216,
    "total_throughput": 1572.6629256944957,
    "itl": 24.338448317349563,
    "ttft": 4422.973085192518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.969347544174735,
    "arrivals": 12326,
    "finished_requests": 12311,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4581504259258509. Arrivals time: 0.045171629171818495 Scheduler time: 1.0439998432993889 Scheduler overhead time: 0.12757775140926242 Adapter cache time: 0.05390973994508386 Engine time: 0.12396887177601457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 1080, 1080, 66, 1080, 66, 540, 66, 1080, 540, 1080, 66, 540, 1080, 1080, 1080, 66, 1080, 540, 540, 66, 66, 66, 1080, 66, 66, 66, 1080, 540, 66, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 66, 540, 540, 1080, 1080, 540, 1080, 66, 1080, 540, 540, 540, 1080, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 1080, 540]
Prompts retrieved: 36486 . Total input tokens: 8090413 . Total output tokens: 7145923
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4329739790409803,
    "estimated_duration": 3599.556441854891,
    "input_throughput": 842.1245920061058,
    "output_throughput": 730.5377877739103,
    "total_throughput": 1572.662379780016,
    "itl": 24.363271293566402,
    "ttft": 4423.193144493416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.358866373189667,
    "arrivals": 12326,
    "finished_requests": 12311,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4330682130530477. Arrivals time: 0.0445334161631763 Scheduler time: 1.0214018244296312 Scheduler overhead time: 0.12833414506167173 Adapter cache time: 0.053576317615807056 Engine time: 0.12209654273465276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 1080, 1080, 33, 1080, 33, 540, 33, 1080, 540, 1080, 33, 540, 1080, 1080, 1080, 33, 1080, 540, 540, 33, 33, 33, 1080, 33, 33, 33, 1080, 540, 33, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 33, 540, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 1080, 540]
Prompts retrieved: 35793 . Total input tokens: 7932571 . Total output tokens: 7011702
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.4403483741916716,
    "estimated_duration": 3599.6362908009505,
    "input_throughput": 830.0119119354699,
    "output_throughput": 718.3486861181295,
    "total_throughput": 1548.3605980535995,
    "itl": 23.94422616433459,
    "ttft": 6309.400456489936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3063,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.2538088648621,
    "arrivals": 12061,
    "finished_requests": 12040,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4404315152205527. Arrivals time: 0.04477320518344641 Scheduler time: 1.0255809701047838 Scheduler overhead time: 0.1279130238108337 Adapter cache time: 0.0520858783274889 Engine time: 0.12639484321698546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 1080, 1080, 33, 1080, 33, 540, 33, 1080, 540, 1080, 33, 540, 1080, 1080, 1080, 33, 1080, 540, 540, 33, 33, 33, 1080, 33, 33, 33, 1080, 540, 33, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 33, 540, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 1080, 540]
Prompts retrieved: 35793 . Total input tokens: 7932571 . Total output tokens: 7011702
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4454613733105361,
    "estimated_duration": 3599.629491285952,
    "input_throughput": 830.0134797852883,
    "output_throughput": 718.3500430418566,
    "total_throughput": 1548.3635228271448,
    "itl": 23.962255271539153,
    "ttft": 6309.623417862769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.407008316562912,
    "arrivals": 12061,
    "finished_requests": 12040,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.445527896285057. Arrivals time: 0.0442645656876266 Scheduler time: 1.0325037245638669 Scheduler overhead time: 0.12791926600039005 Adapter cache time: 0.052347853779792786 Engine time: 0.12487858533859253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 1080, 1080, 33, 1080, 33, 540, 33, 1080, 540, 1080, 33, 540, 1080, 1080, 1080, 33, 1080, 540, 540, 33, 33, 33, 1080, 33, 33, 33, 1080, 540, 33, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 33, 540, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 1080, 540]
Prompts retrieved: 35793 . Total input tokens: 7932571 . Total output tokens: 7011702
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4147831862792373,
    "estimated_duration": 3599.637856471594,
    "input_throughput": 830.0115509199077,
    "output_throughput": 718.3483736707404,
    "total_throughput": 1548.3599245906482,
    "itl": 23.96684301060408,
    "ttft": 6309.711439904488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3069,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.99805769503627,
    "arrivals": 12061,
    "finished_requests": 12040,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4148595798760653. Arrivals time: 0.04412454320117831 Scheduler time: 1.005811860319227 Scheduler overhead time: 0.12774506444111466 Adapter cache time: 0.05187807185575366 Engine time: 0.12223910121247172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 1080, 1080, 33, 1080, 33, 540, 33, 1080, 540, 1080, 33, 540, 1080, 1080, 1080, 33, 1080, 540, 540, 33, 33, 33, 1080, 33, 33, 33, 1080, 540, 33, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 33, 540, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 1080, 540]
Prompts retrieved: 35793 . Total input tokens: 7932571 . Total output tokens: 7011702
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4105966957286,
    "estimated_duration": 3599.6407841032296,
    "input_throughput": 830.0108758614172,
    "output_throughput": 718.3477894292703,
    "total_throughput": 1548.3586652906874,
    "itl": 23.949878408845453,
    "ttft": 6309.434380603769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3069,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.77501763587425,
    "arrivals": 12061,
    "finished_requests": 12040,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4107396099716425. Arrivals time: 0.04362547863274813 Scheduler time: 1.0001511443406343 Scheduler overhead time: 0.12873812345787883 Adapter cache time: 0.05177189689129591 Engine time: 0.12282903958112001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 1080, 1080, 33, 1080, 33, 540, 33, 1080, 540, 1080, 33, 540, 1080, 1080, 1080, 33, 1080, 540, 540, 33, 33, 33, 1080, 33, 33, 33, 1080, 540, 33, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 33, 540, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 1080, 540]
Prompts retrieved: 35793 . Total input tokens: 7932571 . Total output tokens: 7011702
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4206910277716815,
    "estimated_duration": 3599.623673251238,
    "input_throughput": 830.0148213275373,
    "output_throughput": 718.3512041036415,
    "total_throughput": 1548.366025431179,
    "itl": 23.964550882600907,
    "ttft": 6309.6912040049265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.775246633687487,
    "arrivals": 12061,
    "finished_requests": 12040,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4207818997092545. Arrivals time: 0.04436616785824299 Scheduler time: 1.010879254899919 Scheduler overhead time: 0.12825411930680275 Adapter cache time: 0.05161354038864374 Engine time: 0.12174302991479635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 1080, 1080, 33, 1080, 33, 540, 33, 1080, 540, 1080, 33, 540, 1080, 1080, 1080, 33, 1080, 540, 540, 33, 33, 33, 1080, 33, 33, 33, 1080, 540, 33, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 33, 540, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 1080, 540]
Prompts retrieved: 35793 . Total input tokens: 7932571 . Total output tokens: 7011702
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.4245858159847558,
    "estimated_duration": 3599.6307034010215,
    "input_throughput": 830.0132002922153,
    "output_throughput": 718.349801149567,
    "total_throughput": 1548.3630014417824,
    "itl": 23.93792954343493,
    "ttft": 6309.326906925552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3061,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.5411819990879,
    "arrivals": 12061,
    "finished_requests": 12040,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4246810539625585. Arrivals time: 0.04407223407179117 Scheduler time: 1.0090911770239472 Scheduler overhead time: 0.13140327157452703 Adapter cache time: 0.05167162045836449 Engine time: 0.12401040084660053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.1-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 1080, 1080, 33, 1080, 33, 540, 33, 1080, 540, 1080, 33, 540, 1080, 1080, 1080, 33, 1080, 540, 540, 33, 33, 33, 1080, 33, 33, 33, 1080, 540, 33, 1080, 1080, 1080, 1080, 540, 540, 540, 1080, 33, 540, 540, 1080, 1080, 540, 1080, 33, 1080, 540, 540, 540, 1080, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 1080, 540]
Prompts retrieved: 35793 . Total input tokens: 7932571 . Total output tokens: 7011702
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.4271332030184567,
    "estimated_duration": 3599.640576538936,
    "input_throughput": 830.0109237219236,
    "output_throughput": 718.3478308510034,
    "total_throughput": 1548.3587545729272,
    "itl": 23.963735654661917,
    "ttft": 6309.672236513796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3071,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.591794180702838,
    "arrivals": 12061,
    "finished_requests": 12040,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.427229224704206. Arrivals time: 0.04410459380596876 Scheduler time: 1.0136549160815775 Scheduler overhead time: 0.12680577486753464 Adapter cache time: 0.05214650183916092 Engine time: 0.12745141191408038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 1080, 1080, 135, 1080, 135, 270, 135, 1080, 270, 1080, 135, 270, 1080, 1080, 1080, 135, 1080, 270, 270, 135, 135, 135, 1080, 135, 135, 135, 1080, 270, 135, 1080, 1080, 1080, 1080, 270, 270, 270, 1080, 135, 270, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 1080, 270]
Prompts retrieved: 32265 . Total input tokens: 7152847 . Total output tokens: 6336847
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.3469602093100548,
    "estimated_duration": 3599.3457889503215,
    "input_throughput": 745.8099769799725,
    "output_throughput": 652.0673860247423,
    "total_throughput": 1397.8773630047147,
    "itl": 23.54094753057862,
    "ttft": 6315.836942086073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2977,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.68514168811454,
    "arrivals": 10900,
    "finished_requests": 10881,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3470545313321054. Arrivals time: 0.041166802402585745 Scheduler time: 0.9316224041394889 Scheduler overhead time: 0.13218251196667552 Adapter cache time: 0.0503216153010726 Engine time: 0.12665621796622872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 1080, 1080, 135, 1080, 135, 270, 135, 1080, 270, 1080, 135, 270, 1080, 1080, 1080, 135, 1080, 270, 270, 135, 135, 135, 1080, 135, 135, 135, 1080, 270, 135, 1080, 1080, 1080, 1080, 270, 270, 270, 1080, 135, 270, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 1080, 270]
Prompts retrieved: 32265 . Total input tokens: 7152847 . Total output tokens: 6336847
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.3565373760648072,
    "estimated_duration": 3599.335319600396,
    "input_throughput": 745.804089266689,
    "output_throughput": 651.9698198778906,
    "total_throughput": 1397.7739091445796,
    "itl": 23.556030782995283,
    "ttft": 6646.285754550453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.74194721552492,
    "arrivals": 10900,
    "finished_requests": 10880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.35661114519462. Arrivals time: 0.041365101002156734 Scheduler time: 0.9429297484457493 Scheduler overhead time: 0.12949660560116172 Adapter cache time: 0.050362625159323215 Engine time: 0.1269701961427927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 1080, 1080, 135, 1080, 135, 270, 135, 1080, 270, 1080, 135, 270, 1080, 1080, 1080, 135, 1080, 270, 270, 135, 135, 135, 1080, 135, 135, 135, 1080, 270, 135, 1080, 1080, 1080, 1080, 270, 270, 270, 1080, 135, 270, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 1080, 270]
Prompts retrieved: 32265 . Total input tokens: 7152847 . Total output tokens: 6336847
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.356972417794168,
    "estimated_duration": 3599.3589474736727,
    "input_throughput": 745.8072504505707,
    "output_throughput": 652.0650021991637,
    "total_throughput": 1397.8722526497345,
    "itl": 23.562871648767278,
    "ttft": 6316.176974113734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.322943241843397,
    "arrivals": 10900,
    "finished_requests": 10881,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3570432467386127. Arrivals time: 0.04131830856204033 Scheduler time: 0.9437495963647962 Scheduler overhead time: 0.12940610712394118 Adapter cache time: 0.05070491461083293 Engine time: 0.12729087751358747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 1080, 1080, 135, 1080, 135, 270, 135, 1080, 270, 1080, 135, 270, 1080, 1080, 1080, 135, 1080, 270, 270, 135, 135, 135, 1080, 135, 135, 135, 1080, 270, 135, 1080, 1080, 1080, 1080, 270, 270, 270, 1080, 135, 270, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 1080, 270]
Prompts retrieved: 32265 . Total input tokens: 7152847 . Total output tokens: 6336847
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 1.351171214133501,
    "estimated_duration": 3599.348256946342,
    "input_throughput": 745.8094655940425,
    "output_throughput": 652.0669389160996,
    "total_throughput": 1397.876404510142,
    "itl": 23.54548539074339,
    "ttft": 6315.7946809466675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2977,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.29736636623329,
    "arrivals": 10900,
    "finished_requests": 10881,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3512445432133973. Arrivals time: 0.04190820036455989 Scheduler time: 0.9363521263003349 Scheduler overhead time: 0.13004087610170245 Adapter cache time: 0.0503335939720273 Engine time: 0.12730631278827786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 1080, 1080, 135, 1080, 135, 270, 135, 1080, 270, 1080, 135, 270, 1080, 1080, 1080, 135, 1080, 270, 270, 135, 135, 135, 1080, 135, 135, 135, 1080, 270, 135, 1080, 1080, 1080, 1080, 270, 270, 270, 1080, 135, 270, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 1080, 270]
Prompts retrieved: 32265 . Total input tokens: 7152847 . Total output tokens: 6336847
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 1.332051320001483,
    "estimated_duration": 3599.356507799054,
    "input_throughput": 745.8077559651024,
    "output_throughput": 652.065444174398,
    "total_throughput": 1397.8732001395003,
    "itl": 23.55970292925158,
    "ttft": 6316.065583613266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.108576964047167,
    "arrivals": 10900,
    "finished_requests": 10881,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3321482092142105. Arrivals time: 0.04095331393182278 Scheduler time: 0.9223901182413101 Scheduler overhead time: 0.13047570968046784 Adapter cache time: 0.05010216450318694 Engine time: 0.12353001395240426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 1080, 1080, 135, 1080, 135, 270, 135, 1080, 270, 1080, 135, 270, 1080, 1080, 1080, 135, 1080, 270, 270, 135, 135, 135, 1080, 135, 135, 135, 1080, 270, 135, 1080, 1080, 1080, 1080, 270, 270, 270, 1080, 135, 270, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 1080, 270]
Prompts retrieved: 32265 . Total input tokens: 7152847 . Total output tokens: 6336847
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 1.3473032107576728,
    "estimated_duration": 3599.357227758139,
    "input_throughput": 745.8076067853918,
    "output_throughput": 652.0653137454323,
    "total_throughput": 1397.872920530824,
    "itl": 23.534577844865606,
    "ttft": 6315.692731256316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.011316561020475,
    "arrivals": 10900,
    "finished_requests": 10881,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3473978289403021. Arrivals time: 0.04131469735875726 Scheduler time: 0.9321819287724793 Scheduler overhead time: 0.12983997724950314 Adapter cache time: 0.05132284667342901 Engine time: 0.12758121686056256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.1-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 1080, 1080, 135, 1080, 135, 270, 135, 1080, 270, 1080, 135, 270, 1080, 1080, 1080, 135, 1080, 270, 270, 135, 135, 135, 1080, 135, 135, 135, 1080, 270, 135, 1080, 1080, 1080, 1080, 270, 270, 270, 1080, 135, 270, 270, 1080, 1080, 270, 1080, 135, 1080, 270, 270, 270, 1080, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 1080, 270]
Prompts retrieved: 32265 . Total input tokens: 7152847 . Total output tokens: 6336847
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 1.349182209931314,
    "estimated_duration": 3599.3485055293686,
    "input_throughput": 745.8013570723117,
    "output_throughput": 651.9674314379482,
    "total_throughput": 1397.7687885102598,
    "itl": 23.55956033109582,
    "ttft": 6646.263951385364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2977,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.909176488463167,
    "arrivals": 10900,
    "finished_requests": 10880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3493273938074708. Arrivals time: 0.04169558174908161 Scheduler time: 0.934667247813195 Scheduler overhead time: 0.12953069480136037 Adapter cache time: 0.050406606402248144 Engine time: 0.12730491487309337 
