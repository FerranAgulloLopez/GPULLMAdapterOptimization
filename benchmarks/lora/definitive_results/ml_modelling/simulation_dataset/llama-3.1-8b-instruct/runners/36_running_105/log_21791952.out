INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.378503214102238,
    "estimated_duration": 3600.0242752360728,
    "input_throughput": 7147.756524034164,
    "output_throughput": 6184.571074465877,
    "total_throughput": 13332.32759850004,
    "itl": 96.83371631707618,
    "ttft": 960918.7055094985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1001,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.619021440982033,
    "arrivals": 128637,
    "finished_requests": 103710,
    "scheduler_time": 120.7030046891835
}
#Debug simulation 
Total elapsed time: 7.378612146247178. Arrivals time: 0.35941973701119423 Scheduler time: 6.859322772361338 Scheduler overhead time: 0.055952944327145815 Adapter cache time: 0.01915218075737357 Engine time: 0.0581081104464829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.292091573122889,
    "estimated_duration": 3600.0342939072693,
    "input_throughput": 7080.805880972146,
    "output_throughput": 6127.312741806951,
    "total_throughput": 13208.118622779097,
    "itl": 94.1217656872177,
    "ttft": 996698.3083031222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1011,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.407851879582752,
    "arrivals": 128637,
    "finished_requests": 102719,
    "scheduler_time": 122.29644891097995
}
#Debug simulation 
Total elapsed time: 7.29223301820457. Arrivals time: 0.3420849763788283 Scheduler time: 6.7875488586723804 Scheduler overhead time: 0.05713338823989034 Adapter cache time: 0.018658277578651905 Engine time: 0.05930985929444432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.121181769296527,
    "estimated_duration": 3600.028390935897,
    "input_throughput": 6908.955513413029,
    "output_throughput": 5982.789484168687,
    "total_throughput": 12891.744997581716,
    "itl": 87.72231492068813,
    "ttft": 1039582.258924464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1020,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.66856231173968,
    "arrivals": 128637,
    "finished_requests": 100270,
    "scheduler_time": 126.44763173335411
}
#Debug simulation 
Total elapsed time: 7.12130640912801. Arrivals time: 0.3316666395403445 Scheduler time: 6.61844350816682 Scheduler overhead time: 0.060284745413810015 Adapter cache time: 0.019216614309698343 Engine time: 0.06282903766259551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.410204374231398,
    "estimated_duration": 3600.0772303330596,
    "input_throughput": 7082.011959404901,
    "output_throughput": 6128.745465262911,
    "total_throughput": 13210.757424667812,
    "itl": 94.10499579552898,
    "ttft": 995871.038497045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1008,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.723742270544152,
    "arrivals": 128637,
    "finished_requests": 102746,
    "scheduler_time": 122.28703932183183
}
#Debug simulation 
Total elapsed time: 7.410330432001501. Arrivals time: 0.36298626894131303 Scheduler time: 6.882212121039629 Scheduler overhead time: 0.057639578357338905 Adapter cache time: 0.019234633073210716 Engine time: 0.060630313120782375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.085562526248395,
    "estimated_duration": 3600.029154955115,
    "input_throughput": 6908.954324929658,
    "output_throughput": 5982.808769855237,
    "total_throughput": 12891.763094784894,
    "itl": 87.7226535946789,
    "ttft": 1039484.1443896819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1022,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.58358940731736,
    "arrivals": 128637,
    "finished_requests": 100271,
    "scheduler_time": 126.44544404495545
}
#Debug simulation 
Total elapsed time: 7.0857031820341945. Arrivals time: 0.35805863747373223 Scheduler time: 6.555691336747259 Scheduler overhead time: 0.060046182945370674 Adapter cache time: 0.020461988635361195 Engine time: 0.06263675913214684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.3299855422228575,
    "estimated_duration": 3600.0072622601065,
    "input_throughput": 7083.011544813285,
    "output_throughput": 6129.425135144546,
    "total_throughput": 13212.436679957831,
    "itl": 94.09542594248927,
    "ttft": 995395.2325768168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1009,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.441376229035636,
    "arrivals": 128637,
    "finished_requests": 102758,
    "scheduler_time": 122.27970943419226
}
#Debug simulation 
Total elapsed time: 7.330129011068493. Arrivals time: 0.3499211613088846 Scheduler time: 6.814021337311715 Scheduler overhead time: 0.05754469893872738 Adapter cache time: 0.01955147599801421 Engine time: 0.06168310344219208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86296644 . Total output tokens: 75858121
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.188628612086177,
    "estimated_duration": 3600.004669746518,
    "input_throughput": 6909.001315754212,
    "output_throughput": 5982.853628219473,
    "total_throughput": 12891.854943973683,
    "itl": 87.72046128492822,
    "ttft": 1039519.7839127432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1021,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.534141767602434,
    "arrivals": 128637,
    "finished_requests": 100271,
    "scheduler_time": 126.44498938189093
}
#Debug simulation 
Total elapsed time: 7.188729987945408. Arrivals time: 0.3422118010930717 Scheduler time: 6.67431221017614 Scheduler overhead time: 0.06036628782749176 Adapter cache time: 0.01945533649995923 Engine time: 0.06349730677902699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.409569972194731,
    "estimated_duration": 3600.04759830097,
    "input_throughput": 7192.398237240002,
    "output_throughput": 6299.378933407112,
    "total_throughput": 13491.777170647114,
    "itl": 95.57835088109185,
    "ttft": 886389.350409856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 856,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.660222131349261,
    "arrivals": 127906,
    "finished_requests": 105019,
    "scheduler_time": 119.16653949419972
}
#Debug simulation 
Total elapsed time: 7.409707433078438. Arrivals time: 0.33837006986141205 Scheduler time: 6.91189078707248 Scheduler overhead time: 0.05626475624740124 Adapter cache time: 0.017687221989035606 Engine time: 0.05863745464012027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.411157609894872,
    "estimated_duration": 3600.052492497414,
    "input_throughput": 7131.079075513909,
    "output_throughput": 6243.85590122507,
    "total_throughput": 13374.93497673898,
    "itl": 92.90088816326484,
    "ttft": 920538.3737858828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 851,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.228167338133799,
    "arrivals": 127906,
    "finished_requests": 104120,
    "scheduler_time": 120.6769180288278
}
#Debug simulation 
Total elapsed time: 7.411274376790971. Arrivals time: 0.35737692937254906 Scheduler time: 6.889681201428175 Scheduler overhead time: 0.05778627563267946 Adapter cache time: 0.01853582262992859 Engine time: 0.06026133382692933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.299553894903511,
    "estimated_duration": 3600.050800463838,
    "input_throughput": 6962.991465778844,
    "output_throughput": 6096.253696523484,
    "total_throughput": 13059.245162302328,
    "itl": 86.57581990781927,
    "ttft": 1008109.5918272161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 830,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.245180786391747,
    "arrivals": 127906,
    "finished_requests": 101646,
    "scheduler_time": 124.95256125369764
}
#Debug simulation 
Total elapsed time: 7.299748525954783. Arrivals time: 0.3632937357760966 Scheduler time: 6.762228643056005 Scheduler overhead time: 0.06120495684444904 Adapter cache time: 0.019475679844617844 Engine time: 0.06409344868734479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.500981085933745,
    "estimated_duration": 3600.065761835915,
    "input_throughput": 7131.974996730699,
    "output_throughput": 6244.634539268911,
    "total_throughput": 13376.609535999609,
    "itl": 92.8878262493163,
    "ttft": 920030.1309672944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 851,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.760336870127346,
    "arrivals": 127906,
    "finished_requests": 104133,
    "scheduler_time": 120.6700665791234
}
#Debug simulation 
Total elapsed time: 7.501084418967366. Arrivals time: 0.3591555333696306 Scheduler time: 6.977238202467561 Scheduler overhead time: 0.0580686004832387 Adapter cache time: 0.018306393641978502 Engine time: 0.06043915310874581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.258774999063462,
    "estimated_duration": 3600.0019196778358,
    "input_throughput": 6963.20434246971,
    "output_throughput": 6096.36258248552,
    "total_throughput": 13059.56692495523,
    "itl": 86.57235843411776,
    "ttft": 1008016.0028085797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.1853224258357935,
    "arrivals": 127906,
    "finished_requests": 101649,
    "scheduler_time": 124.94820440718317
}
#Debug simulation 
Total elapsed time: 7.258899705950171. Arrivals time: 0.34390462609007955 Scheduler time: 6.74230361264199 Scheduler overhead time: 0.06118956161662936 Adapter cache time: 0.018548880703747272 Engine time: 0.06345992675051093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.4316444848664105,
    "estimated_duration": 3600.0433391579104,
    "input_throughput": 7132.234137494017,
    "output_throughput": 6245.057873458517,
    "total_throughput": 13377.292010952535,
    "itl": 92.87752836489071,
    "ttft": 919772.9350587508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 849,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.419948878544356,
    "arrivals": 127906,
    "finished_requests": 104139,
    "scheduler_time": 120.6656849472867
}
#Debug simulation 
Total elapsed time: 7.431764116045088. Arrivals time: 0.35048438888043165 Scheduler time: 6.916867689695209 Scheduler overhead time: 0.0576774887740612 Adapter cache time: 0.018710619304329157 Engine time: 0.060338451992720366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85811544 . Total output tokens: 75449308
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.373827666044235,
    "estimated_duration": 3600.062036000181,
    "input_throughput": 6963.055566634336,
    "output_throughput": 6096.260225666539,
    "total_throughput": 13059.315792300875,
    "itl": 86.57236384078479,
    "ttft": 1007949.2909077663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 830,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.128780933134285,
    "arrivals": 127906,
    "finished_requests": 101648,
    "scheduler_time": 124.949953437943
}
#Debug simulation 
Total elapsed time: 7.37392501719296. Arrivals time: 0.3646680014207959 Scheduler time: 6.834484802559018 Scheduler overhead time: 0.061478659976273775 Adapter cache time: 0.019415860064327717 Engine time: 0.06428379146382213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.501271652057767,
    "estimated_duration": 3600.00080276622,
    "input_throughput": 7339.381141164651,
    "output_throughput": 6362.683581181264,
    "total_throughput": 13702.064722345915,
    "itl": 94.16813516095871,
    "ttft": 826349.8788938199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.489825732694085,
    "arrivals": 127669,
    "finished_requests": 106476,
    "scheduler_time": 117.91072380189253
}
#Debug simulation 
Total elapsed time: 7.501361363101751. Arrivals time: 0.3664689133875072 Scheduler time: 6.974234911147505 Scheduler overhead time: 0.056904177647084 Adapter cache time: 0.017347778659313917 Engine time: 0.05917140142992139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.5388217768631876,
    "estimated_duration": 3600.0811512736823,
    "input_throughput": 7266.258703847577,
    "output_throughput": 6302.990140089479,
    "total_throughput": 13569.248843937055,
    "itl": 91.53727081006006,
    "ttft": 864620.7490899225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 672,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.945235171467074,
    "arrivals": 127669,
    "finished_requests": 105441,
    "scheduler_time": 119.40315323881973
}
#Debug simulation 
Total elapsed time: 7.538913927972317. Arrivals time: 0.3392394222319126 Scheduler time: 7.034829829353839 Scheduler overhead time: 0.05844766553491354 Adapter cache time: 0.017079373355954885 Engine time: 0.06126841949298978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.322141882963479,
    "estimated_duration": 3600.073572625154,
    "input_throughput": 7098.774090154114,
    "output_throughput": 6155.866693535349,
    "total_throughput": 13254.640783689463,
    "itl": 85.27936649256341,
    "ttft": 956560.0521350239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.905204718373749,
    "arrivals": 127669,
    "finished_requests": 103027,
    "scheduler_time": 123.77729531942347
}
#Debug simulation 
Total elapsed time: 7.322293246164918. Arrivals time: 0.3572226329706609 Scheduler time: 6.791354384738952 Scheduler overhead time: 0.061838462483137846 Adapter cache time: 0.017795948777347803 Engine time: 0.06440893234685063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.505779909901321,
    "estimated_duration": 3600.046471797502,
    "input_throughput": 7266.789249789304,
    "output_throughput": 6303.320020385673,
    "total_throughput": 13570.109270174977,
    "itl": 91.52614440829358,
    "ttft": 864216.6340576386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.515717706787389,
    "arrivals": 127669,
    "finished_requests": 105448,
    "scheduler_time": 119.39815205591866
}
#Debug simulation 
Total elapsed time: 7.50587643077597. Arrivals time: 0.36860864935442805 Scheduler time: 6.971554352436215 Scheduler overhead time: 0.05833842186257243 Adapter cache time: 0.018214177805930376 Engine time: 0.06116002704948187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.360659410711378,
    "estimated_duration": 3600.027030337007,
    "input_throughput": 7099.286695524475,
    "output_throughput": 6156.1462770254075,
    "total_throughput": 13255.432972549883,
    "itl": 85.27611981562266,
    "ttft": 956420.8871353059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.842862448212725,
    "arrivals": 127669,
    "finished_requests": 103030,
    "scheduler_time": 123.77231671857452
}
#Debug simulation 
Total elapsed time: 7.360781400930136. Arrivals time: 0.3526536882854998 Scheduler time: 6.833916831761599 Scheduler overhead time: 0.06175981881096959 Adapter cache time: 0.0178314414806664 Engine time: 0.06494543049484491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.512723271269351,
    "estimated_duration": 3600.017650168155,
    "input_throughput": 7267.095759593846,
    "output_throughput": 6303.7377049915285,
    "total_throughput": 13570.833464585374,
    "itl": 91.52066471155433,
    "ttft": 864035.9195117782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2963787930039485,
    "arrivals": 127669,
    "finished_requests": 105453,
    "scheduler_time": 119.39063600637724
}
#Debug simulation 
Total elapsed time: 7.51283615315333. Arrivals time: 0.33929832046851516 Scheduler time: 7.008984818123281 Scheduler overhead time: 0.05859658261761069 Adapter cache time: 0.01702561369165778 Engine time: 0.06082488875836134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85658887 . Total output tokens: 75321514
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.3763919728808105,
    "estimated_duration": 3600.039623790997,
    "input_throughput": 7099.391581997707,
    "output_throughput": 6156.0816868627435,
    "total_throughput": 13255.47326886045,
    "itl": 85.27870311421755,
    "ttft": 956451.4802126213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.810759285837446,
    "arrivals": 127669,
    "finished_requests": 103030,
    "scheduler_time": 123.77523791304547
}
#Debug simulation 
Total elapsed time: 7.376526764128357. Arrivals time: 0.3749718591570854 Scheduler time: 6.826148614753038 Scheduler overhead time: 0.061990529764443636 Adapter cache time: 0.01856978889554739 Engine time: 0.06519097834825516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.629098598379642,
    "estimated_duration": 3600.0358172846245,
    "input_throughput": 7434.502698972441,
    "output_throughput": 6462.372093160941,
    "total_throughput": 13896.874792133383,
    "itl": 92.84797440020269,
    "ttft": 755639.9283549301,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1012198359845535,
    "arrivals": 127143,
    "finished_requests": 107955,
    "scheduler_time": 116.87644855470681
}
#Debug simulation 
Total elapsed time: 7.629232226405293. Arrivals time: 0.35870955465361476 Scheduler time: 7.109280531294644 Scheduler overhead time: 0.057580342050641775 Adapter cache time: 0.01582946442067623 Engine time: 0.06028225040063262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.584971148986369,
    "estimated_duration": 3600.0946065646935,
    "input_throughput": 7363.5673217199455,
    "output_throughput": 6401.481493840811,
    "total_throughput": 13765.048815560756,
    "itl": 90.28598683588777,
    "ttft": 794929.9992669147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3805510133178953,
    "arrivals": 127143,
    "finished_requests": 106932,
    "scheduler_time": 118.34967593988134
}
#Debug simulation 
Total elapsed time: 7.585221824236214. Arrivals time: 0.3648483161814511 Scheduler time: 7.054623553995043 Scheduler overhead time: 0.0591915063560009 Adapter cache time: 0.016361749731004238 Engine time: 0.06157296011224389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.4127557668834925,
    "estimated_duration": 3600.058213505657,
    "input_throughput": 7170.323219541305,
    "output_throughput": 6241.02491335025,
    "total_throughput": 13411.348132891555,
    "itl": 84.28233258051665,
    "ttft": 897923.70494057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3549246866349396,
    "arrivals": 127143,
    "finished_requests": 104192,
    "scheduler_time": 122.5914862800915
}
#Debug simulation 
Total elapsed time: 7.412856757175177. Arrivals time: 0.3498563230969012 Scheduler time: 6.888922126032412 Scheduler overhead time: 0.06265130406245589 Adapter cache time: 0.01607485394924879 Engine time: 0.06523603992536664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.574403448961675,
    "estimated_duration": 3600.080521564275,
    "input_throughput": 7363.936123428923,
    "output_throughput": 6401.80708791086,
    "total_throughput": 13765.743211339783,
    "itl": 90.2804606063535,
    "ttft": 794559.5504936593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1334476800681963,
    "arrivals": 127143,
    "finished_requests": 106940,
    "scheduler_time": 118.34393725240096
}
#Debug simulation 
Total elapsed time: 7.574550763703883. Arrivals time: 0.33677053824067116 Scheduler time: 7.071364510804415 Scheduler overhead time: 0.05887095537036657 Adapter cache time: 0.015495993196964264 Engine time: 0.06152037624269724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.423860654234886,
    "estimated_duration": 3600.080836095894,
    "input_throughput": 7170.371770872204,
    "output_throughput": 6241.041805151711,
    "total_throughput": 13411.413576023915,
    "itl": 84.28132106109157,
    "ttft": 897923.4650302397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3193005322572104,
    "arrivals": 127143,
    "finished_requests": 104193,
    "scheduler_time": 122.59675738096024
}
#Debug simulation 
Total elapsed time: 7.423966350965202. Arrivals time: 0.362631244584918 Scheduler time: 6.887723620049655 Scheduler overhead time: 0.062245949637144804 Adapter cache time: 0.016293753869831562 Engine time: 0.06503861909732223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.712557352147996,
    "estimated_duration": 3600.0317785572115,
    "input_throughput": 7364.115827506629,
    "output_throughput": 6402.065708774611,
    "total_throughput": 13766.18153628124,
    "itl": 90.27478159751658,
    "ttft": 794451.070712603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.955755395484144,
    "arrivals": 127143,
    "finished_requests": 106942,
    "scheduler_time": 118.33813566994137
}
#Debug simulation 
Total elapsed time: 7.712670650333166. Arrivals time: 0.3677690210752189 Scheduler time: 7.178357366472483 Scheduler overhead time: 0.05919402139261365 Adapter cache time: 0.016496188938617706 Engine time: 0.062383581418544054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334844 . Total output tokens: 75033062
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.419480831827968,
    "estimated_duration": 3600.014218570532,
    "input_throughput": 7170.41084639101,
    "output_throughput": 6241.10118346184,
    "total_throughput": 13411.51202985285,
    "itl": 84.27949326108427,
    "ttft": 897888.714469571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.294032236710217,
    "arrivals": 127143,
    "finished_requests": 104192,
    "scheduler_time": 122.59024270213997
}
#Debug simulation 
Total elapsed time: 7.419664460234344. Arrivals time: 0.3509067455306649 Scheduler time: 6.895173041615635 Scheduler overhead time: 0.062159734312444925 Adapter cache time: 0.015958490781486034 Engine time: 0.06535883573815227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 57.17105785617605,
    "estimated_duration": 3599.9597668501247,
    "input_throughput": 6456.065207733085,
    "output_throughput": 5546.291151323096,
    "total_throughput": 12002.35635905618,
    "itl": 80.8398924421161,
    "ttft": 443331.92778033897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1968460347829373,
    "arrivals": 101151,
    "finished_requests": 93288,
    "scheduler_time": 93.04908037929833
}
#Debug simulation 
Total elapsed time: 57.171261589974165. Arrivals time: 0.4744737078435719 Scheduler time: 56.450463166460395 Scheduler overhead time: 0.09513752488419414 Adapter cache time: 0.017251024954020977 Engine time: 0.09720095712691545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 57.808195039164275,
    "estimated_duration": 3599.9436219415893,
    "input_throughput": 6447.088742873812,
    "output_throughput": 5543.967932818873,
    "total_throughput": 11991.056675692686,
    "itl": 80.48610043908096,
    "ttft": 454857.1274080253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0589377498207622,
    "arrivals": 101151,
    "finished_requests": 93198,
    "scheduler_time": 95.33077140629364
}
#Debug simulation 
Total elapsed time: 57.80836946424097. Arrivals time: 0.48105440754443407 Scheduler time: 57.081996084656566 Scheduler overhead time: 0.09417076781392097 Adapter cache time: 0.016486631240695715 Engine time: 0.09746363712474704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 38.2408365230076,
    "estimated_duration": 3599.995281558732,
    "input_throughput": 6462.8109706649975,
    "output_throughput": 5559.155897375167,
    "total_throughput": 12021.966868040165,
    "itl": 78.74268851383913,
    "ttft": 435977.64483462815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.564865274685438,
    "arrivals": 101151,
    "finished_requests": 93430,
    "scheduler_time": 92.55467600739955
}
#Debug simulation 
Total elapsed time: 38.241026679985225. Arrivals time: 0.4248157814145088 Scheduler time: 37.598874452058226 Scheduler overhead time: 0.08152785571292043 Adapter cache time: 0.015557682141661644 Engine time: 0.08433549711480737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 61.392895644996315,
    "estimated_duration": 3599.9803108351407,
    "input_throughput": 6436.527980516814,
    "output_throughput": 5527.557175829029,
    "total_throughput": 11964.085156345844,
    "itl": 79.98585842794814,
    "ttft": 465042.11069809744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.133306587720289,
    "arrivals": 101151,
    "finished_requests": 92983,
    "scheduler_time": 95.2553126427859
}
#Debug simulation 
Total elapsed time: 61.393074315041304. Arrivals time: 0.4798106043599546 Scheduler time: 60.65610897075385 Scheduler overhead time: 0.09850917477160692 Adapter cache time: 0.017112123779952526 Engine time: 0.10405217530205846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 26.437612819951028,
    "estimated_duration": 3599.9895267022735,
    "input_throughput": 6599.64086666769,
    "output_throughput": 5676.272069246488,
    "total_throughput": 12275.912935914179,
    "itl": 81.25396703804294,
    "ttft": 352185.99817498174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.02912441398949,
    "arrivals": 101151,
    "finished_requests": 95356,
    "scheduler_time": 90.94490416472725
}
#Debug simulation 
Total elapsed time: 26.437775827012956. Arrivals time: 0.38552849600091577 Scheduler time: 25.85271861543879 Scheduler overhead time: 0.07498592184856534 Adapter cache time: 0.014945633709430695 Engine time: 0.07619206374511123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 58.2027090950869,
    "estimated_duration": 3599.9684404411396,
    "input_throughput": 6451.34488933299,
    "output_throughput": 5544.949721157529,
    "total_throughput": 11996.294610490519,
    "itl": 80.46613528863038,
    "ttft": 453815.16490167216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9256685363827256,
    "arrivals": 101151,
    "finished_requests": 93222,
    "scheduler_time": 95.32261170859739
}
#Debug simulation 
Total elapsed time: 58.202893088106066. Arrivals time: 0.477814138866961 Scheduler time: 57.47543804999441 Scheduler overhead time: 0.09610957559198141 Adapter cache time: 0.016747163143008947 Engine time: 0.09952321834862232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67686442 . Total output tokens: 59558690
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 46.238306934945285,
    "estimated_duration": 3599.96463338084,
    "input_throughput": 6430.5598408780215,
    "output_throughput": 5529.9571044130225,
    "total_throughput": 11960.516945291045,
    "itl": 78.14917569233687,
    "ttft": 467498.7482708314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.536490221489224,
    "arrivals": 101151,
    "finished_requests": 92877,
    "scheduler_time": 92.86440304245257
}
#Debug simulation 
Total elapsed time: 46.23851454211399. Arrivals time: 0.4332438106648624 Scheduler time: 45.56965794088319 Scheduler overhead time: 0.0889713866636157 Adapter cache time: 0.017249078024178743 Engine time: 0.09261666005477309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 47.56763469381258,
    "estimated_duration": 3600.0009214393885,
    "input_throughput": 6371.41281364702,
    "output_throughput": 5489.461650498281,
    "total_throughput": 11860.874464145301,
    "itl": 78.39804255158762,
    "ttft": 327694.00962283945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.031535808984189,
    "arrivals": 97469,
    "finished_requests": 91871,
    "scheduler_time": 87.88693846112865
}
#Debug simulation 
Total elapsed time: 47.56793853174895. Arrivals time: 0.4131980831734836 Scheduler time: 46.9260513628833 Scheduler overhead time: 0.0873259655199945 Adapter cache time: 0.01583979744464159 Engine time: 0.0895317136310041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 42.41825465578586,
    "estimated_duration": 3600.025919466853,
    "input_throughput": 6393.251747312015,
    "output_throughput": 5489.049090770566,
    "total_throughput": 11882.300838082581,
    "itl": 78.57176604486428,
    "ttft": 348727.6482330241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.523594712875785,
    "arrivals": 97469,
    "finished_requests": 92073,
    "scheduler_time": 88.37225661801767
}
#Debug simulation 
Total elapsed time: 42.41840477893129. Arrivals time: 0.41071113757789135 Scheduler time: 41.78338290611282 Scheduler overhead time: 0.08590252883732319 Adapter cache time: 0.01584601029753685 Engine time: 0.0865653152577579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 46.58693560631946,
    "estimated_duration": 3600.0079180625103,
    "input_throughput": 6298.4333690585745,
    "output_throughput": 5427.40000708541,
    "total_throughput": 11725.833376143984,
    "itl": 74.97663477527199,
    "ttft": 357181.6186890732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.200364153077827,
    "arrivals": 97469,
    "finished_requests": 90811,
    "scheduler_time": 87.53308903963399
}
#Debug simulation 
Total elapsed time: 46.587086454965174. Arrivals time: 0.42520846240222454 Scheduler time: 45.92972912359983 Scheduler overhead time: 0.08824112173169851 Adapter cache time: 0.015652349684387445 Engine time: 0.09068120643496513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 48.670631096698344,
    "estimated_duration": 3599.999344625183,
    "input_throughput": 6328.702818797903,
    "output_throughput": 5437.994878868028,
    "total_throughput": 11766.697697665932,
    "itl": 77.05053925377516,
    "ttft": 353810.2000361553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0580877574067555,
    "arrivals": 97469,
    "finished_requests": 91172,
    "scheduler_time": 87.86036560826034
}
#Debug simulation 
Total elapsed time: 48.67081071762368. Arrivals time: 0.43260429380461574 Scheduler time: 48.000114894937724 Scheduler overhead time: 0.0908962870016694 Adapter cache time: 0.01677399640902877 Engine time: 0.0930583723820746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 43.586739744991064,
    "estimated_duration": 3600.0556601673306,
    "input_throughput": 6356.200336896017,
    "output_throughput": 5457.6589516082,
    "total_throughput": 11813.859288504216,
    "itl": 75.83264751682097,
    "ttft": 331886.1644365908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.18001629730221,
    "arrivals": 97469,
    "finished_requests": 91586,
    "scheduler_time": 87.54364037348128
}
#Debug simulation 
Total elapsed time: 43.58689666725695. Arrivals time: 0.41675636544823647 Scheduler time: 42.94466503150761 Scheduler overhead time: 0.08611954702064395 Adapter cache time: 0.015607339795678854 Engine time: 0.0871207076124847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 46.63046718109399,
    "estimated_duration": 3600.009425647336,
    "input_throughput": 6372.743592435094,
    "output_throughput": 5477.904824222496,
    "total_throughput": 11850.64841665759,
    "itl": 77.99473972878914,
    "ttft": 330506.0794564302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9895077457884306,
    "arrivals": 97469,
    "finished_requests": 91849,
    "scheduler_time": 88.02048521907251
}
#Debug simulation 
Total elapsed time: 46.63068693317473. Arrivals time: 0.42559701716527343 Scheduler time: 45.97261459659785 Scheduler overhead time: 0.08760714577510953 Adapter cache time: 0.016676760278642178 Engine time: 0.09137898730114102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65183443 . Total output tokens: 57342189
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 43.55055850185454,
    "estimated_duration": 3600.0089542683227,
    "input_throughput": 6356.282801149518,
    "output_throughput": 5457.7297583398085,
    "total_throughput": 11814.012559489327,
    "itl": 75.82097647289802,
    "ttft": 331845.42987088993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1760795744322252,
    "arrivals": 97469,
    "finished_requests": 91586,
    "scheduler_time": 87.54134576517532
}
#Debug simulation 
Total elapsed time: 43.55072817299515. Arrivals time: 0.42239013873040676 Scheduler time: 42.89973424375057 Scheduler overhead time: 0.08663031319156289 Adapter cache time: 0.015624801628291607 Engine time: 0.08954941527917981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 52.29455071315169,
    "estimated_duration": 3600.0690548580155,
    "input_throughput": 6184.415537795314,
    "output_throughput": 5324.037874811246,
    "total_throughput": 11508.45341260656,
    "itl": 74.32551730755517,
    "ttft": 375948.2240606679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.031535808984189,
    "arrivals": 95579,
    "finished_requests": 89290,
    "scheduler_time": 86.253184602298
}
#Debug simulation 
Total elapsed time: 52.29470669943839. Arrivals time: 0.43212776677682996 Scheduler time: 51.62096603959799 Scheduler overhead time: 0.09181847143918276 Adapter cache time: 0.01653260551393032 Engine time: 0.0949148959480226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 50.87696945015341,
    "estimated_duration": 3600.010512450815,
    "input_throughput": 6211.085196186768,
    "output_throughput": 5346.515776393297,
    "total_throughput": 11557.600972580065,
    "itl": 74.08549004275054,
    "ttft": 371556.30965643574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2576798137091112,
    "arrivals": 95579,
    "finished_requests": 89629,
    "scheduler_time": 86.53954447674575
}
#Debug simulation 
Total elapsed time: 50.87713051401079. Arrivals time: 0.42854767059907317 Scheduler time: 50.20423040911555 Scheduler overhead time: 0.09236570633947849 Adapter cache time: 0.016247670631855726 Engine time: 0.09735480276867747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 40.27339006308466,
    "estimated_duration": 3600.03231255889,
    "input_throughput": 6301.423718020838,
    "output_throughput": 5442.597815482659,
    "total_throughput": 11744.021533503497,
    "itl": 75.00012955549064,
    "ttft": 304136.1235989358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.31420206950512,
    "arrivals": 95579,
    "finished_requests": 90933,
    "scheduler_time": 86.30366844867422
}
#Debug simulation 
Total elapsed time: 40.27357926312834. Arrivals time: 0.4141933829523623 Scheduler time: 39.628189124166965 Scheduler overhead time: 0.08708400605246425 Adapter cache time: 0.015519087668508291 Engine time: 0.0915284832008183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 49.97055244585499,
    "estimated_duration": 3600.0388862242994,
    "input_throughput": 6204.081596303187,
    "output_throughput": 5343.130340509755,
    "total_throughput": 11547.211936812942,
    "itl": 74.0715602691775,
    "ttft": 372381.1234112541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1105473459046324,
    "arrivals": 95579,
    "finished_requests": 89578,
    "scheduler_time": 86.46069110256327
}
#Debug simulation 
Total elapsed time: 49.97072494402528. Arrivals time: 0.3271695142611861 Scheduler time: 49.40921860607341 Scheduler overhead time: 0.08897070772945881 Adapter cache time: 0.015337002463638783 Engine time: 0.09234492247924209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 40.61511069396511,
    "estimated_duration": 3600.102171753135,
    "input_throughput": 6280.485364388828,
    "output_throughput": 5416.83320907072,
    "total_throughput": 11697.318573459548,
    "itl": 74.57430865698417,
    "ttft": 311350.7206021783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3278654583543557,
    "arrivals": 95579,
    "finished_requests": 90671,
    "scheduler_time": 86.00686872492626
}
#Debug simulation 
Total elapsed time: 40.61529421200976. Arrivals time: 0.39707377133890986 Scheduler time: 39.99386226711795 Scheduler overhead time: 0.08415309432893991 Adapter cache time: 0.015577870421111584 Engine time: 0.0879738968797028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 50.527963078115135,
    "estimated_duration": 3600.0274081853236,
    "input_throughput": 6206.74107902507,
    "output_throughput": 5345.515691420911,
    "total_throughput": 11552.25677044598,
    "itl": 74.09964984500611,
    "ttft": 370920.03928416304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0150434295507127,
    "arrivals": 95579,
    "finished_requests": 89617,
    "scheduler_time": 86.48840390447884
}
#Debug simulation 
Total elapsed time: 50.528160745278. Arrivals time: 0.42313817888498306 Scheduler time: 49.8636781857349 Scheduler overhead time: 0.09208530141040683 Adapter cache time: 0.016016925685107708 Engine time: 0.0951949069276452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 63896656 . Total output tokens: 56219874
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 38.9810307752341,
    "estimated_duration": 3600.004087724578,
    "input_throughput": 6320.379767785634,
    "output_throughput": 5445.811871950268,
    "total_throughput": 11766.191639735902,
    "itl": 75.33635928657476,
    "ttft": 301352.16111551505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6215470504388247,
    "arrivals": 95579,
    "finished_requests": 91222,
    "scheduler_time": 86.23256879943288
}
#Debug simulation 
Total elapsed time: 38.98133051721379. Arrivals time: 0.4005215889774263 Scheduler time: 38.35493592964485 Scheduler overhead time: 0.08532597869634628 Adapter cache time: 0.01622955547645688 Engine time: 0.08755280170589685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 32.45948074944317,
    "estimated_duration": 3600.0564907869907,
    "input_throughput": 6278.534811285378,
    "output_throughput": 5463.314825846086,
    "total_throughput": 11741.849637131465,
    "itl": 78.53497906237068,
    "ttft": 233485.77436028028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6663270760513822,
    "arrivals": 94646,
    "finished_requests": 91119,
    "scheduler_time": 82.57036124607313
}
#Debug simulation 
Total elapsed time: 32.45965830842033. Arrivals time: 0.3693637428805232 Scheduler time: 31.878440796397626 Scheduler overhead time: 0.08085954003036022 Adapter cache time: 0.015353400725871325 Engine time: 0.08008144423365593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 33.15465035196394,
    "estimated_duration": 3600.0865998621166,
    "input_throughput": 6242.215395835394,
    "output_throughput": 5418.656040315014,
    "total_throughput": 11660.871436150408,
    "itl": 77.31068160918898,
    "ttft": 258529.63924822267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8419692005403363,
    "arrivals": 94646,
    "finished_requests": 90652,
    "scheduler_time": 82.53823809591857
}
#Debug simulation 
Total elapsed time: 33.1548294457607. Arrivals time: 0.35983923776075244 Scheduler time: 32.582591036334634 Scheduler overhead time: 0.08101852145045996 Adapter cache time: 0.015632044058293104 Engine time: 0.07982259290292859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.286267136223614,
    "estimated_duration": 3600.018663879748,
    "input_throughput": 6250.919537108179,
    "output_throughput": 5428.911854289441,
    "total_throughput": 11679.83139139762,
    "itl": 75.65599409208097,
    "ttft": 253277.05983138765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8563768053334255,
    "arrivals": 94646,
    "finished_requests": 90741,
    "scheduler_time": 82.78690565095626
}
#Debug simulation 
Total elapsed time: 30.286481080111116. Arrivals time: 0.34915847051888704 Scheduler time: 29.723076269961894 Scheduler overhead time: 0.08079667761921883 Adapter cache time: 0.01544536603614688 Engine time: 0.0822039763443172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 33.15830961894244,
    "estimated_duration": 3600.0574968129813,
    "input_throughput": 6258.835315810102,
    "output_throughput": 5427.527204023167,
    "total_throughput": 11686.362519833268,
    "itl": 77.3891527809821,
    "ttft": 251320.79462356665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.704818654987028,
    "arrivals": 94646,
    "finished_requests": 90814,
    "scheduler_time": 82.44286597405066
}
#Debug simulation 
Total elapsed time: 33.15846436005086. Arrivals time: 0.36469364166259766 Scheduler time: 32.580399323720485 Scheduler overhead time: 0.081162522546947 Adapter cache time: 0.015780279878526926 Engine time: 0.08051226940006018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 30.932205031160265,
    "estimated_duration": 3600.0162820480487,
    "input_throughput": 6243.2556519476375,
    "output_throughput": 5435.78930394927,
    "total_throughput": 11679.044955896907,
    "itl": 75.72304853815872,
    "ttft": 255224.99835380932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9077799067366918,
    "arrivals": 94646,
    "finished_requests": 90615,
    "scheduler_time": 82.57034574138089
}
#Debug simulation 
Total elapsed time: 30.93232393404469. Arrivals time: 0.3642371688038111 Scheduler time: 30.35318322107196 Scheduler overhead time: 0.08152440190315247 Adapter cache time: 0.015745244454592466 Engine time: 0.08169791661202908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.49655838822946,
    "estimated_duration": 3600.0745038760792,
    "input_throughput": 6311.04494519152,
    "output_throughput": 5480.021865869443,
    "total_throughput": 11791.066811060962,
    "itl": 78.79879143198363,
    "ttft": 214810.29796500196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9598637287551464,
    "arrivals": 94646,
    "finished_requests": 91630,
    "scheduler_time": 82.49227591658973
}
#Debug simulation 
Total elapsed time: 29.496676669921726. Arrivals time: 0.3615696420893073 Scheduler time: 28.923725137021393 Scheduler overhead time: 0.08142197458073497 Adapter cache time: 0.015869416762143373 Engine time: 0.07883973745629191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63257302 . Total output tokens: 55648999
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 32.6299252519384,
    "estimated_duration": 3600.058879102668,
    "input_throughput": 6215.241681151931,
    "output_throughput": 5393.41678901643,
    "total_throughput": 11608.65847016836,
    "itl": 74.67899712901661,
    "ttft": 281698.88298021717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8459627327136738,
    "arrivals": 94646,
    "finished_requests": 90215,
    "scheduler_time": 82.90030878174731
}
#Debug simulation 
Total elapsed time: 32.63009872706607. Arrivals time: 0.365112925413996 Scheduler time: 32.04745002742857 Scheduler overhead time: 0.0832876767963171 Adapter cache time: 0.015974000561982393 Engine time: 0.08192982757464051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 41.63367179175839,
    "estimated_duration": 3600.027905197229,
    "input_throughput": 6219.959008560309,
    "output_throughput": 5448.175546551899,
    "total_throughput": 11668.134555112207,
    "itl": 77.61146897101324,
    "ttft": 265854.3997824446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2563577160704866,
    "arrivals": 94180,
    "finished_requests": 90628,
    "scheduler_time": 85.5975893182343
}
#Debug simulation 
Total elapsed time: 41.633885733783245. Arrivals time: 0.3950010063126683 Scheduler time: 41.01377444155514 Scheduler overhead time: 0.08663392486050725 Adapter cache time: 0.015851639211177826 Engine time: 0.08593257423490286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 42.04167585587129,
    "estimated_duration": 3600.0550847321524,
    "input_throughput": 6181.479859677118,
    "output_throughput": 5416.683228737553,
    "total_throughput": 11598.16308841467,
    "itl": 76.32755026184836,
    "ttft": 289727.6225486913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.32207620704081,
    "arrivals": 94180,
    "finished_requests": 90090,
    "scheduler_time": 85.62088478690046
}
#Debug simulation 
Total elapsed time: 42.04186738654971. Arrivals time: 0.39944930374622345 Scheduler time: 41.410419372841716 Scheduler overhead time: 0.08768172841519117 Adapter cache time: 0.015695674810558558 Engine time: 0.0915123070590198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 41.129317863844335,
    "estimated_duration": 3600.091348823486,
    "input_throughput": 6055.981609223596,
    "output_throughput": 5307.222830955115,
    "total_throughput": 11363.204440178712,
    "itl": 72.27531994907379,
    "ttft": 359158.74209578393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.444660091279077,
    "arrivals": 94180,
    "finished_requests": 88290,
    "scheduler_time": 84.48676890937577
}
#Debug simulation 
Total elapsed time: 41.12965212203562. Arrivals time: 0.3950457563623786 Scheduler time: 40.50545641966164 Scheduler overhead time: 0.08720600232481956 Adapter cache time: 0.01643607532605529 Engine time: 0.08750204602256417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 46.73185686720535,
    "estimated_duration": 3600.0028976774447,
    "input_throughput": 6107.072862131309,
    "output_throughput": 5354.64180110423,
    "total_throughput": 11461.71466323554,
    "itl": 74.58876768018175,
    "ttft": 329690.11090018926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1269226667797188,
    "arrivals": 94180,
    "finished_requests": 89020,
    "scheduler_time": 85.2264700813233
}
#Debug simulation 
Total elapsed time: 46.73201892198995. Arrivals time: 0.4170227232389152 Scheduler time: 46.077663813252 Scheduler overhead time: 0.0903984708711505 Adapter cache time: 0.01594264293089509 Engine time: 0.09314797772094607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 36.888918668963015,
    "estimated_duration": 3600.0081283740656,
    "input_throughput": 6175.801611326208,
    "output_throughput": 5419.9791512172305,
    "total_throughput": 11595.780762543438,
    "itl": 74.14612038871223,
    "ttft": 268708.0973878194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4086780610727172,
    "arrivals": 94180,
    "finished_requests": 90018,
    "scheduler_time": 83.79261267889376
}
#Debug simulation 
Total elapsed time: 36.8890722701326. Arrivals time: 0.38609591126441956 Scheduler time: 36.28015087451786 Scheduler overhead time: 0.08440318098291755 Adapter cache time: 0.015332335140556097 Engine time: 0.08660401543602347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 47.40973797021434,
    "estimated_duration": 3600.0821560306513,
    "input_throughput": 6101.031878732769,
    "output_throughput": 5347.613794799212,
    "total_throughput": 11448.64567353198,
    "itl": 74.33426131262334,
    "ttft": 332627.16718389106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0661147970752767,
    "arrivals": 94180,
    "finished_requests": 88913,
    "scheduler_time": 85.13113285284034
}
#Debug simulation 
Total elapsed time: 47.410054268315434. Arrivals time: 0.42110486421734095 Scheduler time: 46.752153547946364 Scheduler overhead time: 0.0893910969607532 Adapter cache time: 0.016290682833641768 Engine time: 0.0934070204384625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 62957245 . Total output tokens: 55350843
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 41.02238402655348,
    "estimated_duration": 3600.05334371552,
    "input_throughput": 6056.0030417490425,
    "output_throughput": 5307.164137818338,
    "total_throughput": 11363.16717956738,
    "itl": 72.2765736688501,
    "ttft": 359199.56897368893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4162331691011811,
    "arrivals": 94180,
    "finished_requests": 88288,
    "scheduler_time": 84.48695241572204
}
#Debug simulation 
Total elapsed time: 41.02256919583306. Arrivals time: 0.39588787127286196 Scheduler time: 40.39493777556345 Scheduler overhead time: 0.08798864809796214 Adapter cache time: 0.016085118986666203 Engine time: 0.0893784100189805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 37.94474339205772,
    "estimated_duration": 3600.0049849291695,
    "input_throughput": 6046.183294501258,
    "output_throughput": 5329.693453293234,
    "total_throughput": 11375.876747794491,
    "itl": 74.05464029880542,
    "ttft": 277516.0961176169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0050861728563893,
    "arrivals": 93946,
    "finished_requests": 88344,
    "scheduler_time": 80.04262588763035
}
#Debug simulation 
Total elapsed time: 37.94490879494697. Arrivals time: 0.3605971853248775 Scheduler time: 37.35989398229867 Scheduler overhead time: 0.08555990271270275 Adapter cache time: 0.015270361676812172 Engine time: 0.08637347025796771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 28.943484276998788,
    "estimated_duration": 3600.0244681097665,
    "input_throughput": 6198.613425457307,
    "output_throughput": 5466.180625803457,
    "total_throughput": 11664.794051260764,
    "itl": 77.53733241170194,
    "ttft": 209714.93319510133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.526371154822411,
    "arrivals": 93946,
    "finished_requests": 90542,
    "scheduler_time": 80.60759559817869
}
#Debug simulation 
Total elapsed time: 28.94362820778042. Arrivals time: 0.3347298647277057 Scheduler time: 28.40008307993412 Scheduler overhead time: 0.07989715551957488 Adapter cache time: 0.015220134984701872 Engine time: 0.07812116900458932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 27.49239772092551,
    "estimated_duration": 3600.061594431583,
    "input_throughput": 6182.339778415415,
    "output_throughput": 5454.069183252456,
    "total_throughput": 11636.408961667872,
    "itl": 75.24678761553784,
    "ttft": 220118.05640262004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.930302313491707,
    "arrivals": 93946,
    "finished_requests": 90331,
    "scheduler_time": 80.67448647480343
}
#Debug simulation 
Total elapsed time: 27.492507135029882. Arrivals time: 0.3302457109093666 Scheduler time: 26.953414595220238 Scheduler overhead time: 0.07919234596192837 Adapter cache time: 0.01528249541297555 Engine time: 0.07893659127876163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 30.87649551825598,
    "estimated_duration": 3600.022685507386,
    "input_throughput": 6198.616494788812,
    "output_throughput": 5466.183332460455,
    "total_throughput": 11664.799827249268,
    "itl": 77.53394506934767,
    "ttft": 209715.02808189046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4200353238033119,
    "arrivals": 93946,
    "finished_requests": 90542,
    "scheduler_time": 80.60864115346989
}
#Debug simulation 
Total elapsed time: 30.876664247363806. Arrivals time: 0.33187175123021007 Scheduler time: 30.33672387013212 Scheduler overhead time: 0.07935051620006561 Adapter cache time: 0.015019466634839773 Engine time: 0.07844721712172031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 27.24731401912868,
    "estimated_duration": 3600.006222333728,
    "input_throughput": 6182.191258984115,
    "output_throughput": 5454.040850871572,
    "total_throughput": 11636.232109855688,
    "itl": 75.24583471251371,
    "ttft": 220270.85468559596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9205663096765104,
    "arrivals": 93946,
    "finished_requests": 90327,
    "scheduler_time": 80.67299353915399
}
#Debug simulation 
Total elapsed time: 27.247507445979863. Arrivals time: 0.33534160256385803 Scheduler time: 26.702434449456632 Scheduler overhead time: 0.07930531492456794 Adapter cache time: 0.015317435842007399 Engine time: 0.07943520555272698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.186302072834224,
    "estimated_duration": 3600.0304008878643,
    "input_throughput": 6198.6032102663585,
    "output_throughput": 5466.171617647113,
    "total_throughput": 11664.77482791347,
    "itl": 77.52648008920477,
    "ttft": 209706.67769723482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3214716346980966,
    "arrivals": 93946,
    "finished_requests": 90542,
    "scheduler_time": 80.60631400129452
}
#Debug simulation 
Total elapsed time: 29.18642634060234. Arrivals time: 0.34010913176462054 Scheduler time: 28.637147867586464 Scheduler overhead time: 0.08024773420765996 Adapter cache time: 0.014793120324611664 Engine time: 0.07893606415018439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62806846 . Total output tokens: 55215887
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 27.350325007922947,
    "estimated_duration": 3600.0465413584543,
    "input_throughput": 6168.194701066501,
    "output_throughput": 5442.331307363277,
    "total_throughput": 11610.526008429777,
    "itl": 75.05283941130202,
    "ttft": 228304.53929773238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8961798483319627,
    "arrivals": 93946,
    "finished_requests": 90109,
    "scheduler_time": 80.56778372178864
}
#Debug simulation 
Total elapsed time: 27.350418905261904. Arrivals time: 0.3203242067247629 Scheduler time: 26.818359165918082 Scheduler overhead time: 0.07903780043125153 Adapter cache time: 0.01764313643798232 Engine time: 0.07964401645585895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.55956514691934,
    "estimated_duration": 3600.037035751039,
    "input_throughput": 5140.123786570869,
    "output_throughput": 4467.174598564922,
    "total_throughput": 9607.298385135791,
    "itl": 53.50031954238192,
    "ttft": 27350.204612181755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 920,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.083416309394071,
    "arrivals": 74944,
    "finished_requests": 74521,
    "scheduler_time": 54.05430685965386
}
#Debug simulation 
Total elapsed time: 7.559707789681852. Arrivals time: 0.23889183439314365 Scheduler time: 7.083284910302609 Scheduler overhead time: 0.08709662919864058 Adapter cache time: 0.02081272192299366 Engine time: 0.08827601186931133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.640845907386392,
    "estimated_duration": 3600.017101126561,
    "input_throughput": 5137.375318081804,
    "output_throughput": 4465.9785074267575,
    "total_throughput": 9603.35382550856,
    "itl": 53.5018771565243,
    "ttft": 29207.40180932873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 897,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.58290942422581,
    "arrivals": 74944,
    "finished_requests": 74487,
    "scheduler_time": 54.07332991063082
}
#Debug simulation 
Total elapsed time: 7.640965073369443. Arrivals time: 0.22639917442575097 Scheduler time: 7.173627156298608 Scheduler overhead time: 0.08835810329765081 Adapter cache time: 0.020886550191789865 Engine time: 0.0900872116908431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.538458562921733,
    "estimated_duration": 3600.058487516087,
    "input_throughput": 5140.093157977426,
    "output_throughput": 4467.147979891851,
    "total_throughput": 9607.241137869276,
    "itl": 53.526368674404225,
    "ttft": 27400.32398018061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 920,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.942977681313668,
    "arrivals": 74944,
    "finished_requests": 74521,
    "scheduler_time": 54.06037675803154
}
#Debug simulation 
Total elapsed time: 7.5386055978015065. Arrivals time: 0.2197882323525846 Scheduler time: 7.080217046663165 Scheduler overhead time: 0.08735667308792472 Adapter cache time: 0.02055351808667183 Engine time: 0.08929703291505575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.5628972868435085,
    "estimated_duration": 3600.010069845473,
    "input_throughput": 5139.162847062297,
    "output_throughput": 4467.69611416403,
    "total_throughput": 9606.858961226328,
    "itl": 53.506478288926424,
    "ttft": 27767.391105522063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.214171396200534,
    "arrivals": 74944,
    "finished_requests": 74514,
    "scheduler_time": 54.06332211142319
}
#Debug simulation 
Total elapsed time: 7.563041178975254. Arrivals time: 0.23024803632870317 Scheduler time: 7.094380600843579 Scheduler overhead time: 0.08742318442091346 Adapter cache time: 0.020950423553586006 Engine time: 0.08869045879691839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.60818477300927,
    "estimated_duration": 3600.045766040995,
    "input_throughput": 5139.111889776271,
    "output_throughput": 4467.651814795526,
    "total_throughput": 9606.763704571797,
    "itl": 53.524958707341696,
    "ttft": 27768.95113336714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.806087054950213,
    "arrivals": 74944,
    "finished_requests": 74514,
    "scheduler_time": 54.06785350455068
}
#Debug simulation 
Total elapsed time: 7.608372464310378. Arrivals time: 0.22978610591962934 Scheduler time: 7.137335943989456 Scheduler overhead time: 0.08871608413755894 Adapter cache time: 0.020957622211426497 Engine time: 0.08983571548014879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.626949733123183,
    "estimated_duration": 3600.047860436714,
    "input_throughput": 5137.931415655178,
    "output_throughput": 4465.528688290783,
    "total_throughput": 9603.460103945961,
    "itl": 53.45415632253985,
    "ttft": 28802.845978936926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 898,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.7327610046323105,
    "arrivals": 74944,
    "finished_requests": 74494,
    "scheduler_time": 54.046731711345885
}
#Debug simulation 
Total elapsed time: 7.627054559066892. Arrivals time: 0.22598581947386265 Scheduler time: 7.161312466021627 Scheduler overhead time: 0.08765635080635548 Adapter cache time: 0.02077232440933585 Engine time: 0.08988554449751973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 49902949 . Total output tokens: 43982410
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.599139382597059,
    "estimated_duration": 3600.015182916561,
    "input_throughput": 5137.378055449345,
    "output_throughput": 4465.980887051342,
    "total_throughput": 9603.358942500687,
    "itl": 53.503570413095616,
    "ttft": 29207.512375763676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 897,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.637980727721032,
    "arrivals": 74944,
    "finished_requests": 74487,
    "scheduler_time": 54.073613628858176
}
#Debug simulation 
Total elapsed time: 7.599231256637722. Arrivals time: 0.22113377787172794 Scheduler time: 7.139683397021145 Scheduler overhead time: 0.0877456828020513 Adapter cache time: 0.020166081376373768 Engine time: 0.08946180995553732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.790317284874618,
    "estimated_duration": 3600.048590294879,
    "input_throughput": 4974.531746120992,
    "output_throughput": 4390.00185792089,
    "total_throughput": 9364.533604041882,
    "itl": 52.47886242640828,
    "ttft": 23010.42086053428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.691757940333485,
    "arrivals": 73090,
    "finished_requests": 72727,
    "scheduler_time": 52.398157161154145
}
#Debug simulation 
Total elapsed time: 6.790468575898558. Arrivals time: 0.2095588818192482 Scheduler time: 6.3453886029310524 Scheduler overhead time: 0.08627527859061956 Adapter cache time: 0.020924179814755917 Engine time: 0.0871919458732009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.840002998244017,
    "estimated_duration": 3600.0535848944423,
    "input_throughput": 4974.465956601891,
    "output_throughput": 4389.941601511854,
    "total_throughput": 9364.407558113746,
    "itl": 52.50097621088448,
    "ttft": 23060.90461606355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.43311371022371,
    "arrivals": 73090,
    "finished_requests": 72726,
    "scheduler_time": 52.40331589980231
}
#Debug simulation 
Total elapsed time: 6.840168437920511. Arrivals time: 0.2190710143186152 Scheduler time: 6.382730874232948 Scheduler overhead time: 0.08720238413661718 Adapter cache time: 0.02134352596476674 Engine time: 0.08845557225868106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.840410725213587,
    "estimated_duration": 3600.0310028959107,
    "input_throughput": 4974.49716005065,
    "output_throughput": 4389.969138401042,
    "total_throughput": 9364.46629845169,
    "itl": 52.506649517283336,
    "ttft": 23061.345744062797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.639370700437598,
    "arrivals": 73090,
    "finished_requests": 72726,
    "scheduler_time": 52.404487581175964
}
#Debug simulation 
Total elapsed time: 6.840535750146955. Arrivals time: 0.2207112805917859 Scheduler time: 6.381633929908276 Scheduler overhead time: 0.08692373847588897 Adapter cache time: 0.021266608964651823 Engine time: 0.0887025180272758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.8292480409145355,
    "estimated_duration": 3600.035472829447,
    "input_throughput": 4974.490983536043,
    "output_throughput": 4389.963687657452,
    "total_throughput": 9364.454671193496,
    "itl": 52.48545488320908,
    "ttft": 23059.589006712722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.902813298418174,
    "arrivals": 73090,
    "finished_requests": 72726,
    "scheduler_time": 52.399430459898255
}
#Debug simulation 
Total elapsed time: 6.829348402097821. Arrivals time: 0.22713169269263744 Scheduler time: 6.355586989782751 Scheduler overhead time: 0.09168183850124478 Adapter cache time: 0.02136193634942174 Engine time: 0.0916675073094666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.805986455176026,
    "estimated_duration": 3600.024355243373,
    "input_throughput": 4974.233569814115,
    "output_throughput": 4390.052799776567,
    "total_throughput": 9364.286369590682,
    "itl": 52.506186284809814,
    "ttft": 22961.413238928377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.560251938970782,
    "arrivals": 73090,
    "finished_requests": 72727,
    "scheduler_time": 52.40684124215784
}
#Debug simulation 
Total elapsed time: 6.806084273848683. Arrivals time: 0.2133696866221726 Scheduler time: 6.356109942309558 Scheduler overhead time: 0.08634155616164207 Adapter cache time: 0.02113912906497717 Engine time: 0.08791508059948683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.825141887180507,
    "estimated_duration": 3600.0420979030237,
    "input_throughput": 4974.481829096213,
    "output_throughput": 4389.955608909583,
    "total_throughput": 9364.437438005796,
    "itl": 52.472136082110154,
    "ttft": 23058.736919259525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.466911912797918,
    "arrivals": 73090,
    "finished_requests": 72726,
    "scheduler_time": 52.396634459813185
}
#Debug simulation 
Total elapsed time: 6.825271889101714. Arrivals time: 0.21003936789929867 Scheduler time: 6.3774149916134775 Scheduler overhead time: 0.08710246812552214 Adapter cache time: 0.02106114150956273 Engine time: 0.08834152948111296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48638517 . Total output tokens: 42886182
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.763135438784957,
    "estimated_duration": 3600.0256504251774,
    "input_throughput": 4974.598999839046,
    "output_throughput": 4390.571494437332,
    "total_throughput": 9365.170494276379,
    "itl": 52.51485956169959,
    "ttft": 22873.876282363886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1016,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.515820635668956,
    "arrivals": 73090,
    "finished_requests": 72728,
    "scheduler_time": 52.4081689739693
}
#Debug simulation 
Total elapsed time: 6.763254057150334. Arrivals time: 0.20793326338753104 Scheduler time: 6.318277871701866 Scheduler overhead time: 0.08675940008834004 Adapter cache time: 0.020922775380313396 Engine time: 0.08810477517545223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.059295645914972,
    "estimated_duration": 3599.9922610333156,
    "input_throughput": 4959.753439824065,
    "output_throughput": 4298.354240227851,
    "total_throughput": 9258.107680051915,
    "itl": 51.28228943112771,
    "ttft": 20025.80557106498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.498471842231403,
    "arrivals": 72131,
    "finished_requests": 71792,
    "scheduler_time": 50.56369824014541
}
#Debug simulation 
Total elapsed time: 6.059464627876878. Arrivals time: 0.21052298927679658 Scheduler time: 5.612740741111338 Scheduler overhead time: 0.08622694108635187 Adapter cache time: 0.021641334518790245 Engine time: 0.08715878939256072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.053159445058554,
    "estimated_duration": 3600.0094377941064,
    "input_throughput": 4959.63616443751,
    "output_throughput": 4298.484008831377,
    "total_throughput": 9258.120173268886,
    "itl": 51.3064846054658,
    "ttft": 19850.287602187564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.300524354143995,
    "arrivals": 72131,
    "finished_requests": 71795,
    "scheduler_time": 50.57132548529416
}
#Debug simulation 
Total elapsed time: 6.053273607976735. Arrivals time: 0.21200931817293167 Scheduler time: 5.60471004107967 Scheduler overhead time: 0.0864660139195621 Adapter cache time: 0.02161112381145358 Engine time: 0.08713378617540002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.097763987723738,
    "estimated_duration": 3600.02741312133,
    "input_throughput": 4959.61140043637,
    "output_throughput": 4298.462546034637,
    "total_throughput": 9258.073946471006,
    "itl": 51.31300447335408,
    "ttft": 19850.64919718711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.528185604731544,
    "arrivals": 72131,
    "finished_requests": 71795,
    "scheduler_time": 50.573170962962855
}
#Debug simulation 
Total elapsed time: 6.097862014081329. Arrivals time: 0.21256797993555665 Scheduler time: 5.647217390593141 Scheduler overhead time: 0.08710249047726393 Adapter cache time: 0.02191729797050357 Engine time: 0.08763141697272658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.129456559661776,
    "estimated_duration": 3600.008806058228,
    "input_throughput": 4959.637034763189,
    "output_throughput": 4298.484763136912,
    "total_throughput": 9258.121797900101,
    "itl": 51.288563081139216,
    "ttft": 19848.976285331843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.670272032259947,
    "arrivals": 72131,
    "finished_requests": 71795,
    "scheduler_time": 50.56684209655846
}
#Debug simulation 
Total elapsed time: 6.129712637979537. Arrivals time: 0.21352537535130978 Scheduler time: 5.676569652277976 Scheduler overhead time: 0.08728120010346174 Adapter cache time: 0.021886298432946205 Engine time: 0.0885063330642879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.08092991495505,
    "estimated_duration": 3600.00659241359,
    "input_throughput": 4959.725084289153,
    "output_throughput": 4298.4918507121965,
    "total_throughput": 9258.216935001348,
    "itl": 51.30326693102053,
    "ttft": 20055.38145861307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.402264347407034,
    "arrivals": 72131,
    "finished_requests": 71791,
    "scheduler_time": 50.56227155042929
}
#Debug simulation 
Total elapsed time: 6.08102949289605. Arrivals time: 0.214214360807091 Scheduler time: 5.626865833066404 Scheduler overhead time: 0.08734679594635963 Adapter cache time: 0.02172129414975643 Engine time: 0.08932607853785157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.099595584906638,
    "estimated_duration": 3600.0206680682522,
    "input_throughput": 4959.833469395371,
    "output_throughput": 4298.467266384766,
    "total_throughput": 9258.300735780136,
    "itl": 51.27569096908237,
    "ttft": 19875.449787514117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.239366346606948,
    "arrivals": 72131,
    "finished_requests": 71795,
    "scheduler_time": 50.56225193921699
}
#Debug simulation 
Total elapsed time: 6.099736093077809. Arrivals time: 0.2203478366136551 Scheduler time: 5.641754970885813 Scheduler overhead time: 0.08698706142604351 Adapter cache time: 0.0218028393574059 Engine time: 0.08732093591243029 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 47997764 . Total output tokens: 42329825
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.1397509053349495,
    "estimated_duration": 3599.992117794578,
    "input_throughput": 4959.753637165836,
    "output_throughput": 4298.354411253457,
    "total_throughput": 9258.108048419293,
    "itl": 51.307584504649114,
    "ttft": 20027.450469481977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.37828314132988,
    "arrivals": 72131,
    "finished_requests": 71792,
    "scheduler_time": 50.5697318799089
}
#Debug simulation 
Total elapsed time: 6.139843733981252. Arrivals time: 0.2167510292492807 Scheduler time: 5.68417478306219 Scheduler overhead time: 0.08766405889764428 Adapter cache time: 0.02184594701975584 Engine time: 0.08785711647942662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.782860457897186,
    "estimated_duration": 3600.0113478518474,
    "input_throughput": 4943.155529389832,
    "output_throughput": 4265.0460002471855,
    "total_throughput": 9208.201529637017,
    "itl": 50.82134838530719,
    "ttft": 19135.04170988723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1097,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.253812708049248,
    "arrivals": 71695,
    "finished_requests": 71356,
    "scheduler_time": 49.875215678981384
}
#Debug simulation 
Total elapsed time: 5.783002834767103. Arrivals time: 0.20541418017819524 Scheduler time: 5.340889271814376 Scheduler overhead time: 0.08647289359942079 Adapter cache time: 0.021452137269079685 Engine time: 0.08744669985026121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.737401969265193,
    "estimated_duration": 3600.0317456449156,
    "input_throughput": 4943.498351515747,
    "output_throughput": 4265.582107318073,
    "total_throughput": 9209.080458833821,
    "itl": 50.843851811033446,
    "ttft": 18800.713894549768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.062921580686247,
    "arrivals": 71695,
    "finished_requests": 71363,
    "scheduler_time": 49.87593913470461
}
#Debug simulation 
Total elapsed time: 5.737507399171591. Arrivals time: 0.2022132812999189 Scheduler time: 5.299990667961538 Scheduler overhead time: 0.08584921201691031 Adapter cache time: 0.02130227768793702 Engine time: 0.08691605133935809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.79006222775206,
    "estimated_duration": 3600.0207676196765,
    "input_throughput": 4943.195650442429,
    "output_throughput": 4265.214283792198,
    "total_throughput": 9208.409934234627,
    "itl": 50.843515321269265,
    "ttft": 19129.151860616035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1097,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.263079059198507,
    "arrivals": 71695,
    "finished_requests": 71356,
    "scheduler_time": 49.87949857468531
}
#Debug simulation 
Total elapsed time: 5.790154155809432. Arrivals time: 0.20637436117976904 Scheduler time: 5.34366411389783 Scheduler overhead time: 0.08967015985399485 Adapter cache time: 0.021563288290053606 Engine time: 0.08712921850383282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.764174185693264,
    "estimated_duration": 3600.0342384001497,
    "input_throughput": 4943.21882002667,
    "output_throughput": 4264.9177711218745,
    "total_throughput": 9208.136591148545,
    "itl": 50.81766598614552,
    "ttft": 19267.7596321167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1093,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.356609913757985,
    "arrivals": 71695,
    "finished_requests": 71355,
    "scheduler_time": 49.87924080201799
}
#Debug simulation 
Total elapsed time: 5.764308380894363. Arrivals time: 0.19811272248625755 Scheduler time: 5.329635836649686 Scheduler overhead time: 0.08636027807369828 Adapter cache time: 0.021464495453983545 Engine time: 0.08725610375404358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.779366800095886,
    "estimated_duration": 3600.0022424946724,
    "input_throughput": 4943.051087565075,
    "output_throughput": 4264.746232313693,
    "total_throughput": 9207.797319878768,
    "itl": 50.8419526479961,
    "ttft": 19290.737161154615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.19317084665877,
    "arrivals": 71695,
    "finished_requests": 71352,
    "scheduler_time": 49.8812460952736
}
#Debug simulation 
Total elapsed time: 5.779477125965059. Arrivals time: 0.20914327399805188 Scheduler time: 5.332287880126387 Scheduler overhead time: 0.0870820926502347 Adapter cache time: 0.021587551571428776 Engine time: 0.08795266738161445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.75523217022419,
    "estimated_duration": 3600.0285461552003,
    "input_throughput": 4943.098025987939,
    "output_throughput": 4264.962847696841,
    "total_throughput": 9208.060873684779,
    "itl": 50.8091684205491,
    "ttft": 18992.483345059358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.067000481211545,
    "arrivals": 71695,
    "finished_requests": 71358,
    "scheduler_time": 49.8640374205417
}
#Debug simulation 
Total elapsed time: 5.755345460958779. Arrivals time: 0.21388015197589993 Scheduler time: 5.3044034778140485 Scheduler overhead time: 0.08703764388337731 Adapter cache time: 0.0215429007075727 Engine time: 0.08697087410837412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47667435 . Total output tokens: 42034375
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.771591721102595,
    "estimated_duration": 3600.027718584643,
    "input_throughput": 4943.362215832222,
    "output_throughput": 4265.471324214461,
    "total_throughput": 9208.833540046682,
    "itl": 50.84461554624655,
    "ttft": 18807.86073064108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.12761772213505,
    "arrivals": 71695,
    "finished_requests": 71362,
    "scheduler_time": 49.87566121485531
}
#Debug simulation 
Total elapsed time: 5.771804171148688. Arrivals time: 0.21355622122064233 Scheduler time: 5.320696098264307 Scheduler overhead time: 0.0871290834620595 Adapter cache time: 0.02153609972447157 Engine time: 0.08735493430867791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.652396047953516,
    "estimated_duration": 3600.018186489642,
    "input_throughput": 4896.176098817804,
    "output_throughput": 4276.3778410274135,
    "total_throughput": 9172.553939845217,
    "itl": 50.96115602209164,
    "ttft": 18807.566024465923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.0752776641865935,
    "arrivals": 71480,
    "finished_requests": 71141,
    "scheduler_time": 50.02280004117311
}
#Debug simulation 
Total elapsed time: 5.652580574154854. Arrivals time: 0.2101626880466938 Scheduler time: 5.206771016120911 Scheduler overhead time: 0.08611306175589561 Adapter cache time: 0.02139592869207263 Engine time: 0.08689621137455106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.697933082003146,
    "estimated_duration": 3599.995693755433,
    "input_throughput": 4896.2066900731825,
    "output_throughput": 4276.404559789973,
    "total_throughput": 9172.611249863156,
    "itl": 50.98129035570581,
    "ttft": 18808.61241061451,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.85530248471537,
    "arrivals": 71480,
    "finished_requests": 71141,
    "scheduler_time": 50.02799422373655
}
#Debug simulation 
Total elapsed time: 5.698031189851463. Arrivals time: 0.21472505666315556 Scheduler time: 5.245574722997844 Scheduler overhead time: 0.08763807686045766 Adapter cache time: 0.02143889432772994 Engine time: 0.08719898434355855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.636605141684413,
    "estimated_duration": 3600.0230203231795,
    "input_throughput": 4896.3753566269115,
    "output_throughput": 4276.5209869735545,
    "total_throughput": 9172.896343600467,
    "itl": 50.99844109249618,
    "ttft": 18977.15585088248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1068,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.057922640070341,
    "arrivals": 71480,
    "finished_requests": 71138,
    "scheduler_time": 50.0321701265713
}
#Debug simulation 
Total elapsed time: 5.636783333960921. Arrivals time: 0.21249994775280356 Scheduler time: 5.188599000684917 Scheduler overhead time: 0.08664408465847373 Adapter cache time: 0.021299568470567465 Engine time: 0.08641865756362677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.6291356780566275,
    "estimated_duration": 3599.978055180262,
    "input_throughput": 4896.277346645489,
    "output_throughput": 4276.529957688618,
    "total_throughput": 9172.807304334106,
    "itl": 50.972946033415205,
    "ttft": 19026.347664887166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1068,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.135930167417953,
    "arrivals": 71480,
    "finished_requests": 71137,
    "scheduler_time": 50.024921052921265
}
#Debug simulation 
Total elapsed time: 5.629229567013681. Arrivals time: 0.20665424084290862 Scheduler time: 5.186821830458939 Scheduler overhead time: 0.08614119607955217 Adapter cache time: 0.021365575958043337 Engine time: 0.08698585815727711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.626126367133111,
    "estimated_duration": 3599.971216058533,
    "input_throughput": 4896.254148192446,
    "output_throughput": 4276.483915017415,
    "total_throughput": 9172.738063209861,
    "itl": 50.99578993457367,
    "ttft": 19077.747527238676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1068,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.952914231526689,
    "arrivals": 71480,
    "finished_requests": 71136,
    "scheduler_time": 50.030608166150024
}
#Debug simulation 
Total elapsed time: 5.626223090104759. Arrivals time: 0.20172550808638334 Scheduler time: 5.18896224303171 Scheduler overhead time: 0.08595316717401147 Adapter cache time: 0.02124467771500349 Engine time: 0.08715308969840407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.625964813865721,
    "estimated_duration": 3600.000219286715,
    "input_throughput": 4896.406368411981,
    "output_throughput": 4276.548072836062,
    "total_throughput": 9172.954441248043,
    "itl": 50.96447867393381,
    "ttft": 18975.51573582363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1068,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.818027564529295,
    "arrivals": 71480,
    "finished_requests": 71138,
    "scheduler_time": 50.02311992390734
}
#Debug simulation 
Total elapsed time: 5.6260565598495305. Arrivals time: 0.19927838118746877 Scheduler time: 5.191844612825662 Scheduler overhead time: 0.08609018148854375 Adapter cache time: 0.021209489554166794 Engine time: 0.08635474881157279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_32_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47511579 . Total output tokens: 41897736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.677531179971993,
    "estimated_duration": 3599.984653814329,
    "input_throughput": 4896.427539301707,
    "output_throughput": 4276.5665636068115,
    "total_throughput": 9172.994102908518,
    "itl": 50.99413956893497,
    "ttft": 18976.870004085806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1068,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.9054843980819225,
    "arrivals": 71480,
    "finished_requests": 71138,
    "scheduler_time": 50.030445275164624
}
#Debug simulation 
Total elapsed time: 5.677638796623796. Arrivals time: 0.1969651379622519 Scheduler time: 5.242877567652613 Scheduler overhead time: 0.08758270647376776 Adapter cache time: 0.021373070776462555 Engine time: 0.08733886433765292 
